# Aprendizaje por refuerzo profundo

El aprendizaje por refuerzo (RL) se considera uno de los paradigmas b치sicos del aprendizaje autom치tico, junto al aprendizaje supervisado y el aprendizaje no supervisado. Mientras que en el aprendizaje supervisado nos basamos en el conjunto de datos con resultados conocidos, RL se basa en **aprender haciendo**. Por ejemplo, cuando vemos por primera vez un juego de computadora, comenzamos a jugar, incluso sin conocer las reglas, y pronto somos capaces de mejorar nuestras habilidades simplemente jugando y ajustando nuestro comportamiento.

## [Pre-lecture quiz](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/122)

Para realizar RL, necesitamos:

* Un **entorno** o **simulador** que marca las reglas del juego. Deber칤amos poder ejecutar los experimentos en el simulador y observar los resultados.
* Alguna **Funci칩n de recompensa**, que indica el 칠xito de nuestro experimento. En caso de aprender a jugar a un juego de ordenador, la recompensa ser칤a nuestra puntuaci칩n final.

Bas치ndonos en la funci칩n de recompensa, deber칤amos poder ajustar nuestro comportamiento y mejorar nuestras habilidades, para que la pr칩xima vez juguemos mejor. La principal diferencia entre otros tipos de aprendizaje autom치tico y RL es que en RL normalmente no sabemos si ganamos o perdemos hasta que terminamos el juego. Por lo tanto, no podemos decir si un determinado movimiento por s칤 solo es bueno o no; solo recibimos una recompensa al final del juego.

Durante RL, normalmente realizamos muchos experimentos. Durante cada experimento, debemos equilibrar entre seguir la estrategia 칩ptima que hemos aprendido hasta ahora (**explotaci칩n**) y explorar nuevos estados posibles (**exploraci칩n**).

## Gimnasio OpenAI

Una gran herramienta para RL es el [OpenAI Gym](https://gym.openai.com/) - a **simulation environment**, which can simulate many different environments starting from Atari games, to the physics behind pole balancing. It is one of the most popular simulation environments for training reinforcement learning algorithms, and is maintained by [OpenAI](https://openai.com/).

> **Nota**: Puedes ver todos los entornos disponibles en OpenAI Gym [here](https://gym.openai.com/envs/#classic_control).

## Equilibrio de postes de carrito

Probablemente todos hab칠is visto dispositivos de equilibrio modernos como el *Segway* o los *Gyroscooters*. Pueden equilibrarse autom치ticamente ajustando sus ruedas en respuesta a una se침al de un aceler칩metro o giroscopio. En esta secci칩n, aprenderemos c칩mo resolver un problema similar: equilibrar un poste. Es similar a una situaci칩n en la que un artista de circo necesita equilibrar un poste en su mano, pero este equilibrio del poste solo ocurre en 1D.

Una versi칩n simplificada del equilibrio se conoce como problema **CartPole**. En el mundo de los postes, tenemos un control deslizante horizontal que se puede mover hacia la izquierda o hacia la derecha, y el objetivo es equilibrar un poste vertical encima del control deslizante a medida que se mueve.

<img alt="a cartpole" src="images/cartpole.png" width="200"/>

Para crear y utilizar este entorno, necesitamos un par de l칤neas de c칩digo Python:

```pit칩n
importar gimnasio
env = gimnasio.make("CartPole-v1")

entorno.reset()
hecho = falso
recompensa_total = 0
mientras no haya terminado:
    entorno.render()
    acci칩n = env.action_space.sample()
    observaci칩n, recompensa, hecho, informaci칩n = env.step(acci칩n)
    recompensa_total += recompensa

print(f"Recompensa total: {recompensa_total}")
```

Se puede acceder a cada entorno exactamente de la misma manera:
* `env.reset` inicia un nuevo experimento
* `env.step` realiza un paso de simulaci칩n. Recibe una **acci칩n** del **espacio de acci칩n** y devuelve una **observaci칩n** (del espacio de observaci칩n), as칤 como una recompensa y un indicador de terminaci칩n.

En el ejemplo anterior realizamos una acci칩n aleatoria en cada paso, raz칩n por la cual la duraci칩n del experimento es muy corta:

![non-balancing cartpole](images/cartpole-nobalance.gif)

El objetivo de un algoritmo RL es entrenar un modelo: la llamada **pol칤tica** &pi; - que devolver치 la acci칩n en respuesta a un estado determinado. Tambi칠n podemos considerar que la pol칤tica es probabil칤stica, por ejemplo. para cualquier estado *s* y acci칩n *a* devolver치 la probabilidad &pi;(*a*|*s*) de que debamos tomar *a* en el estado *s*.

## Algoritmo de gradientes de pol칤ticas

La forma m치s obvia de modelar una pol칤tica es crear una red neuronal que tome estados como entrada y devuelva las acciones correspondientes (o m치s bien las probabilidades de todas las acciones). En cierto sentido, ser칤a similar a una tarea de clasificaci칩n normal, con una diferencia importante: no sabemos de antemano qu칠 acciones debemos realizar en cada uno de los pasos.

La idea aqu칤 es estimar esas probabilidades. Creamos un vector de **recompensas acumulativas** que muestra nuestra recompensa total en cada paso del experimento. Tambi칠n aplicamos **descuento de recompensas** multiplicando las recompensas anteriores por alg칰n coeficiente 풥=0,99, para disminuir el papel de las recompensas anteriores. Luego, reforzamos aquellos pasos a lo largo del camino del experimento que generan mayores recompensas.

> Obtenga m치s informaci칩n sobre el algoritmo de gradiente de pol칤ticas y v칠alo en acci칩n en el [example notebook](CartPole-RL-TF.ipynb).

## Actor-Critic Algorithm

An improved version of the Policy Gradients approach is called **Actor-Critic**. The main idea behind it is that the neural network would be trained to return two things:

* The policy, which determines which action to take. This part is called **actor**
* The estimation of the total reward we can expect to get at this state - this part is called **critic**.

In a sense, this architecture resembles a [GAN](../../4-ComputerVision/10-GANs/README.md), donde tenemos dos redes que se entrenan entre s칤. En el modelo actor-cr칤tico, el actor propone la acci칩n que debemos realizar y el cr칤tico intenta ser cr칤tico y estimar el resultado. Sin embargo, nuestro objetivo es entrenar esas redes al un칤sono.

Como conocemos tanto las recompensas acumuladas reales como los resultados arrojados por el cr칤tico durante el experimento, es relativamente f치cil construir una funci칩n de p칠rdida que minimice la diferencia entre ellos. Eso nos dar칤a una **p칠rdida cr칤tica**. Podemos calcular la **p칠rdida de actor** utilizando el mismo enfoque que en el algoritmo de gradiente de pol칤ticas.

Despu칠s de ejecutar uno de esos algoritmos, podemos esperar que nuestro CartPole se comporte as칤:

![a balancing cartpole](images/cartpole-balance.gif)

## 九꽲잺 Ejercicios: gradientes de pol칤ticas y RL actor-cr칤tico

Contin칰a tu aprendizaje en los siguientes cuadernos:

* [RL in TensorFlow](CartPole-RL-TF.ipynb)
* [RL in PyTorch](CartPole-RL-PyTorch.ipynb)

## Otras tareas de RL

El aprendizaje por refuerzo es hoy en d칤a un campo de investigaci칩n de r치pido crecimiento. Algunos de los ejemplos interesantes de aprendizaje por refuerzo son:

* Ense침ar a una computadora a jugar **Juegos Atari**. La parte desafiante de este problema es que no tenemos un estado simple representado como un vector, sino m치s bien una captura de pantalla, y necesitamos usar CNN para convertir esta imagen de pantalla en un vector de caracter칤sticas o para extraer informaci칩n de recompensa. Los juegos de Atari est치n disponibles en el gimnasio.
* Ense침ar a una computadora a jugar juegos de mesa, como Chess and Go. Recientemente, dos agentes entrenaron programas de 칰ltima generaci칩n como **Alpha Zero** desde cero, jugando uno contra el otro y mejorando en cada paso.
* En la industria, RL se utiliza para crear sistemas de control a partir de simulaci칩n. un servicio llamado [Bonsai](https://azure.microsoft.com/services/project-bonsai/?WT.mc_id=academic-77998-cacaste) est치 dise침ado espec칤ficamente para eso.

* ## Conclusi칩n

Ahora hemos aprendido c칩mo entrenar agentes para lograr buenos resultados simplemente proporcion치ndoles una funci칩n de recompensa que defina el estado deseado del juego y d치ndoles la oportunidad de explorar inteligentemente el espacio de b칰squeda. Probamos con 칠xito dos algoritmos y logramos un buen resultado en un per칤odo de tiempo relativamente corto. Sin embargo, este es solo el comienzo de su viaje hacia la vida real y definitivamente deber칤a considerar tomar un curso por separado si desea profundizar m치s.

## 游 Desaf칤o

Explore las aplicaciones enumeradas en la secci칩n 'Otras tareas de RL' e intente implementar una.

## [Post-lecture quiz](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/222)

## Revisi칩n y autoestudio

Obtenga m치s informaci칩n sobre el aprendizaje por refuerzo cl치sico en nuestro [Machine Learning for Beginners Curriculum](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/README.md).

Mirar [this great video](https://www.youtube.com/watch?v=qv6UVOQ0F44) hablando de c칩mo una computadora puede aprender a jugar Super Mario.

## Asignaci칩n: [Train a Mountain Car](lab/README.md)

Su objetivo durante esta tarea ser칤a entrenar en un ambiente de gimnasio diferente -  [Mountain Car](https://www.gymlibrary.ml/environments/classic_control/mountain_car/).
