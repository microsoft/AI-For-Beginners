# Modelado de lenguaje

Las incrustaciones sem치nticas, como Word2Vec y GloVe, son de hecho un primer paso hacia el **modelado del lenguaje**: crear modelos que de alguna manera *comprenden* (o *representan*) la naturaleza del lenguaje.

## [Pre-lecture quiz](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/115)

La idea principal detr치s del modelado de lenguajes es entrenarlos en conjuntos de datos sin etiquetar y sin supervisi칩n. Esto es importante porque tenemos grandes cantidades de texto sin etiquetar disponibles, mientras que la cantidad de texto etiquetado siempre estar치 limitada por la cantidad de esfuerzo que podamos dedicar al etiquetado. En la mayor칤a de los casos, podemos crear modelos de lenguaje que pueden **predecir las palabras que faltan** en el texto, porque es f치cil enmascarar una palabra aleatoria en el texto y usarla como muestra de entrenamiento.

## Integraciones de entrenamiento

En nuestros ejemplos anteriores, utilizamos incorporaciones sem치nticas previamente entrenadas, pero es interesante ver c칩mo se pueden entrenar esas incorporaciones. Hay varias ideas posibles que se pueden utilizar:

* Modelado de lenguaje **N-Gram**, cuando predecimos un token observando N tokens anteriores (N-gram)
* **Bolsa de palabras continua** (CBoW), cuando predecimos el token medio $W_0$ en una secuencia de tokens $W_{-N}$, ..., $W_N$.
* **Skip-gram**, donde predecimos un conjunto de tokens vecinos {$W_{-N},\dots, W_{-1}, W_1,\dots, W_N$} del token del medio $W_0$.

![image from paper on converting words to vectors](../14-Embeddings/images/example-algorithms-for-converting-words-to-vectors.png)

> Imagen de [this paper](https://arxiv.org/pdf/1301.3781.pdf)

## 九꽲잺 Cuadernos de ejemplo: Entrenamiento del modelo CBoW

Contin칰a tu aprendizaje en los siguientes cuadernos:

* [Training CBoW Word2Vec with TensorFlow](CBoW-TF.ipynb)
* [Training CBoW Word2Vec with PyTorch](CBoW-PyTorch.ipynb)

## Conclusi칩n

춰En la lecci칩n anterior vimos que las incrustaciones de palabras funcionan como magia! Ahora sabemos que entrenar incrustaciones de palabras no es una tarea muy compleja y, si es necesario, deber칤amos poder entrenar nuestras propias incrustaciones de palabras para texto de dominio espec칤fico.

## [Post-lecture quiz](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/215)

## Revisi칩n y autoestudio

* [Official PyTorch tutorial on Language Modeling](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html).
* [Official TensorFlow tutorial on training Word2Vec model](https://www.TensorFlow.org/tutorials/text/word2vec).
* Se describe el uso del marco **gensim** para entrenar las incrustaciones m치s utilizadas en unas pocas l칤neas de c칩digo [in this documentation](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html).

## 游 [Assignment: Train Skip-Gram Model](lab/README.md)

In the lab, we challenge you to modify the code from this lesson to train skip-gram model instead of CBoW.[Read the details](lab/README.md)

