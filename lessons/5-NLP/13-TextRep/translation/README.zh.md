# 将文本表示为张量

## [课前测验](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/113)

## 文本分类

在这一部分的前面，我们将集中讨论**文本分类**任务。我们将使用[AG News](https://www.kaggle.com/amananandrai/ag-news-classification-dataset)数据集，该数据集包含以下新闻文章：

* 类别：科技/技术
* 标题：KY公司获得研究肽化合物的资助金（美联社）* 主题：AP - 路易斯维尔大学的化学研究员创办的一个公司赢得了一项开发资金...
  
我们的目标是根据文本将新闻项目分类为其中一种类别。

## 表示文本

如果我们想要用神经网络解决自然语言处理（NLP）任务，我们需要一种将文本表示为张量的方法。计算机已经通过编码（如ASCII或UTF-8）将文本字符表示为将映射到屏幕上的字体的数字。

<img alt="显示将字符映射到ASCII和二进制表示的图像" src="../images/ascii-character-map.png" width="50%"/>

> [图片来源](https://www.seobility.net/zh/wiki/ASCII)

作为人类，我们理解每个字母代表的含义，以及所有字符如何组合形成句子中的单词。然而，计算机本身没有这样的理解，神经网络在训练过程中需要学习这个含义。

因此，在表示文本时可以使用不同的方法：

* **字符级表示**，通过将每个字符都视为一个数字来表示文本。假设我们的文本语料库中有*C*种不同的字符，那么单词*Hello*将被表示为一个5x*C*的张量。每个字母将对应于一个以one-hot编码的张量列。
* **单词级表示**，我们创建一个包含文本中所有单词的**词汇表**，然后使用one-hot编码来表示单词。这种方法更好一些，因为每个字母本身并没有太多含义，因此通过使用更高级的语义概念-单词，我们简化了神经网络的任务。然而，由于字典大小较大，我们需要处理高维稀疏张量。

无论采用何种表示方式，我们首先需要将文本转换为一系列**标记**（token），其中一个标记可以是一个字符、一个单词，或者有时甚至是一个单词的一部分。然后，我们使用**词汇表**将标记转换为数字，通常使用one-hot编码来将这个数字输入神经网络。

## N-Grams

在自然语言中，一个词的确切含义只能在上下文中确定。例如，*神经网络*和*捕鱼网*的含义完全不同。为了考虑到这一点，一种方法是构建我们的模型以词对的形式，并将词对视为单独的词汇标记。这样，句子*I like to go fishing*将由以下标记序列表示：*I like*，*like to*，*to go*，*go fishing*。这种方法的问题在于词典大小显著增长，而像*go fishing*和*go shopping*这样的组合被不同的标记表示，尽管它们是相同的动词，它们之间没有任何语义相似性。

在一些情况下，我们可以考虑使用三元组 - 三个词的组合。这种方法通常被称为**n-grams**。此外，使用字符级表示的n-grams也是有意义的，这种情况下n-grams大致对应于不同的音节。

## 词袋和TF/IDF

在解决文本分类等任务时，我们需要能够用一个固定大小的向量来表示文本，我们将使用这个向量作为最终密集分类器的输入。其中一种最简单的方法是将所有单词的表示组合在一起，例如通过加法。如果我们将每个单词的独热编码相加，我们将得到一个频率向量，显示每个单词在文本中出现的次数。这种文本表示被称为**词袋**（Bag-of-Words）。
<img src="../images/bow.png" width="90%"/>

> 图片由作者提供

BoW基本上表示出现在文本中的单词及其数量，这实际上可以很好地表明文本的内容。例如，政治新闻文章可能包含诸如"总统"和"国家"之类的词汇，而科学出版物可能会使用"对撞机"、"发现"等词汇。因此，单词频率在许多情况下可以很好地指示文本内容。

BoW的问题在于一些常见词汇，例如"and"、"is"等，在大多数文本中都出现，并且它们的频率最高，掩盖了真正重要的词汇。我们可以通过考虑单词在整个文档集中出现的频率来降低这些词汇的重要性。这是TF/IDF方法的主要思想，更详细的内容可以在本课程附带的笔记本中找到。

然而，以上方法都不能充分考虑文本的语义。我们需要更强大的神经网络模型来实现这一点，这将在本节的后续讨论中介绍。

## ✍️练习: 文本表示

在以下笔记本中继续学习:

* [使用PyTorch进行文本表示](../TextRepresentationPyTorch.ipynb)
* [使用TensorFlow进行文本表示](../TextRepresentationTF.ipynb)

## 结论

到目前为止，我们已经学习了一些可以为不同的单词添加频率权重的技术。然而，它们无法表示含义或顺序。正如著名语言学家J.R. Firth在1935年所说：“一个单词的完整含义始终是依赖于上下文的，任何脱离上下文的含义研究都不能被认真对待。”在后面的课程中，我们将学习如何使用语言模型从文本中捕获上下文信息。

## 🚀 挑战

尝试一些使用词袋模型和不同数据模型的其他练习。你可以参考这个[Kaggle竞赛](https://www.kaggle.com/competitions/word2vec-nlp-tutorial/overview/part-1-for-beginners-bag-of-words)。

## [讲座后测验](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/213)

## 复习与自学

在[Microsoft Learn](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-pytorch/?WT.mc_id=academic-77998-cacaste)上练习文本嵌入和词袋技术的技能。

## [作业: 笔记本](assignment.zh.md)