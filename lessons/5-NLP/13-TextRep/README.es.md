# Representando el texto como tensoras

## [Pre-lecture quiz](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/113)

## Clasificaci칩n de texto

A lo largo de la primera parte de esta secci칩n, nos centraremos en la tarea de **clasificaci칩n de texto**. Usaremos el [AG News](https://www.kaggle.com/amananandrai/ag-news-classification-dataset) Conjunto de datos, que contiene art칤culos de noticias como los siguientes:

* Categor칤a: Ciencia/Tecnolog칤a
* T칤tulo: Ky. Company gana una subvenci칩n para estudiar p칠ptidos (AP)
*Cuerpo: AP - Una empresa fundada por un investigador de qu칤mica de la Universidad de Louisville gan칩 una subvenci칩n para desarrollar...

Nuestro objetivo ser치 clasificar la noticia en una de las categor칤as basadas en texto.

## Representando texto

Si queremos resolver tareas de procesamiento del lenguaje natural (PNL) con redes neuronales, necesitamos alguna forma de representar el texto como tensores. Las computadoras ya representan caracteres textuales como n칰meros que se asignan a fuentes en la pantalla mediante codificaciones como ASCII o UTF-8.

<img alt="Image showing diagram mapping a character to an ASCII and binary representation" src="images/ascii-character-map.png" width="50%"/>

> [Image source](https://www.seobility.net/en/wiki/ASCII)

Como seres humanos, entendemos lo que **representa** cada letra y c칩mo todos los caracteres se unen para formar las palabras de una oraci칩n. Sin embargo, los ordenadores por s칤 solos no tienen esa comprensi칩n y las redes neuronales tienen que aprender el significado durante el entrenamiento.

Por tanto, podemos utilizar diferentes enfoques a la hora de representar texto:

* **Representaci칩n a nivel de car치cter**, cuando representamos texto tratando cada car치cter como un n칰mero. Dado que tenemos *C* caracteres diferentes en nuestro corpus de texto, la palabra *Hola* estar칤a representada por el tensor 5x*C*. Cada letra corresponder칤a a una columna tensorial en codificaci칩n one-hot.
* **Representaci칩n a nivel de palabra**, en la que creamos un **vocabulario** de todas las palabras de nuestro texto y luego las representamos usando codificaci칩n one-hot. Este enfoque es algo mejor, porque cada letra por s칤 sola no tiene mucho significado y, por lo tanto, al utilizar conceptos sem치nticos de nivel superior (palabras), simplificamos la tarea de la red neuronal. Sin embargo, dado el gran tama침o del diccionario, debemos lidiar con tensores dispersos de alta dimensi칩n.

Independientemente de la representaci칩n, primero debemos convertir el texto en una secuencia de **tokens**, siendo un token un car치cter, una palabra o, a veces, incluso parte de una palabra. Luego, convertimos el token en un n칰mero, normalmente usando **vocabulario**, y este n칰mero se puede introducir en una red neuronal mediante codificaci칩n one-hot.

## N-Gramos

En el lenguaje natural, el significado preciso de las palabras s칩lo puede determinarse en contexto. Por ejemplo, los significados de *red neuronal* y *red de pesca* son completamente diferentes. Una de las formas de tener esto en cuenta es construir nuestro modelo sobre pares de palabras y considerar los pares de palabras como fichas de vocabulario separadas. De esta forma, la frase *me gusta ir a pescar* quedar치 representada por la siguiente secuencia de fichas: *me gusta*, *me gusta*, *ir*, *ir a pescar*. El problema con este enfoque es que el tama침o del diccionario crece significativamente y combinaciones como *ir a pescar* y *ir de compras* se presentan mediante tokens diferentes, que no comparten ninguna similitud sem치ntica a pesar del mismo verbo.

En algunos casos, tambi칠n podemos considerar el uso de trigramas (combinaciones de tres palabras). Por lo tanto, el enfoque a menudo se denomina **n-gramas**. Adem치s, tiene sentido utilizar n-gramas con representaci칩n a nivel de car치cter, en cuyo caso los n-gramas corresponder치n aproximadamente a diferentes programas de estudio.

## Bolsa de palabras y TF/IDF

Al resolver tareas como la clasificaci칩n de texto, debemos poder representar el texto mediante un vector de tama침o fijo, que usaremos como entrada para el clasificador denso final. Una de las formas m치s sencillas de hacerlo es combinar todas las representaciones de palabras individuales, por ejemplo. agreg치ndolos. Si agregamos codificaciones one-hot de cada palabra, terminaremos con un vector de frecuencias, que muestra cu치ntas veces aparece cada palabra dentro del texto. Esta representaci칩n de texto se llama **bolsa de palabras** (BoW).

<img src="images/bow.png" width="90%"/>

> Image by the author

Un BoW esencialmente representa qu칠 palabras aparecen en el texto y en qu칠 cantidades, lo que de hecho puede ser una buena indicaci칩n de de qu칠 trata el texto. Por ejemplo, es probable que un art칤culo de noticias sobre pol칤tica contenga palabras como *presidente* y *pa칤s*, mientras que una publicaci칩n cient칤fica tendr칤a algo como *colisionador*, *descubierto*, etc. Por lo tanto, la frecuencia de las palabras puede en muchos casos ser una buena indicador del contenido del texto.

El problema con BoW es que ciertas palabras comunes, como *and*, *is*, etc. aparecen en la mayor칤a de los textos y tienen frecuencias m치s altas, enmascarando las palabras que son realmente importantes. Podemos reducir la importancia de esas palabras teniendo en cuenta la frecuencia con la que aparecen en toda la colecci칩n de documentos. Esta es la idea principal detr치s del enfoque TF/IDF, que se trata con m치s detalle en los cuadernos adjuntos a esta lecci칩n.

Sin embargo, ninguno de esos enfoques puede tener en cuenta plenamente la **sem치ntica** del texto. Necesitamos modelos de redes neuronales m치s potentes para hacer esto, lo cual discutiremos m치s adelante en esta secci칩n.

## 九꽲잺 Ejercicios: Representaci칩n de Texto

Contin칰a tu aprendizaje en los siguientes cuadernos:

* [Text Representation with PyTorch](TextRepresentationPyTorch.ipynb)
* [Text Representation with TensorFlow](TextRepresentationTF.ipynb)

## Conclusi칩n

Hasta ahora, hemos estudiado t칠cnicas que pueden agregar peso de frecuencia a diferentes palabras. Sin embargo, no pueden representar significado u orden. Como dijo el famoso ling칲ista J. R. Firth en 1935: "El significado completo de una palabra es siempre contextual, y ning칰n estudio del significado aparte del contexto puede tomarse en serio". M치s adelante en el curso aprenderemos c칩mo capturar informaci칩n contextual del texto utilizando modelos de lenguaje.

## 游 Desaf칤o

Pruebe otros ejercicios utilizando una bolsa de palabras y diferentes modelos de datos. Quiz치s te inspire esto [competition on Kaggle](https://www.kaggle.com/competitions/word2vec-nlp-tutorial/overview/part-1-for-beginners-bag-of-words)

## [Post-lecture quiz](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/213)

## Revisi칩n y autoestudio

Practica tus habilidades con incrustaciones de texto y t칠cnicas de bolsa de palabras en [Microsoft Learn](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-pytorch/?WT.mc_id=academic-77998-cacaste)

## [Assignment: Notebooks](assignment.md)
