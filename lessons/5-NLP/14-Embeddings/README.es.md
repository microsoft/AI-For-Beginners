# incrustaciones

## [Pre-lecture quiz](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/114)

Cuando entren谩bamos clasificadores basados en BoW o TF/IDF, operamos con vectores de bolsa de palabras de alta dimensi贸n con longitud "vocab_size", y convert铆amos expl铆citamente desde vectores de representaci贸n posicional de baja dimensi贸n a una representaci贸n 煤nica y dispersa. Sin embargo, esta representaci贸n 煤nica no es eficiente en cuanto a memoria. Adem谩s, cada palabra se trata de forma independiente entre s铆, es decir, los vectores codificados en caliente no expresan ninguna similitud sem谩ntica entre palabras.

La idea de **incrustar** es representar palabras mediante vectores densos de dimensiones inferiores, que de alguna manera reflejan el significado sem谩ntico de una palabra. M谩s adelante discutiremos c贸mo crear incrustaciones de palabras significativas, pero por ahora pensemos en las incrustaciones como una forma de reducir la dimensionalidad de un vector de palabras.

Entonces, la capa de incrustaci贸n tomar铆a una palabra como entrada y producir铆a un vector de salida del `embedding_size` especificado. En cierto sentido, es muy similar a una capa "Lineal", pero en lugar de tomar un vector codificado en caliente, podr谩 tomar un n煤mero de palabra como entrada, lo que nos permitir谩 evitar la creaci贸n de grandes archivos codificados en caliente. vectores.

Al usar una capa de incrustaci贸n como primera capa en nuestra red clasificadora, podemos cambiar de una bolsa de palabras al modelo de **bolsa de incrustaci贸n**, donde primero convertimos cada palabra en nuestro texto en la incrustaci贸n correspondiente y luego calculamos algunas funci贸n agregada sobre todas esas incrustaciones, como "suma", "promedio" o "m谩ximo".

![Image showing an embedding classifier for five sequence words.](images/embedding-classifier-example.png)

> Imagen del autor

## 锔 Ejercicios: Incrustaciones

Contin煤a tu aprendizaje en los siguientes cuadernos:
* [Embeddings with PyTorch](EmbeddingsPyTorch.ipynb)
* [Embeddings TensorFlow](EmbeddingsTF.ipynb)
  
## Incrustaciones sem谩nticas: Word2Vec

Si bien la capa de incrustaci贸n aprendi贸 a asignar palabras a una representaci贸n vectorial, esta representaci贸n no necesariamente ten铆a mucho significado sem谩ntico. Ser铆a bueno aprender una representaci贸n vectorial de modo que palabras similares o sin贸nimos correspondan a vectores que est茅n cerca entre s铆 en t茅rminos de alguna distancia vectorial (por ejemplo, distancia euclidiana).

Para hacer eso, necesitamos entrenar previamente nuestro modelo de incrustaci贸n en una gran colecci贸n de texto de una manera espec铆fica. Una forma de entrenar incrustaciones sem谩nticas se llama [Word2Vec](https://en.wikipedia.org/wiki/Word2vec). Se basa en dos arquitecturas principales que se utilizan para producir una representaci贸n distribuida de palabras:

  - **Bolsa de palabras continua** (CBoW): en esta arquitectura, entrenamos el modelo para predecir una palabra a partir del contexto circundante. Dado el ngrama $(W_{-2},W_{-1},W_0,W_1,W_2)$, el objetivo del modelo es predecir $W_0$ a partir de $(W_{-2},W_{-1} ,W_1,W_2)$.
  - **Salto continuo de gramo** es opuesto a CBoW. El modelo utiliza una ventana circundante de palabras de contexto para predecir la palabra actual.

CBoW es m谩s r谩pido, mientras que skip-gram es m谩s lento, pero representa mejor palabras poco frecuentes.

![Image showing both CBoW and Skip-Gram algorithms to convert words to vectors.](./images/example-algorithms-for-converting-words-to-vectors.png)

> Imagen de [this paper](https://arxiv.org/pdf/1301.3781.pdf)

Las incrustaciones previamente entrenadas de Word2Vec (as铆 como otros modelos similares, como GloVe) tambi茅n se pueden usar en lugar de la capa de incrustaci贸n en redes neuronales. Sin embargo, debemos ocuparnos de los vocabularios, porque es probable que el vocabulario utilizado para entrenar previamente Word2Vec/GloVe difiera del vocabulario de nuestro corpus de texto. Eche un vistazo a los cuadernos anteriores para ver c贸mo se puede resolver este problema.

## Incrustaciones contextuales

Una limitaci贸n clave de las representaciones de incrustaci贸n tradicionales previamente entrenadas, como Word2Vec, es el problema de la desambiguaci贸n del sentido de las palabras. Si bien las incrustaciones previamente entrenadas pueden capturar parte del significado de las palabras en contexto, cada significado posible de una palabra est谩 codificado en la misma incrustaci贸n. Esto puede causar problemas en los modelos posteriores, ya que muchas palabras, como la palabra "jugar", tienen diferentes significados seg煤n el contexto en el que se utilizan.

Por ejemplo, la palabra "jugar" en esas dos oraciones diferentes tiene un significado bastante diferente:

- Fui a una **obra** en el teatro.
- John quiere **jugar** con sus amigos.

Las incrustaciones previamente entrenadas anteriores representan ambos significados de la palabra "jugar" en la misma incrustaci贸n. Para superar esta limitaci贸n, necesitamos crear incrustaciones basadas en el **modelo de lenguaje**, que se entrena en un gran corpus de texto y *sabe* c贸mo se pueden juntar las palabras en diferentes contextos. Hablar de incrustaciones contextuales est谩 fuera del alcance de este tutorial, pero volveremos a ellas cuando hablemos de modelos de lenguaje m谩s adelante en el curso.

## Conclusi贸n

En esta lecci贸n, descubri贸 c贸mo crear y utilizar capas incrustadas en TensorFlow y Pytorch para reflejar mejor los significados sem谩nticos de las palabras.

##  Desaf铆o

Word2Vec se ha utilizado para algunas aplicaciones interesantes, incluida la generaci贸n de letras de canciones y poes铆a. Eche un vistazo a [este art铆culo](https://www.politetype.com/blog/word2vec-color-poems) que explica c贸mo el autor utiliz贸 Word2Vec para generar poes铆a. Mirar [this video by Dan Shiffmann](https://www.youtube.com/watch?v=LSS_bos_TPI&ab_channel=TheCodingTrain) as铆 como descubrir una explicaci贸n diferente de esta t茅cnica. Luego intente aplicar estas t茅cnicas a su propio corpus de texto, tal vez obtenido de Kaggle.

## [Post-lecture quiz](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/214)

## Revisi贸n y autoestudio

Lea este documento sobre Word2Vec: [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf)

## [Assignment: Notebooks](assignment.md)

