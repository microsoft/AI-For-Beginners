<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "186bf7eeab776b36f557357ea56d4751",
  "translation_date": "2025-08-25T23:45:20+00:00",
  "source_file": "lessons/3-NeuralNetworks/04-OwnFramework/README.md",
  "language_code": "sk"
}
-->
# Ãšvod do neurÃ³novÃ½ch sietÃ­. ViacvrstvovÃ½ perceptron

V predchÃ¡dzajÃºcej Äasti ste sa oboznÃ¡mili s najjednoduchÅ¡Ã­m modelom neurÃ³novej siete â€“ jednovrstvovÃ½m perceptronom, lineÃ¡rnym modelom pre dvojtriednu klasifikÃ¡ciu.

V tejto Äasti rozÅ¡Ã­rime tento model na flexibilnejÅ¡Ã­ rÃ¡mec, ktorÃ½ nÃ¡m umoÅ¾nÃ­:

* vykonÃ¡vaÅ¥ **viactriednu klasifikÃ¡ciu** okrem dvojtriednej
* rieÅ¡iÅ¥ **regresnÃ© problÃ©my** okrem klasifikÃ¡cie
* oddeliÅ¥ triedy, ktorÃ© nie sÃº lineÃ¡rne separovateÄ¾nÃ©

TaktieÅ¾ si vyvinieme vlastnÃ½ modulÃ¡rny rÃ¡mec v Pythone, ktorÃ½ nÃ¡m umoÅ¾nÃ­ konÅ¡truovaÅ¥ rÃ´zne architektÃºry neurÃ³novÃ½ch sietÃ­.

## [KvÃ­z pred prednÃ¡Å¡kou](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/104)

## FormalizÃ¡cia strojovÃ©ho uÄenia

ZaÄnime formalizÃ¡ciou problÃ©mu strojovÃ©ho uÄenia. Predpokladajme, Å¾e mÃ¡me trÃ©ningovÃº mnoÅ¾inu dÃ¡t **X** s oznaÄeniami **Y**, a potrebujeme vytvoriÅ¥ model *f*, ktorÃ½ bude robiÅ¥ Äo najpresnejÅ¡ie predikcie. Kvalita predikciÃ­ sa meria pomocou **stratovej funkcie** â„’. ÄŒasto sa pouÅ¾Ã­vajÃº nasledujÃºce stratovÃ© funkcie:

* Pre regresnÃ© problÃ©my, keÄ potrebujeme predpovedaÅ¥ ÄÃ­slo, mÃ´Å¾eme pouÅ¾iÅ¥ **absolÃºtnu chybu** âˆ‘<sub>i</sub>|f(x<sup>(i)</sup>)-y<sup>(i)</sup>| alebo **kvadratickÃº chybu** âˆ‘<sub>i</sub>(f(x<sup>(i)</sup>)-y<sup>(i)</sup>)<sup>2</sup>
* Pre klasifikÃ¡ciu pouÅ¾Ã­vame **0-1 stratu** (Äo je v podstate to istÃ© ako **presnosÅ¥** modelu) alebo **logistickÃº stratu**.

Pre jednovrstvovÃ½ perceptron bola funkcia *f* definovanÃ¡ ako lineÃ¡rna funkcia *f(x)=wx+b* (kde *w* je matica vÃ¡h, *x* je vektor vstupnÃ½ch vlastnostÃ­ a *b* je vektor biasu). Pre rÃ´zne architektÃºry neurÃ³novÃ½ch sietÃ­ mÃ´Å¾e maÅ¥ tÃ¡to funkcia zloÅ¾itejÅ¡iu formu.

> V prÃ­pade klasifikÃ¡cie je Äasto Å¾iaduce zÃ­skaÅ¥ pravdepodobnosti prÃ­sluÅ¡nÃ½ch tried ako vÃ½stup siete. Na konverziu Ä¾ubovoÄ¾nÃ½ch ÄÃ­sel na pravdepodobnosti (napr. na normalizÃ¡ciu vÃ½stupu) Äasto pouÅ¾Ã­vame funkciu **softmax** Ïƒ, a funkcia *f* sa stÃ¡va *f(x)=Ïƒ(wx+b)*.

V definÃ­cii *f* vyÅ¡Å¡ie sa *w* a *b* nazÃ½vajÃº **parametre** Î¸=âŸ¨*w,b*âŸ©. Na zÃ¡klade mnoÅ¾iny dÃ¡t âŸ¨**X**,**Y**âŸ© mÃ´Å¾eme vypoÄÃ­taÅ¥ celkovÃº chybu na celej mnoÅ¾ine dÃ¡t ako funkciu parametrov Î¸.

> âœ… **CieÄ¾om trÃ©novania neurÃ³novej siete je minimalizovaÅ¥ chybu zmenou parametrov Î¸**

## OptimalizÃ¡cia pomocou gradientnÃ©ho zostupu

Existuje znÃ¡ma metÃ³da optimalizÃ¡cie funkciÃ­ nazÃ½vanÃ¡ **gradientnÃ½ zostup**. MyÅ¡lienka spoÄÃ­va v tom, Å¾e mÃ´Å¾eme vypoÄÃ­taÅ¥ derivÃ¡ciu (v multidimenzionÃ¡lnom prÃ­pade nazÃ½vanÃº **gradient**) stratovej funkcie vzhÄ¾adom na parametre a meniÅ¥ parametre tak, aby sa chyba zniÅ¾ovala. To mÃ´Å¾eme formalizovaÅ¥ nasledovne:

* Inicializujte parametre nÃ¡hodnÃ½mi hodnotami w<sup>(0)</sup>, b<sup>(0)</sup>
* Opakujte nasledujÃºci krok mnohokrÃ¡t:
    - w<sup>(i+1)</sup> = w<sup>(i)</sup>-Î·âˆ‚â„’/âˆ‚w
    - b<sup>(i+1)</sup> = b<sup>(i)</sup>-Î·âˆ‚â„’/âˆ‚b

PoÄas trÃ©novania by sa optimalizaÄnÃ© kroky mali poÄÃ­taÅ¥ s ohÄ¾adom na celÃº mnoÅ¾inu dÃ¡t (pamÃ¤tajte, Å¾e strata sa poÄÃ­ta ako sÃºÄet cez vÅ¡etky trÃ©ningovÃ© vzorky). V praxi vÅ¡ak berieme malÃ© Äasti mnoÅ¾iny dÃ¡t nazÃ½vanÃ© **minibatch-e** a poÄÃ­tame gradienty na zÃ¡klade podmnoÅ¾iny dÃ¡t. PretoÅ¾e podmnoÅ¾ina sa zakaÅ¾dÃ½m vyberÃ¡ nÃ¡hodne, takÃ¡to metÃ³da sa nazÃ½va **stochastickÃ½ gradientnÃ½ zostup** (SGD).

## ViacvrstvovÃ© perceptrony a spÃ¤tnÃ¡ propagÃ¡cia

JednovrstvovÃ¡ sieÅ¥, ako sme videli vyÅ¡Å¡ie, je schopnÃ¡ klasifikovaÅ¥ lineÃ¡rne separovateÄ¾nÃ© triedy. Na vytvorenie bohatÅ¡ieho modelu mÃ´Å¾eme kombinovaÅ¥ niekoÄ¾ko vrstiev siete. Matematicky to znamenÃ¡, Å¾e funkcia *f* bude maÅ¥ zloÅ¾itejÅ¡iu formu a bude sa poÄÃ­taÅ¥ v niekoÄ¾kÃ½ch krokoch:
* z<sub>1</sub>=w<sub>1</sub>x+b<sub>1</sub>
* z<sub>2</sub>=w<sub>2</sub>Î±(z<sub>1</sub>)+b<sub>2</sub>
* f = Ïƒ(z<sub>2</sub>)

Tu je Î± **nelineÃ¡rna aktivaÄnÃ¡ funkcia**, Ïƒ je softmax funkcia a parametre Î¸=<*w<sub>1</sub>,b<sub>1</sub>,w<sub>2</sub>,b<sub>2</sub>*>.

Algoritmus gradientnÃ©ho zostupu zostÃ¡va rovnakÃ½, ale vÃ½poÄet gradientov je zloÅ¾itejÅ¡Ã­. Na zÃ¡klade pravidla reÅ¥azovej derivÃ¡cie mÃ´Å¾eme vypoÄÃ­taÅ¥ derivÃ¡cie nasledovne:

* âˆ‚â„’/âˆ‚w<sub>2</sub> = (âˆ‚â„’/âˆ‚Ïƒ)(âˆ‚Ïƒ/âˆ‚z<sub>2</sub>)(âˆ‚z<sub>2</sub>/âˆ‚w<sub>2</sub>)
* âˆ‚â„’/âˆ‚w<sub>1</sub> = (âˆ‚â„’/âˆ‚Ïƒ)(âˆ‚Ïƒ/âˆ‚z<sub>2</sub>)(âˆ‚z<sub>2</sub>/âˆ‚Î±)(âˆ‚Î±/âˆ‚z<sub>1</sub>)(âˆ‚z<sub>1</sub>/âˆ‚w<sub>1</sub>)

> âœ… Pravidlo reÅ¥azovej derivÃ¡cie sa pouÅ¾Ã­va na vÃ½poÄet derivÃ¡ciÃ­ stratovej funkcie vzhÄ¾adom na parametre.

VÅ¡imnite si, Å¾e Ä¾avÃ¡ ÄasÅ¥ vÅ¡etkÃ½ch tÃ½chto vÃ½razov je rovnakÃ¡, a preto mÃ´Å¾eme efektÃ­vne poÄÃ­taÅ¥ derivÃ¡cie zaÄÃ­najÃºc stratovou funkciou a postupujÃºc "spÃ¤Å¥" cez vÃ½poÄtovÃ½ graf. Preto sa metÃ³da trÃ©novania viacvrstvovÃ©ho perceptronu nazÃ½va **spÃ¤tnÃ¡ propagÃ¡cia** alebo 'backprop'.

<img alt="vÃ½poÄtovÃ½ graf" src="images/ComputeGraphGrad.png"/>

> TODO: citÃ¡cia obrÃ¡zku

> âœ… SpÃ¤tnÃº propagÃ¡ciu si podrobnejÅ¡ie preberieme v naÅ¡om prÃ­klade v notebooku.  

## ZÃ¡ver

V tejto lekcii sme si vytvorili vlastnÃº kniÅ¾nicu neurÃ³novÃ½ch sietÃ­ a pouÅ¾ili sme ju na jednoduchÃº dvojrozmernÃº klasifikaÄnÃº Ãºlohu.

## ğŸš€ VÃ½zva

V priloÅ¾enom notebooku implementujete vlastnÃ½ rÃ¡mec na vytvÃ¡ranie a trÃ©novanie viacvrstvovÃ½ch perceptronov. Budete mÃ´cÅ¥ detailne vidieÅ¥, ako modernÃ© neurÃ³novÃ© siete fungujÃº.

PokraÄujte do notebooku [OwnFramework](../../../../../lessons/3-NeuralNetworks/04-OwnFramework/OwnFramework.ipynb) a prejdite si ho.

## [KvÃ­z po prednÃ¡Å¡ke](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/204)

## PrehÄ¾ad a samoÅ¡tÃºdium

SpÃ¤tnÃ¡ propagÃ¡cia je beÅ¾nÃ½ algoritmus pouÅ¾Ã­vanÃ½ v AI a ML, stojÃ­ za to ju [Å¡tudovaÅ¥ podrobnejÅ¡ie](https://wikipedia.org/wiki/Backpropagation).

## [Zadanie](lab/README.md)

V tomto laboratÃ³riu mÃ¡te za Ãºlohu pouÅ¾iÅ¥ rÃ¡mec, ktorÃ½ ste vytvorili v tejto lekcii, na rieÅ¡enie klasifikÃ¡cie ruÄne pÃ­sanÃ½ch ÄÃ­slic MNIST.

* [InÅ¡trukcie](lab/README.md)
* [Notebook](../../../../../lessons/3-NeuralNetworks/04-OwnFramework/lab/MyFW_MNIST.ipynb)

**Zrieknutie sa zodpovednosti**:  
Tento dokument bol preloÅ¾enÃ½ pomocou sluÅ¾by AI prekladu [Co-op Translator](https://github.com/Azure/co-op-translator). Aj keÄ sa snaÅ¾Ã­me o presnosÅ¥, prosÃ­m, berte na vedomie, Å¾e automatizovanÃ© preklady mÃ´Å¾u obsahovaÅ¥ chyby alebo nepresnosti. PÃ´vodnÃ½ dokument v jeho rodnom jazyku by mal byÅ¥ povaÅ¾ovanÃ½ za autoritatÃ­vny zdroj. Pre kritickÃ© informÃ¡cie sa odporÃºÄa profesionÃ¡lny Ä¾udskÃ½ preklad. Nie sme zodpovednÃ­ za Å¾iadne nedorozumenia alebo nesprÃ¡vne interpretÃ¡cie vyplÃ½vajÃºce z pouÅ¾itia tohto prekladu.