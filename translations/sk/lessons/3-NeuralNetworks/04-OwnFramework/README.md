# Ãšvod do neurÃ³novÃ½ch sietÃ­. ViacvrstvovÃ½ perceptron

V predchÃ¡dzajÃºcej sekcii ste sa nauÄili o najjednoduchÅ¡om modeli neurÃ³novej siete â€“ jednovrstvovom perceptrone, lineÃ¡rnom modeli pre dvojtriednu klasifikÃ¡ciu.

V tejto sekcii rozÅ¡Ã­rime tento model na flexibilnejÅ¡Ã­ rÃ¡mec, ktorÃ½ nÃ¡m umoÅ¾nÃ­:

* vykonÃ¡vaÅ¥ **viactriednu klasifikÃ¡ciu** okrem dvojtriednej
* rieÅ¡iÅ¥ **regresnÃ© problÃ©my** okrem klasifikÃ¡cie
* oddeÄ¾ovaÅ¥ triedy, ktorÃ© nie sÃº lineÃ¡rne oddeliteÄ¾nÃ©

TaktieÅ¾ si vyvineme vlastnÃ½ modulÃ¡rny rÃ¡mec v Pythone, ktorÃ½ nÃ¡m umoÅ¾nÃ­ konÅ¡truovaÅ¥ rÃ´zne architektÃºry neurÃ³novÃ½ch sietÃ­.

## [KvÃ­z pred prednÃ¡Å¡kou](https://ff-quizzes.netlify.app/en/ai/quiz/7)

## FormalizÃ¡cia strojovÃ©ho uÄenia

ZaÄnime formalizÃ¡ciou problÃ©mu strojovÃ©ho uÄenia. Predpokladajme, Å¾e mÃ¡me trÃ©ningovÃº dÃ¡tovÃº sadu **X** s oznaÄeniami **Y**, a potrebujeme vytvoriÅ¥ model *f*, ktorÃ½ bude robiÅ¥ Äo najpresnejÅ¡ie predpovede. Kvalita predpovedÃ­ sa meria pomocou **stratovej funkcie** &lagran;. ÄŒasto sa pouÅ¾Ã­vajÃº nasledujÃºce stratovÃ© funkcie:

* Pri regresnom problÃ©me, keÄ potrebujeme predpovedaÅ¥ ÄÃ­slo, mÃ´Å¾eme pouÅ¾iÅ¥ **absolÃºtnu chybu** &sum;<sub>i</sub>|f(x<sup>(i)</sup>)-y<sup>(i)</sup>| alebo **kvadratickÃº chybu** &sum;<sub>i</sub>(f(x<sup>(i)</sup>)-y<sup>(i)</sup>)<sup>2</sup>
* Pri klasifikÃ¡cii pouÅ¾Ã­vame **0-1 stratu** (Äo je v podstate to istÃ© ako **presnosÅ¥** modelu) alebo **logistickÃº stratu**.

Pre jednovrstvovÃ½ perceptron bola funkcia *f* definovanÃ¡ ako lineÃ¡rna funkcia *f(x)=wx+b* (tu *w* je matica vÃ¡h, *x* je vektor vstupnÃ½ch vlastnostÃ­ a *b* je vektor biasu). Pre rÃ´zne architektÃºry neurÃ³novÃ½ch sietÃ­ mÃ´Å¾e tÃ¡to funkcia nadobudnÃºÅ¥ zloÅ¾itejÅ¡iu formu.

> V prÃ­pade klasifikÃ¡cie je Äasto Å¾iaduce zÃ­skaÅ¥ pravdepodobnosti prÃ­sluÅ¡nÃ½ch tried ako vÃ½stup siete. Na konverziu Ä¾ubovoÄ¾nÃ½ch ÄÃ­sel na pravdepodobnosti (napr. na normalizÃ¡ciu vÃ½stupu) Äasto pouÅ¾Ã­vame funkciu **softmax** &sigma;, a funkcia *f* sa stÃ¡va *f(x)=&sigma;(wx+b)*.

V definÃ­cii *f* vyÅ¡Å¡ie sa *w* a *b* nazÃ½vajÃº **parametre** &theta;=âŸ¨*w,b*âŸ©. Na zÃ¡klade dÃ¡tovej sady âŸ¨**X**,**Y**âŸ© mÃ´Å¾eme vypoÄÃ­taÅ¥ celkovÃº chybu na celej dÃ¡tovej sade ako funkciu parametrov &theta;.

> âœ… **CieÄ¾om trÃ©ningu neurÃ³novej siete je minimalizovaÅ¥ chybu zmenou parametrov &theta;**

## OptimalizÃ¡cia pomocou gradientnÃ©ho zostupu

Existuje znÃ¡ma metÃ³da optimalizÃ¡cie funkciÃ­ nazÃ½vanÃ¡ **gradientnÃ½ zostup**. MyÅ¡lienka je, Å¾e mÃ´Å¾eme vypoÄÃ­taÅ¥ derivÃ¡ciu (v multidimenzionÃ¡lnom prÃ­pade nazÃ½vanÃº **gradient**) stratovej funkcie vzhÄ¾adom na parametre a meniÅ¥ parametre tak, aby sa chyba zniÅ¾ovala. To sa dÃ¡ formalizovaÅ¥ nasledovne:

* Inicializujte parametre nÃ¡hodnÃ½mi hodnotami w<sup>(0)</sup>, b<sup>(0)</sup>
* Opakujte nasledujÃºci krok mnohokrÃ¡t:
    - w<sup>(i+1)</sup> = w<sup>(i)</sup>-&eta;&part;&lagran;/&part;w
    - b<sup>(i+1)</sup> = b<sup>(i)</sup>-&eta;&part;&lagran;/&part;b

PoÄas trÃ©ningu sa optimalizaÄnÃ© kroky majÃº poÄÃ­taÅ¥ s ohÄ¾adom na celÃº dÃ¡tovÃº sadu (pamÃ¤tajte, Å¾e strata sa poÄÃ­ta ako sÃºÄet cez vÅ¡etky trÃ©ningovÃ© vzorky). AvÅ¡ak v praxi berieme malÃ© Äasti dÃ¡tovej sady nazÃ½vanÃ© **minibatch-e** a poÄÃ­tame gradienty na zÃ¡klade podmnoÅ¾iny dÃ¡t. KeÄÅ¾e podmnoÅ¾ina sa berie nÃ¡hodne pri kaÅ¾dom kroku, takÃ¡to metÃ³da sa nazÃ½va **stochastickÃ½ gradientnÃ½ zostup** (SGD).

## ViacvrstvovÃ© perceptrony a spÃ¤tnÃ¡ propagÃ¡cia

JednovrstvovÃ¡ sieÅ¥, ako sme videli vyÅ¡Å¡ie, je schopnÃ¡ klasifikovaÅ¥ lineÃ¡rne oddeliteÄ¾nÃ© triedy. Na vytvorenie bohatÅ¡ieho modelu mÃ´Å¾eme kombinovaÅ¥ niekoÄ¾ko vrstiev siete. Matematicky by to znamenalo, Å¾e funkcia *f* by mala zloÅ¾itejÅ¡iu formu a bude sa poÄÃ­taÅ¥ v niekoÄ¾kÃ½ch krokoch:
* z<sub>1</sub>=w<sub>1</sub>x+b<sub>1</sub>
* z<sub>2</sub>=w<sub>2</sub>&alpha;(z<sub>1</sub>)+b<sub>2</sub>
* f = &sigma;(z<sub>2</sub>)

Tu &alpha; je **nelineÃ¡rna aktivaÄnÃ¡ funkcia**, &sigma; je softmax funkcia a parametre &theta;=<*w<sub>1</sub>,b<sub>1</sub>,w<sub>2</sub>,b<sub>2</sub>*>.

Algoritmus gradientnÃ©ho zostupu by zostal rovnakÃ½, ale vÃ½poÄet gradientov by bol zloÅ¾itejÅ¡Ã­. Na zÃ¡klade pravidla reÅ¥azovej diferenciÃ¡cie mÃ´Å¾eme vypoÄÃ­taÅ¥ derivÃ¡cie nasledovne:

* &part;&lagran;/&part;w<sub>2</sub> = (&part;&lagran;/&part;&sigma;)(&part;&sigma;/&part;z<sub>2</sub>)(&part;z<sub>2</sub>/&part;w<sub>2</sub>)
* &part;&lagran;/&part;w<sub>1</sub> = (&part;&lagran;/&part;&sigma;)(&part;&sigma;/&part;z<sub>2</sub>)(&part;z<sub>2</sub>/&part;&alpha;)(&part;&alpha;/&part;z<sub>1</sub>)(&part;z<sub>1</sub>/&part;w<sub>1</sub>)

> âœ… Pravidlo reÅ¥azovej diferenciÃ¡cie sa pouÅ¾Ã­va na vÃ½poÄet derivÃ¡ciÃ­ stratovej funkcie vzhÄ¾adom na parametre.

VÅ¡imnite si, Å¾e Ä¾avÃ¡ ÄasÅ¥ vÅ¡etkÃ½ch tÃ½chto vÃ½razov je rovnakÃ¡, a preto mÃ´Å¾eme efektÃ­vne poÄÃ­taÅ¥ derivÃ¡cie zaÄÃ­najÃºc stratovou funkciou a postupovaÅ¥ "spÃ¤Å¥" cez vÃ½poÄtovÃ½ graf. Preto sa metÃ³da trÃ©ningu viacvrstvovÃ©ho perceptronu nazÃ½va **spÃ¤tnÃ¡ propagÃ¡cia**, alebo 'backprop'.

<img alt="vÃ½poÄtovÃ½ graf" src="../../../../../translated_images/sk/ComputeGraphGrad.4626252c0de03507.webp"/>

> TODO: citÃ¡cia obrÃ¡zku

> âœ… SpÃ¤tnÃº propagÃ¡ciu pokryjeme oveÄ¾a podrobnejÅ¡ie v naÅ¡om prÃ­klade v notebooku.  

## ZÃ¡ver

V tejto lekcii sme si vytvorili vlastnÃº kniÅ¾nicu neurÃ³novÃ½ch sietÃ­ a pouÅ¾ili sme ju na jednoduchÃº dvojrozmernÃº klasifikaÄnÃº Ãºlohu.

## ğŸš€ VÃ½zva

V priloÅ¾enom notebooku implementujete vlastnÃ½ rÃ¡mec na vytvÃ¡ranie a trÃ©ning viacvrstvovÃ½ch perceptronov. Budete mÃ´cÅ¥ detailne vidieÅ¥, ako modernÃ© neurÃ³novÃ© siete fungujÃº.

PokraÄujte do notebooku [OwnFramework](OwnFramework.ipynb) a prejdite si ho.

## [KvÃ­z po prednÃ¡Å¡ke](https://ff-quizzes.netlify.app/en/ai/quiz/8)

## PrehÄ¾ad a samostatnÃ© Å¡tÃºdium

SpÃ¤tnÃ¡ propagÃ¡cia je beÅ¾nÃ½ algoritmus pouÅ¾Ã­vanÃ½ v AI a ML, stojÃ­ za to ju [Å¡tudovaÅ¥ podrobnejÅ¡ie](https://wikipedia.org/wiki/Backpropagation).

## [Ãšloha](lab/README.md)

V tomto laboratÃ³riu sa od vÃ¡s poÅ¾aduje pouÅ¾iÅ¥ rÃ¡mec, ktorÃ½ ste si vytvorili v tejto lekcii, na rieÅ¡enie klasifikÃ¡cie ruÄne pÃ­sanÃ½ch ÄÃ­slic MNIST.

* [Pokyny](lab/README.md)
* [Notebook](lab/MyFW_MNIST.ipynb)

---

