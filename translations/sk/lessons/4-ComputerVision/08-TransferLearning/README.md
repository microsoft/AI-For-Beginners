# PredtrÃ©novanÃ© siete a transferovÃ© uÄenie

TrÃ©novanie CNN mÃ´Å¾e zabraÅ¥ veÄ¾a Äasu a vyÅ¾aduje si veÄ¾kÃ© mnoÅ¾stvo dÃ¡t. AvÅ¡ak veÄ¾kÃ¡ ÄasÅ¥ Äasu sa venuje uÄeniu najlepÅ¡Ã­ch nÃ­zkoÃºrovÅˆovÃ½ch filtrov, ktorÃ© sieÅ¥ mÃ´Å¾e pouÅ¾iÅ¥ na extrahovanie vzorov z obrÃ¡zkov. Prirodzene sa vynÃ¡ra otÃ¡zka - mÃ´Å¾eme pouÅ¾iÅ¥ neurÃ³novÃº sieÅ¥ trÃ©novanÃº na jednom datasete a prispÃ´sobiÅ¥ ju na klasifikÃ¡ciu inÃ½ch obrÃ¡zkov bez potreby kompletnÃ©ho procesu trÃ©novania?

## [KvÃ­z pred prednÃ¡Å¡kou](https://ff-quizzes.netlify.app/en/ai/quiz/15)

Tento prÃ­stup sa nazÃ½va **transferovÃ© uÄenie**, pretoÅ¾e prenÃ¡Å¡ame urÄitÃº znalosÅ¥ z jednÃ©ho modelu neurÃ³novej siete na druhÃ½. Pri transferovom uÄenÃ­ zvyÄajne zaÄÃ­name s predtrÃ©novanÃ½m modelom, ktorÃ½ bol trÃ©novanÃ½ na veÄ¾kom datasete obrÃ¡zkov, ako je naprÃ­klad **ImageNet**. Tieto modely uÅ¾ dokÃ¡Å¾u dobre extrahovaÅ¥ rÃ´zne Ärty z generickÃ½ch obrÃ¡zkov, a v mnohÃ½ch prÃ­padoch staÄÃ­ postaviÅ¥ klasifikÃ¡tor na vrchole tÃ½chto extrahovanÃ½ch ÄÅ•t, aby sme dosiahli dobrÃ½ vÃ½sledok.

> âœ… TransferovÃ© uÄenie je termÃ­n, ktorÃ½ nÃ¡jdete aj v inÃ½ch akademickÃ½ch oblastiach, ako je vzdelÃ¡vanie. OznaÄuje proces prenÃ¡Å¡ania znalostÃ­ z jednej oblasti do druhej.

## PredtrÃ©novanÃ© modely ako extraktory ÄÅ•t

KonvoluÄnÃ© siete, o ktorÃ½ch sme hovorili v predchÃ¡dzajÃºcej sekcii, obsahovali mnoÅ¾stvo vrstiev, z ktorÃ½ch kaÅ¾dÃ¡ mala za Ãºlohu extrahovaÅ¥ urÄitÃ© Ärty z obrÃ¡zku, od nÃ­zkoÃºrovÅˆovÃ½ch kombinÃ¡ciÃ­ pixelov (ako horizontÃ¡lna/vertikÃ¡lna lÃ­nia alebo Å¥ah) aÅ¾ po vyÅ¡Å¡ie Ãºrovne kombinÃ¡ciÃ­ ÄÅ•t, ktorÃ© zodpovedajÃº veciam ako oko alebo plameÅˆ. Ak trÃ©nujeme CNN na dostatoÄne veÄ¾kom datasete generickÃ½ch a rÃ´znorodÃ½ch obrÃ¡zkov, sieÅ¥ by sa mala nauÄiÅ¥ extrahovaÅ¥ tieto spoloÄnÃ© Ärty.

Keras aj PyTorch obsahujÃº funkcie na jednoduchÃ© naÄÃ­tanie predtrÃ©novanÃ½ch vÃ¡h neurÃ³novÃ½ch sietÃ­ pre niektorÃ© beÅ¾nÃ© architektÃºry, z ktorÃ½ch vÃ¤ÄÅ¡ina bola trÃ©novanÃ¡ na obrÃ¡zkoch z ImageNet. NajÄastejÅ¡ie pouÅ¾Ã­vanÃ© sÃº popÃ­sanÃ© na strÃ¡nke [ArchitektÃºry CNN](../07-ConvNets/CNN_Architectures.md) z predchÃ¡dzajÃºcej lekcie. KonkrÃ©tne mÃ´Å¾ete zvÃ¡Å¾iÅ¥ pouÅ¾itie jednej z nasledujÃºcich:

* **VGG-16/VGG-19**, ktorÃ© sÃº relatÃ­vne jednoduchÃ© modely, ale stÃ¡le poskytujÃº dobrÃº presnosÅ¥. PouÅ¾itie VGG ako prvÃ½ pokus je Äasto dobrÃ¡ voÄ¾ba na zistenie, ako transferovÃ© uÄenie funguje.
* **ResNet** je rodina modelov navrhnutÃ½ch Microsoft Research v roku 2015. MajÃº viac vrstiev, a teda vyÅ¾adujÃº viac zdrojov.
* **MobileNet** je rodina modelov so zmenÅ¡enou veÄ¾kosÅ¥ou, vhodnÃ¡ pre mobilnÃ© zariadenia. PouÅ¾ite ich, ak mÃ¡te obmedzenÃ© zdroje a mÃ´Å¾ete obetovaÅ¥ trochu presnosti.

Tu sÃº ukÃ¡Å¾kovÃ© Ärty extrahovanÃ© z obrÃ¡zku maÄky pomocou siete VGG-16:

![ÄŒrty extrahovanÃ© VGG-16](../../../../../translated_images/sk/features.6291f9c7ba3a0b95.webp)

## Dataset MaÄky vs. Psy

V tomto prÃ­klade pouÅ¾ijeme dataset [MaÄky a Psy](https://www.microsoft.com/download/details.aspx?id=54765&WT.mc_id=academic-77998-cacaste), ktorÃ½ je veÄ¾mi blÃ­zky reÃ¡lnemu scenÃ¡ru klasifikÃ¡cie obrÃ¡zkov.

## âœï¸ CviÄenie: TransferovÃ© uÄenie

Pozrime sa na transferovÃ© uÄenie v praxi v prÃ­sluÅ¡nÃ½ch notebookoch:

* [TransferovÃ© uÄenie - PyTorch](TransferLearningPyTorch.ipynb)
* [TransferovÃ© uÄenie - TensorFlow](TransferLearningTF.ipynb)

## VizualizÃ¡cia ideÃ¡lnej maÄky

PredtrÃ©novanÃ¡ neurÃ³novÃ¡ sieÅ¥ obsahuje rÃ´zne vzory vo svojej *pamÃ¤ti*, vrÃ¡tane predstÃ¡v o **ideÃ¡lnej maÄke** (ako aj ideÃ¡lnom psovi, ideÃ¡lnej zebre, atÄ.). Bolo by zaujÃ­mavÃ© nejako **vizualizovaÅ¥ tento obrÃ¡zok**. AvÅ¡ak nie je to jednoduchÃ©, pretoÅ¾e vzory sÃº rozptÃ½lenÃ© po celÃ½ch vÃ¡hach siete a tieÅ¾ organizovanÃ© v hierarchickej Å¡truktÃºre.

Jeden prÃ­stup, ktorÃ½ mÃ´Å¾eme pouÅ¾iÅ¥, je zaÄaÅ¥ s nÃ¡hodnÃ½m obrÃ¡zkom a potom sa pokÃºsiÅ¥ pouÅ¾iÅ¥ techniku **optimalizÃ¡cie pomocou gradientnÃ©ho zostupu**, aby sme upravili tento obrÃ¡zok tak, Å¾e sieÅ¥ zaÄne myslieÅ¥, Å¾e je to maÄka.

![OptimalizaÄnÃ½ cyklus obrÃ¡zku](../../../../../translated_images/sk/ideal-cat-loop.999fbb8ff306e044.webp)

Ak to vÅ¡ak urobÃ­me, dostaneme nieÄo veÄ¾mi podobnÃ© nÃ¡hodnÃ©mu Å¡umu. Je to preto, Å¾e *existuje mnoho spÃ´sobov, ako presvedÄiÅ¥ sieÅ¥, Å¾e vstupnÃ½ obrÃ¡zok je maÄka*, vrÃ¡tane niektorÃ½ch, ktorÃ© vizuÃ¡lne nedÃ¡vajÃº zmysel. Hoci tieto obrÃ¡zky obsahujÃº veÄ¾a vzorov typickÃ½ch pre maÄku, niÄ ich neobmedzuje, aby boli vizuÃ¡lne zreteÄ¾nÃ©.

Na zlepÅ¡enie vÃ½sledku mÃ´Å¾eme do funkcie straty pridaÅ¥ ÄalÅ¡Ã­ Älen, ktorÃ½ sa nazÃ½va **variÃ¡cia straty**. Je to metrika, ktorÃ¡ ukazuje, ako podobnÃ© sÃº susednÃ© pixely obrÃ¡zku. MinimalizÃ¡cia variÃ¡cie straty robÃ­ obrÃ¡zok hladÅ¡Ã­m a zbavuje sa Å¡umu - ÄÃ­m odhaÄ¾uje vizuÃ¡lne prÃ­Å¥aÅ¾livejÅ¡ie vzory. Tu je prÃ­klad takÃ½chto "ideÃ¡lnych" obrÃ¡zkov, ktorÃ© sÃº klasifikovanÃ© ako maÄka a zebra s vysokou pravdepodobnosÅ¥ou:

![IdeÃ¡lna maÄka](../../../../../translated_images/sk/ideal-cat.203dd4597643d6b0.webp) | ![IdeÃ¡lna zebra](../../../../../translated_images/sk/ideal-zebra.7f70e8b54ee15a7a.webp)
-----|-----
 *IdeÃ¡lna maÄka* | *IdeÃ¡lna zebra*

PodobnÃ½ prÃ­stup mÃ´Å¾e byÅ¥ pouÅ¾itÃ½ na vykonanie tzv. **adversariÃ¡lnych Ãºtokov** na neurÃ³novÃº sieÅ¥. Predpokladajme, Å¾e chceme oklamaÅ¥ neurÃ³novÃº sieÅ¥ a urobiÅ¥ z psa maÄku. Ak vezmeme obrÃ¡zok psa, ktorÃ½ je sieÅ¥ou rozpoznanÃ½ ako pes, mÃ´Å¾eme ho trochu upraviÅ¥ pomocou optimalizÃ¡cie gradientnÃ©ho zostupu, aÅ¾ kÃ½m sieÅ¥ nezaÄne klasifikovaÅ¥ obrÃ¡zok ako maÄku:

![ObrÃ¡zok psa](../../../../../translated_images/sk/original-dog.8f68a67d2fe0911f.webp) | ![ObrÃ¡zok psa klasifikovanÃ½ ako maÄka](../../../../../translated_images/sk/adversarial-dog.d9fc7773b0142b89.webp)
-----|-----
*PÃ´vodnÃ½ obrÃ¡zok psa* | *ObrÃ¡zok psa klasifikovanÃ½ ako maÄka*

Pozrite si kÃ³d na reprodukciu vyÅ¡Å¡ie uvedenÃ½ch vÃ½sledkov v nasledujÃºcom notebooku:

* [IdeÃ¡lna a adversariÃ¡lna maÄka - TensorFlow](AdversarialCat_TF.ipynb)

## ZÃ¡ver

Pomocou transferovÃ©ho uÄenia mÃ´Å¾ete rÃ½chlo zostaviÅ¥ klasifikÃ¡tor pre Ãºlohu klasifikÃ¡cie vlastnÃ½ch objektov a dosiahnuÅ¥ vysokÃº presnosÅ¥. VidÃ­te, Å¾e zloÅ¾itejÅ¡ie Ãºlohy, ktorÃ© teraz rieÅ¡ime, vyÅ¾adujÃº vyÅ¡Å¡Ã­ vÃ½poÄtovÃ½ vÃ½kon a nemÃ´Å¾u byÅ¥ Ä¾ahko vyrieÅ¡enÃ© na CPU. V ÄalÅ¡ej jednotke sa pokÃºsime pouÅ¾iÅ¥ Ä¾ahÅ¡iu implementÃ¡ciu na trÃ©novanie rovnakÃ©ho modelu s niÅ¾Å¡Ã­mi vÃ½poÄtovÃ½mi zdrojmi, Äo vedie len k mierne niÅ¾Å¡ej presnosti.

## ğŸš€ VÃ½zva

V sprievodnÃ½ch notebookoch sÃº poznÃ¡mky na konci o tom, ako transferovÃ© uÄenie najlepÅ¡ie funguje s podobnÃ½mi trÃ©novacÃ­mi dÃ¡tami (naprÃ­klad novÃ½ typ zvieraÅ¥a). Urobte experimenty s Ãºplne novÃ½mi typmi obrÃ¡zkov, aby ste zistili, ako dobre alebo zle vaÅ¡e modely transferovÃ©ho uÄenia fungujÃº.

## [KvÃ­z po prednÃ¡Å¡ke](https://ff-quizzes.netlify.app/en/ai/quiz/16)

## PrehÄ¾ad a samostatnÃ© Å¡tÃºdium

PreÄÃ­tajte si [TrainingTricks.md](TrainingTricks.md), aby ste si prehÄºbili znalosti o ÄalÅ¡Ã­ch spÃ´soboch trÃ©novania modelov.

## [Ãšloha](lab/README.md)

V tomto laboratÃ³riu pouÅ¾ijeme reÃ¡lny dataset [Oxford-IIIT](https://www.robots.ox.ac.uk/~vgg/data/pets/) domÃ¡cich milÃ¡Äikov s 35 plemenami maÄiek a psov a vytvorÃ­me klasifikÃ¡tor pomocou transferovÃ©ho uÄenia.

---

