<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "717775c4050ccbffbe0c961ad8bf7bf7",
  "translation_date": "2025-08-25T23:07:14+00:00",
  "source_file": "lessons/4-ComputerVision/08-TransferLearning/README.md",
  "language_code": "sk"
}
-->
# PredtrÃ©novanÃ© siete a transferovÃ© uÄenie

TrÃ©novanie CNN mÃ´Å¾e zabraÅ¥ veÄ¾a Äasu a vyÅ¾aduje veÄ¾kÃ© mnoÅ¾stvo dÃ¡t. AvÅ¡ak veÄ¾kÃ¡ ÄasÅ¥ Äasu sa venuje uÄeniu najlepÅ¡Ã­ch nÃ­zkoÃºrovÅˆovÃ½ch filtrov, ktorÃ© sieÅ¥ mÃ´Å¾e pouÅ¾iÅ¥ na extrahovanie vzorov z obrÃ¡zkov. Prirodzene sa vynÃ¡ra otÃ¡zka - mÃ´Å¾eme pouÅ¾iÅ¥ neurÃ³novÃº sieÅ¥ natrÃ©novanÃº na jednom datasete a prispÃ´sobiÅ¥ ju na klasifikÃ¡ciu inÃ½ch obrÃ¡zkov bez potreby kompletnÃ©ho procesu trÃ©novania?

## [Pre-lecture quiz](https://ff-quizzes.netlify.app/en/ai/quiz/15)

Tento prÃ­stup sa nazÃ½va **transferovÃ© uÄenie**, pretoÅ¾e prenÃ¡Å¡ame urÄitÃº znalosÅ¥ z jednÃ©ho modelu neurÃ³novej siete na druhÃ½. Pri transferovom uÄenÃ­ zvyÄajne zaÄÃ­name s predtrÃ©novanÃ½m modelom, ktorÃ½ bol natrÃ©novanÃ½ na veÄ¾kom datasete obrÃ¡zkov, ako je naprÃ­klad **ImageNet**. Tieto modely uÅ¾ dokÃ¡Å¾u dobre extrahovaÅ¥ rÃ´zne vlastnosti z generickÃ½ch obrÃ¡zkov, a v mnohÃ½ch prÃ­padoch staÄÃ­ postaviÅ¥ klasifikÃ¡tor na vrchole tÃ½chto extrahovanÃ½ch vlastnostÃ­, aby sme dosiahli dobrÃ½ vÃ½sledok.

> âœ… TransferovÃ© uÄenie je termÃ­n, ktorÃ½ nÃ¡jdete aj v inÃ½ch akademickÃ½ch oblastiach, ako je vzdelÃ¡vanie. OznaÄuje proces prenÃ¡Å¡ania znalostÃ­ z jednej oblasti do druhej.

## PredtrÃ©novanÃ© modely ako extraktory vlastnostÃ­

KonvoluÄnÃ© siete, o ktorÃ½ch sme hovorili v predchÃ¡dzajÃºcej sekcii, obsahovali mnoÅ¾stvo vrstiev, z ktorÃ½ch kaÅ¾dÃ¡ mÃ¡ za Ãºlohu extrahovaÅ¥ urÄitÃ© vlastnosti z obrÃ¡zku, od nÃ­zkoÃºrovÅˆovÃ½ch kombinÃ¡ciÃ­ pixelov (ako horizontÃ¡lne/vertikÃ¡lne Äiary alebo Å¥ahy), aÅ¾ po vyÅ¡Å¡ie Ãºrovne kombinÃ¡ciÃ­ vlastnostÃ­, ktorÃ© zodpovedajÃº veciam ako oko alebo plameÅˆ. Ak trÃ©nujeme CNN na dostatoÄne veÄ¾kom datasete generickÃ½ch a rÃ´znorodÃ½ch obrÃ¡zkov, sieÅ¥ by sa mala nauÄiÅ¥ extrahovaÅ¥ tieto spoloÄnÃ© vlastnosti.

Keras aj PyTorch obsahujÃº funkcie na jednoduchÃ© naÄÃ­tanie predtrÃ©novanÃ½ch vÃ¡h neurÃ³novÃ½ch sietÃ­ pre niektorÃ© beÅ¾nÃ© architektÃºry, z ktorÃ½ch vÃ¤ÄÅ¡ina bola trÃ©novanÃ¡ na obrÃ¡zkoch z ImageNet. NajÄastejÅ¡ie pouÅ¾Ã­vanÃ© sÃº popÃ­sanÃ© na strÃ¡nke [CNN ArchitektÃºry](../07-ConvNets/CNN_Architectures.md) z predchÃ¡dzajÃºcej lekcie. KonkrÃ©tne mÃ´Å¾ete zvÃ¡Å¾iÅ¥ pouÅ¾itie jednej z nasledujÃºcich:

* **VGG-16/VGG-19**, ktorÃ© sÃº relatÃ­vne jednoduchÃ© modely, ale stÃ¡le poskytujÃº dobrÃº presnosÅ¥. PouÅ¾itie VGG ako prvÃ½ pokus je Äasto dobrÃ¡ voÄ¾ba na zistenie, ako transferovÃ© uÄenie funguje.
* **ResNet** je rodina modelov navrhnutÃ½ch Microsoft Research v roku 2015. MajÃº viac vrstiev, a preto vyÅ¾adujÃº viac zdrojov.
* **MobileNet** je rodina modelov s redukovanou veÄ¾kosÅ¥ou, vhodnÃ¡ pre mobilnÃ© zariadenia. PouÅ¾ite ich, ak mÃ¡te obmedzenÃ© zdroje a mÃ´Å¾ete obetovaÅ¥ trochu presnosti.

Tu sÃº ukÃ¡Å¾kovÃ© vlastnosti extrahovanÃ© z obrÃ¡zku maÄky pomocou siete VGG-16:

![Vlastnosti extrahovanÃ© VGG-16](../../../../../translated_images/features.6291f9c7ba3a0b951af88fc9864632b9115365410765680680d30c927dd67354.sk.png)

## Dataset MaÄky vs. Psy

V tomto prÃ­klade pouÅ¾ijeme dataset [MaÄky a Psy](https://www.microsoft.com/download/details.aspx?id=54765&WT.mc_id=academic-77998-cacaste), ktorÃ½ je veÄ¾mi blÃ­zky reÃ¡lnemu scenÃ¡ru klasifikÃ¡cie obrÃ¡zkov.

## âœï¸ CviÄenie: TransferovÃ© uÄenie

Pozrime sa na transferovÃ© uÄenie v praxi v prÃ­sluÅ¡nÃ½ch notebookoch:

* [TransferovÃ© uÄenie - PyTorch](../../../../../lessons/4-ComputerVision/08-TransferLearning/TransferLearningPyTorch.ipynb)
* [TransferovÃ© uÄenie - TensorFlow](../../../../../lessons/4-ComputerVision/08-TransferLearning/TransferLearningTF.ipynb)

## VizualizÃ¡cia AdversariÃ¡lnej MaÄky

PredtrÃ©novanÃ¡ neurÃ³novÃ¡ sieÅ¥ obsahuje rÃ´zne vzory vo svojej *pamÃ¤ti*, vrÃ¡tane predstÃ¡v o **ideÃ¡lnej maÄke** (ako aj ideÃ¡lnom psovi, ideÃ¡lnej zebre, atÄ.). Bolo by zaujÃ­mavÃ© nejako **vizualizovaÅ¥ tento obrÃ¡zok**. AvÅ¡ak nie je to jednoduchÃ©, pretoÅ¾e vzory sÃº rozptÃ½lenÃ© po celej vÃ¡he siete a tieÅ¾ organizovanÃ© v hierarchickej Å¡truktÃºre.

JednÃ½m prÃ­stupom, ktorÃ½ mÃ´Å¾eme pouÅ¾iÅ¥, je zaÄaÅ¥ s nÃ¡hodnÃ½m obrÃ¡zkom a potom sa pokÃºsiÅ¥ pouÅ¾iÅ¥ techniku **gradientnÃ©ho zostupu** na Ãºpravu tohto obrÃ¡zku tak, aby si sieÅ¥ zaÄala myslieÅ¥, Å¾e je to maÄka.

![OptimalizaÄnÃ¡ sluÄka obrÃ¡zku](../../../../../translated_images/ideal-cat-loop.999fbb8ff306e044f997032f4eef9152b453e6a990e449bbfb107de2493cc37e.sk.png)

Ak to vÅ¡ak urobÃ­me, dostaneme nieÄo veÄ¾mi podobnÃ© nÃ¡hodnÃ©mu Å¡umu. Je to preto, Å¾e *existuje mnoho spÃ´sobov, ako presvedÄiÅ¥ sieÅ¥, Å¾e vstupnÃ½ obrÃ¡zok je maÄka*, vrÃ¡tane niektorÃ½ch, ktorÃ© vizuÃ¡lne nedÃ¡vajÃº zmysel. ZatiaÄ¾ Äo tieto obrÃ¡zky obsahujÃº veÄ¾a vzorov typickÃ½ch pre maÄku, niÄ ich neobmedzuje, aby boli vizuÃ¡lne rozpoznateÄ¾nÃ©.

Na zlepÅ¡enie vÃ½sledku mÃ´Å¾eme pridaÅ¥ ÄalÅ¡Ã­ Älen do stratovej funkcie, ktorÃ½ sa nazÃ½va **variÃ¡cia strata**. Je to metrika, ktorÃ¡ ukazuje, ako podobnÃ© sÃº susednÃ© pixely obrÃ¡zku. MinimalizÃ¡cia variÃ¡cie straty robÃ­ obrÃ¡zok hladÅ¡Ã­m a zbavuje sa Å¡umu - ÄÃ­m odhaÄ¾uje vizuÃ¡lne prÃ­Å¥aÅ¾livejÅ¡ie vzory. Tu je prÃ­klad takÃ½chto "ideÃ¡lnych" obrÃ¡zkov, ktorÃ© sÃº klasifikovanÃ© ako maÄka a zebra s vysokou pravdepodobnosÅ¥ou:

![IdeÃ¡lna MaÄka](../../../../../translated_images/ideal-cat.203dd4597643d6b0bd73038b87f9c0464322725e3a06ab145d25d4a861c70592.sk.png) | ![IdeÃ¡lna Zebra](../../../../../translated_images/ideal-zebra.7f70e8b54ee15a7a314000bb5df38a6cfe086ea04d60df4d3ef313d046b98a2b.sk.png)
-----|-----
 *IdeÃ¡lna MaÄka* | *IdeÃ¡lna Zebra*

PodobnÃ½ prÃ­stup mÃ´Å¾e byÅ¥ pouÅ¾itÃ½ na vykonanie tzv. **adversariÃ¡lnych Ãºtokov** na neurÃ³novÃº sieÅ¥. Predpokladajme, Å¾e chceme oklamaÅ¥ neurÃ³novÃº sieÅ¥ a urobiÅ¥ z psa maÄku. Ak vezmeme obrÃ¡zok psa, ktorÃ½ je sieÅ¥ou rozpoznanÃ½ ako pes, mÃ´Å¾eme ho trochu upraviÅ¥ pomocou gradientnÃ©ho zostupu, aÅ¾ kÃ½m ho sieÅ¥ nezaÄne klasifikovaÅ¥ ako maÄku:

![ObrÃ¡zok psa](../../../../../translated_images/original-dog.8f68a67d2fe0911f33041c0f7fce8aa4ea919f9d3917ec4b468298522aeb6356.sk.png) | ![ObrÃ¡zok psa klasifikovanÃ½ ako maÄka](../../../../../translated_images/adversarial-dog.d9fc7773b0142b89752539bfbf884118de845b3851c5162146ea0b8809fc820f.sk.png)
-----|-----
*PÃ´vodnÃ½ obrÃ¡zok psa* | *ObrÃ¡zok psa klasifikovanÃ½ ako maÄka*

Pozrite si kÃ³d na reprodukciu vyÅ¡Å¡ie uvedenÃ½ch vÃ½sledkov v nasledujÃºcom notebooku:

* [IdeÃ¡lna a AdversariÃ¡lna MaÄka - TensorFlow](../../../../../lessons/4-ComputerVision/08-TransferLearning/AdversarialCat_TF.ipynb)

## ZÃ¡ver

Pomocou transferovÃ©ho uÄenia mÃ´Å¾ete rÃ½chlo zostaviÅ¥ klasifikÃ¡tor pre Ãºlohu klasifikÃ¡cie vlastnÃ½ch objektov a dosiahnuÅ¥ vysokÃº presnosÅ¥. VidÃ­te, Å¾e zloÅ¾itejÅ¡ie Ãºlohy, ktorÃ© teraz rieÅ¡ime, vyÅ¾adujÃº vyÅ¡Å¡Ã­ vÃ½poÄtovÃ½ vÃ½kon a nemÃ´Å¾u byÅ¥ Ä¾ahko vyrieÅ¡enÃ© na CPU. V ÄalÅ¡ej jednotke sa pokÃºsime pouÅ¾iÅ¥ Ä¾ahÅ¡iu implementÃ¡ciu na trÃ©novanie rovnakÃ©ho modelu s niÅ¾Å¡Ã­mi vÃ½poÄtovÃ½mi zdrojmi, Äo vedie len k mierne niÅ¾Å¡ej presnosti.

## ğŸš€ VÃ½zva

V sprievodnÃ½ch notebookoch sÃº poznÃ¡mky na spodku o tom, ako transferovÃ© znalosti najlepÅ¡ie fungujÃº s trochu podobnÃ½mi trÃ©novacÃ­mi dÃ¡tami (naprÃ­klad novÃ½ typ zvieraÅ¥a). Urobte experimenty s Ãºplne novÃ½mi typmi obrÃ¡zkov, aby ste zistili, ako dobre alebo zle vaÅ¡e modely transferovÃ½ch znalostÃ­ fungujÃº.

## [Post-lecture quiz](https://ff-quizzes.netlify.app/en/ai/quiz/16)

## PrehÄ¾ad & SamoÅ¡tÃºdium

PreÄÃ­tajte si [TrainingTricks.md](TrainingTricks.md), aby ste si prehÄºbili znalosti o ÄalÅ¡Ã­ch spÃ´soboch trÃ©novania vaÅ¡ich modelov.

## [Ãšloha](lab/README.md)

V tomto laboratÃ³riu pouÅ¾ijeme reÃ¡lny dataset [Oxford-IIIT](https://www.robots.ox.ac.uk/~vgg/data/pets/) domÃ¡cich milÃ¡Äikov s 35 plemenami maÄiek a psov a vytvorÃ­me klasifikÃ¡tor pomocou transferovÃ©ho uÄenia.

**Upozornenie**:  
Tento dokument bol preloÅ¾enÃ½ pomocou sluÅ¾by AI prekladu [Co-op Translator](https://github.com/Azure/co-op-translator). Aj keÄ sa snaÅ¾Ã­me o presnosÅ¥, prosÃ­m, berte na vedomie, Å¾e automatizovanÃ© preklady mÃ´Å¾u obsahovaÅ¥ chyby alebo nepresnosti. PÃ´vodnÃ½ dokument v jeho rodnom jazyku by mal byÅ¥ povaÅ¾ovanÃ½ za autoritatÃ­vny zdroj. Pre kritickÃ© informÃ¡cie sa odporÃºÄa profesionÃ¡lny Ä¾udskÃ½ preklad. Nie sme zodpovednÃ­ za akÃ©koÄ¾vek nedorozumenia alebo nesprÃ¡vne interpretÃ¡cie vyplÃ½vajÃºce z pouÅ¾itia tohto prekladu.