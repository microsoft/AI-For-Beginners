# GeneratÃ­vne AdverzÃ¡rne Siete

V predchÃ¡dzajÃºcej sekcii sme sa nauÄili o **generatÃ­vnych modeloch**: modeloch, ktorÃ© dokÃ¡Å¾u generovaÅ¥ novÃ© obrÃ¡zky podobnÃ© tÃ½m v trÃ©ningovej mnoÅ¾ine. VAE bol dobrÃ½m prÃ­kladom generatÃ­vneho modelu.

## [KvÃ­z pred prednÃ¡Å¡kou](https://ff-quizzes.netlify.app/en/ai/quiz/19)

Ak sa vÅ¡ak pokÃºsime generovaÅ¥ nieÄo naozaj zmysluplnÃ©, naprÃ­klad maÄ¾bu v rozumnej kvalite, pomocou VAE, zistÃ­me, Å¾e trÃ©ning neprebieha dobre. Pre tento prÃ­pad pouÅ¾itia by sme sa mali nauÄiÅ¥ o inej architektÃºre Å¡pecificky zameranej na generatÃ­vne modely - **GeneratÃ­vne AdverzÃ¡rne Siete**, alebo GANs.

HlavnÃ¡ myÅ¡lienka GAN je maÅ¥ dve neurÃ³novÃ© siete, ktorÃ© sa budÃº trÃ©novaÅ¥ proti sebe:

<img src="../../../../../translated_images/sk/gan_architecture.8f3a5ab62b8d5d69.webp" width="70%"/>

> ObrÃ¡zok od [Dmitry Soshnikov](http://soshnikov.com)

> âœ… MalÃ½ slovnÃ­k:
> * **GenerÃ¡tor** je sieÅ¥, ktorÃ¡ prijÃ­ma nÃ¡hodnÃ½ vektor a ako vÃ½sledok produkuje obrÃ¡zok.
> * **DiskriminÃ¡tor** je sieÅ¥, ktorÃ¡ prijÃ­ma obrÃ¡zok a mÃ¡ urÄiÅ¥, Äi ide o skutoÄnÃ½ obrÃ¡zok (z trÃ©ningovej mnoÅ¾iny), alebo bol generovanÃ½ generÃ¡torom. V podstate ide o klasifikÃ¡tor obrÃ¡zkov.

### DiskriminÃ¡tor

ArchitektÃºra diskriminÃ¡tora sa nelÃ­Å¡i od beÅ¾nej siete na klasifikÃ¡ciu obrÃ¡zkov. V najjednoduchÅ¡om prÃ­pade mÃ´Å¾e Ã­sÅ¥ o plne prepojenÃ½ klasifikÃ¡tor, ale pravdepodobne to bude [konvoluÄnÃ¡ sieÅ¥](../07-ConvNets/README.md).

> âœ… GAN zaloÅ¾enÃ½ na konvoluÄnÃ½ch sieÅ¥ach sa nazÃ½va [DCGAN](https://arxiv.org/pdf/1511.06434.pdf)

DiskriminÃ¡tor CNN pozostÃ¡va z nasledujÃºcich vrstiev: niekoÄ¾ko konvolÃºciÃ­+poolingov (s klesajÃºcou priestorovou veÄ¾kosÅ¥ou) a jednej alebo viacerÃ½ch plne prepojenÃ½ch vrstiev na zÃ­skanie "vektora vlastnostÃ­", finÃ¡lneho binÃ¡rneho klasifikÃ¡tora.

> âœ… 'Pooling' v tomto kontexte je technika, ktorÃ¡ zmenÅ¡uje veÄ¾kosÅ¥ obrÃ¡zku. "PoolingovÃ© vrstvy zniÅ¾ujÃº rozmery dÃ¡t kombinovanÃ­m vÃ½stupov klastrov neurÃ³nov na jednej vrstve do jednÃ©ho neurÃ³nu na nasledujÃºcej vrstve." - [zdroj](https://wikipedia.org/wiki/Convolutional_neural_network#Pooling_layers)

### GenerÃ¡tor

GenerÃ¡tor je o nieÄo zloÅ¾itejÅ¡Ã­. MÃ´Å¾ete ho povaÅ¾ovaÅ¥ za obrÃ¡tenÃ½ diskriminÃ¡tor. ZaÄÃ­na od latentnÃ©ho vektora (namiesto vektora vlastnostÃ­), mÃ¡ plne prepojenÃº vrstvu na jeho konverziu do poÅ¾adovanej veÄ¾kosti/tvaru, nasledovanÃº dekonvolÃºciami+zvÃ¤ÄÅ¡ovanÃ­m. Toto je podobnÃ© *dekodÃ©ru* v [autoenkÃ³deri](../09-Autoencoders/README.md).

> âœ… KeÄÅ¾e konvoluÄnÃ¡ vrstva je implementovanÃ¡ ako lineÃ¡rny filter prechÃ¡dzajÃºci obrÃ¡zkom, dekonvolÃºcia je v podstate podobnÃ¡ konvolÃºcii a mÃ´Å¾e byÅ¥ implementovanÃ¡ pomocou rovnakej logiky vrstvy.

<img src="../../../../../translated_images/sk/gan_arch_detail.46b95fd366f8e543.webp" width="70%"/>

> ObrÃ¡zok od [Dmitry Soshnikov](http://soshnikov.com)

### TrÃ©ning GAN

GAN sa nazÃ½vajÃº **adverzÃ¡rne**, pretoÅ¾e medzi generÃ¡torom a diskriminÃ¡torom prebieha neustÃ¡la sÃºÅ¥aÅ¾. PoÄas tejto sÃºÅ¥aÅ¾e sa obidva generÃ¡tor a diskriminÃ¡tor zlepÅ¡ujÃº, ÄÃ­m sa sieÅ¥ uÄÃ­ produkovaÅ¥ stÃ¡le lepÅ¡ie obrÃ¡zky.

TrÃ©ning prebieha v dvoch fÃ¡zach:

* **TrÃ©ning diskriminÃ¡tora**. TÃ¡to Ãºloha je pomerne jednoduchÃ¡: generujeme dÃ¡vku obrÃ¡zkov pomocou generÃ¡tora, oznaÄÃ­me ich 0, Äo znamenÃ¡ faloÅ¡nÃ½ obrÃ¡zok, a vezmeme dÃ¡vku obrÃ¡zkov z vstupnej mnoÅ¾iny (s oznaÄenÃ­m 1, skutoÄnÃ½ obrÃ¡zok). ZÃ­skame nejakÃº *stratu diskriminÃ¡tora* a vykonÃ¡me spÃ¤tnÃ© Å¡Ã­renie.
* **TrÃ©ning generÃ¡tora**. Toto je o nieÄo zloÅ¾itejÅ¡ie, pretoÅ¾e nepoznÃ¡me oÄakÃ¡vanÃ½ vÃ½stup generÃ¡tora priamo. Vezmeme celÃº GAN sieÅ¥ pozostÃ¡vajÃºcu z generÃ¡tora nasledovanÃ©ho diskriminÃ¡torom, nakÅ•mime ju nÃ¡hodnÃ½mi vektormi a oÄakÃ¡vame vÃ½sledok 1 (zodpovedajÃºci skutoÄnÃ½m obrÃ¡zkom). Potom zmrazÃ­me parametre diskriminÃ¡tora (nechceme, aby sa v tomto kroku trÃ©noval) a vykonÃ¡me spÃ¤tnÃ© Å¡Ã­renie.

PoÄas tohto procesu straty generÃ¡tora aj diskriminÃ¡tora vÃ½razne neklesajÃº. V ideÃ¡lnej situÃ¡cii by mali oscilovaÅ¥, Äo zodpovedÃ¡ zlepÅ¡ovaniu vÃ½konu oboch sietÃ­.

## âœï¸ CviÄenia: GANs

* [GAN Notebook v TensorFlow/Keras](GANTF.ipynb)
* [GAN Notebook v PyTorch](GANPyTorch.ipynb)

### ProblÃ©my s trÃ©ningom GAN

GAN sÃº znÃ¡me tÃ½m, Å¾e sÃº obzvlÃ¡Å¡Å¥ nÃ¡roÄnÃ© na trÃ©ning. Tu je niekoÄ¾ko problÃ©mov:

* **Kolaps mÃ³du**. Tento termÃ­n znamenÃ¡, Å¾e generÃ¡tor sa nauÄÃ­ produkovaÅ¥ jeden ÃºspeÅ¡nÃ½ obrÃ¡zok, ktorÃ½ oklame diskriminÃ¡tor, a nie rÃ´znorodÃ© obrÃ¡zky.
* **CitlivosÅ¥ na hyperparametre**. ÄŒasto mÃ´Å¾ete vidieÅ¥, Å¾e GAN vÃ´bec nekonverguje, a potom nÃ¡hle znÃ­Å¾enie rÃ½chlosti uÄenia vedie ku konvergencii.
* UdrÅ¾iavanie **rovnovÃ¡hy** medzi generÃ¡torom a diskriminÃ¡torom. V mnohÃ½ch prÃ­padoch strata diskriminÃ¡tora mÃ´Å¾e relatÃ­vne rÃ½chlo klesnÃºÅ¥ na nulu, Äo spÃ´sobÃ­, Å¾e generÃ¡tor uÅ¾ nie je schopnÃ½ Äalej trÃ©novaÅ¥. Na prekonanie tohto problÃ©mu mÃ´Å¾eme skÃºsiÅ¥ nastaviÅ¥ rÃ´zne rÃ½chlosti uÄenia pre generÃ¡tor a diskriminÃ¡tor, alebo preskoÄiÅ¥ trÃ©ning diskriminÃ¡tora, ak je strata uÅ¾ prÃ­liÅ¡ nÃ­zka.
* TrÃ©ning pre **vysokÃ© rozlÃ­Å¡enie**. Tento problÃ©m, podobnÃ½ problÃ©mu s autoenkÃ³dermi, nastÃ¡va, pretoÅ¾e rekonÅ¡trukcia prÃ­liÅ¡ mnohÃ½ch vrstiev konvoluÄnej siete vedie k artefaktom. Tento problÃ©m sa zvyÄajne rieÅ¡i tzv. **progresÃ­vnym rastom**, keÄ sa najprv niekoÄ¾ko vrstiev trÃ©nuje na obrÃ¡zkoch s nÃ­zkym rozlÃ­Å¡enÃ­m, a potom sa vrstvy "odomknÃº" alebo pridajÃº. ÄalÅ¡Ã­m rieÅ¡enÃ­m by bolo pridanie extra spojenÃ­ medzi vrstvami a trÃ©ning viacerÃ½ch rozlÃ­Å¡enÃ­ naraz - podrobnosti nÃ¡jdete v tomto [Multi-Scale Gradient GANs ÄlÃ¡nku](https://arxiv.org/abs/1903.06048).

## Prenos Å¡tÃ½lu

GAN sÃº skvelÃ½m spÃ´sobom, ako generovaÅ¥ umeleckÃ© obrÃ¡zky. ÄalÅ¡ou zaujÃ­mavou technikou je tzv. **prenos Å¡tÃ½lu**, ktorÃ½ vezme jeden **obrÃ¡zok obsahu** a prekreslÃ­ ho v inom Å¡tÃ½le, aplikujÃºc filtre z **obrÃ¡zku Å¡tÃ½lu**.

Ako to funguje:
* ZaÄÃ­name s nÃ¡hodnÃ½m Å¡umovÃ½m obrÃ¡zkom (alebo s obrÃ¡zkom obsahu, ale pre lepÅ¡ie pochopenie je jednoduchÅ¡ie zaÄaÅ¥ s nÃ¡hodnÃ½m Å¡umom).
* NaÅ¡Ã­m cieÄ¾om je vytvoriÅ¥ takÃ½ obrÃ¡zok, ktorÃ½ bude blÃ­zky obrÃ¡zku obsahu aj obrÃ¡zku Å¡tÃ½lu. Toto bude urÄenÃ© dvoma stratovÃ½mi funkciami:
   - **Strata obsahu** sa vypoÄÃ­ta na zÃ¡klade vlastnostÃ­ extrahovanÃ½ch CNN na niektorÃ½ch vrstvÃ¡ch z aktuÃ¡lneho obrÃ¡zku a obrÃ¡zku obsahu.
   - **Strata Å¡tÃ½lu** sa vypoÄÃ­ta medzi aktuÃ¡lnym obrÃ¡zkom a obrÃ¡zkom Å¡tÃ½lu Å¡ikovnÃ½m spÃ´sobom pomocou GramovÃ½ch matÃ­c (viac podrobnostÃ­ v [prÃ­kladovom notebooku](StyleTransfer.ipynb)).
* Aby bol obrÃ¡zok hladÅ¡Ã­ a odstrÃ¡nil Å¡um, zavÃ¡dzame tieÅ¾ **stratu variÃ¡cie**, ktorÃ¡ vypoÄÃ­ta priemernÃº vzdialenosÅ¥ medzi susednÃ½mi pixelmi.
* HlavnÃ¡ optimalizaÄnÃ¡ sluÄka upravuje aktuÃ¡lny obrÃ¡zok pomocou gradientnÃ©ho zostupu (alebo inÃ©ho optimalizaÄnÃ©ho algoritmu) na minimalizÃ¡ciu celkovej straty, ktorÃ¡ je vÃ¡Å¾enÃ½m sÃºÄtom vÅ¡etkÃ½ch troch strÃ¡t.

## âœï¸ PrÃ­klad: [Prenos Å¡tÃ½lu](StyleTransfer.ipynb)

## [KvÃ­z po prednÃ¡Å¡ke](https://ff-quizzes.netlify.app/en/ai/quiz/20)

## ZÃ¡ver

V tejto lekcii ste sa nauÄili o GAN a ako ich trÃ©novaÅ¥. TieÅ¾ ste sa nauÄili o Å¡peciÃ¡lnych vÃ½zvach, ktorÃ½m tento typ neurÃ³novej siete mÃ´Å¾e ÄeliÅ¥, a o niektorÃ½ch stratÃ©giÃ¡ch, ako ich prekonaÅ¥.

## ğŸš€ VÃ½zva

Prejdite si [notebook Prenos Å¡tÃ½lu](StyleTransfer.ipynb) s pouÅ¾itÃ­m vlastnÃ½ch obrÃ¡zkov.

## PrehÄ¾ad a samostatnÃ© Å¡tÃºdium

Pre referenciu si preÄÃ­tajte viac o GAN v tÃ½chto zdrojoch:

* Marco Pasini, [10 lekciÃ­, ktorÃ© som sa nauÄil pri trÃ©ningu GAN poÄas jednÃ©ho roka](https://towardsdatascience.com/10-lessons-i-learned-training-generative-adversarial-networks-gans-for-a-year-c9071159628)
* [StyleGAN](https://en.wikipedia.org/wiki/StyleGAN), *de facto* GAN architektÃºra na zvÃ¡Å¾enie
* [VytvÃ¡ranie generatÃ­vneho umenia pomocou GAN na Azure ML](https://soshnikov.com/scienceart/creating-generative-art-using-gan-on-azureml/)

## Zadanie

Znovu si prejdite jeden z dvoch notebookov priradenÃ½ch k tejto lekcii a znovu natrÃ©nujte GAN na vlastnÃ½ch obrÃ¡zkoch. ÄŒo dokÃ¡Å¾ete vytvoriÅ¥?

---

