{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mechanizmy pozornosti a transformery\n",
    "\n",
    "Jednou z hlavných nevýhod rekurentných sietí je, že všetky slová v sekvencii majú rovnaký vplyv na výsledok. To spôsobuje suboptimálny výkon pri štandardných modeloch LSTM encoder-decoder pre úlohy sekvencie na sekvenciu, ako je rozpoznávanie pomenovaných entít alebo strojový preklad. V skutočnosti majú konkrétne slová v vstupnej sekvencii často väčší vplyv na výstupy ako ostatné.\n",
    "\n",
    "Zvážte model sekvencie na sekvenciu, ako je strojový preklad. Ten je implementovaný pomocou dvoch rekurentných sietí, kde jedna sieť (**encoder**) zhrnie vstupnú sekvenciu do skrytého stavu a druhá (**decoder**) rozvinie tento skrytý stav do preloženého výsledku. Problém s týmto prístupom je, že konečný stav siete má problém zapamätať si začiatok vety, čo vedie k nízkej kvalite modelu pri dlhých vetách.\n",
    "\n",
    "**Mechanizmy pozornosti** poskytujú spôsob, ako vážiť kontextuálny vplyv každého vstupného vektora na každú výstupnú predikciu RNN. Implementuje sa to vytvorením skratiek medzi medzistavmi vstupnej RNN a výstupnej RNN. Týmto spôsobom, pri generovaní výstupného symbolu $y_t$, zohľadníme všetky skryté stavy vstupu $h_i$, s rôznymi váhovými koeficientmi $\\alpha_{t,i}$.\n",
    "\n",
    "![Obrázok zobrazujúci model encoder/decoder s vrstvou aditívnej pozornosti](../../../../../translated_images/sk/encoder-decoder-attention.7a726296894fb567.webp)\n",
    "*Model encoder-decoder s mechanizmom aditívnej pozornosti podľa [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf), citované z [tohto blogového príspevku](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)*\n",
    "\n",
    "Maticu pozornosti $\\{\\alpha_{i,j}\\}$ môžeme interpretovať ako mieru, do akej konkrétne vstupné slová ovplyvňujú generovanie daného slova vo výstupnej sekvencii. Nižšie je príklad takejto matice:\n",
    "\n",
    "![Obrázok zobrazujúci vzorové zarovnanie nájdené RNNsearch-50, prevzaté z Bahdanau - arviz.org](../../../../../translated_images/sk/bahdanau-fig3.09ba2d37f202a6af.webp)\n",
    "\n",
    "*Obrázok prevzatý z [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) (Obr.3)*\n",
    "\n",
    "Mechanizmy pozornosti sú zodpovedné za veľkú časť súčasného alebo takmer súčasného stavu techniky v spracovaní prirodzeného jazyka. Pridanie pozornosti však výrazne zvyšuje počet parametrov modelu, čo viedlo k problémom so škálovaním RNN. Kľúčovým obmedzením škálovania RNN je, že rekurentná povaha modelov sťažuje dávkovanie a paralelizáciu tréningu. V RNN musí byť každý prvok sekvencie spracovaný v sekvenčnom poradí, čo znamená, že ho nemožno ľahko paralelizovať.\n",
    "\n",
    "Prijatie mechanizmov pozornosti v kombinácii s týmto obmedzením viedlo k vzniku dnes už štandardných modelov Transformer, ktoré poznáme a používame, od BERT po OpenGPT3.\n",
    "\n",
    "## Modely Transformer\n",
    "\n",
    "Namiesto prenášania kontextu každej predchádzajúcej predikcie do ďalšieho kroku hodnotenia používajú **modely Transformer** **pozíciové kódovania** a pozornosť na zachytenie kontextu daného vstupu v rámci poskytnutého okna textu. Obrázok nižšie ukazuje, ako pozíciové kódovania s pozornosťou dokážu zachytiť kontext v rámci daného okna.\n",
    "\n",
    "![Animovaný GIF zobrazujúci, ako sa hodnotenia vykonávajú v modeloch Transformer.](../../../../../lessons/5-NLP/18-Transformers/images/transformer-animated-explanation.gif)\n",
    "\n",
    "Keďže každá vstupná pozícia je mapovaná nezávisle na každú výstupnú pozíciu, transformery dokážu lepšie paralelizovať ako RNN, čo umožňuje oveľa väčšie a expresívnejšie jazykové modely. Každá hlava pozornosti môže byť použitá na učenie rôznych vzťahov medzi slovami, čo zlepšuje následné úlohy spracovania prirodzeného jazyka.\n",
    "\n",
    "**BERT** (Bidirectional Encoder Representations from Transformers) je veľmi veľká viacvrstvová sieť Transformer s 12 vrstvami pre *BERT-base* a 24 pre *BERT-large*. Model je najprv predtrénovaný na veľkom korpuse textových dát (Wikipedia + knihy) pomocou nesupervidovaného tréningu (predikcia maskovaných slov vo vete). Počas predtrénovania model absorbuje významnú úroveň porozumenia jazyka, ktorú je možné následne využiť s inými dátovými súbormi pomocou jemného doladenia. Tento proces sa nazýva **transfer learning**.\n",
    "\n",
    "![Obrázok z http://jalammar.github.io/illustrated-bert/](../../../../../translated_images/sk/jalammarBERT-language-modeling-masked-lm.34f113ea5fec4362.webp)\n",
    "\n",
    "Existuje mnoho variácií architektúr Transformer, vrátane BERT, DistilBERT, BigBird, OpenGPT3 a ďalších, ktoré je možné jemne doladiť. Balík [HuggingFace](https://github.com/huggingface/) poskytuje úložisko na tréning mnohých z týchto architektúr pomocou PyTorch.\n",
    "\n",
    "## Použitie BERT na klasifikáciu textu\n",
    "\n",
    "Pozrime sa, ako môžeme použiť predtrénovaný model BERT na riešenie našej tradičnej úlohy: klasifikácie sekvencií. Budeme klasifikovať náš pôvodný dataset AG News.\n",
    "\n",
    "Najprv načítajme knižnicu HuggingFace a náš dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Building vocab...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "from torchnlp import *\n",
    "import transformers\n",
    "train_dataset, test_dataset, classes, vocab = load_dataset()\n",
    "vocab_len = len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keďže budeme používať predtrénovaný model BERT, budeme potrebovať špecifický tokenizer. Najskôr načítame tokenizer spojený s predtrénovaným modelom BERT.\n",
    "\n",
    "Knižnica HuggingFace obsahuje úložisko predtrénovaných modelov, ktoré môžete použiť jednoducho tým, že ich názvy zadáte ako argumenty funkcií `from_pretrained`. Všetky potrebné binárne súbory pre model sa automaticky stiahnu.\n",
    "\n",
    "Avšak, v určitých prípadoch budete potrebovať načítať vlastné modely. V takom prípade môžete špecifikovať adresár, ktorý obsahuje všetky relevantné súbory, vrátane parametrov pre tokenizer, súboru `config.json` s parametrami modelu, binárnych váh, atď.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load the model from Internet repository using model name. \n",
    "# Use this if you are running from your own copy of the notebooks\n",
    "bert_model = 'bert-base-uncased' \n",
    "\n",
    "# To load the model from the directory on disk. Use this for Microsoft Learn module, because we have\n",
    "# prepared all required files for you.\n",
    "bert_model = './bert'\n",
    "\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(bert_model)\n",
    "\n",
    "MAX_SEQ_LEN = 128\n",
    "PAD_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "UNK_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objekt `tokenizer` obsahuje funkciu `encode`, ktorú je možné priamo použiť na kódovanie textu:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 1052, 22123, 2953, 2818, 2003, 1037, 2307, 7705, 2005, 17953, 2361, 102]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('PyTorch is a great framework for NLP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Potom vytvorme iterátory, ktoré budeme používať počas tréningu na prístup k dátam. Keďže BERT používa svoju vlastnú funkciu kódovania, budeme musieť definovať funkciu na doplnenie podobnú `padify`, ktorú sme definovali predtým:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_bert(b):\n",
    "    # b is the list of tuples of length batch_size\n",
    "    #   - first element of a tuple = label, \n",
    "    #   - second = feature (text sequence)\n",
    "    # build vectorized sequence\n",
    "    v = [tokenizer.encode(x[1]) for x in b]\n",
    "    # compute max length of a sequence in this minibatch\n",
    "    l = max(map(len,v))\n",
    "    return ( # tuple of two tensors - labels and features\n",
    "        torch.LongTensor([t[0] for t in b]),\n",
    "        torch.stack([torch.nn.functional.pad(torch.tensor(t),(0,l-len(t)),mode='constant',value=0) for t in v])\n",
    "    )\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=8, collate_fn=pad_bert, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=8, collate_fn=pad_bert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "V našom prípade budeme používať predtrénovaný model BERT nazvaný `bert-base-uncased`. Načítajme model pomocou balíka `BertForSequenceClassfication`. To zabezpečí, že náš model už má požadovanú architektúru pre klasifikáciu, vrátane finálneho klasifikátora. Uvidíte varovnú správu, ktorá uvádza, že váhy finálneho klasifikátora nie sú inicializované a model by vyžadoval predtréning - to je úplne v poriadku, pretože presne to sa chystáme urobiť!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./bert were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./bert and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = transformers.BertForSequenceClassification.from_pretrained(bert_model,num_labels=4).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teraz sme pripravení začať tréning! Keďže BERT je už predtrénovaný, chceme začať s pomerne malou rýchlosťou učenia, aby sme nezničili počiatočné váhy.\n",
    "\n",
    "Všetku náročnú prácu vykonáva model `BertForSequenceClassification`. Keď zavoláme model na tréningových dátach, vráti nám stratu aj výstup siete pre vstupný minibatch. Stratu používame na optimalizáciu parametrov (`loss.backward()` vykonáva spätný prechod) a `out` na výpočet presnosti tréningu porovnaním získaných štítkov `labs` (vypočítaných pomocou `argmax`) s očakávanými `labels`.\n",
    "\n",
    "Na kontrolu procesu akumulujeme stratu a presnosť počas niekoľkých iterácií a tlačíme ich každých `report_freq` tréningových cyklov.\n",
    "\n",
    "Tento tréning pravdepodobne zaberie pomerne veľa času, preto obmedzujeme počet iterácií.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 1.1254194641113282, Accuracy = 0.585\n",
      "Loss = 0.6194715118408203, Accuracy = 0.83\n",
      "Loss = 0.46665248870849607, Accuracy = 0.8475\n",
      "Loss = 0.4309701919555664, Accuracy = 0.8575\n",
      "Loss = 0.35427074432373046, Accuracy = 0.8825\n",
      "Loss = 0.3306886291503906, Accuracy = 0.8975\n",
      "Loss = 0.30340143203735354, Accuracy = 0.8975\n",
      "Loss = 0.26139299392700194, Accuracy = 0.915\n",
      "Loss = 0.26708646774291994, Accuracy = 0.9225\n",
      "Loss = 0.3667240524291992, Accuracy = 0.8675\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
    "\n",
    "report_freq = 50\n",
    "iterations = 500 # make this larger to train for longer time!\n",
    "\n",
    "model.train()\n",
    "\n",
    "i,c = 0,0\n",
    "acc_loss = 0\n",
    "acc_acc = 0\n",
    "\n",
    "for labels,texts in train_loader:\n",
    "    labels = labels.to(device)-1 # get labels in the range 0-3         \n",
    "    texts = texts.to(device)\n",
    "    loss, out = model(texts, labels=labels)[:2]\n",
    "    labs = out.argmax(dim=1)\n",
    "    acc = torch.mean((labs==labels).type(torch.float32))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    acc_loss += loss\n",
    "    acc_acc += acc\n",
    "    i+=1\n",
    "    c+=1\n",
    "    if i%report_freq==0:\n",
    "        print(f\"Loss = {acc_loss.item()/c}, Accuracy = {acc_acc.item()/c}\")\n",
    "        c = 0\n",
    "        acc_loss = 0\n",
    "        acc_acc = 0\n",
    "    iterations-=1\n",
    "    if not iterations:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Môžete vidieť (najmä ak zvýšite počet iterácií a počkáte dostatočne dlho), že klasifikácia pomocou BERT nám poskytuje celkom dobrú presnosť! Je to preto, že BERT už veľmi dobre rozumie štruktúre jazyka a my potrebujeme iba doladiť záverečný klasifikátor. Avšak, keďže BERT je veľký model, celý proces trénovania trvá dlho a vyžaduje značný výpočtový výkon! (GPU, a ideálne viac ako jeden).\n",
    "\n",
    "> **Note:** V našom príklade sme používali jeden z najmenších predtrénovaných modelov BERT. Existujú väčšie modely, ktoré pravdepodobne prinesú lepšie výsledky.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hodnotenie výkonu modelu\n",
    "\n",
    "Teraz môžeme vyhodnotiť výkon nášho modelu na testovacej množine údajov. Cyklus hodnotenia je veľmi podobný tréningovému cyklu, ale nemali by sme zabudnúť prepnúť model do hodnotiaceho režimu zavolaním `model.eval()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy: 0.9047029702970297\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "iterations = 100\n",
    "acc = 0\n",
    "i = 0\n",
    "for labels,texts in test_loader:\n",
    "    labels = labels.to(device)-1      \n",
    "    texts = texts.to(device)\n",
    "    _, out = model(texts, labels=labels)[:2]\n",
    "    labs = out.argmax(dim=1)\n",
    "    acc += torch.mean((labs==labels).type(torch.float32))\n",
    "    i+=1\n",
    "    if i>iterations: break\n",
    "        \n",
    "print(f\"Final accuracy: {acc.item()/i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hlavné body\n",
    "\n",
    "V tejto jednotke sme videli, aké jednoduché je použiť predtrénovaný jazykový model z knižnice **transformers** a prispôsobiť ho na úlohu klasifikácie textu. Podobne je možné modely BERT použiť na extrakciu entít, odpovedanie na otázky a ďalšie úlohy spracovania prirodzeného jazyka.\n",
    "\n",
    "Modely transformerov predstavujú súčasný špičkový stav v oblasti NLP a vo väčšine prípadov by mali byť prvým riešením, s ktorým začnete experimentovať pri implementácii vlastných NLP riešení. Avšak pochopenie základných princípov rekurentných neurónových sietí, o ktorých sme diskutovali v tomto module, je mimoriadne dôležité, ak chcete vytvárať pokročilé neurónové modely.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Upozornenie**:  \nTento dokument bol preložený pomocou služby AI prekladu [Co-op Translator](https://github.com/Azure/co-op-translator). Hoci sa snažíme o presnosť, prosím, berte na vedomie, že automatizované preklady môžu obsahovať chyby alebo nepresnosti. Pôvodný dokument v jeho pôvodnom jazyku by mal byť považovaný za autoritatívny zdroj. Pre kritické informácie sa odporúča profesionálny ľudský preklad. Nie sme zodpovední za akékoľvek nedorozumenia alebo nesprávne interpretácie vyplývajúce z použitia tohto prekladu.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37_pytorch",
   "language": "python",
   "name": "conda-env-py37_pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "coopTranslator": {
   "original_hash": "753865967678a92dbce7d7efbd36d980",
   "translation_date": "2025-08-30T00:43:57+00:00",
   "source_file": "lessons/5-NLP/18-Transformers/TransformersPyTorch.ipynb",
   "language_code": "sk"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}