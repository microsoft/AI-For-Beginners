<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "7e617f0b8de85a43957a853aba09bfeb",
  "translation_date": "2025-08-25T22:00:55+00:00",
  "source_file": "lessons/5-NLP/18-Transformers/README.md",
  "language_code": "sk"
}
-->
# Mechanizmy pozornosti a transformery

## [KvÃ­z pred prednÃ¡Å¡kou](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/118)

JednÃ½m z najdÃ´leÅ¾itejÅ¡Ã­ch problÃ©mov v oblasti NLP je **strojovÃ½ preklad**, zÃ¡kladnÃ¡ Ãºloha, ktorÃ¡ je zÃ¡kladom nÃ¡strojov ako Google Translate. V tejto sekcii sa zameriame na strojovÃ½ preklad, alebo vÅ¡eobecnejÅ¡ie na akÃºkoÄ¾vek Ãºlohu *sekvencia na sekvenciu* (ktorÃ¡ sa tieÅ¾ nazÃ½va **transformÃ¡cia viet**).

Pri RNN je sekvencia na sekvenciu implementovanÃ¡ dvoma rekurentnÃ½mi sieÅ¥ami, kde jedna sieÅ¥, **enkÃ³der**, zhrnie vstupnÃº sekvenciu do skrytÃ©ho stavu, zatiaÄ¾ Äo druhÃ¡ sieÅ¥, **dekÃ³der**, rozvinie tento skrytÃ½ stav do preloÅ¾enÃ©ho vÃ½sledku. Tento prÃ­stup mÃ¡ vÅ¡ak niekoÄ¾ko problÃ©mov:

* KoneÄnÃ½ stav enkÃ³derovej siete mÃ¡ problÃ©m zapamÃ¤taÅ¥ si zaÄiatok vety, Äo spÃ´sobuje nÃ­zku kvalitu modelu pri dlhÃ½ch vetÃ¡ch.
* VÅ¡etky slovÃ¡ v sekvencii majÃº rovnakÃ½ vplyv na vÃ½sledok. V skutoÄnosti vÅ¡ak konkrÃ©tne slovÃ¡ vo vstupnej sekvencii Äasto majÃº vÃ¤ÄÅ¡Ã­ vplyv na vÃ½stupy sekvencie neÅ¾ inÃ©.

**Mechanizmy pozornosti** poskytujÃº spÃ´sob vÃ¡Å¾enia kontextovÃ©ho vplyvu kaÅ¾dÃ©ho vstupnÃ©ho vektora na kaÅ¾dÃº predikciu vÃ½stupu RNN. Implementuje sa to vytvorenÃ­m skratiek medzi medzistavmi vstupnÃ©ho RNN a vÃ½stupnÃ©ho RNN. TÃ½mto spÃ´sobom, pri generovanÃ­ vÃ½stupnÃ©ho symbolu y<sub>t</sub>, zohÄ¾adnÃ­me vÅ¡etky skrytÃ© stavy vstupu h<sub>i</sub>, s rÃ´znymi vÃ¡hovÃ½mi koeficientmi Î±<sub>t,i</sub>.

![ObrÃ¡zok zobrazujÃºci model enkÃ³der/dekÃ³der s aditÃ­vnou vrstvou pozornosti](../../../../../translated_images/encoder-decoder-attention.7a726296894fb567aa2898c94b17b3289087f6705c11907df8301df9e5eeb3de.sk.png)

> Model enkÃ³der-dekÃ³der s mechanizmom aditÃ­vnej pozornosti podÄ¾a [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf), citovanÃ© z [tohto blogovÃ©ho prÃ­spevku](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)

Matica pozornosti {Î±<sub>i,j</sub>} by reprezentovala mieru, do akej urÄitÃ© vstupnÃ© slovÃ¡ zohrÃ¡vajÃº Ãºlohu pri generovanÃ­ danÃ©ho slova vo vÃ½stupnej sekvencii. NiÅ¾Å¡ie je prÃ­klad takejto matice:

![ObrÃ¡zok zobrazujÃºci vzorovÃ© zarovnanie nÃ¡jdenÃ© RNNsearch-50, prevzatÃ© z Bahdanau - arviz.org](../../../../../translated_images/bahdanau-fig3.09ba2d37f202a6af11de6c82d2d197830ba5f4528d9ea430eb65fd3a75065973.sk.png)

> ObrÃ¡zok z [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) (Obr.3)

Mechanizmy pozornosti sÃº zodpovednÃ© za veÄ¾kÃº ÄasÅ¥ sÃºÄasnÃ©ho alebo takmer sÃºÄasnÃ©ho stavu umenia v NLP. Pridanie pozornosti vÅ¡ak vÃ½razne zvyÅ¡uje poÄet parametrov modelu, Äo viedlo k problÃ©mom so Å¡kÃ¡lovanÃ­m RNN. KÄ¾ÃºÄovÃ½m obmedzenÃ­m Å¡kÃ¡lovania RNN je, Å¾e rekurentnÃ¡ povaha modelov sÅ¥aÅ¾uje dÃ¡vkovanie a paralelizÃ¡ciu trÃ©ningu. V RNN musÃ­ byÅ¥ kaÅ¾dÃ½ prvok sekvencie spracovanÃ½ v sekvenÄnom poradÃ­, Äo znamenÃ¡, Å¾e ho nemoÅ¾no Ä¾ahko paralelizovaÅ¥.

![EnkÃ³der DekÃ³der s PozornosÅ¥ou](../../../../../lessons/5-NLP/18-Transformers/images/EncDecAttention.gif)

> ObrÃ¡zok z [Google Blogu](https://research.googleblog.com/2016/09/a-neural-network-for-machine.html)

Prijatie mechanizmov pozornosti v kombinÃ¡cii s tÃ½mto obmedzenÃ­m viedlo k vytvoreniu dnes znÃ¡mych a pouÅ¾Ã­vanÃ½ch modelov transformÃ©rov, ako sÃº BERT a Open-GPT3.

## Modely transformÃ©rov

Jednou z hlavnÃ½ch myÅ¡lienok transformÃ©rov je vyhnÃºÅ¥ sa sekvenÄnej povahe RNN a vytvoriÅ¥ model, ktorÃ½ je paralelizovateÄ¾nÃ½ poÄas trÃ©ningu. To sa dosahuje implementÃ¡ciou dvoch myÅ¡lienok:

* poziÄnÃ© kÃ³dovanie
* pouÅ¾itie mechanizmu vlastnej pozornosti na zachytenie vzorov namiesto RNN (alebo CNN) (preto sa ÄlÃ¡nok, ktorÃ½ predstavuje transformÃ©ry, nazÃ½va *[Attention is all you need](https://arxiv.org/abs/1706.03762)*)

### PoziÄnÃ© kÃ³dovanie/vkladanie

MyÅ¡lienka poziÄnÃ©ho kÃ³dovania je nasledovnÃ¡. 
1. Pri pouÅ¾itÃ­ RNN je relatÃ­vna pozÃ­cia tokenov reprezentovanÃ¡ poÄtom krokov, a teda ju netreba explicitne reprezentovaÅ¥. 
2. AvÅ¡ak, keÄ prejdeme na pozornosÅ¥, potrebujeme vedieÅ¥ relatÃ­vne pozÃ­cie tokenov v rÃ¡mci sekvencie. 
3. Na zÃ­skanie poziÄnÃ©ho kÃ³dovania rozÅ¡Ã­rime naÅ¡u sekvenciu tokenov o sekvenciu pozÃ­ciÃ­ tokenov v sekvencii (t.j. sekvenciu ÄÃ­sel 0,1, ...).
4. Potom zmieÅ¡ame pozÃ­ciu tokenu s vektorom vkladania tokenu. Na transformÃ¡ciu pozÃ­cie (celÃ©ho ÄÃ­sla) na vektor mÃ´Å¾eme pouÅ¾iÅ¥ rÃ´zne prÃ­stupy:

* TrÃ©novateÄ¾nÃ© vkladanie, podobnÃ© vkladaniu tokenov. Toto je prÃ­stup, ktorÃ½ tu zvaÅ¾ujeme. Aplikujeme vrstvy vkladania na tokeny aj ich pozÃ­cie, ÄÃ­m zÃ­skame vektory vkladania rovnakÃ½ch rozmerov, ktorÃ© potom sÄÃ­tame.
* FixnÃ¡ funkcia poziÄnÃ©ho kÃ³dovania, ako je navrhnutÃ© v pÃ´vodnom ÄlÃ¡nku.

<img src="images/pos-embedding.png" width="50%"/>

> ObrÃ¡zok od autora

VÃ½sledok, ktorÃ½ zÃ­skame s poziÄnÃ½m vkladanÃ­m, vkladÃ¡ pÃ´vodnÃ½ token aj jeho pozÃ­ciu v rÃ¡mci sekvencie.

### Multi-Head Self-Attention

Äalej potrebujeme zachytiÅ¥ niektorÃ© vzory v rÃ¡mci naÅ¡ej sekvencie. Na tento ÃºÄel transformÃ©ry pouÅ¾Ã­vajÃº mechanizmus **vlastnej pozornosti**, ktorÃ½ je v podstate pozornosÅ¥ aplikovanÃ¡ na tÃº istÃº sekvenciu ako vstup a vÃ½stup. Aplikovanie vlastnej pozornosti nÃ¡m umoÅ¾Åˆuje zohÄ¾adniÅ¥ **kontext** v rÃ¡mci vety a vidieÅ¥, ktorÃ© slovÃ¡ sÃº navzÃ¡jom prepojenÃ©. NaprÃ­klad nÃ¡m umoÅ¾Åˆuje vidieÅ¥, na ktorÃ© slovÃ¡ sa odkazuje pomocou koreferenciÃ­, ako *to*, a tieÅ¾ zohÄ¾adniÅ¥ kontext:

![](../../../../../translated_images/CoreferenceResolution.861924d6d384a7d68d8d0039d06a71a151f18a796b8b1330239d3590bd4947eb.sk.png)

> ObrÃ¡zok z [Google Blogu](https://research.googleblog.com/2017/08/transformer-novel-neural-network.html)

V transformÃ©roch pouÅ¾Ã­vame **Multi-Head Attention**, aby sme dali sieti schopnosÅ¥ zachytiÅ¥ niekoÄ¾ko rÃ´znych typov zÃ¡vislostÃ­, napr. dlhodobÃ© vs. krÃ¡tkodobÃ© vzÅ¥ahy medzi slovami, koreferencie vs. nieÄo inÃ©, atÄ.

[TensorFlow Notebook](../../../../../lessons/5-NLP/18-Transformers/TransformersTF.ipynb) obsahuje viac detailov o implementÃ¡cii vrstiev transformÃ©rov.

### PozornosÅ¥ enkÃ³der-dekÃ³der

V transformÃ©roch sa pozornosÅ¥ pouÅ¾Ã­va na dvoch miestach:

* Na zachytenie vzorov v rÃ¡mci vstupnÃ©ho textu pomocou vlastnej pozornosti.
* Na vykonanie prekladu sekvencie - ide o vrstvu pozornosti medzi enkÃ³derom a dekÃ³derom.

PozornosÅ¥ enkÃ³der-dekÃ³der je veÄ¾mi podobnÃ¡ mechanizmu pozornosti pouÅ¾Ã­vanÃ©mu v RNN, ako je opÃ­sanÃ© na zaÄiatku tejto sekcie. Tento animovanÃ½ diagram vysvetÄ¾uje Ãºlohu pozornosti enkÃ³der-dekÃ³der.

![AnimovanÃ½ GIF zobrazujÃºci, ako sa hodnotenia vykonÃ¡vajÃº v modeloch transformÃ©rov.](../../../../../lessons/5-NLP/18-Transformers/images/transformer-animated-explanation.gif)

KeÄÅ¾e kaÅ¾dÃ¡ vstupnÃ¡ pozÃ­cia je mapovanÃ¡ nezÃ¡visle na kaÅ¾dÃº vÃ½stupnÃº pozÃ­ciu, transformÃ©ry mÃ´Å¾u lepÅ¡ie paralelizovaÅ¥ neÅ¾ RNN, Äo umoÅ¾Åˆuje oveÄ¾a vÃ¤ÄÅ¡ie a expresÃ­vnejÅ¡ie jazykovÃ© modely. KaÅ¾dÃ¡ hlava pozornosti mÃ´Å¾e byÅ¥ pouÅ¾itÃ¡ na uÄenie rÃ´znych vzÅ¥ahov medzi slovami, Äo zlepÅ¡uje nÃ¡slednÃ© Ãºlohy spracovania prirodzenÃ©ho jazyka.

## BERT

**BERT** (Bidirectional Encoder Representations from Transformers) je veÄ¾mi veÄ¾kÃ¡ viacvrstvovÃ¡ sieÅ¥ transformÃ©rov s 12 vrstvami pre *BERT-base* a 24 pre *BERT-large*. Model je najprv predtrÃ©novanÃ½ na veÄ¾kom korpuse textovÃ½ch dÃ¡t (WikiPedia + knihy) pomocou nesupervidovanÃ©ho trÃ©ningu (predpovedanie maskovanÃ½ch slov vo vete). PoÄas predtrÃ©novania model absorbuje vÃ½znamnÃ© Ãºrovne porozumenia jazyka, ktorÃ© mÃ´Å¾u byÅ¥ nÃ¡sledne vyuÅ¾itÃ© s inÃ½mi datasetmi pomocou jemnÃ©ho doladenia. Tento proces sa nazÃ½va **transfer learning**.

![obrÃ¡zok z http://jalammar.github.io/illustrated-bert/](../../../../../translated_images/jalammarBERT-language-modeling-masked-lm.34f113ea5fec4362e39ee4381aab7cad06b5465a0b5f053a0f2aa05fbe14e746.sk.png)

> ObrÃ¡zok [zdroj](http://jalammar.github.io/illustrated-bert/)

## âœï¸ CviÄenia: TransformÃ©ry

PokraÄujte vo svojom uÄenÃ­ v nasledujÃºcich notebookoch:

* [TransformÃ©ry v PyTorch](../../../../../lessons/5-NLP/18-Transformers/TransformersPyTorch.ipynb)
* [TransformÃ©ry v TensorFlow](../../../../../lessons/5-NLP/18-Transformers/TransformersTF.ipynb)

## ZÃ¡ver

V tejto lekcii ste sa nauÄili o transformÃ©roch a mechanizmoch pozornosti, vÅ¡etko zÃ¡kladnÃ© nÃ¡stroje v NLP. Existuje mnoho variÃ¡ciÃ­ architektÃºr transformÃ©rov vrÃ¡tane BERT, DistilBERT, BigBird, OpenGPT3 a ÄalÅ¡Ã­ch, ktorÃ© je moÅ¾nÃ© jemne doladiÅ¥. BalÃ­k [HuggingFace](https://github.com/huggingface/) poskytuje ÃºloÅ¾isko na trÃ©ning mnohÃ½ch z tÃ½chto architektÃºr s pouÅ¾itÃ­m PyTorch aj TensorFlow.

## ğŸš€ VÃ½zva

## [KvÃ­z po prednÃ¡Å¡ke](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/218)

## PrehÄ¾ad a samostatnÃ© Å¡tÃºdium

* [BlogovÃ½ prÃ­spevok](https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/), vysvetÄ¾ujÃºci klasickÃ½ ÄlÃ¡nok [Attention is all you need](https://arxiv.org/abs/1706.03762) o transformÃ©roch.
* [SÃ©ria blogovÃ½ch prÃ­spevkov](https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452) o transformÃ©roch, vysvetÄ¾ujÃºca architektÃºru podrobne.

## [Ãšloha](assignment.md)

**Zrieknutie sa zodpovednosti**:  
Tento dokument bol preloÅ¾enÃ½ pomocou sluÅ¾by AI prekladu [Co-op Translator](https://github.com/Azure/co-op-translator). Aj keÄ sa snaÅ¾Ã­me o presnosÅ¥, prosÃ­m, berte na vedomie, Å¾e automatizovanÃ© preklady mÃ´Å¾u obsahovaÅ¥ chyby alebo nepresnosti. PÃ´vodnÃ½ dokument v jeho rodnom jazyku by mal byÅ¥ povaÅ¾ovanÃ½ za autoritatÃ­vny zdroj. Pre kritickÃ© informÃ¡cie sa odporÃºÄa profesionÃ¡lny Ä¾udskÃ½ preklad. Nenesieme zodpovednosÅ¥ za akÃ©koÄ¾vek nedorozumenia alebo nesprÃ¡vne interpretÃ¡cie vyplÃ½vajÃºce z pouÅ¾itia tohto prekladu.