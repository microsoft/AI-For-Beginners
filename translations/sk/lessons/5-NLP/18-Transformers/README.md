# Mechanizmy pozornosti a transformery

## [KvÃ­z pred prednÃ¡Å¡kou](https://ff-quizzes.netlify.app/en/ai/quiz/35)

JednÃ½m z najdÃ´leÅ¾itejÅ¡Ã­ch problÃ©mov v oblasti NLP je **strojovÃ½ preklad**, zÃ¡kladnÃ¡ Ãºloha, ktorÃ¡ je zÃ¡kladom nÃ¡strojov ako Google Translate. V tejto sekcii sa zameriame na strojovÃ½ preklad, alebo vÅ¡eobecnejÅ¡ie na akÃºkoÄ¾vek Ãºlohu *sekvencia na sekvenciu* (ktorÃ¡ sa tieÅ¾ nazÃ½va **transformÃ¡cia viet**).

Pri RNN sa sekvencia na sekvenciu implementuje pomocou dvoch rekurentnÃ½ch sietÃ­, kde jedna sieÅ¥, **enkodÃ©r**, zhrnie vstupnÃº sekvenciu do skrytÃ©ho stavu, zatiaÄ¾ Äo druhÃ¡ sieÅ¥, **dekodÃ©r**, rozvinie tento skrytÃ½ stav do preloÅ¾enÃ©ho vÃ½sledku. Tento prÃ­stup mÃ¡ niekoÄ¾ko problÃ©mov:

* KoneÄnÃ½ stav enkodÃ©rovej siete mÃ¡ problÃ©m zapamÃ¤taÅ¥ si zaÄiatok vety, Äo spÃ´sobuje nÃ­zku kvalitu modelu pri dlhÃ½ch vetÃ¡ch.
* VÅ¡etky slovÃ¡ v sekvencii majÃº rovnakÃ½ vplyv na vÃ½sledok. V skutoÄnosti vÅ¡ak konkrÃ©tne slovÃ¡ vo vstupnej sekvencii Äasto majÃº vÃ¤ÄÅ¡Ã­ vplyv na vÃ½stupy sekvencie neÅ¾ inÃ©.

**Mechanizmy pozornosti** poskytujÃº spÃ´sob vÃ¡Å¾enia kontextovÃ©ho vplyvu kaÅ¾dÃ©ho vstupnÃ©ho vektora na kaÅ¾dÃº vÃ½stupnÃº predikciu RNN. Implementuje sa to vytvorenÃ­m skratiek medzi medzistavmi vstupnÃ©ho RNN a vÃ½stupnÃ©ho RNN. TÃ½mto spÃ´sobom, pri generovanÃ­ vÃ½stupnÃ©ho symbolu y<sub>t</sub>, zohÄ¾adnÃ­me vÅ¡etky skrytÃ© stavy vstupu h<sub>i</sub>, s rÃ´znymi vÃ¡hovÃ½mi koeficientmi &alpha;<sub>t,i</sub>.

![ObrÃ¡zok zobrazujÃºci model enkodÃ©r/dekodÃ©r s aditÃ­vnou vrstvou pozornosti](../../../../../translated_images/sk/encoder-decoder-attention.7a726296894fb567.webp)

> Model enkodÃ©r-dekodÃ©r s aditÃ­vnym mechanizmom pozornosti podÄ¾a [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf), citovanÃ© z [tohto blogovÃ©ho prÃ­spevku](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)

Maticu pozornosti {&alpha;<sub>i,j</sub>} by sme mohli interpretovaÅ¥ ako mieru, do akej urÄitÃ© vstupnÃ© slovÃ¡ ovplyvÅˆujÃº generovanie danÃ©ho slova vo vÃ½stupnej sekvencii. NiÅ¾Å¡ie je prÃ­klad takejto matice:

![ObrÃ¡zok zobrazujÃºci vzorovÃ© zarovnanie nÃ¡jdenÃ© RNNsearch-50, prevzatÃ© z Bahdanau - arviz.org](../../../../../translated_images/sk/bahdanau-fig3.09ba2d37f202a6af.webp)

> ObrÃ¡zok z [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) (Obr.3)

Mechanizmy pozornosti sÃº zodpovednÃ© za veÄ¾kÃº ÄasÅ¥ sÃºÄasnÃ©ho alebo takmer sÃºÄasnÃ©ho stavu umenia v NLP. Pridanie pozornosti vÅ¡ak vÃ½razne zvyÅ¡uje poÄet parametrov modelu, Äo viedlo k problÃ©mom so Å¡kÃ¡lovanÃ­m RNN. KÄ¾ÃºÄovÃ½m obmedzenÃ­m Å¡kÃ¡lovania RNN je, Å¾e rekurentnÃ¡ povaha modelov sÅ¥aÅ¾uje dÃ¡vkovanie a paralelizÃ¡ciu trÃ©ningu. V RNN musÃ­ byÅ¥ kaÅ¾dÃ½ prvok sekvencie spracovanÃ½ v sekvenÄnom poradÃ­, Äo znamenÃ¡, Å¾e ho nemoÅ¾no Ä¾ahko paralelizovaÅ¥.

![EnkodÃ©r DekodÃ©r s PozornosÅ¥ou](../../../../../lessons/5-NLP/18-Transformers/images/EncDecAttention.gif)

> ObrÃ¡zok z [Google Blogu](https://research.googleblog.com/2016/09/a-neural-network-for-machine.html)

Adopcia mechanizmov pozornosti v kombinÃ¡cii s tÃ½mto obmedzenÃ­m viedla k vytvoreniu dnes uÅ¾ Å¡piÄkovÃ½ch modelov Transformer, ktorÃ© poznÃ¡me a pouÅ¾Ã­vame, ako naprÃ­klad BERT a Open-GPT3.

## Modely Transformer

Jednou z hlavnÃ½ch myÅ¡lienok za transformermi je vyhnÃºÅ¥ sa sekvenÄnej povahe RNN a vytvoriÅ¥ model, ktorÃ½ je paralelizovateÄ¾nÃ½ poÄas trÃ©ningu. To sa dosahuje implementÃ¡ciou dvoch myÅ¡lienok:

* poziÄnÃ© kÃ³dovanie
* pouÅ¾itie mechanizmu vlastnej pozornosti na zachytenie vzorov namiesto RNN (alebo CNN) (preto sa ÄlÃ¡nok, ktorÃ½ predstavuje transformery, nazÃ½va *[Attention is all you need](https://arxiv.org/abs/1706.03762)*)

### PoziÄnÃ© kÃ³dovanie/embedding

MyÅ¡lienka poziÄnÃ©ho kÃ³dovania je nasledovnÃ¡. 
1. Pri pouÅ¾itÃ­ RNN je relatÃ­vna pozÃ­cia tokenov reprezentovanÃ¡ poÄtom krokov, a teda ju netreba explicitne reprezentovaÅ¥. 
2. AvÅ¡ak, keÄ prejdeme na pozornosÅ¥, potrebujeme vedieÅ¥ relatÃ­vne pozÃ­cie tokenov v rÃ¡mci sekvencie. 
3. Na zÃ­skanie poziÄnÃ©ho kÃ³dovania doplnÃ­me naÅ¡u sekvenciu tokenov o sekvenciu pozÃ­ciÃ­ tokenov v sekvencii (t.j. sekvenciu ÄÃ­sel 0,1, ...).
4. Potom zmieÅ¡ame pozÃ­ciu tokenu s vektorom embeddingu tokenu. Na transformÃ¡ciu pozÃ­cie (celÃ©ho ÄÃ­sla) na vektor mÃ´Å¾eme pouÅ¾iÅ¥ rÃ´zne prÃ­stupy:

* TrÃ©novateÄ¾nÃ½ embedding, podobnÃ½ embeddingu tokenov. Toto je prÃ­stup, ktorÃ½ tu zvaÅ¾ujeme. Aplikujeme vrstvy embeddingu na tokeny aj ich pozÃ­cie, ÄÃ­m zÃ­skame embeddingovÃ© vektory rovnakÃ½ch rozmerov, ktorÃ© potom sÄÃ­tame.
* FixnÃ¡ funkcia poziÄnÃ©ho kÃ³dovania, ako je navrhnutÃ© v pÃ´vodnom ÄlÃ¡nku.

<img src="../../../../../translated_images/sk/pos-embedding.e41ce9b6cf6078af.webp" width="50%"/>

> ObrÃ¡zok od autora

VÃ½sledok, ktorÃ½ zÃ­skame s poziÄnÃ½m embeddingom, zahÅ•Åˆa pÃ´vodnÃ½ token aj jeho pozÃ­ciu v rÃ¡mci sekvencie.

### Multi-Head Self-Attention

Äalej potrebujeme zachytiÅ¥ nejakÃ© vzory v rÃ¡mci naÅ¡ej sekvencie. Na tento ÃºÄel transformery pouÅ¾Ã­vajÃº mechanizmus **vlastnej pozornosti**, ktorÃ½ je v podstate pozornosÅ¥ aplikovanÃ¡ na tÃº istÃº sekvenciu ako vstup a vÃ½stup. Aplikovanie vlastnej pozornosti nÃ¡m umoÅ¾Åˆuje zohÄ¾adniÅ¥ **kontext** v rÃ¡mci vety a vidieÅ¥, ktorÃ© slovÃ¡ sÃº navzÃ¡jom prepojenÃ©. NaprÃ­klad nÃ¡m umoÅ¾Åˆuje vidieÅ¥, na ktorÃ© slovÃ¡ odkazujÃº koreferencie, ako *to*, a tieÅ¾ zohÄ¾adniÅ¥ kontext:

![](../../../../../translated_images/sk/CoreferenceResolution.861924d6d384a7d6.webp)

> ObrÃ¡zok z [Google Blogu](https://research.googleblog.com/2017/08/transformer-novel-neural-network.html)

V transformeroch pouÅ¾Ã­vame **Multi-Head Attention**, aby sme dali sieti schopnosÅ¥ zachytiÅ¥ niekoÄ¾ko rÃ´znych typov zÃ¡vislostÃ­, napr. dlhodobÃ© vs. krÃ¡tkodobÃ© vzÅ¥ahy medzi slovami, koreferencie vs. nieÄo inÃ©, atÄ.

[TensorFlow Notebook](TransformersTF.ipynb) obsahuje viac detailov o implementÃ¡cii vrstiev transformera.

### PozornosÅ¥ enkodÃ©r-dekodÃ©r

V transformeroch sa pozornosÅ¥ pouÅ¾Ã­va na dvoch miestach:

* Na zachytenie vzorov v rÃ¡mci vstupnÃ©ho textu pomocou vlastnej pozornosti.
* Na vykonanie prekladu sekvencie - ide o vrstvu pozornosti medzi enkodÃ©rom a dekodÃ©rom.

PozornosÅ¥ enkodÃ©r-dekodÃ©r je veÄ¾mi podobnÃ¡ mechanizmu pozornosti pouÅ¾Ã­vanÃ©mu v RNN, ako je opÃ­sanÃ© na zaÄiatku tejto sekcie. Tento animovanÃ½ diagram vysvetÄ¾uje Ãºlohu pozornosti enkodÃ©r-dekodÃ©r.

![AnimovanÃ½ GIF zobrazujÃºci, ako sa hodnotenia vykonÃ¡vajÃº v modeloch transformera.](../../../../../lessons/5-NLP/18-Transformers/images/transformer-animated-explanation.gif)

KeÄÅ¾e kaÅ¾dÃ¡ vstupnÃ¡ pozÃ­cia je nezÃ¡visle mapovanÃ¡ na kaÅ¾dÃº vÃ½stupnÃº pozÃ­ciu, transformery mÃ´Å¾u lepÅ¡ie paralelizovaÅ¥ neÅ¾ RNN, Äo umoÅ¾Åˆuje oveÄ¾a vÃ¤ÄÅ¡ie a expresÃ­vnejÅ¡ie jazykovÃ© modely. KaÅ¾dÃ¡ hlava pozornosti mÃ´Å¾e byÅ¥ pouÅ¾itÃ¡ na uÄenie rÃ´znych vzÅ¥ahov medzi slovami, Äo zlepÅ¡uje Ãºlohy spracovania prirodzenÃ©ho jazyka.

## BERT

**BERT** (Bidirectional Encoder Representations from Transformers) je veÄ¾mi veÄ¾kÃ¡ viacvrstvovÃ¡ sieÅ¥ transformera s 12 vrstvami pre *BERT-base* a 24 pre *BERT-large*. Model je najprv predtrÃ©novanÃ½ na veÄ¾kom korpuse textovÃ½ch dÃ¡t (WikiPedia + knihy) pomocou nesupervidovanÃ©ho trÃ©ningu (predikcia maskovanÃ½ch slov vo vete). PoÄas predtrÃ©novania model absorbuje vÃ½znamnÃ© Ãºrovne porozumenia jazyka, ktorÃ© mÃ´Å¾u byÅ¥ nÃ¡sledne vyuÅ¾itÃ© s inÃ½mi datasetmi pomocou jemnÃ©ho doladenia. Tento proces sa nazÃ½va **transfer learning**.

![obrÃ¡zok z http://jalammar.github.io/illustrated-bert/](../../../../../translated_images/sk/jalammarBERT-language-modeling-masked-lm.34f113ea5fec4362.webp)

> ObrÃ¡zok [zdroj](http://jalammar.github.io/illustrated-bert/)

## âœï¸ CviÄenia: Transformery

PokraÄujte vo svojom uÄenÃ­ v nasledujÃºcich notebookoch:

* [Transformery v PyTorch](TransformersPyTorch.ipynb)
* [Transformery v TensorFlow](TransformersTF.ipynb)

## ZÃ¡ver

V tejto lekcii ste sa nauÄili o Transformeroch a Mechanizmoch pozornosti, vÅ¡etko zÃ¡kladnÃ© nÃ¡stroje v NLP. Existuje mnoho variÃ¡ciÃ­ architektÃºr Transformera vrÃ¡tane BERT, DistilBERT, BigBird, OpenGPT3 a ÄalÅ¡Ã­ch, ktorÃ© je moÅ¾nÃ© jemne doladiÅ¥. BalÃ­k [HuggingFace](https://github.com/huggingface/) poskytuje ÃºloÅ¾isko na trÃ©ning mnohÃ½ch z tÃ½chto architektÃºr s pouÅ¾itÃ­m PyTorch aj TensorFlow.

## ğŸš€ VÃ½zva

## [KvÃ­z po prednÃ¡Å¡ke](https://ff-quizzes.netlify.app/en/ai/quiz/36)

## PrehÄ¾ad & SamoÅ¡tÃºdium

* [BlogovÃ½ prÃ­spevok](https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/), vysvetÄ¾ujÃºci klasickÃ½ ÄlÃ¡nok [Attention is all you need](https://arxiv.org/abs/1706.03762) o transformeroch.
* [SÃ©ria blogovÃ½ch prÃ­spevkov](https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452) o transformeroch, vysvetÄ¾ujÃºca architektÃºru podrobne.

## [Ãšloha](assignment.md)

---

