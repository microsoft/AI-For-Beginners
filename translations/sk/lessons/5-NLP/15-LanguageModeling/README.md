# Jazykov√© modelovanie

S√©mantick√© vektory, ako Word2Vec a GloVe, s√∫ v skutoƒçnosti prv√Ωm krokom k **jazykov√©mu modelovaniu** ‚Äì vytv√°raniu modelov, ktor√© nejako *rozumej√∫* (alebo *reprezentuj√∫*) podstatu jazyka.

## [Kv√≠z pred predn√°≈°kou](https://ff-quizzes.netlify.app/en/ai/quiz/29)

Hlavnou my≈°lienkou jazykov√©ho modelovania je ich tr√©novanie na neoznaƒçen√Ωch d√°tach v nesupervidovanom re≈æime. To je d√¥le≈æit√©, preto≈æe m√°me k dispoz√≠cii obrovsk√© mno≈æstvo neoznaƒçen√©ho textu, zatiaƒæ ƒço mno≈æstvo oznaƒçen√©ho textu je v≈ædy obmedzen√© √∫sil√≠m, ktor√© m√¥≈æeme venova≈• jeho oznaƒçovaniu. Najƒçastej≈°ie m√¥≈æeme vytvori≈• jazykov√© modely, ktor√© dok√°≈æu **predpoveda≈• ch√Ωbaj√∫ce slov√°** v texte, preto≈æe je jednoduch√© n√°hodne vynecha≈• slovo v texte a pou≈æi≈• ho ako tr√©ningov√Ω vzor.

## Tr√©novanie vektorov

V na≈°ich predch√°dzaj√∫cich pr√≠kladoch sme pou≈æ√≠vali predtr√©novan√© s√©mantick√© vektory, ale je zauj√≠mav√© vidie≈•, ako sa tieto vektory daj√∫ tr√©nova≈•. Existuje niekoƒæko mo≈æn√Ωch pr√≠stupov, ktor√© sa daj√∫ pou≈æi≈•:

* **Jazykov√© modelovanie pomocou N-Gramov**, kde predpoved√°me token na z√°klade N predch√°dzaj√∫cich tokenov (N-gram).
* **Continuous Bag-of-Words** (CBoW), kde predpoved√°me stredn√Ω token $W_0$ v sekvencii tokenov $W_{-N}$, ..., $W_N$.
* **Skip-gram**, kde predpoved√°me mno≈æinu susedn√Ωch tokenov {$W_{-N},\dots, W_{-1}, W_1,\dots, W_N$} zo stredn√©ho tokenu $W_0$.

![obr√°zok z ƒçl√°nku o konverzii slov na vektory](../../../../../translated_images/sk/example-algorithms-for-converting-words-to-vectors.fbe9207a726922f6.webp)

> Obr√°zok z [tohto ƒçl√°nku](https://arxiv.org/pdf/1301.3781.pdf)

## ‚úçÔ∏è Pr√≠kladov√© notebooky: Tr√©novanie CBoW modelu

Pokraƒçujte vo svojom uƒçen√≠ v nasleduj√∫cich notebookoch:

* [Tr√©novanie CBoW Word2Vec pomocou TensorFlow](CBoW-TF.ipynb)
* [Tr√©novanie CBoW Word2Vec pomocou PyTorch](CBoW-PyTorch.ipynb)

## Z√°ver

V predch√°dzaj√∫cej lekcii sme videli, ≈æe vektory slov funguj√∫ ako k√∫zlo! Teraz vieme, ≈æe tr√©novanie vektorov slov nie je veƒæmi zlo≈æit√° √∫loha, a mali by sme by≈• schopn√≠ tr√©nova≈• vlastn√© vektory slov pre texty ≈°pecifick√© pre dan√∫ oblas≈•, ak to bude potrebn√©.

## [Kv√≠z po predn√°≈°ke](https://ff-quizzes.netlify.app/en/ai/quiz/30)

## Prehƒæad a samostatn√© ≈°t√∫dium

* [Ofici√°lny PyTorch tutori√°l o jazykovom modelovan√≠](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html).
* [Ofici√°lny TensorFlow tutori√°l o tr√©novan√≠ Word2Vec modelu](https://www.TensorFlow.org/tutorials/text/word2vec).
* Pou≈æitie frameworku **gensim** na tr√©novanie najbe≈ænej≈°ie pou≈æ√≠van√Ωch vektorov v niekoƒæk√Ωch riadkoch k√≥du je pop√≠san√© [v tejto dokument√°cii](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html).

## üöÄ [√öloha: Tr√©novanie Skip-Gram modelu](lab/README.md)

V laborat√≥riu v√°s vyz√Ωvame, aby ste upravili k√≥d z tejto lekcie na tr√©novanie Skip-Gram modelu namiesto CBoW. [Preƒç√≠tajte si podrobnosti](lab/README.md)

---

