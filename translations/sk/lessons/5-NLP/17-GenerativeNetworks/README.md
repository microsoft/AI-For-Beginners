# Generat√≠vne siete

## [Kv√≠z pred predn√°≈°kou](https://ff-quizzes.netlify.app/en/ai/quiz/33)

Rekurentn√© neur√≥nov√© siete (RNN) a ich varianty s br√°nami, ako s√∫ Long Short Term Memory Cells (LSTM) a Gated Recurrent Units (GRU), poskytuj√∫ mechanizmus na modelovanie jazyka, preto≈æe dok√°≈æu uƒçi≈• poradie slov a predpoveda≈• ƒèal≈°ie slovo v sekvencii. To n√°m umo≈æ≈àuje pou≈æ√≠va≈• RNN na **generat√≠vne √∫lohy**, ako je generovanie textu, strojov√Ω preklad a dokonca aj popisovanie obr√°zkov.

> ‚úÖ Zamyslite sa nad v≈°etk√Ωmi situ√°ciami, kedy ste vyu≈æili generat√≠vne √∫lohy, ako napr√≠klad dopƒ∫≈àanie textu poƒças p√≠sania. Presk√∫majte svoje obƒæ√∫ben√© aplik√°cie a zistite, ƒçi vyu≈æ√≠vaj√∫ RNN.

V architekt√∫re RNN, ktor√∫ sme preberali v predch√°dzaj√∫cej jednotke, ka≈æd√° jednotka RNN produkovala ako v√Ωstup ƒèal≈°√≠ skryt√Ω stav. M√¥≈æeme v≈°ak prida≈• ƒèal≈°√≠ v√Ωstup ku ka≈ædej rekurentnej jednotke, ƒço n√°m umo≈æn√≠ generova≈• **sekvenciu** (rovnako dlh√∫ ako p√¥vodn√° sekvencia). Navy≈°e m√¥≈æeme pou≈æi≈• RNN jednotky, ktor√© neprij√≠maj√∫ vstup na ka≈ædom kroku, ale iba poƒçiatoƒçn√Ω stavov√Ω vektor, a potom generuj√∫ sekvenciu v√Ωstupov.

To umo≈æ≈àuje r√¥zne neur√≥nov√© architekt√∫ry, ktor√© s√∫ zn√°zornen√© na obr√°zku ni≈æ≈°ie:

![Obr√°zok zobrazuj√∫ci be≈æn√© vzory rekurentn√Ωch neur√≥nov√Ωch siet√≠.](../../../../../translated_images/sk/unreasonable-effectiveness-of-rnn.541ead816778f42d.webp)

> Obr√°zok z blogov√©ho pr√≠spevku [Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) od [Andreja Karpatyho](http://karpathy.github.io/)

* **One-to-one** je tradiƒçn√° neur√≥nov√° sie≈• s jedn√Ωm vstupom a jedn√Ωm v√Ωstupom.
* **One-to-many** je generat√≠vna architekt√∫ra, ktor√° prij√≠ma jednu vstupn√∫ hodnotu a generuje sekvenciu v√Ωstupn√Ωch hodn√¥t. Napr√≠klad, ak chceme tr√©nova≈• sie≈• na **popisovanie obr√°zkov**, ktor√° by vytvorila textov√Ω popis obr√°zka, m√¥≈æeme ako vstup pou≈æi≈• obr√°zok, prehna≈• ho cez CNN na z√≠skanie skryt√©ho stavu a potom necha≈• rekurentn√Ω re≈•azec generova≈• popis slovo po slove.
* **Many-to-one** zodpoved√° architekt√∫ram RNN, ktor√© sme pop√≠sali v predch√°dzaj√∫cej jednotke, ako je klasifik√°cia textu.
* **Many-to-many**, alebo **sekvencia na sekvenciu**, zodpoved√° √∫loh√°m, ako je **strojov√Ω preklad**, kde prv√° RNN zhroma≈æƒèuje v≈°etky inform√°cie zo vstupnej sekvencie do skryt√©ho stavu a ƒèal≈°√≠ re≈•azec RNN rozvinie tento stav do v√Ωstupnej sekvencie.

V tejto jednotke sa zameriame na jednoduch√© generat√≠vne modely, ktor√© n√°m pom√°haj√∫ generova≈• text. Pre jednoduchos≈• pou≈æijeme tokeniz√°ciu na √∫rovni znakov.

Budeme tr√©nova≈• t√∫to RNN na generovanie textu krok za krokom. Na ka≈ædom kroku vezmeme sekvenciu znakov dƒ∫≈æky `nchars` a po≈æiadame sie≈•, aby pre ka≈æd√Ω vstupn√Ω znak vygenerovala ƒèal≈°√≠ v√Ωstupn√Ω znak:

![Obr√°zok zobrazuj√∫ci pr√≠klad generovania slova 'HELLO' pomocou RNN.](../../../../../translated_images/sk/rnn-generate.56c54afb52f9781d.webp)

Pri generovan√≠ textu (poƒças inferencie) zaƒç√≠name s nejak√Ωm **podnetom**, ktor√Ω prech√°dza cez RNN bunky na generovanie jeho medzistavu, a potom z tohto stavu zaƒç√≠na generovanie. Generujeme jeden znak naraz a stav spolu s vygenerovan√Ωm znakom posielame ƒèal≈°ej RNN bunke na generovanie ƒèal≈°ieho znaku, a≈æ k√Ωm nevygenerujeme dostatok znakov.

<img src="../../../../../translated_images/sk/rnn-generate-inf.5168dc65e0370eea.webp" width="60%"/>

> Obr√°zok od autora

## ‚úçÔ∏è Cviƒçenia: Generat√≠vne siete

Pokraƒçujte vo svojom uƒçen√≠ v nasleduj√∫cich notebookoch:

* [Generat√≠vne siete s PyTorch](GenerativePyTorch.ipynb)
* [Generat√≠vne siete s TensorFlow](GenerativeTF.ipynb)

## M√§kk√© generovanie textu a teplota

V√Ωstup ka≈ædej RNN bunky je pravdepodobnostn√© rozdelenie znakov. Ak v≈ædy vyberieme znak s najvy≈°≈°ou pravdepodobnos≈•ou ako ƒèal≈°√≠ znak v generovanom texte, text sa ƒçasto m√¥≈æe "cyklova≈•" medzi rovnak√Ωmi sekvenciami znakov znova a znova, ako v tomto pr√≠klade:

```
today of the second the company and a second the company ...
```

Ak sa v≈°ak pozrieme na pravdepodobnostn√© rozdelenie pre ƒèal≈°√≠ znak, m√¥≈æe sa sta≈•, ≈æe rozdiel medzi niekoƒæk√Ωmi najvy≈°≈°√≠mi pravdepodobnos≈•ami nie je veƒæk√Ω, napr√≠klad jeden znak m√¥≈æe ma≈• pravdepodobnos≈• 0,2, in√Ω 0,19 atƒè. Napr√≠klad, keƒè hƒæad√°me ƒèal≈°√≠ znak v sekvencii '*play*', ƒèal≈°√≠m znakom m√¥≈æe by≈• rovnako dobre medzera alebo **e** (ako v slove *player*).

To n√°s vedie k z√°veru, ≈æe nie je v≈ædy "spravodliv√©" vybra≈• znak s vy≈°≈°ou pravdepodobnos≈•ou, preto≈æe v√Ωber druh√©ho najvy≈°≈°ieho m√¥≈æe st√°le vies≈• k zmyslupln√©mu textu. Je rozumnej≈°ie **vzorkova≈•** znaky z pravdepodobnostn√©ho rozdelenia dan√©ho v√Ωstupom siete. M√¥≈æeme tie≈æ pou≈æi≈• parameter **teplota**, ktor√Ω vyhlad√≠ pravdepodobnostn√© rozdelenie, ak chceme prida≈• viac n√°hodnosti, alebo ho urobi≈• strm≈°√≠m, ak chceme viac dodr≈æiava≈• znaky s najvy≈°≈°ou pravdepodobnos≈•ou.

Presk√∫majte, ako je toto m√§kk√© generovanie textu implementovan√© v notebookoch uveden√Ωch vy≈°≈°ie.

## Z√°ver

Aj keƒè generovanie textu m√¥≈æe by≈• u≈æitoƒçn√© samo o sebe, hlavn√© v√Ωhody prich√°dzaj√∫ zo schopnosti generova≈• text pomocou RNN z nejak√©ho poƒçiatoƒçn√©ho vektorov√©ho znaku. Napr√≠klad generovanie textu sa pou≈æ√≠va ako s√∫ƒças≈• strojov√©ho prekladu (sekvencia na sekvenciu, v tomto pr√≠pade sa stavov√Ω vektor z *enk√≥dera* pou≈æ√≠va na generovanie alebo *dek√≥dovanie* prelo≈æen√©ho textu) alebo na generovanie textov√©ho popisu obr√°zka (v tomto pr√≠pade by vektor znakov poch√°dzal z CNN extraktora).

## üöÄ V√Ωzva

Absolvujte niektor√© lekcie na Microsoft Learn na t√∫to t√©mu:

* Generovanie textu s [PyTorch](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-pytorch/6-generative-networks/?WT.mc_id=academic-77998-cacaste)/[TensorFlow](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-tensorflow/5-generative-networks/?WT.mc_id=academic-77998-cacaste)

## [Kv√≠z po predn√°≈°ke](https://ff-quizzes.netlify.app/en/ai/quiz/34)

## Prehƒæad a samo≈°t√∫dium

Tu s√∫ niektor√© ƒçl√°nky na roz≈°√≠renie va≈°ich vedomost√≠:

* R√¥zne pr√≠stupy k generovaniu textu s Markovov√Ωm re≈•azcom, LSTM a GPT-2: [blogov√Ω pr√≠spevok](https://towardsdatascience.com/text-generation-gpt-2-lstm-markov-chain-9ea371820e1e)
* Uk√°≈æka generovania textu v [dokument√°cii Keras](https://keras.io/examples/generative/lstm_character_level_text_generation/)

## [√öloha](lab/README.md)

Videli sme, ako generova≈• text znak po znaku. V laborat√≥riu presk√∫mate generovanie textu na √∫rovni slov.

---

