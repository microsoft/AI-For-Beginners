<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "4522e22e150be0845e03aa41209a39d5",
  "translation_date": "2025-08-25T21:50:01+00:00",
  "source_file": "lessons/5-NLP/13-TextRep/README.md",
  "language_code": "sk"
}
-->
# ReprezentÃ¡cia textu ako tenzorov

## [KvÃ­z pred prednÃ¡Å¡kou](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/113)

## KlasifikÃ¡cia textu

V prvej Äasti tejto sekcie sa zameriame na Ãºlohu **klasifikÃ¡cie textu**. PouÅ¾ijeme dataset [AG News](https://www.kaggle.com/amananandrai/ag-news-classification-dataset), ktorÃ½ obsahuje spravodajskÃ© ÄlÃ¡nky, ako naprÃ­klad:

* KategÃ³ria: Veda/TechnolÃ³gie  
* NÃ¡zov: Ky. spoloÄnosÅ¥ zÃ­skala grant na Å¡tÃºdium peptidov (AP)  
* Text: AP - SpoloÄnosÅ¥ zaloÅ¾enÃ¡ chemickÃ½m vÃ½skumnÃ­kom na Univerzite v Louisville zÃ­skala grant na vÃ½voj...

NaÅ¡Ã­m cieÄ¾om bude klasifikovaÅ¥ spravodajskÃ½ ÄlÃ¡nok do jednej z kategÃ³riÃ­ na zÃ¡klade textu.

## ReprezentÃ¡cia textu

Ak chceme rieÅ¡iÅ¥ Ãºlohy spracovania prirodzenÃ©ho jazyka (NLP) pomocou neurÃ³novÃ½ch sietÃ­, potrebujeme spÃ´sob, ako reprezentovaÅ¥ text ako tenzory. PoÄÃ­taÄe uÅ¾ reprezentujÃº textovÃ© znaky ako ÄÃ­sla, ktorÃ© mapujÃº na fonty na vaÅ¡ej obrazovke pomocou kÃ³dovanÃ­, ako sÃº ASCII alebo UTF-8.

<img alt="ObrÃ¡zok zobrazujÃºci diagram mapujÃºci znak na ASCII a binÃ¡rnu reprezentÃ¡ciu" src="images/ascii-character-map.png" width="50%"/>

> [Zdroj obrÃ¡zku](https://www.seobility.net/en/wiki/ASCII)

Ako Ä¾udia rozumieme, Äo kaÅ¾dÃ½ znak **reprezentuje**, a ako vÅ¡etky znaky spolu tvoria slovÃ¡ vo vete. PoÄÃ­taÄe vÅ¡ak samy o sebe takÃ©muto vÃ½znamu nerozumejÃº, a neurÃ³novÃ¡ sieÅ¥ sa musÃ­ tento vÃ½znam nauÄiÅ¥ poÄas trÃ©ningu.

Preto mÃ´Å¾eme pouÅ¾iÅ¥ rÃ´zne prÃ­stupy na reprezentÃ¡ciu textu:

* **ReprezentÃ¡cia na Ãºrovni znakov**, kde reprezentujeme text tak, Å¾e kaÅ¾dÃ½ znak povaÅ¾ujeme za ÄÃ­slo. Ak mÃ¡me *C* rÃ´znych znakov v naÅ¡om textovom korpuse, slovo *Hello* by bolo reprezentovanÃ© ako tenzor veÄ¾kosti 5x*C*. KaÅ¾dÃ© pÃ­smeno by zodpovedalo stÄºpcu tenzora v one-hot kÃ³dovanÃ­.  
* **ReprezentÃ¡cia na Ãºrovni slov**, kde vytvorÃ­me **slovnÃ­k** vÅ¡etkÃ½ch slov v naÅ¡om texte a potom slovÃ¡ reprezentujeme pomocou one-hot kÃ³dovania. Tento prÃ­stup je o nieÄo lepÅ¡Ã­, pretoÅ¾e jednotlivÃ© pÃ­smenÃ¡ samy o sebe nemajÃº veÄ¾kÃ½ vÃ½znam, a tak pouÅ¾itÃ­m vyÅ¡Å¡Ã­ch sÃ©mantickÃ½ch konceptov - slov - zjednoduÅ¡ujeme Ãºlohu pre neurÃ³novÃº sieÅ¥. AvÅ¡ak vzhÄ¾adom na veÄ¾kosÅ¥ slovnÃ­ka musÃ­me pracovaÅ¥ s vysoko dimenzionÃ¡lnymi riedkymi tenzormi.

Bez ohÄ¾adu na spÃ´sob reprezentÃ¡cie musÃ­me najprv text previesÅ¥ na sekvenciu **tokenov**, priÄom jeden token mÃ´Å¾e byÅ¥ znak, slovo alebo niekedy aj ÄasÅ¥ slova. Potom token prevedieme na ÄÃ­slo, zvyÄajne pomocou **slovnÃ­ka**, a toto ÄÃ­slo mÃ´Å¾eme vloÅ¾iÅ¥ do neurÃ³novej siete pomocou one-hot kÃ³dovania.

## N-Gramy

V prirodzenom jazyku je presnÃ½ vÃ½znam slov urÄenÃ½ iba v kontexte. NaprÃ­klad vÃ½znamy *neurÃ³novÃ¡ sieÅ¥* a *rybÃ¡rska sieÅ¥* sÃº Ãºplne odliÅ¡nÃ©. JednÃ½m zo spÃ´sobov, ako to zohÄ¾adniÅ¥, je vytvoriÅ¥ model na zÃ¡klade dvojÃ­c slov a povaÅ¾ovaÅ¥ dvojice slov za samostatnÃ© tokeny slovnÃ­ka. TÃ½mto spÃ´sobom bude veta *RÃ¡d chodÃ­m na ryby* reprezentovanÃ¡ nasledujÃºcou sekvenciou tokenov: *RÃ¡d chodÃ­m*, *chodÃ­m na*, *na ryby*. ProblÃ©mom tohto prÃ­stupu je, Å¾e veÄ¾kosÅ¥ slovnÃ­ka sa vÃ½razne zvÃ¤ÄÅ¡uje a kombinÃ¡cie ako *na ryby* a *na nÃ¡kupy* sÃº reprezentovanÃ© rÃ´znymi tokenmi, ktorÃ© nezdieÄ¾ajÃº Å¾iadnu sÃ©mantickÃº podobnosÅ¥ napriek rovnakÃ©mu slovesu.

V niektorÃ½ch prÃ­padoch mÃ´Å¾eme zvÃ¡Å¾iÅ¥ pouÅ¾itie tri-gramov â€“ kombinÃ¡ciÃ­ troch slov. Tento prÃ­stup sa preto Äasto nazÃ½va **n-gramy**. TieÅ¾ mÃ¡ zmysel pouÅ¾Ã­vaÅ¥ n-gramy s reprezentÃ¡ciou na Ãºrovni znakov, v takom prÃ­pade n-gramy pribliÅ¾ne zodpovedajÃº rÃ´znym slabikÃ¡m.

## Bag-of-Words a TF/IDF

Pri rieÅ¡enÃ­ Ãºloh, ako je klasifikÃ¡cia textu, potrebujeme byÅ¥ schopnÃ­ reprezentovaÅ¥ text jednÃ½m vektorom pevnej veÄ¾kosti, ktorÃ½ pouÅ¾ijeme ako vstup do koneÄnÃ©ho hustÃ©ho klasifikÃ¡tora. JednÃ½m z najjednoduchÅ¡Ã­ch spÃ´sobov, ako to dosiahnuÅ¥, je kombinovaÅ¥ vÅ¡etky individuÃ¡lne reprezentÃ¡cie slov, napr. ich sÄÃ­tanÃ­m. Ak sÄÃ­tame one-hot kÃ³dovania kaÅ¾dÃ©ho slova, skonÄÃ­me s vektorom frekvenciÃ­, ktorÃ½ ukazuje, koÄ¾kokrÃ¡t sa kaÅ¾dÃ© slovo v texte objavilo. TakÃ¡to reprezentÃ¡cia textu sa nazÃ½va **bag of words** (BoW).

<img src="images/bow.png" width="90%"/>

> ObrÃ¡zok od autora

BoW v podstate reprezentuje, ktorÃ© slovÃ¡ sa v texte objavujÃº a v akÃ½ch mnoÅ¾stvÃ¡ch, Äo mÃ´Å¾e byÅ¥ dobrÃ½m indikÃ¡torom toho, o Äom text je. NaprÃ­klad spravodajskÃ½ ÄlÃ¡nok o politike pravdepodobne obsahuje slovÃ¡ ako *prezident* a *krajina*, zatiaÄ¾ Äo vedeckÃ¡ publikÃ¡cia by mohla obsahovaÅ¥ slovÃ¡ ako *urÃ½chÄ¾ovaÄ*, *objavenÃ½* atÄ. Frekvencie slov mÃ´Å¾u teda v mnohÃ½ch prÃ­padoch byÅ¥ dobrÃ½m indikÃ¡torom obsahu textu.

ProblÃ©mom BoW je, Å¾e urÄitÃ© beÅ¾nÃ© slovÃ¡, ako *a*, *je* atÄ., sa objavujÃº vo vÃ¤ÄÅ¡ine textov a majÃº najvyÅ¡Å¡ie frekvencie, ÄÃ­m zakrÃ½vajÃº slovÃ¡, ktorÃ© sÃº skutoÄne dÃ´leÅ¾itÃ©. MÃ´Å¾eme znÃ­Å¾iÅ¥ dÃ´leÅ¾itosÅ¥ tÃ½chto slov tÃ½m, Å¾e zohÄ¾adnÃ­me frekvenciu, s akou sa slovÃ¡ vyskytujÃº v celej kolekcii dokumentov. Toto je hlavnÃ¡ myÅ¡lienka prÃ­stupu TF/IDF, ktorÃ½ je podrobnejÅ¡ie vysvetlenÃ½ v poznÃ¡mkovÃ½ch blokoch pripojenÃ½ch k tejto lekcii.

Å½iadny z tÃ½chto prÃ­stupov vÅ¡ak nedokÃ¡Å¾e plne zohÄ¾adniÅ¥ **sÃ©mantiku** textu. Na to potrebujeme vÃ½konnejÅ¡ie modely neurÃ³novÃ½ch sietÃ­, o ktorÃ½ch budeme diskutovaÅ¥ neskÃ´r v tejto sekcii.

## âœï¸ CviÄenia: ReprezentÃ¡cia textu

PokraÄujte vo svojom uÄenÃ­ v nasledujÃºcich poznÃ¡mkovÃ½ch blokoch:

* [ReprezentÃ¡cia textu s PyTorch](../../../../../lessons/5-NLP/13-TextRep/TextRepresentationPyTorch.ipynb)  
* [ReprezentÃ¡cia textu s TensorFlow](../../../../../lessons/5-NLP/13-TextRep/TextRepresentationTF.ipynb)  

## ZÃ¡ver

Doteraz sme Å¡tudovali techniky, ktorÃ© mÃ´Å¾u pridaÅ¥ vÃ¡hu frekvencie rÃ´znym slovÃ¡m. NedokÃ¡Å¾u vÅ¡ak reprezentovaÅ¥ vÃ½znam alebo poradie. Ako povedal slÃ¡vny lingvista J. R. Firth v roku 1935: "ÃšplnÃ½ vÃ½znam slova je vÅ¾dy kontextuÃ¡lny a Å¾iadna Å¡tÃºdia vÃ½znamu mimo kontextu nemÃ´Å¾e byÅ¥ branÃ¡ vÃ¡Å¾ne." NeskÃ´r v kurze sa nauÄÃ­me, ako zachytiÅ¥ kontextovÃ© informÃ¡cie z textu pomocou jazykovÃ©ho modelovania.

## ğŸš€ VÃ½zva

VyskÃºÅ¡ajte ÄalÅ¡ie cviÄenia s bag-of-words a rÃ´znymi dÃ¡tovÃ½mi modelmi. MÃ´Å¾ete sa inÅ¡pirovaÅ¥ touto [sÃºÅ¥aÅ¾ou na Kaggle](https://www.kaggle.com/competitions/word2vec-nlp-tutorial/overview/part-1-for-beginners-bag-of-words).

## [KvÃ­z po prednÃ¡Å¡ke](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/213)

## PrehÄ¾ad a samostatnÃ© Å¡tÃºdium

PrecviÄte si svoje zruÄnosti s textovÃ½mi embeddingmi a technikami bag-of-words na [Microsoft Learn](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-pytorch/?WT.mc_id=academic-77998-cacaste).

## [Ãšloha: PoznÃ¡mkovÃ© bloky](assignment.md)

**Upozornenie**:  
Tento dokument bol preloÅ¾enÃ½ pomocou sluÅ¾by AI prekladu [Co-op Translator](https://github.com/Azure/co-op-translator). Aj keÄ sa snaÅ¾Ã­me o presnosÅ¥, prosÃ­m, berte na vedomie, Å¾e automatizovanÃ© preklady mÃ´Å¾u obsahovaÅ¥ chyby alebo nepresnosti. PÃ´vodnÃ½ dokument v jeho rodnom jazyku by mal byÅ¥ povaÅ¾ovanÃ½ za autoritatÃ­vny zdroj. Pre kritickÃ© informÃ¡cie sa odporÃºÄa profesionÃ¡lny Ä¾udskÃ½ preklad. Nie sme zodpovednÃ­ za akÃ©koÄ¾vek nedorozumenia alebo nesprÃ¡vne interpretÃ¡cie vyplÃ½vajÃºce z pouÅ¾itia tohto prekladu.