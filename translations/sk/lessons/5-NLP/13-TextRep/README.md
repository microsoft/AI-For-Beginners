# Reprezent치cia textu ako tenzorov

## [Kv칤z pred predn치코kou](https://ff-quizzes.netlify.app/en/ai/quiz/25)

## Klasifik치cia textu

V prvej 캜asti tejto sekcie sa zameriame na 칰lohu **klasifik치cie textu**. Pou쬴jeme dataset [AG News](https://www.kaggle.com/amananandrai/ag-news-classification-dataset), ktor칳 obsahuje spravodajsk칠 캜l치nky ako napr칤klad:

* Kateg칩ria: Veda/Technol칩gie
* N치zov: Ky. spolo캜nos콘 z칤skala grant na 코t칰dium peptidov (AP)
* Text: AP - Spolo캜nos콘 zalo쬰n치 chemick칳m v칳skumn칤kom na Univerzite v Louisville z칤skala grant na v칳voj...

Na코칤m cie쬺m bude klasifikova콘 spravodajsk칳 캜l치nok do jednej z kateg칩ri칤 na z치klade textu.

## Reprezent치cia textu

Ak chceme rie코i콘 칰lohy spracovania prirodzen칠ho jazyka (NLP) pomocou neur칩nov칳ch siet칤, potrebujeme sp칪sob, ako reprezentova콘 text ako tenzory. Po캜칤ta캜e u reprezentuj칰 textov칠 znaky ako 캜칤sla, ktor칠 mapuj칰 na fonty na va코ej obrazovke pomocou k칩dovan칤, ako s칰 ASCII alebo UTF-8.

<img alt="Obr치zok zobrazuj칰ci diagram mapovania znaku na ASCII a bin치rnu reprezent치ciu" src="../../../../../translated_images/sk/ascii-character-map.18ed6aa7f3b0a7ff.webp" width="50%"/>

> [Zdroj obr치zku](https://www.seobility.net/en/wiki/ASCII)

Ako 쬿dia rozumieme, 캜o ka쬯칳 znak **reprezentuje**, a ako v코etky znaky spolu tvoria slov치 vo vete. Po캜칤ta캜e v코ak samy o sebe tak칠mu porozumeniu nemaj칰, a neur칩nov치 sie콘 sa mus칤 nau캜i콘 v칳znam po캜as tr칠ningu.

Preto m칪쬰me pou쬴콘 r칪zne pr칤stupy pri reprezent치cii textu:

* **Reprezent치cia na 칰rovni znakov**, ke캞 reprezentujeme text tak, 쬰 ka쬯칳 znak pova쬿jeme za 캜칤slo. Ak m치me *C* r칪znych znakov v na코om textovom korpuse, slovo *Hello* by bolo reprezentovan칠 ako tenzor 5x*C*. Ka쬯칠 p칤smeno by zodpovedalo st컄pcu tenzora v one-hot k칩dovan칤.
* **Reprezent치cia na 칰rovni slov**, pri ktorej vytvor칤me **slovn칤k** v코etk칳ch slov v na코om texte a potom reprezentujeme slov치 pomocou one-hot k칩dovania. Tento pr칤stup je o nie캜o lep코칤, preto쬰 ka쬯칠 p칤smeno samo o sebe nem치 ve쬶칳 v칳znam, a tak pou쬴t칤m vy코코칤ch semantick칳ch konceptov - slov - zjednodu코ujeme 칰lohu pre neur칩nov칰 sie콘. Av코ak vzh쬬dom na ve쬶os콘 slovn칤ka mus칤me pracova콘 s vysoko dimenzion치lnymi riedkymi tenzormi.

Bez oh쬬du na reprezent치ciu mus칤me najsk칪r konvertova콘 text na sekvenciu **tokenov**, pri캜om jeden token m칪쬰 by콘 znak, slovo alebo dokonca 캜as콘 slova. Potom token konvertujeme na 캜칤slo, zvy캜ajne pomocou **slovn칤ka**, a toto 캜칤slo m칪쬰 by콘 vlo쬰n칠 do neur칩novej siete pomocou one-hot k칩dovania.

## N-Gramy

V prirodzenom jazyku je presn칳 v칳znam slov mo쬹칠 ur캜i콘 iba v kontexte. Napr칤klad v칳znamy *neur칩nov치 sie콘* a *ryb치rska sie콘* s칰 칰plne odli코n칠. Jedn칳m zo sp칪sobov, ako to zoh쬬dni콘, je vytvori콘 model na z치klade dvoj칤c slov a pova쬺va콘 dvojice slov za samostatn칠 tokeny slovn칤ka. T칳mto sp칪sobom bude veta *R치d chod칤m na ryby* reprezentovan치 nasleduj칰cou sekvenciou tokenov: *R치d chod칤m*, *chod칤m na*, *na ryby*. Probl칠mom tohto pr칤stupu je, 쬰 ve쬶os콘 slovn칤ka sa v칳razne zv칛캜코uje a kombin치cie ako *na ryby* a *na n치kupy* s칰 reprezentovan칠 r칪znymi tokenmi, ktor칠 nezdie쬬j칰 쬴adnu semantick칰 podobnos콘 napriek rovnak칠mu slovesu.

V niektor칳ch pr칤padoch m칪쬰me zv치쬴콘 pou쬴tie tri-gramov -- kombin치ci칤 troch slov -- tie. Tento pr칤stup sa 캜asto naz칳va **n-gramy**. Tie m치 zmysel pou쮂셨a콘 n-gramy s reprezent치ciou na 칰rovni znakov, v takom pr칤pade n-gramy pribli쬹e zodpovedaj칰 r칪znym slabik치m.

## Bag-of-Words a TF/IDF

Pri rie코en칤 칰loh, ako je klasifik치cia textu, mus칤me by콘 schopn칤 reprezentova콘 text jedn칳m vektorom pevnej ve쬶osti, ktor칳 pou쬴jeme ako vstup pre kone캜n칳 hust칳 klasifik치tor. Jedn칳m z najjednoduch코칤ch sp칪sobov, ako to urobi콘, je kombinova콘 v코etky individu치lne reprezent치cie slov, napr. ich s캜칤tan칤m. Ak s캜칤tame one-hot k칩dovania ka쬯칠ho slova, skon캜칤me s vektorom frekvenci칤, ktor칳 ukazuje, ko쬶okr치t sa ka쬯칠 slovo objav칤 v texte. Tak치to reprezent치cia textu sa naz칳va **bag of words** (BoW).

<img src="../../../../../translated_images/sk/bow.3811869cff59368d.webp" width="90%"/>

> Obr치zok od autora

BoW v podstate reprezentuje, ktor칠 slov치 sa objavuj칰 v texte a v ak칳ch mno쬽tv치ch, 캜o m칪쬰 by콘 dobr칳m indik치torom toho, o 캜om text je. Napr칤klad spravodajsk칳 캜l치nok o politike pravdepodobne obsahuje slov치 ako *prezident* a *krajina*, zatia 캜o vedeck치 publik치cia by mala nie캜o ako *ur칳ch쬺va캜*, *objaven칠*, at캞. Frekvencie slov m칪쬿 v mnoh칳ch pr칤padoch by콘 dobr칳m indik치torom obsahu textu.

Probl칠mom BoW je, 쬰 ur캜it칠 be쬹칠 slov치, ako *a*, *je*, at캞., sa objavuj칰 vo v칛캜코ine textov a maj칰 najvy코코ie frekvencie, 캜칤m zakr칳vaj칰 slov치, ktor칠 s칰 skuto캜ne d칪le쬴t칠. M칪쬰me zn칤쬴콘 d칪le쬴tos콘 t칳chto slov t칳m, 쬰 zoh쬬dn칤me frekvenciu, s akou sa slov치 vyskytuj칰 v celej kolekcii dokumentov. Toto je hlavn치 my코lienka pr칤stupu TF/IDF, ktor칳 je podrobnej코ie pokryt칳 v notebookoch pripojen칳ch k tejto lekcii.

Av코ak 쬴adny z t칳chto pr칤stupov nedok치쬰 plne zoh쬬dni콘 **semantiku** textu. Na to potrebujeme v칳konnej코ie modely neur칩nov칳ch siet칤, o ktor칳ch budeme diskutova콘 nesk칪r v tejto sekcii.

## 九꽲잺 Cvi캜enia: Reprezent치cia textu

Pokra캜ujte vo svojom u캜en칤 v nasleduj칰cich notebookoch:

* [Reprezent치cia textu s PyTorch](TextRepresentationPyTorch.ipynb)
* [Reprezent치cia textu s TensorFlow](TextRepresentationTF.ipynb)

## Z치ver

Doteraz sme 코tudovali techniky, ktor칠 m칪쬿 prida콘 v치hu frekvencie r칪znym slov치m. Nie s칰 v코ak schopn칠 reprezentova콘 v칳znam alebo poradie. Ako povedal sl치vny lingvista J. R. Firth v roku 1935: "칔pln칳 v칳znam slova je v쬯y kontextov칳 a 쬴adna 코t칰dia v칳znamu mimo kontextu nem칪쬰 by콘 bran치 v치쬹e." Nesk칪r v kurze sa nau캜칤me, ako zachyti콘 kontextov칠 inform치cie z textu pomocou jazykov칠ho modelovania.

## 游 V칳zva

Vysk칰코ajte niektor칠 캞al코ie cvi캜enia s bag-of-words a r칪znymi d치tov칳mi modelmi. M칪쬰te sa in코pirova콘 touto [s칰콘a쬺u na Kaggle](https://www.kaggle.com/competitions/word2vec-nlp-tutorial/overview/part-1-for-beginners-bag-of-words)

## [Kv칤z po predn치코ke](https://ff-quizzes.netlify.app/en/ai/quiz/26)

## Preh쬬d & Samo코t칰dium

Precvi캜te si svoje zru캜nosti s textov칳mi embeddingmi a technikami bag-of-words na [Microsoft Learn](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-pytorch/?WT.mc_id=academic-77998-cacaste)

## [칔loha: Notebooky](assignment.md)

---

