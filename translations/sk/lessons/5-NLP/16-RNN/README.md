# Rekurentn√© neur√≥nov√© siete

## [Kv√≠z pred predn√°≈°kou](https://ff-quizzes.netlify.app/en/ai/quiz/31)

V predch√°dzaj√∫cich sekci√°ch sme pou≈æ√≠vali bohat√© s√©mantick√© reprezent√°cie textu a jednoduch√Ω line√°rny klasifik√°tor nad embeddingami. T√°to architekt√∫ra zachyt√°va agregovan√Ω v√Ωznam slov vo vete, ale neberie do √∫vahy **poradie** slov, preto≈æe oper√°cia agreg√°cie nad embeddingami odstr√°nila t√∫to inform√°ciu z p√¥vodn√©ho textu. Keƒè≈æe tieto modely nedok√°≈æu modelova≈• poradie slov, nem√¥≈æu rie≈°i≈• zlo≈æitej≈°ie alebo nejednoznaƒçn√© √∫lohy, ako je generovanie textu alebo odpovedanie na ot√°zky.

Na zachytenie v√Ωznamu textovej sekvencie potrebujeme pou≈æi≈• in√∫ architekt√∫ru neur√≥novej siete, ktor√° sa naz√Ωva **rekurentn√° neur√≥nov√° sie≈•** alebo RNN. V RNN prech√°dzame vetou cez sie≈• jeden symbol po druhom a sie≈• produkuje urƒçit√Ω **stav**, ktor√Ω n√°sledne posunieme do siete spolu s ƒèal≈°√≠m symbolom.

![RNN](../../../../../translated_images/sk/rnn.27f5c29c53d727b5.webp)

> Obr√°zok od autora

Pri danej vstupnej sekvencii tokenov X<sub>0</sub>,...,X<sub>n</sub>, RNN vytv√°ra sekvenciu blokov neur√≥novej siete a tr√©nuje t√∫to sekvenciu end-to-end pomocou backpropag√°cie. Ka≈æd√Ω blok siete prij√≠ma dvojicu (X<sub>i</sub>,S<sub>i</sub>) ako vstup a produkuje S<sub>i+1</sub> ako v√Ωsledok. Koneƒçn√Ω stav S<sub>n</sub> alebo (v√Ωstup Y<sub>n</sub>) ide do line√°rneho klasifik√°tora, ktor√Ω produkuje v√Ωsledok. V≈°etky bloky siete zdieƒæaj√∫ rovnak√© v√°hy a s√∫ tr√©novan√© end-to-end pomocou jednej backpropagaƒçnej pas√°≈æe.

Keƒè≈æe stavov√© vektory S<sub>0</sub>,...,S<sub>n</sub> prech√°dzaj√∫ sie≈•ou, dok√°≈æe sa nauƒçi≈• sekvenƒçn√© z√°vislosti medzi slovami. Napr√≠klad, keƒè sa v sekvencii objav√≠ slovo *nie*, sie≈• sa m√¥≈æe nauƒçi≈• negova≈• urƒçit√© prvky v stavovom vektore, ƒço vedie k neg√°cii.

> ‚úÖ Keƒè≈æe v√°hy v≈°etk√Ωch blokov RNN na obr√°zku vy≈°≈°ie s√∫ zdieƒæan√©, rovnak√Ω obr√°zok m√¥≈æe by≈• reprezentovan√Ω ako jeden blok (vpravo) s rekurentnou sp√§tnou v√§zbou, ktor√° pos√∫va v√Ωstupn√Ω stav siete sp√§≈• na vstup.

## Anat√≥mia bunky RNN

Pozrime sa, ako je organizovan√° jednoduch√° bunka RNN. Prij√≠ma predch√°dzaj√∫ci stav S<sub>i-1</sub> a aktu√°lny symbol X<sub>i</sub> ako vstupy a mus√≠ produkova≈• v√Ωstupn√Ω stav S<sub>i</sub> (a niekedy n√°s zauj√≠ma aj in√Ω v√Ωstup Y<sub>i</sub>, ako v pr√≠pade generat√≠vnych siet√≠).

Jednoduch√° bunka RNN m√° vo vn√∫tri dve matice v√°h: jedna transformuje vstupn√Ω symbol (nazvime ju W) a druh√° transformuje vstupn√Ω stav (H). V tomto pr√≠pade sa v√Ωstup siete vypoƒç√≠ta ako &sigma;(W&times;X<sub>i</sub>+H&times;S<sub>i-1</sub>+b), kde &sigma; je aktivaƒçn√° funkcia a b je dodatoƒçn√° bias.

<img alt="Anat√≥mia bunky RNN" src="../../../../../translated_images/sk/rnn-anatomy.79ee3f3920b3294b.webp" width="50%"/>

> Obr√°zok od autora

V mnoh√Ωch pr√≠padoch s√∫ vstupn√© tokeny pred vstupom do RNN prech√°dzan√© cez embedding vrstvu na zn√≠≈æenie dimenzionality. V tomto pr√≠pade, ak je dimenzia vstupn√Ωch vektorov *emb_size* a stavov√Ω vektor je *hid_size* - veƒækos≈• W je *emb_size*&times;*hid_size* a veƒækos≈• H je *hid_size*&times;*hid_size*.

## Long Short Term Memory (LSTM)

Jedn√Ωm z hlavn√Ωch probl√©mov klasick√Ωch RNN je tzv. **probl√©m mizn√∫cich gradientov**. Keƒè≈æe RNN s√∫ tr√©novan√© end-to-end v jednej backpropagaƒçnej pas√°≈æi, maj√∫ probl√©m propagova≈• chybu do prv√Ωch vrstiev siete, a teda sie≈• nedok√°≈æe nauƒçi≈• vz≈•ahy medzi vzdialen√Ωmi tokenmi. Jedn√Ωm zo sp√¥sobov, ako sa tomuto probl√©mu vyhn√∫≈•, je zavedenie **explicitn√©ho riadenia stavu** pomocou tzv. **br√°n**. Existuj√∫ dve zn√°me architekt√∫ry tohto typu: **Long Short Term Memory** (LSTM) a **Gated Relay Unit** (GRU).

![Obr√°zok zobrazuj√∫ci pr√≠klad bunky LSTM](../../../../../lessons/5-NLP/16-RNN/images/long-short-term-memory-cell.svg)

> Zdroj obr√°zku TBD

LSTM sie≈• je organizovan√° podobne ako RNN, ale existuj√∫ dva stavy, ktor√© sa pren√°≈°aj√∫ z vrstvy do vrstvy: aktu√°lny stav C a skryt√Ω vektor H. V ka≈ædej jednotke je skryt√Ω vektor H<sub>i</sub> spojen√Ω so vstupom X<sub>i</sub>, a tieto kontroluj√∫, ƒço sa stane so stavom C prostredn√≠ctvom **br√°n**. Ka≈æd√° br√°na je neur√≥nov√° sie≈• so sigmoidnou aktiv√°ciou (v√Ωstup v rozsahu [0,1]), ktor√∫ mo≈æno pova≈æova≈• za bitov√∫ masku pri n√°soben√≠ stavov√Ωm vektorom. Existuj√∫ nasleduj√∫ce br√°ny (zƒæava doprava na obr√°zku vy≈°≈°ie):

* **Br√°na zabudnutia** prij√≠ma skryt√Ω vektor a urƒçuje, ktor√© komponenty vektora C je potrebn√© zabudn√∫≈• a ktor√© prenies≈• ƒèalej.
* **Vstupn√° br√°na** prij√≠ma inform√°cie zo vstupn√©ho a skryt√©ho vektora a vklad√° ich do stavu.
* **V√Ωstupn√° br√°na** transformuje stav cez line√°rnu vrstvu s *tanh* aktiv√°ciou, potom vyber√° niektor√© z jeho komponentov pomocou skryt√©ho vektora H<sub>i</sub>, aby produkovala nov√Ω stav C<sub>i+1</sub>.

Komponenty stavu C mo≈æno pova≈æova≈• za urƒçit√© pr√≠znaky, ktor√© mo≈æno zapn√∫≈• a vypn√∫≈•. Napr√≠klad, keƒè v sekvencii naraz√≠me na meno *Alice*, m√¥≈æeme predpoklada≈•, ≈æe ide o ≈æensk√∫ postavu, a nastavi≈• pr√≠znak v stave, ≈æe m√°me ≈æensk√© podstatn√© meno vo vete. Keƒè ƒèalej naraz√≠me na fr√°zu *a Tom*, nastav√≠me pr√≠znak, ≈æe m√°me mno≈æn√© ƒç√≠slo. T√Ωmto sp√¥sobom manipul√°ciou so stavom m√¥≈æeme √∫dajne sledova≈• gramatick√© vlastnosti ƒçast√≠ vety.

> ‚úÖ Skvel√Ωm zdrojom na pochopenie vn√∫torn√©ho fungovania LSTM je tento v√Ωborn√Ω ƒçl√°nok [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) od Christophera Olaha.

## Obojstrann√© a viacvrstvov√© RNN

Diskutovali sme o rekurentn√Ωch sie≈•ach, ktor√© funguj√∫ jedn√Ωm smerom, od zaƒçiatku sekvencie po jej koniec. Vyzer√° to prirodzene, preto≈æe to pripom√≠na sp√¥sob, ak√Ωm ƒç√≠tame a poƒç√∫vame reƒç. Av≈°ak, keƒè≈æe v mnoh√Ωch praktick√Ωch pr√≠padoch m√°me n√°hodn√Ω pr√≠stup k vstupnej sekvencii, m√¥≈æe d√°va≈• zmysel spusti≈• rekurentn√© v√Ωpoƒçty v oboch smeroch. Tak√©to siete sa naz√Ωvaj√∫ **obojstrann√©** RNN. Pri pr√°ci s obojstrannou sie≈•ou by sme potrebovali dva skryt√© stavov√© vektory, jeden pre ka≈æd√Ω smer.

Rekurentn√° sie≈•, ƒçi u≈æ jednosmern√° alebo obojstrann√°, zachyt√°va urƒçit√© vzory v r√°mci sekvencie a m√¥≈æe ich ulo≈æi≈• do stavov√©ho vektora alebo prenies≈• do v√Ωstupu. Rovnako ako pri konvoluƒçn√Ωch sie≈•ach, m√¥≈æeme na prv√∫ vrstvu postavi≈• ƒèal≈°iu rekurentn√∫ vrstvu, aby sme zachytili vzory vy≈°≈°ej √∫rovne a stavali na vzoroch ni≈æ≈°ej √∫rovne extrahovan√Ωch prvou vrstvou. To n√°s priv√°dza k pojmu **viacvrstvov√° RNN**, ktor√° pozost√°va z dvoch alebo viacer√Ωch rekurentn√Ωch siet√≠, kde v√Ωstup predch√°dzaj√∫cej vrstvy je posunut√Ω do ƒèal≈°ej vrstvy ako vstup.

![Obr√°zok zobrazuj√∫ci viacvrstvov√∫ LSTM RNN](../../../../../translated_images/sk/multi-layer-lstm.dd975e29bb2a59fe.webp)

*Obr√°zok z [tohto skvel√©ho pr√≠spevku](https://towardsdatascience.com/from-a-lstm-cell-to-a-multilayer-lstm-network-with-pytorch-2899eb5696f3) od Fernanda L√≥peza*

## ‚úçÔ∏è Cviƒçenia: Embeddingy

Pokraƒçujte vo svojom uƒçen√≠ v nasleduj√∫cich notebookoch:

* [RNNs s PyTorch](RNNPyTorch.ipynb)
* [RNNs s TensorFlow](RNNTF.ipynb)

## Z√°ver

V tejto jednotke sme videli, ≈æe RNN m√¥≈æu by≈• pou≈æit√© na klasifik√°ciu sekvenci√≠, ale v skutoƒçnosti dok√°≈æu zvl√°dnu≈• oveƒæa viac √∫loh, ako je generovanie textu, strojov√Ω preklad a ƒèal≈°ie. Tieto √∫lohy budeme sk√∫ma≈• v ƒèal≈°ej jednotke.

## üöÄ V√Ωzva

Preƒç√≠tajte si literat√∫ru o LSTM a zv√°≈æte ich aplik√°cie:

- [Grid Long Short-Term Memory](https://arxiv.org/pdf/1507.01526v1.pdf)
- [Show, Attend and Tell: Neural Image Caption
Generation with Visual Attention](https://arxiv.org/pdf/1502.03044v2.pdf)

## [Kv√≠z po predn√°≈°ke](https://ff-quizzes.netlify.app/en/ai/quiz/32)

## Prehƒæad a samostatn√© ≈°t√∫dium

- [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) od Christophera Olaha.

## [√öloha: Notebooky](assignment.md)

---

