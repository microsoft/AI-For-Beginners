<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "58bf4adb210aab53e8f78c8082040e7c",
  "translation_date": "2025-08-25T21:32:45+00:00",
  "source_file": "lessons/5-NLP/16-RNN/README.md",
  "language_code": "sk"
}
-->
# RekurentnÃ© neurÃ³novÃ© siete

## [KvÃ­z pred prednÃ¡Å¡kou](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/116)

V predchÃ¡dzajÃºcich sekciÃ¡ch sme pouÅ¾Ã­vali bohatÃ© sÃ©mantickÃ© reprezentÃ¡cie textu a jednoduchÃ½ lineÃ¡rny klasifikÃ¡tor nad embeddingmi. TÃ¡to architektÃºra zachytÃ¡va agregovanÃ½ vÃ½znam slov vo vete, ale neberie do Ãºvahy **poradie** slov, pretoÅ¾e operÃ¡cia agregÃ¡cie nad embeddingmi tÃºto informÃ¡ciu z pÃ´vodnÃ©ho textu odstrÃ¡nila. KeÄÅ¾e tieto modely nedokÃ¡Å¾u modelovaÅ¥ poradie slov, nemÃ´Å¾u rieÅ¡iÅ¥ zloÅ¾itejÅ¡ie alebo nejednoznaÄnÃ© Ãºlohy, ako je generovanie textu alebo odpovedanie na otÃ¡zky.

Na zachytenie vÃ½znamu textovej sekvencie potrebujeme pouÅ¾iÅ¥ inÃº architektÃºru neurÃ³novej siete, ktorÃ¡ sa nazÃ½va **rekurentnÃ¡ neurÃ³novÃ¡ sieÅ¥** alebo RNN. V RNN prechÃ¡dzame vetou cez sieÅ¥ jeden symbol po druhom a sieÅ¥ produkuje urÄitÃ½ **stav**, ktorÃ½ nÃ¡sledne odovzdÃ¡vame s ÄalÅ¡Ã­m symbolom spÃ¤Å¥ do siete.

![RNN](../../../../../translated_images/rnn.27f5c29c53d727b546ad3961637a267f0fe9ec5ab01f2a26a853c92fcefbb574.sk.png)

> ObrÃ¡zok od autora

Pri danej vstupnej sekvencii tokenov X<sub>0</sub>,...,X<sub>n</sub> RNN vytvÃ¡ra sekvenciu blokov neurÃ³novej siete a trÃ©nuje tÃºto sekvenciu end-to-end pomocou backpropagÃ¡cie. KaÅ¾dÃ½ blok siete prijÃ­ma dvojicu (X<sub>i</sub>,S<sub>i</sub>) ako vstup a produkuje S<sub>i+1</sub> ako vÃ½sledok. KoneÄnÃ½ stav S<sub>n</sub> (alebo vÃ½stup Y<sub>n</sub>) prechÃ¡dza do lineÃ¡rneho klasifikÃ¡tora, aby sa vytvoril vÃ½sledok. VÅ¡etky bloky siete zdieÄ¾ajÃº rovnakÃ© vÃ¡hy a sÃº trÃ©novanÃ© end-to-end pomocou jednÃ©ho prechodu backpropagÃ¡cie.

KeÄÅ¾e stavovÃ© vektory S<sub>0</sub>,...,S<sub>n</sub> prechÃ¡dzajÃº sieÅ¥ou, dokÃ¡Å¾e sa nauÄiÅ¥ sekvenÄnÃ© zÃ¡vislosti medzi slovami. NaprÃ­klad, keÄ sa niekde v sekvencii objavÃ­ slovo *not*, sieÅ¥ sa mÃ´Å¾e nauÄiÅ¥ negovaÅ¥ urÄitÃ© prvky v stavovom vektore, Äo vedie k negÃ¡cii.

> âœ… KeÄÅ¾e vÃ¡hy vÅ¡etkÃ½ch RNN blokov na obrÃ¡zku vyÅ¡Å¡ie sÃº zdieÄ¾anÃ©, ten istÃ½ obrÃ¡zok mÃ´Å¾e byÅ¥ reprezentovanÃ½ ako jeden blok (vpravo) s rekurentnou spÃ¤tnou vÃ¤zbou, ktorÃ¡ odovzdÃ¡va vÃ½stupnÃ½ stav siete spÃ¤Å¥ na vstup.

## AnatÃ³mia RNN bunky

Pozrime sa, ako je organizovanÃ¡ jednoduchÃ¡ RNN bunka. PrijÃ­ma predchÃ¡dzajÃºci stav S<sub>i-1</sub> a aktuÃ¡lny symbol X<sub>i</sub> ako vstupy a musÃ­ produkovaÅ¥ vÃ½stupnÃ½ stav S<sub>i</sub> (a niekedy nÃ¡s zaujÃ­ma aj inÃ½ vÃ½stup Y<sub>i</sub>, ako v prÃ­pade generatÃ­vnych sietÃ­).

JednoduchÃ¡ RNN bunka mÃ¡ vo vnÃºtri dve matice vÃ¡h: jedna transformuje vstupnÃ½ symbol (nazvime ju W) a druhÃ¡ transformuje vstupnÃ½ stav (H). V tomto prÃ­pade sa vÃ½stup siete vypoÄÃ­ta ako Ïƒ(WÃ—X<sub>i</sub>+HÃ—S<sub>i-1</sub>+b), kde Ïƒ je aktivaÄnÃ¡ funkcia a b je dodatoÄnÃ½ bias.

<img alt="AnatÃ³mia RNN bunky" src="images/rnn-anatomy.png" width="50%"/>

> ObrÃ¡zok od autora

V mnohÃ½ch prÃ­padoch prechÃ¡dzajÃº vstupnÃ© tokeny pred vstupom do RNN cez embedding vrstvu, aby sa znÃ­Å¾ila dimenzionalita. V tomto prÃ­pade, ak je dimenzia vstupnÃ½ch vektorov *emb_size* a stavovÃ©ho vektora *hid_size*, veÄ¾kosÅ¥ W je *emb_size*Ã—*hid_size* a veÄ¾kosÅ¥ H je *hid_size*Ã—*hid_size*.

## Long Short Term Memory (LSTM)

JednÃ½m z hlavnÃ½ch problÃ©mov klasickÃ½ch RNN je tzv. problÃ©m **miznÃºcich gradientov**. KeÄÅ¾e RNN sÃº trÃ©novanÃ© end-to-end v jednom prechode backpropagÃ¡cie, majÃº problÃ©m s propagÃ¡ciou chyby do prvÃ½ch vrstiev siete, a preto sa sieÅ¥ nedokÃ¡Å¾e nauÄiÅ¥ vzÅ¥ahy medzi vzdialenÃ½mi tokenmi. JednÃ½m zo spÃ´sobov, ako sa tomuto problÃ©mu vyhnÃºÅ¥, je zavedenie **explicitnÃ©ho riadenia stavu** pomocou tzv. **brÃ¡n**. ExistujÃº dve znÃ¡me architektÃºry tohto typu: **Long Short Term Memory** (LSTM) a **Gated Relay Unit** (GRU).

![ObrÃ¡zok zobrazujÃºci prÃ­klad bunky LSTM](../../../../../lessons/5-NLP/16-RNN/images/long-short-term-memory-cell.svg)

> Zdroj obrÃ¡zku TBD

LSTM sieÅ¥ je organizovanÃ¡ podobne ako RNN, ale existujÃº dva stavy, ktorÃ© sa prenÃ¡Å¡ajÃº z vrstvy do vrstvy: aktuÃ¡lny stav C a skrytÃ½ vektor H. V kaÅ¾dej jednotke je skrytÃ½ vektor H<sub>i</sub> spojenÃ½ so vstupom X<sub>i</sub> a tieto kontrolujÃº, Äo sa deje so stavom C prostrednÃ­ctvom **brÃ¡n**. KaÅ¾dÃ¡ brÃ¡na je neurÃ³novÃ¡ sieÅ¥ so sigmoidnou aktivÃ¡ciou (vÃ½stup v rozsahu [0,1]), ktorÃº si mÃ´Å¾eme predstaviÅ¥ ako bitovÃº masku, keÄ sa vynÃ¡sobÃ­ stavovÃ½m vektorom. ExistujÃº nasledujÃºce brÃ¡ny (zÄ¾ava doprava na obrÃ¡zku vyÅ¡Å¡ie):

* **Forget gate** (brÃ¡na zabÃºdania) prijÃ­ma skrytÃ½ vektor a urÄuje, ktorÃ© komponenty vektora C je potrebnÃ© zabudnÃºÅ¥ a ktorÃ© preniesÅ¥ Äalej.
* **Input gate** (vstupnÃ¡ brÃ¡na) prijÃ­ma informÃ¡cie zo vstupnÃ©ho a skrytÃ©ho vektora a vkladÃ¡ ich do stavu.
* **Output gate** (vÃ½stupnÃ¡ brÃ¡na) transformuje stav cez lineÃ¡rnu vrstvu s *tanh* aktivÃ¡ciou, potom vyberÃ¡ niektorÃ© z jeho komponentov pomocou skrytÃ©ho vektora H<sub>i</sub>, aby vytvorila novÃ½ stav C<sub>i+1</sub>.

Komponenty stavu C si mÃ´Å¾eme predstaviÅ¥ ako urÄitÃ© prÃ­znaky, ktorÃ© mÃ´Å¾u byÅ¥ zapnutÃ© alebo vypnutÃ©. NaprÃ­klad, keÄ v sekvencii narazÃ­me na meno *Alice*, mÃ´Å¾eme predpokladaÅ¥, Å¾e ide o Å¾enskÃº postavu, a zvÃ½Å¡iÅ¥ prÃ­znak v stave, Å¾e vo vete mÃ¡me Å¾enskÃ© podstatnÃ© meno. KeÄ Äalej narazÃ­me na frÃ¡zu *and Tom*, zvÃ½Å¡ime prÃ­znak, Å¾e mÃ¡me mnoÅ¾nÃ© ÄÃ­slo. Takto mÃ´Å¾eme manipulÃ¡ciou so stavom Ãºdajne sledovaÅ¥ gramatickÃ© vlastnosti ÄastÃ­ vety.

> âœ… SkvelÃ½m zdrojom na pochopenie vnÃºtornÃ©ho fungovania LSTM je tento vÃ½bornÃ½ ÄlÃ¡nok [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) od Christophera Olaha.

## BidirekÄnÃ© a viacvrstvovÃ© RNN

Diskutovali sme o rekurentnÃ½ch sieÅ¥ach, ktorÃ© fungujÃº jednÃ½m smerom, od zaÄiatku sekvencie po jej koniec. VyzerÃ¡ to prirodzene, pretoÅ¾e to pripomÃ­na spÃ´sob, akÃ½m ÄÃ­tame a poÄÃºvame reÄ. AvÅ¡ak, keÄÅ¾e v mnohÃ½ch praktickÃ½ch prÃ­padoch mÃ¡me nÃ¡hodnÃ½ prÃ­stup k vstupnej sekvencii, mÃ´Å¾e maÅ¥ zmysel spustiÅ¥ rekurentnÃ© vÃ½poÄty v oboch smeroch. TakÃ©to siete sa nazÃ½vajÃº **bidirekÄnÃ©** RNN. Pri prÃ¡ci s bidirekÄnou sieÅ¥ou by sme potrebovali dva skrytÃ© stavovÃ© vektory, jeden pre kaÅ¾dÃ½ smer.

RekurentnÃ¡ sieÅ¥, Äi uÅ¾ jednosmernÃ¡ alebo bidirekÄnÃ¡, zachytÃ¡va urÄitÃ© vzory v sekvencii a mÃ´Å¾e ich uloÅ¾iÅ¥ do stavovÃ©ho vektora alebo odovzdaÅ¥ do vÃ½stupu. Rovnako ako pri konvoluÄnÃ½ch sieÅ¥ach mÃ´Å¾eme na prvÃº vrstvu postaviÅ¥ ÄalÅ¡iu rekurentnÃº vrstvu, aby sme zachytili vzory vyÅ¡Å¡ej Ãºrovne a vytvorili ich z nÃ­zkoÃºrovÅˆovÃ½ch vzorov extrahovanÃ½ch prvou vrstvou. To nÃ¡s privÃ¡dza k pojmu **viacvrstvovÃ¡ RNN**, ktorÃ¡ pozostÃ¡va z dvoch alebo viacerÃ½ch rekurentnÃ½ch sietÃ­, kde vÃ½stup predchÃ¡dzajÃºcej vrstvy prechÃ¡dza do ÄalÅ¡ej vrstvy ako vstup.

![ObrÃ¡zok zobrazujÃºci viacvrstvovÃº LSTM RNN](../../../../../translated_images/multi-layer-lstm.dd975e29bb2a59fe58b429db833932d734c81f211cad2783797a9608984acb8c.sk.jpg)

*ObrÃ¡zok z [tohto skvelÃ©ho ÄlÃ¡nku](https://towardsdatascience.com/from-a-lstm-cell-to-a-multilayer-lstm-network-with-pytorch-2899eb5696f3) od Fernanda LÃ³peza*

## âœï¸ CviÄenia: Embeddingy

PokraÄujte vo svojom uÄenÃ­ v nasledujÃºcich notebookoch:

* [RNNs s PyTorch](../../../../../lessons/5-NLP/16-RNN/RNNPyTorch.ipynb)
* [RNNs s TensorFlow](../../../../../lessons/5-NLP/16-RNN/RNNTF.ipynb)

## ZÃ¡ver

V tejto jednotke sme videli, Å¾e RNN mÃ´Å¾u byÅ¥ pouÅ¾itÃ© na klasifikÃ¡ciu sekvenciÃ­, ale v skutoÄnosti dokÃ¡Å¾u zvlÃ¡dnuÅ¥ oveÄ¾a viac Ãºloh, ako je generovanie textu, strojovÃ½ preklad a ÄalÅ¡ie. TÃ½mto ÃºlohÃ¡m sa budeme venovaÅ¥ v ÄalÅ¡ej jednotke.

## ğŸš€ VÃ½zva

PreÄÃ­tajte si niektorÃ© materiÃ¡ly o LSTM a zvÃ¡Å¾te ich aplikÃ¡cie:

- [Grid Long Short-Term Memory](https://arxiv.org/pdf/1507.01526v1.pdf)
- [Show, Attend and Tell: Neural Image Caption
Generation with Visual Attention](https://arxiv.org/pdf/1502.03044v2.pdf)

## [KvÃ­z po prednÃ¡Å¡ke](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/216)

## PrehÄ¾ad a samostatnÃ© Å¡tÃºdium

- [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) od Christophera Olaha.

## [Ãšloha: Notebooky](assignment.md)

**Upozornenie**:  
Tento dokument bol preloÅ¾enÃ½ pomocou sluÅ¾by AI prekladu [Co-op Translator](https://github.com/Azure/co-op-translator). Aj keÄ sa snaÅ¾Ã­me o presnosÅ¥, prosÃ­m, berte na vedomie, Å¾e automatizovanÃ© preklady mÃ´Å¾u obsahovaÅ¥ chyby alebo nepresnosti. PÃ´vodnÃ½ dokument v jeho rodnom jazyku by mal byÅ¥ povaÅ¾ovanÃ½ za autoritatÃ­vny zdroj. Pre kritickÃ© informÃ¡cie sa odporÃºÄa profesionÃ¡lny Ä¾udskÃ½ preklad. Nie sme zodpovednÃ­ za akÃ©koÄ¾vek nedorozumenia alebo nesprÃ¡vne interpretÃ¡cie vyplÃ½vajÃºce z pouÅ¾itia tohto prekladu.