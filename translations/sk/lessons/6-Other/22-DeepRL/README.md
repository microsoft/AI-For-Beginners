# Hlbok칠 posil켿ovacie u캜enie

Posil켿ovacie u캜enie (RL) je pova쬺van칠 za jeden zo z치kladn칳ch paradigmov strojov칠ho u캜enia, ved쬬 u캜enia s u캜ite쬺m a u캜enia bez u캜ite쬬. Zatia 캜o pri u캜en칤 s u캜ite쬺m sa spoliehame na dataset so zn치mymi v칳sledkami, RL je zalo쬰n칠 na **u캜en칤 sa prostredn칤ctvom sk칰senost칤**. Napr칤klad, ke캞 prv칳kr치t vid칤me po캜칤ta캜ov칰 hru, za캜neme ju hra콘, aj ke캞 nepozn치me pravidl치, a 캜oskoro dok치쬰me zlep코i콘 svoje schopnosti len procesom hrania a prisp칪sobovania svojho spr치vania.

## [Kv칤z pred predn치코kou](https://ff-quizzes.netlify.app/en/ai/quiz/43)

Na vykonanie RL potrebujeme:

* **Prostredie** alebo **simul치tor**, ktor칳 nastavuje pravidl치 hry. Mali by sme by콘 schopn칤 vykon치va콘 experimenty v simul치tore a pozorova콘 v칳sledky.
* **Funkciu odmeny**, ktor치 nazna캜uje, ako 칰spe코n칳 bol n치코 experiment. V pr칤pade u캜enia sa hra콘 po캜칤ta캜ov칰 hru by odmenou bolo na코e kone캜n칠 sk칩re.

Na z치klade funkcie odmeny by sme mali by콘 schopn칤 prisp칪sobi콘 svoje spr치vanie a zlep코i콘 svoje schopnosti, aby sme nabud칰ce hrali lep코ie. Hlavn칳 rozdiel medzi in칳mi typmi strojov칠ho u캜enia a RL je, 쬰 pri RL zvy캜ajne nevieme, 캜i vyhr치me alebo prehr치me, k칳m nedokon캜칤me hru. Preto nem칪쬰me poveda콘, 캜i je ur캜it칳 krok s치m o sebe dobr칳 alebo nie - odmenu dostaneme a na konci hry.

Po캜as RL zvy캜ajne vykon치vame mnoho experimentov. Po캜as ka쬯칠ho experimentu mus칤me vyv치쬴콘 medzi nasledovan칤m optim치lnej strat칠gie, ktor칰 sme sa doteraz nau캜ili (**exploat치cia**), a sk칰man칤m nov칳ch mo쬹칳ch stavov (**explor치cia**).

## OpenAI Gym

Skvel칳m n치strojom pre RL je [OpenAI Gym](https://gym.openai.com/) - **simula캜n칠 prostredie**, ktor칠 dok치쬰 simulova콘 mnoho r칪znych prostred칤, od hier Atari a po fyziku za balansovan칤m ty캜e. Je to jedno z najpopul치rnej코칤ch simula캜n칳ch prostred칤 na tr칠novanie algoritmov posil켿ovacieho u캜enia a je udr쬴avan칠 spolo캜nos콘ou [OpenAI](https://openai.com/).

> **Note**: V코etky dostupn칠 prostredia z OpenAI Gym si m칪쬰te pozrie콘 [tu](https://gym.openai.com/envs/#classic_control).

## Balansovanie CartPole

Pravdepodobne ste u videli modern칠 balansovacie zariadenia, ako napr칤klad *Segway* alebo *Gyrosk칰tre*. Dok치쬿 automaticky balansova콘 t칳m, 쬰 upravuj칰 svoje koles치 na z치klade sign치lu z akcelerometra alebo gyroskopu. V tejto sekcii sa nau캜칤me, ako vyrie코i콘 podobn칳 probl칠m - balansovanie ty캜e. Je to podobn칠 situ치cii, ke캞 cirkusov칳 umelec potrebuje balansova콘 ty캜 na svojej ruke - ale toto balansovanie ty캜e sa odohr치va iba v 1D.

Zjednodu코en치 verzia balansovania je zn치ma ako probl칠m **CartPole**. Vo svete CartPole m치me horizont치lny pos칰va캜, ktor칳 sa m칪쬰 pohybova콘 do쬬va alebo doprava, a cie쬺m je balansova콘 vertik치lnu ty캜 na vrchu pos칰va캜a, ke캞 sa pohybuje.

<img alt="cartpole" src="../../../../../translated_images/sk/cartpole.f52a67f27e058170.webp" width="200"/>

Na vytvorenie a pou쬴tie tohto prostredia potrebujeme nieko쬶o riadkov k칩du v Pythone:

```python
import gym
env = gym.make("CartPole-v1")

env.reset()
done = False
total_reward = 0
while not done:
   env.render()
   action = env.action_space.sample()
   observaton, reward, done, info = env.step(action)
   total_reward += reward

print(f"Total reward: {total_reward}")
```

Ka쬯칠 prostredie je mo쬹칠 pristupova콘 presne rovnak칳m sp칪sobom:
* `env.reset` spust칤 nov칳 experiment
* `env.step` vykon치 simula캜n칳 krok. Prij칤ma **akciu** z **ak캜n칠ho priestoru** a vracia **pozorovanie** (z pozorovacieho priestoru), ako aj odmenu a pr칤znak ukon캜enia.

V pr칤klade vy코코ie vykon치vame n치hodn칰 akciu pri ka쬯om kroku, 캜o je d칪vod, pre캜o je 쬴vot experimentu ve쬸i kr치tky:

![nebalansuj칰ci cartpole](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-nobalance.gif)

Cie쬺m RL algoritmu je vytr칠nova콘 model - tzv. **politiku** &pi; - ktor치 bude vraca콘 akciu v reakcii na dan칳 stav. Politiku m칪쬰me tie pova쬺va콘 za pravdepodobnostn칰, napr. pre ak칳ko쭀ek stav *s* a akciu *a* bude vraca콘 pravdepodobnos콘 &pi;(*a*|*s*), 쬰 by sme mali vykona콘 *a* v stave *s*.

## Algoritmus Policy Gradients

Najzjavnej코칤 sp칪sob, ako modelova콘 politiku, je vytvori콘 neur칩nov칰 sie콘, ktor치 bude bra콘 stavy ako vstup a vraca콘 zodpovedaj칰ce akcie (alebo sk칪r pravdepodobnosti v코etk칳ch akci칤). V istom zmysle by to bolo podobn칠 be쬹ej klasifika캜nej 칰lohe, s jedn칳m hlavn칳m rozdielom - vopred nevieme, ktor칠 akcie by sme mali vykona콘 pri ka쬯om kroku.

My코lienka je tu odhadn칰콘 tieto pravdepodobnosti. Vytvor칤me vektor **kumulat칤vnych odmien**, ktor칳 ukazuje na코u celkov칰 odmenu pri ka쬯om kroku experimentu. Tie aplikujeme **diskontovanie odmien** n치soben칤m skor코칤ch odmien ur캜it칳m koeficientom &gamma;=0.99, aby sme zn칤쬴li v칳znam skor코칤ch odmien. Potom posiln칤me tie kroky pozd컄 experiment치lnej cesty, ktor칠 prin치코aj칰 v칛캜코ie odmeny.

> Viac o algoritme Policy Gradient a jeho uk치쬶u n치jdete v [pr칤kladovom notebooku](CartPole-RL-TF.ipynb).

## Algoritmus Actor-Critic

Vylep코en치 verzia pr칤stupu Policy Gradients sa naz칳va **Actor-Critic**. Hlavn치 my코lienka spo캜칤va v tom, 쬰 neur칩nov치 sie콘 by bola tr칠novan치 na n치vrat dvoch vec칤:

* Politiku, ktor치 ur캜uje, ak칰 akciu vykona콘. T치to 캜as콘 sa naz칳va **actor**.
* Odhad celkovej odmeny, ktor칰 m칪쬰me o캜ak치va콘 v tomto stave - t치to 캜as콘 sa naz칳va **critic**.

V istom zmysle t치to architekt칰ra pripom칤na [GAN](../../4-ComputerVision/10-GANs/README.md), kde m치me dve siete, ktor칠 s칰 tr칠novan칠 proti sebe. V modeli actor-critic navrhuje actor akciu, ktor칰 potrebujeme vykona콘, a critic sa sna쮂 by콘 kritick칳 a odhadn칰콘 v칳sledok. Na코칤m cie쬺m je v코ak tr칠nova콘 tieto siete v s칰lade.

Ke캞쬰 pozn치me skuto캜n칠 kumulat칤vne odmeny a v칳sledky vr치ten칠 criticom po캜as experimentu, je relat칤vne jednoduch칠 vytvori콘 funkciu straty, ktor치 minimalizuje rozdiel medzi nimi. To n치m d치va **critic loss**. **Actor loss** m칪쬰me vypo캜칤ta콘 pou쬴t칤m rovnak칠ho pr칤stupu ako v algoritme Policy Gradient.

Po spusten칤 jedn칠ho z t칳chto algoritmov m칪쬰me o캜ak치va콘, 쬰 n치코 CartPole sa bude spr치va콘 takto:

![balansuj칰ci cartpole](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-balance.gif)

## 九꽲잺 Cvi캜enia: Policy Gradients a Actor-Critic RL

Pokra캜ujte vo svojom u캜en칤 v nasleduj칰cich notebookoch:

* [RL v TensorFlow](CartPole-RL-TF.ipynb)
* [RL v PyTorch](CartPole-RL-PyTorch.ipynb)

## 캝al코ie 칰lohy RL

Posil켿ovacie u캜enie je dnes r칳chlo rast칰cou oblas콘ou v칳skumu. Niektor칠 zauj칤mav칠 pr칤klady posil켿ovacieho u캜enia s칰:

* U캜enie po캜칤ta캜a hra콘 **hry Atari**. V칳zvou v tomto probl칠me je, 쬰 nem치me jednoduch칳 stav reprezentovan칳 ako vektor, ale sk칪r sn칤mku obrazovky - a mus칤me pou쬴콘 CNN na konverziu tohto obrazov칠ho sn칤mku na vektor vlastnost칤 alebo na extrakciu inform치ci칤 o odmene. Hry Atari s칰 dostupn칠 v Gym.
* U캜enie po캜칤ta캜a hra콘 stolov칠 hry, ako 코ach a Go. Ned치vno boli 코pi캜kov칠 programy ako **Alpha Zero** tr칠novan칠 od nuly dvoma agentmi, ktor칤 hrali proti sebe a zlep코ovali sa pri ka쬯om kroku.
* V priemysle sa RL pou쮂셨a na vytv치ranie riadiacich syst칠mov zo simul치cie. Slu쬭a naz칳van치 [Bonsai](https://azure.microsoft.com/services/project-bonsai/?WT.mc_id=academic-77998-cacaste) je 코peci치lne navrhnut치 na tento 칰캜el.

## Z치ver

Teraz sme sa nau캜ili, ako tr칠nova콘 agentov na dosiahnutie dobr칳ch v칳sledkov len poskytnut칤m funkcie odmeny, ktor치 definuje po쬬dovan칳 stav hry, a umo쬹en칤m inteligentn칠ho sk칰mania priestoru h쬬dania. 칔spe코ne sme vysk칰코ali dva algoritmy a dosiahli dobr칳 v칳sledok v relat칤vne kr치tkom 캜ase. Toto je v코ak len za캜iatok va코ej cesty do RL, a ur캜ite by ste mali zv치쬴콘 absolvovanie samostatn칠ho kurzu, ak chcete 칤s콘 hlb코ie.

## 游 V칳zva

Presk칰majte aplik치cie uveden칠 v sekcii '캝al코ie 칰lohy RL' a sk칰ste implementova콘 jednu z nich!

## [Kv칤z po predn치코ke](https://ff-quizzes.netlify.app/en/ai/quiz/44)

## Preh쬬d a samostatn칠 코t칰dium

Viac o klasickom posil켿ovacom u캜en칤 sa dozviete v na코om [u캜ebnom pl치ne Strojov칠ho u캜enia pre za캜iato캜n칤kov](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/README.md).

Pozrite si [toto skvel칠 video](https://www.youtube.com/watch?v=qv6UVOQ0F44), ktor칠 hovor칤 o tom, ako sa po캜칤ta캜 m칪쬰 nau캜i콘 hra콘 Super Mario.

## Zadanie: [Vytr칠nujte Mountain Car](lab/README.md)

Va코칤m cie쬺m po캜as tohto zadania bude vytr칠nova콘 in칠 prostredie Gym - [Mountain Car](https://www.gymlibrary.ml/environments/classic_control/mountain_car/).

---

