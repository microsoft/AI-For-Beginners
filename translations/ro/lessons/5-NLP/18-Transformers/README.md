# Mecanisme de Aten탵ie 탳i Transformere

## [Chestionar 칥nainte de lec탵ie](https://ff-quizzes.netlify.app/en/ai/quiz/35)

Una dintre cele mai importante probleme din domeniul NLP este **traducerea automat캒**, o sarcin캒 esen탵ial캒 care st캒 la baza unor instrumente precum Google Translate. 칉n aceast캒 sec탵iune, ne vom concentra pe traducerea automat캒 sau, mai general, pe orice sarcin캒 de tip *sequence-to-sequence* (care este numit캒 탳i **transduc탵ia propozi탵iilor**).

Cu RNN-uri, sarcinile de tip sequence-to-sequence sunt implementate prin dou캒 re탵ele recurente, unde o re탵ea, **encoder-ul**, comprim캒 o secven탵캒 de intrare 칥ntr-o stare ascuns캒, iar cealalt캒 re탵ea, **decoder-ul**, desf캒탳oar캒 aceast캒 stare ascuns캒 칥ntr-un rezultat tradus. Exist캒 c칙teva probleme cu aceast캒 abordare:

* Starea final캒 a re탵elei encoder are dificult캒탵i 칥n a-탳i aminti 칥nceputul unei propozi탵ii, ceea ce duce la o calitate slab캒 a modelului pentru propozi탵ii lungi.
* Toate cuvintele dintr-o secven탵캒 au acela탳i impact asupra rezultatului. 칉n realitate, 칥ns캒, anumite cuvinte din secven탵a de intrare au un impact mai mare asupra ie탳irilor secven탵iale dec칙t altele.

**Mecanismele de Aten탵ie** ofer캒 o modalitate de a pondera impactul contextual al fiec캒rui vector de intrare asupra fiec캒rei predic탵ii de ie탳ire a RNN-ului. Modul 칥n care este implementat const캒 칥n crearea unor scurt캒turi 칥ntre st캒rile intermediare ale RNN-ului de intrare 탳i RNN-ul de ie탳ire. Astfel, atunci c칙nd gener캒m simbolul de ie탳ire y<sub>t</sub>, vom lua 칥n considerare toate st캒rile ascunse de intrare h<sub>i</sub>, cu diferi탵i coeficien탵i de greutate &alpha;<sub>t,i</sub>.

![Imagine care arat캒 un model encoder/decoder cu un strat de aten탵ie aditiv](../../../../../translated_images/ro/encoder-decoder-attention.7a726296894fb567.webp)

> Modelul encoder-decoder cu mecanism de aten탵ie aditiv din [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf), citat din [acest articol de blog](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)

Matricea de aten탵ie {&alpha;<sub>i,j</sub>} ar reprezenta gradul 칥n care anumite cuvinte de intrare contribuie la generarea unui cuv칙nt dat 칥n secven탵a de ie탳ire. Mai jos este un exemplu al unei astfel de matrice:

![Imagine care arat캒 o aliniere exemplar캒 g캒sit캒 de RNNsearch-50, preluat캒 din Bahdanau - arviz.org](../../../../../translated_images/ro/bahdanau-fig3.09ba2d37f202a6af.webp)

> Figur캒 din [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) (Fig.3)

Mecanismele de aten탵ie sunt responsabile pentru o mare parte din starea actual캒 sau aproape actual캒 a artei 칥n NLP. Totu탳i, ad캒ugarea aten탵iei cre탳te semnificativ num캒rul de parametri ai modelului, ceea ce a dus la probleme de scalare cu RNN-urile. O constr칙ngere cheie 칥n scalarea RNN-urilor este c캒 natura recurent캒 a modelelor face dificil캒 gruparea 탳i paralelizarea antren캒rii. 칉n cazul unui RNN, fiecare element al unei secven탵e trebuie procesat 칥n ordine secven탵ial캒, ceea ce 칥nseamn캒 c캒 nu poate fi paralelizat cu u탳urin탵캒.

![Encoder Decoder cu Aten탵ie](../../../../../lessons/5-NLP/18-Transformers/images/EncDecAttention.gif)

> Figur캒 din [Blogul Google](https://research.googleblog.com/2016/09/a-neural-network-for-machine.html)

Adoptarea mecanismelor de aten탵ie combinat캒 cu aceast캒 constr칙ngere a dus la crearea modelelor Transformer, care reprezint캒 acum starea de art캒, precum BERT sau Open-GPT3.

## Modele Transformer

Una dintre ideile principale din spatele transformatoarelor este evitarea naturii secven탵iale a RNN-urilor 탳i crearea unui model care poate fi paralelizat 칥n timpul antren캒rii. Acest lucru este realizat prin implementarea a dou캒 idei:

* codificarea pozi탵ional캒
* utilizarea mecanismului de auto-aten탵ie pentru a captura tipare 칥n loc de RNN-uri (sau CNN-uri) (de aceea lucrarea care introduce transformerele se nume탳te *[Attention is all you need](https://arxiv.org/abs/1706.03762)*)

### Codificare/칉ncorporare pozi탵ional캒

Ideea codific캒rii pozi탵ionale este urm캒toarea. 
1. C칙nd folosim RNN-uri, pozi탵ia relativ캒 a tokenilor este reprezentat캒 de num캒rul de pa탳i 탳i, astfel, nu trebuie s캒 fie reprezentat캒 explicit. 
2. Totu탳i, odat캒 ce trecem la aten탵ie, trebuie s캒 탳tim pozi탵iile relative ale tokenilor 칥ntr-o secven탵캒. 
3. Pentru a ob탵ine codificarea pozi탵ional캒, augment캒m secven탵a noastr캒 de tokeni cu o secven탵캒 de pozi탵ii ale tokenilor 칥n secven탵캒 (adic캒 o secven탵캒 de numere 0,1, ...).
4. Apoi combin캒m pozi탵ia tokenului cu un vector de 칥ncorporare a tokenului. Pentru a transforma pozi탵ia (un num캒r 칥ntreg) 칥ntr-un vector, putem folosi diferite abord캒ri:

* 칉ncorporare antrenabil캒, similar캒 cu 칥ncorporarea tokenilor. Aceasta este abordarea pe care o consider캒m aici. Aplic캒m straturi de 칥ncorporare at칙t pe tokeni, c칙t 탳i pe pozi탵iile lor, rezult칙nd vectori de 칥ncorporare de aceea탳i dimensiune, pe care 칥i adun캒m.
* Func탵ie fix캒 de codificare pozi탵ional캒, a탳a cum este propus캒 칥n lucrarea original캒.

<img src="../../../../../translated_images/ro/pos-embedding.e41ce9b6cf6078af.webp" width="50%"/>

> Imagine de autor

Rezultatul pe care 칥l ob탵inem cu 칥ncorporarea pozi탵ional캒 칥ncorporeaz캒 at칙t tokenul original, c칙t 탳i pozi탵ia sa 칥ntr-o secven탵캒.

### Auto-aten탵ie Multi-Head

Urm캒torul pas este capturarea unor tipare 칥n cadrul secven탵ei noastre. Pentru a face acest lucru, transformerele folosesc un mecanism de **auto-aten탵ie**, care este, 칥n esen탵캒, aten탵ie aplicat캒 acelea탳i secven탵e ca intrare 탳i ie탳ire. Aplicarea auto-aten탵iei ne permite s캒 lu캒m 칥n considerare **contextul** din propozi탵ie 탳i s캒 vedem care cuvinte sunt inter-rela탵ionate. De exemplu, ne permite s캒 vedem care cuvinte sunt referite prin coreferin탵e, cum ar fi *it*, 탳i s캒 lu캒m contextul 칥n considerare:

![](../../../../../translated_images/ro/CoreferenceResolution.861924d6d384a7d6.webp)

> Imagine din [Blogul Google](https://research.googleblog.com/2017/08/transformer-novel-neural-network.html)

칉n transformere, folosim **Aten탵ie Multi-Head** pentru a oferi re탵elei puterea de a captura mai multe tipuri de dependen탵e, de exemplu, rela탵ii pe termen lung vs. termen scurt 칥ntre cuvinte, coreferin탵e vs. altceva, etc.

[Notebook TensorFlow](TransformersTF.ipynb) con탵ine mai multe detalii despre implementarea straturilor transformer.

### Aten탵ie Encoder-Decoder

칉n transformere, aten탵ia este utilizat캒 칥n dou캒 locuri:

* Pentru a captura tipare 칥n textul de intrare folosind auto-aten탵ie
* Pentru a realiza traducerea secven탵ei - este stratul de aten탵ie 칥ntre encoder 탳i decoder.

Aten탵ia encoder-decoder este foarte similar캒 cu mecanismul de aten탵ie utilizat 칥n RNN-uri, a탳a cum este descris la 칥nceputul acestei sec탵iuni. Acest diagram캒 animat캒 explic캒 rolul aten탵iei encoder-decoder.

![GIF animat care arat캒 cum sunt realizate evalu캒rile 칥n modelele transformer.](../../../../../lessons/5-NLP/18-Transformers/images/transformer-animated-explanation.gif)

Deoarece fiecare pozi탵ie de intrare este mapat캒 independent la fiecare pozi탵ie de ie탳ire, transformerele pot paraleliza mai bine dec칙t RNN-urile, ceea ce permite modele de limbaj mult mai mari 탳i mai expresive. Fiecare cap de aten탵ie poate fi utilizat pentru a 칥nv캒탵a rela탵ii diferite 칥ntre cuvinte, ceea ce 칥mbun캒t캒탵e탳te sarcinile de procesare a limbajului natural.

## BERT

**BERT** (Bidirectional Encoder Representations from Transformers) este o re탵ea transformer foarte mare, cu mai multe straturi: 12 straturi pentru *BERT-base* 탳i 24 pentru *BERT-large*. Modelul este mai 칥nt칙i pre-antrenat pe un corpus mare de date text (Wikipedia + c캒r탵i) folosind antrenare nesupravegheat캒 (prezicerea cuvintelor mascate 칥ntr-o propozi탵ie). 칉n timpul pre-antren캒rii, modelul dob칙nde탳te niveluri semnificative de 칥n탵elegere a limbajului, care pot fi apoi utilizate cu alte seturi de date prin ajustare fin캒. Acest proces se nume탳te **칥nv캒탵are transferabil캒**.

![imagine de pe http://jalammar.github.io/illustrated-bert/](../../../../../translated_images/ro/jalammarBERT-language-modeling-masked-lm.34f113ea5fec4362.webp)

> Imagine [surs캒](http://jalammar.github.io/illustrated-bert/)

## 九꽲잺 Exerci탵ii: Transformere

Continu캒 칥nv캒탵area 칥n urm캒toarele notebook-uri:

* [Transformere 칥n PyTorch](TransformersPyTorch.ipynb)
* [Transformere 칥n TensorFlow](TransformersTF.ipynb)

## Concluzie

칉n aceast캒 lec탵ie ai 칥nv캒탵at despre Transformere 탳i Mecanismele de Aten탵ie, instrumente esen탵iale 칥n trusa NLP. Exist캒 multe varia탵ii ale arhitecturilor Transformer, inclusiv BERT, DistilBERT, BigBird, OpenGPT3 탳i altele, care pot fi ajustate fin. Pachetul [HuggingFace](https://github.com/huggingface/) ofer캒 un depozit pentru antrenarea multor dintre aceste arhitecturi at칙t cu PyTorch, c칙t 탳i cu TensorFlow.

## 游 Provocare

## [Chestionar dup캒 lec탵ie](https://ff-quizzes.netlify.app/en/ai/quiz/36)

## Recapitulare 탳i Studiu Individual

* [Articol de blog](https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/), care explic캒 lucrarea clasic캒 [Attention is all you need](https://arxiv.org/abs/1706.03762) despre transformere.
* [O serie de articole de blog](https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452) despre transformere, care explic캒 arhitectura 칥n detaliu.

## [Tem캒](assignment.md)

---

