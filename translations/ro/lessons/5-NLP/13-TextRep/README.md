# Reprezentarea textului ca tensori

## [Chestionar 칥nainte de lec탵ie](https://ff-quizzes.netlify.app/en/ai/quiz/25)

## Clasificarea textului

칉n prima parte a acestei sec탵iuni, ne vom concentra pe sarcina de **clasificare a textului**. Vom utiliza setul de date [AG News](https://www.kaggle.com/amananandrai/ag-news-classification-dataset), care con탵ine articole de 탳tiri precum urm캒torul:

* Categorie: 탲tiin탵캒/Tehnologie  
* Titlu: Ky. Company Wins Grant to Study Peptides (AP)  
* Corp: AP - O companie fondat캒 de un cercet캒tor 칥n chimie de la Universitatea din Louisville a c칙탳tigat un grant pentru a dezvolta...

Obiectivul nostru va fi s캒 clasific캒m articolul de 탳tiri 칥ntr-una dintre categorii pe baza textului.

## Reprezentarea textului

Dac캒 dorim s캒 rezolv캒m sarcini de Procesare a Limbajului Natural (NLP) cu re탵ele neuronale, avem nevoie de o modalitate de a reprezenta textul ca tensori. Calculatoarele deja reprezint캒 caracterele textuale ca numere care se mapeaz캒 la fonturi pe ecranul t캒u, utiliz칙nd codific캒ri precum ASCII sau UTF-8.

<img alt="Imagine care arat캒 o diagram캒 ce mapeaz캒 un caracter la o reprezentare ASCII 탳i binar캒" src="../../../../../translated_images/ro/ascii-character-map.18ed6aa7f3b0a7ff.webp" width="50%"/>

> [Sursa imaginii](https://www.seobility.net/en/wiki/ASCII)

Ca oameni, 칥n탵elegem ce **reprezint캒** fiecare liter캒 탳i cum toate caracterele se unesc pentru a forma cuvintele unei propozi탵ii. Totu탳i, calculatoarele, prin ele 칥nsele, nu au o astfel de 칥n탵elegere, iar re탵eaua neuronal캒 trebuie s캒 칥nve탵e semnifica탵ia 칥n timpul antren캒rii.

Prin urmare, putem utiliza diferite abord캒ri pentru a reprezenta textul:

* **Reprezentarea la nivel de caracter**, 칥n care trat캒m fiecare caracter ca un num캒r. Av칙nd *C* caractere diferite 칥n corpusul nostru de text, cuv칙ntul *Hello* ar fi reprezentat printr-un tensor de 5x*C*. Fiecare liter캒 ar corespunde unei coloane tensoriale 칥n codificarea one-hot.  
* **Reprezentarea la nivel de cuv칙nt**, 칥n care cre캒m un **vocabular** al tuturor cuvintelor din textul nostru 탳i apoi reprezent캒m cuvintele utiliz칙nd codificarea one-hot. Aceast캒 abordare este oarecum mai bun캒, deoarece fiecare liter캒, 칥n sine, nu are prea mult캒 semnifica탵ie, iar utiliz칙nd concepte semantice de nivel superior - cuvintele - simplific캒m sarcina pentru re탵eaua neuronal캒. Totu탳i, av칙nd 칥n vedere dimensiunea mare a dic탵ionarului, trebuie s캒 gestion캒m tensori sparse de dimensiuni mari.

Indiferent de reprezentare, mai 칥nt칙i trebuie s캒 convertim textul 칥ntr-o secven탵캒 de **token-uri**, un token fiind fie un caracter, un cuv칙nt sau, uneori, chiar o parte a unui cuv칙nt. Apoi, convertim token-ul 칥ntr-un num캒r, de obicei utiliz칙nd un **vocabular**, iar acest num캒r poate fi introdus 칥ntr-o re탵ea neuronal캒 utiliz칙nd codificarea one-hot.

## N-Grame

칉n limbajul natural, semnifica탵ia precis캒 a cuvintelor poate fi determinat캒 doar 칥n context. De exemplu, semnifica탵iile *re탵ea neuronal캒* 탳i *re탵ea de pescuit* sunt complet diferite. Una dintre modalit캒탵ile de a 탵ine cont de acest lucru este s캒 construim modelul nostru pe perechi de cuvinte 탳i s캒 consider캒m perechile de cuvinte ca token-uri separate 칥n vocabular. 칉n acest fel, propozi탵ia *칉mi place s캒 merg la pescuit* va fi reprezentat캒 prin urm캒toarea secven탵캒 de token-uri: *칉mi place*, *place s캒*, *s캒 merg*, *merg la pescuit*. Problema cu aceast캒 abordare este c캒 dimensiunea dic탵ionarului cre탳te semnificativ, iar combina탵ii precum *merg la pescuit* 탳i *merg la cump캒r캒turi* sunt prezentate prin token-uri diferite, care nu 칥mp캒rt캒탳esc nicio similaritate semantic캒, 칥n ciuda aceluia탳i verb.

칉n unele cazuri, putem lua 칥n considerare utilizarea tri-gramelor -- combina탵ii de trei cuvinte -- de asemenea. Astfel, aceast캒 abordare este adesea numit캒 **n-grame**. De asemenea, are sens s캒 utiliz캒m n-grame cu reprezentarea la nivel de caracter, caz 칥n care n-gramele vor corespunde aproximativ diferitelor silabe.

## Bag-of-Words 탳i TF/IDF

C칙nd rezolv캒m sarcini precum clasificarea textului, trebuie s캒 fim capabili s캒 reprezent캒m textul printr-un vector de dimensiune fix캒, pe care 칥l vom utiliza ca intrare pentru clasificatorul dens final. Una dintre cele mai simple modalit캒탵i de a face acest lucru este s캒 combin캒m toate reprezent캒rile individuale ale cuvintelor, de exemplu, prin adunarea lor. Dac캒 adun캒m codific캒rile one-hot ale fiec캒rui cuv칙nt, vom ob탵ine un vector de frecven탵e, care arat캒 de c칙te ori apare fiecare cuv칙nt 칥n text. O astfel de reprezentare a textului se nume탳te **bag of words** (BoW).

<img src="../../../../../translated_images/ro/bow.3811869cff59368d.webp" width="90%"/>

> Imagine de autor

Un BoW reprezint캒 esen탵ialmente ce cuvinte apar 칥n text 탳i 칥n ce cantit캒탵i, ceea ce poate fi 칥ntr-adev캒r un bun indicator al subiectului textului. De exemplu, un articol de 탳tiri despre politic캒 este probabil s캒 con탵in캒 cuvinte precum *pre탳edinte* 탳i *탵ar캒*, 칥n timp ce o publica탵ie 탳tiin탵ific캒 ar avea ceva de genul *colizor*, *descoperit*, etc. Astfel, frecven탵ele cuvintelor pot fi, 칥n multe cazuri, un bun indicator al con탵inutului textului.

Problema cu BoW este c캒 anumite cuvinte comune, precum *탳i*, *este*, etc., apar 칥n majoritatea textelor 탳i au cele mai mari frecven탵e, masc칙nd cuvintele care sunt cu adev캒rat importante. Putem reduce importan탵a acestor cuvinte 탵in칙nd cont de frecven탵a cu care apar 칥n 칥ntreaga colec탵ie de documente. Aceasta este ideea principal캒 din spatele abord캒rii TF/IDF, care este acoperit캒 칥n detaliu 칥n notebook-urile ata탳ate acestei lec탵ii.

Totu탳i, niciuna dintre aceste abord캒ri nu poate 탵ine pe deplin cont de **semantica** textului. Avem nevoie de modele de re탵ele neuronale mai puternice pentru a face acest lucru, pe care le vom discuta mai t칙rziu 칥n aceast캒 sec탵iune.

## 九꽲잺 Exerci탵ii: Reprezentarea textului

Continu캒 칥nv캒탵area 칥n urm캒toarele notebook-uri:

* [Reprezentarea textului cu PyTorch](TextRepresentationPyTorch.ipynb)  
* [Reprezentarea textului cu TensorFlow](TextRepresentationTF.ipynb)  

## Concluzie

P칙n캒 acum, am studiat tehnici care pot ad캒uga greutate frecven탵ei diferitelor cuvinte. Totu탳i, ele nu sunt capabile s캒 reprezinte semnifica탵ia sau ordinea. Dup캒 cum a spus faimosul lingvist J. R. Firth 칥n 1935, "Semnifica탵ia complet캒 a unui cuv칙nt este 칥ntotdeauna contextual캒, 탳i niciun studiu al semnifica탵iei 칥n afara contextului nu poate fi luat 칥n serios." Vom 칥nv캒탵a mai t칙rziu 칥n curs cum s캒 capt캒m informa탵ii contextuale din text utiliz칙nd modelarea limbajului.

## 游 Provocare

칉ncearc캒 alte exerci탵ii utiliz칙nd bag-of-words 탳i diferite modele de date. Po탵i fi inspirat de aceast캒 [competi탵ie pe Kaggle](https://www.kaggle.com/competitions/word2vec-nlp-tutorial/overview/part-1-for-beginners-bag-of-words)

## [Chestionar dup캒 lec탵ie](https://ff-quizzes.netlify.app/en/ai/quiz/26)

## Recapitulare 탳i Studiu Individual

Exerseaz캒-탵i abilit캒탵ile cu tehnicile de embedding text 탳i bag-of-words pe [Microsoft Learn](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-pytorch/?WT.mc_id=academic-77998-cacaste)

## [Tem캒: Notebook-uri](assignment.md)

---

