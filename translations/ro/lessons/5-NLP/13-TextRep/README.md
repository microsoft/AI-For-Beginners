<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "4522e22e150be0845e03aa41209a39d5",
  "translation_date": "2025-08-25T21:50:27+00:00",
  "source_file": "lessons/5-NLP/13-TextRep/README.md",
  "language_code": "ro"
}
-->
# Reprezentarea textului ca tensori

## [Chestionar 칥nainte de curs](https://ff-quizzes.netlify.app/en/ai/quiz/25)

## Clasificarea textului

칉n prima parte a acestei sec탵iuni, ne vom concentra pe sarcina de **clasificare a textului**. Vom folosi setul de date [AG News](https://www.kaggle.com/amananandrai/ag-news-classification-dataset), care con탵ine articole de 탳tiri precum urm캒torul exemplu:

* Categorie: 탲tiin탵캒/Tehnologie  
* Titlu: Compania Ky. c칙탳tig캒 un grant pentru a studia peptidele (AP)  
* Corp: AP - O companie fondat캒 de un cercet캒tor 칥n chimie de la Universitatea din Louisville a c칙탳tigat un grant pentru a dezvolta...

Obiectivul nostru va fi s캒 clasific캒m articolul de 탳tiri 칥ntr-una dintre categorii pe baza textului.

## Reprezentarea textului

Dac캒 dorim s캒 rezolv캒m sarcini de procesare a limbajului natural (NLP) cu re탵ele neuronale, avem nevoie de o metod캒 pentru a reprezenta textul ca tensori. Calculatoarele deja reprezint캒 caracterele textuale ca numere care corespund fonturilor de pe ecranul t캒u, folosind codific캒ri precum ASCII sau UTF-8.

<img alt="Imagine care arat캒 o diagram캒 ce mapeaz캒 un caracter la o reprezentare ASCII 탳i binar캒" src="images/ascii-character-map.png" width="50%"/>

> [Sursa imaginii](https://www.seobility.net/en/wiki/ASCII)

Ca oameni, 칥n탵elegem ce **reprezint캒** fiecare liter캒 탳i cum toate caracterele se unesc pentru a forma cuvintele unei propozi탵ii. Totu탳i, calculatoarele, prin ele 칥nsele, nu au o astfel de 칥n탵elegere, iar re탵eaua neuronal캒 trebuie s캒 칥nve탵e semnifica탵ia 칥n timpul antren캒rii.

Prin urmare, putem folosi diferite abord캒ri pentru a reprezenta textul:

* **Reprezentarea la nivel de caracter**, 칥n care trat캒m fiecare caracter ca un num캒r. Av칙nd *C* caractere diferite 칥n corpusul nostru de text, cuv칙ntul *Hello* ar fi reprezentat de un tensor de dimensiune 5x*C*. Fiecare liter캒 ar corespunde unei coloane tensoriale 칥n codificarea one-hot.  
* **Reprezentarea la nivel de cuv칙nt**, 칥n care cre캒m un **vocabular** al tuturor cuvintelor din textul nostru 탳i apoi reprezent캒m cuvintele folosind codificarea one-hot. Aceast캒 abordare este oarecum mai bun캒, deoarece fiecare liter캒, 칥n sine, nu are prea mult캒 semnifica탵ie, iar utiliz칙nd concepte semantice de nivel mai 칥nalt - cuvintele - simplific캒m sarcina pentru re탵eaua neuronal캒. Totu탳i, av칙nd 칥n vedere dimensiunea mare a dic탵ionarului, trebuie s캒 gestion캒m tensori dispersa탵i de dimensiuni mari.

Indiferent de reprezentare, mai 칥nt칙i trebuie s캒 convertim textul 칥ntr-o secven탵캒 de **token-uri**, un token fiind fie un caracter, un cuv칙nt sau, uneori, chiar o parte a unui cuv칙nt. Apoi, convertim token-ul 칥ntr-un num캒r, de obicei folosind un **vocabular**, iar acest num캒r poate fi introdus 칥ntr-o re탵ea neuronal캒 folosind codificarea one-hot.

## N-Grame

칉n limbajul natural, semnifica탵ia precis캒 a cuvintelor poate fi determinat캒 doar 칥n context. De exemplu, semnifica탵iile expresiilor *re탵ea neuronal캒* 탳i *re탵ea de pescuit* sunt complet diferite. Una dintre modalit캒탵ile de a 탵ine cont de acest lucru este s캒 construim modelul nostru pe perechi de cuvinte, consider칙nd perechile de cuvinte ca token-uri separate 칥n vocabular. 칉n acest fel, propozi탵ia *칉mi place s캒 merg la pescuit* va fi reprezentat캒 de urm캒toarea secven탵캒 de token-uri: *칉mi place*, *place s캒*, *s캒 merg*, *merg la pescuit*. Problema cu aceast캒 abordare este c캒 dimensiunea dic탵ionarului cre탳te semnificativ, iar combina탵ii precum *merg la pescuit* 탳i *merg la cump캒r캒turi* sunt reprezentate de token-uri diferite, care nu 칥mp캒rt캒탳esc nicio similaritate semantic캒, 칥n ciuda aceluia탳i verb.

칉n unele cazuri, putem lua 칥n considerare utilizarea tri-gramelor -- combina탵ii de trei cuvinte -- de asemenea. Astfel, aceast캒 abordare este adesea numit캒 **n-grame**. De asemenea, are sens s캒 folosim n-grame cu reprezentarea la nivel de caracter, caz 칥n care n-gramele vor corespunde aproximativ diferitelor silabe.

## Bag-of-Words 탳i TF/IDF

C칙nd rezolv캒m sarcini precum clasificarea textului, trebuie s캒 putem reprezenta textul printr-un vector de dimensiune fix캒, pe care 칥l vom folosi ca intrare pentru clasificatorul dens final. Una dintre cele mai simple metode de a face acest lucru este s캒 combin캒m toate reprezent캒rile individuale ale cuvintelor, de exemplu, prin adunarea lor. Dac캒 adun캒m codific캒rile one-hot ale fiec캒rui cuv칙nt, vom ob탵ine un vector de frecven탵e, care arat캒 de c칙te ori apare fiecare cuv칙nt 칥n text. O astfel de reprezentare a textului se nume탳te **bag of words** (BoW).

<img src="images/bow.png" width="90%"/>

> Imagine realizat캒 de autor

Un BoW reprezint캒, 칥n esen탵캒, ce cuvinte apar 칥n text 탳i 칥n ce cantit캒탵i, ceea ce poate fi 칥ntr-adev캒r un bun indicator al subiectului textului. De exemplu, un articol de 탳tiri despre politic캒 este probabil s캒 con탵in캒 cuvinte precum *pre탳edinte* 탳i *탵ar캒*, 칥n timp ce o publica탵ie 탳tiin탵ific캒 ar avea termeni precum *colizor*, *descoperit*, etc. Astfel, frecven탵ele cuvintelor pot fi, 칥n multe cazuri, un bun indicator al con탵inutului textului.

Problema cu BoW este c캒 anumite cuvinte comune, precum *탳i*, *este*, etc., apar 칥n majoritatea textelor 탳i au frecven탵e ridicate, masc칙nd cuvintele care sunt cu adev캒rat importante. Putem reduce importan탵a acestor cuvinte lu칙nd 칥n considerare frecven탵a cu care apar 칥n 칥ntreaga colec탵ie de documente. Aceasta este ideea principal캒 din spatele abord캒rii TF/IDF, care este acoperit캒 칥n detaliu 칥n caietele ata탳ate acestei lec탵ii.

Totu탳i, niciuna dintre aceste abord캒ri nu poate lua 칥n considerare pe deplin **semantica** textului. Avem nevoie de modele mai puternice de re탵ele neuronale pentru a face acest lucru, pe care le vom discuta mai t칙rziu 칥n aceast캒 sec탵iune.

## 九꽲잺 Exerci탵ii: Reprezentarea textului

Continu캒 칥nv캒탵area 칥n urm캒toarele caiete:

* [Reprezentarea textului cu PyTorch](../../../../../lessons/5-NLP/13-TextRep/TextRepresentationPyTorch.ipynb)  
* [Reprezentarea textului cu TensorFlow](../../../../../lessons/5-NLP/13-TextRep/TextRepresentationTF.ipynb)  

## Concluzie

P칙n캒 acum, am studiat tehnici care pot ad캒uga greutate frecven탵ei diferitelor cuvinte. Totu탳i, acestea nu pot reprezenta semnifica탵ia sau ordinea. A탳a cum a spus faimosul lingvist J. R. Firth 칥n 1935, "Semnifica탵ia complet캒 a unui cuv칙nt este 칥ntotdeauna contextual캒, iar niciun studiu al semnifica탵iei 칥n afara contextului nu poate fi luat 칥n serios." Vom 칥nv캒탵a mai t칙rziu 칥n curs cum s캒 captur캒m informa탵ii contextuale din text folosind modelarea limbajului.

## 游 Provocare

칉ncearc캒 alte exerci탵ii folosind bag-of-words 탳i diferite modele de date. Po탵i g캒si inspira탵ie 칥n aceast캒 [competi탵ie pe Kaggle](https://www.kaggle.com/competitions/word2vec-nlp-tutorial/overview/part-1-for-beginners-bag-of-words).

## [Chestionar dup캒 curs](https://ff-quizzes.netlify.app/en/ai/quiz/26)

## Recapitulare 탳i studiu individual

Exerseaz캒-탵i abilit캒탵ile cu tehnicile de embedding 탳i bag-of-words pe [Microsoft Learn](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-pytorch/?WT.mc_id=academic-77998-cacaste).

## [Tem캒: Caiete](assignment.md)

**Declinare de responsabilitate**:  
Acest document a fost tradus folosind serviciul de traducere AI [Co-op Translator](https://github.com/Azure/co-op-translator). De탳i ne str캒duim s캒 asigur캒m acurate탵ea, v캒 rug캒m s캒 fi탵i con탳tien탵i c캒 traducerile automate pot con탵ine erori sau inexactit캒탵i. Documentul original 칥n limba sa natal캒 ar trebui considerat sursa autoritar캒. Pentru informa탵ii critice, se recomand캒 traducerea profesional캒 realizat캒 de un specialist uman. Nu ne asum캒m responsabilitatea pentru eventualele ne칥n탵elegeri sau interpret캒ri gre탳ite care pot ap캒rea din utilizarea acestei traduceri.