# ReÈ›ele generative

## [Chestionar Ã®nainte de curs](https://ff-quizzes.netlify.app/en/ai/quiz/33)

ReÈ›elele Neuronale Recurente (RNN) È™i variantele lor cu celule cu porÈ›i, cum ar fi Celulele cu Memorie pe Termen Lung (LSTM) È™i UnitÄƒÈ›ile Recurente cu PorÈ›i (GRU), oferÄƒ un mecanism pentru modelarea limbajului, deoarece pot Ã®nvÄƒÈ›a ordinea cuvintelor È™i pot oferi predicÈ›ii pentru urmÄƒtorul cuvÃ¢nt dintr-o secvenÈ›Äƒ. Acest lucru ne permite sÄƒ folosim RNN-urile pentru **sarcini generative**, cum ar fi generarea obiÈ™nuitÄƒ de text, traducerea automatÄƒ È™i chiar generarea de descrieri pentru imagini.

> âœ… GÃ¢ndeÈ™te-te la toate momentele Ã®n care ai beneficiat de sarcini generative, cum ar fi completarea textului Ã®n timp ce tastezi. CerceteazÄƒ aplicaÈ›iile tale preferate pentru a vedea dacÄƒ au utilizat RNN-uri.

Ãn arhitectura RNN discutatÄƒ Ã®n unitatea anterioarÄƒ, fiecare unitate RNN producea urmÄƒtoarea stare ascunsÄƒ ca ieÈ™ire. TotuÈ™i, putem adÄƒuga È™i o altÄƒ ieÈ™ire fiecÄƒrei unitÄƒÈ›i recurente, ceea ce ne-ar permite sÄƒ generÄƒm o **secvenÈ›Äƒ** (egalÄƒ ca lungime cu secvenÈ›a originalÄƒ). Mai mult, putem folosi unitÄƒÈ›i RNN care nu acceptÄƒ o intrare la fiecare pas, ci doar iau un vector de stare iniÈ›ialÄƒ È™i apoi produc o secvenÈ›Äƒ de ieÈ™iri.

Acest lucru permite diferite arhitecturi neuronale, aÈ™a cum sunt prezentate Ã®n imaginea de mai jos:

![Imagine care aratÄƒ modele comune de reÈ›ele neuronale recurente.](../../../../../translated_images/ro/unreasonable-effectiveness-of-rnn.541ead816778f42d.webp)

> Imagine din articolul [Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) de [Andrej Karpathy](http://karpathy.github.io/)

* **One-to-one** este o reÈ›ea neuronalÄƒ tradiÈ›ionalÄƒ cu o singurÄƒ intrare È™i o singurÄƒ ieÈ™ire.
* **One-to-many** este o arhitecturÄƒ generativÄƒ care acceptÄƒ o valoare de intrare È™i genereazÄƒ o secvenÈ›Äƒ de valori de ieÈ™ire. De exemplu, dacÄƒ dorim sÄƒ antrenÄƒm o reÈ›ea pentru **descrierea imaginilor** care sÄƒ producÄƒ o descriere textualÄƒ a unei imagini, putem lua o imagine ca intrare, sÄƒ o trecem printr-o CNN pentru a obÈ›ine starea ascunsÄƒ, iar apoi sÄƒ folosim un lanÈ› recurent pentru a genera descrierea cuvÃ¢nt cu cuvÃ¢nt.
* **Many-to-one** corespunde arhitecturilor RNN descrise Ã®n unitatea anterioarÄƒ, cum ar fi clasificarea textului.
* **Many-to-many**, sau **secvenÈ›Äƒ-la-secvenÈ›Äƒ**, corespunde sarcinilor precum **traducerea automatÄƒ**, unde avem un prim RNN care colecteazÄƒ toate informaÈ›iile din secvenÈ›a de intrare Ã®n starea ascunsÄƒ, iar un alt lanÈ› RNN desfÄƒÈ™oarÄƒ aceastÄƒ stare Ã®ntr-o secvenÈ›Äƒ de ieÈ™ire.

Ãn aceastÄƒ unitate, ne vom concentra pe modele generative simple care ne ajutÄƒ sÄƒ generÄƒm text. Pentru simplitate, vom folosi tokenizarea la nivel de caracter.

Vom antrena acest RNN pentru a genera text pas cu pas. La fiecare pas, vom lua o secvenÈ›Äƒ de caractere de lungime `nchars` È™i vom cere reÈ›elei sÄƒ genereze urmÄƒtorul caracter de ieÈ™ire pentru fiecare caracter de intrare:

![Imagine care aratÄƒ un exemplu de generare RNN a cuvÃ¢ntului 'HELLO'.](../../../../../translated_images/ro/rnn-generate.56c54afb52f9781d.webp)

CÃ¢nd generÄƒm text (Ã®n timpul inferenÈ›ei), Ã®ncepem cu un **prompt**, care este trecut prin celulele RNN pentru a genera starea intermediarÄƒ, iar apoi, din aceastÄƒ stare, Ã®ncepe generarea. GenerÄƒm un caracter pe rÃ¢nd È™i transmitem starea È™i caracterul generat unei alte celule RNN pentru a genera urmÄƒtorul, pÃ¢nÄƒ cÃ¢nd generÄƒm suficiente caractere.

<img src="../../../../../translated_images/ro/rnn-generate-inf.5168dc65e0370eea.webp" width="60%"/>

> Imagine realizatÄƒ de autor

## âœï¸ ExerciÈ›ii: ReÈ›ele Generative

ContinuÄƒ Ã®nvÄƒÈ›area Ã®n urmÄƒtoarele notebook-uri:

* [ReÈ›ele Generative cu PyTorch](GenerativePyTorch.ipynb)
* [ReÈ›ele Generative cu TensorFlow](GenerativeTF.ipynb)

## Generare moale de text È™i temperaturÄƒ

IeÈ™irea fiecÄƒrei celule RNN este o distribuÈ›ie de probabilitate a caracterelor. DacÄƒ alegem Ã®ntotdeauna caracterul cu cea mai mare probabilitate ca urmÄƒtor caracter Ã®n textul generat, textul poate deveni adesea "ciclic", repetÃ¢nd aceleaÈ™i secvenÈ›e de caractere din nou È™i din nou, ca Ã®n acest exemplu:

```
today of the second the company and a second the company ...
```

TotuÈ™i, dacÄƒ ne uitÄƒm la distribuÈ›ia de probabilitate pentru urmÄƒtorul caracter, este posibil ca diferenÈ›a dintre cÃ¢teva dintre cele mai mari probabilitÄƒÈ›i sÄƒ nu fie mare, de exemplu, un caracter poate avea probabilitatea 0.2, iar altul 0.19 etc. De exemplu, cÃ¢nd cÄƒutÄƒm urmÄƒtorul caracter Ã®n secvenÈ›a '*play*', urmÄƒtorul caracter poate fi la fel de bine un spaÈ›iu sau **e** (ca Ã®n cuvÃ¢ntul *player*).

Acest lucru ne conduce la concluzia cÄƒ nu este Ã®ntotdeauna "corect" sÄƒ selectÄƒm caracterul cu probabilitatea cea mai mare, deoarece alegerea celui de-al doilea cel mai probabil poate duce totuÈ™i la un text semnificativ. Este mai Ã®nÈ›elept sÄƒ **eÈ™antionÄƒm** caracterele din distribuÈ›ia de probabilitate oferitÄƒ de ieÈ™irea reÈ›elei. Putem folosi È™i un parametru, **temperatura**, care va aplatiza distribuÈ›ia de probabilitate, Ã®n cazul Ã®n care dorim sÄƒ adÄƒugÄƒm mai multÄƒ aleatorie, sau o va face mai abruptÄƒ, dacÄƒ dorim sÄƒ ne concentrÄƒm mai mult pe caracterele cu probabilitate mai mare.

ExploreazÄƒ cum este implementatÄƒ aceastÄƒ generare moale de text Ã®n notebook-urile menÈ›ionate mai sus.

## Concluzie

DeÈ™i generarea de text poate fi utilÄƒ Ã®n sine, beneficiile majore vin din abilitatea de a genera text folosind RNN-uri dintr-un vector de caracteristici iniÈ›ial. De exemplu, generarea de text este utilizatÄƒ ca parte a traducerii automate (secvenÈ›Äƒ-la-secvenÈ›Äƒ, Ã®n acest caz vectorul de stare de la *encoder* este folosit pentru a genera sau *decoda* mesajul tradus) sau pentru generarea unei descrieri textuale a unei imagini (Ã®n acest caz, vectorul de caracteristici ar proveni dintr-un extractor CNN).

## ğŸš€ Provocare

UrmeazÄƒ cÃ¢teva lecÈ›ii pe Microsoft Learn pe acest subiect:

* Generare de text cu [PyTorch](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-pytorch/6-generative-networks/?WT.mc_id=academic-77998-cacaste)/[TensorFlow](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-tensorflow/5-generative-networks/?WT.mc_id=academic-77998-cacaste)

## [Chestionar dupÄƒ curs](https://ff-quizzes.netlify.app/en/ai/quiz/34)

## Recapitulare È™i Studiu Individual

IatÄƒ cÃ¢teva articole pentru a-È›i extinde cunoÈ™tinÈ›ele:

* Diferite abordÄƒri pentru generarea de text cu Markov Chain, LSTM È™i GPT-2: [articol](https://towardsdatascience.com/text-generation-gpt-2-lstm-markov-chain-9ea371820e1e)
* Exemplu de generare de text Ã®n [documentaÈ›ia Keras](https://keras.io/examples/generative/lstm_character_level_text_generation/)

## [TemÄƒ](lab/README.md)

Am vÄƒzut cum sÄƒ generÄƒm text caracter cu caracter. Ãn laborator, vei explora generarea de text la nivel de cuvÃ¢nt.

---

