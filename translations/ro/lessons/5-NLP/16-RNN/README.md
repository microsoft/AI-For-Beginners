<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "58bf4adb210aab53e8f78c8082040e7c",
  "translation_date": "2025-08-25T21:33:18+00:00",
  "source_file": "lessons/5-NLP/16-RNN/README.md",
  "language_code": "ro"
}
-->
# ReÈ›ele Neuronale Recurente

## [Chestionar Ã®nainte de curs](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/116)

Ãn secÈ›iunile anterioare, am utilizat reprezentÄƒri semantice bogate ale textului È™i un clasificator liniar simplu deasupra acestor embedding-uri. AceastÄƒ arhitecturÄƒ capteazÄƒ sensul agregat al cuvintelor dintr-o propoziÈ›ie, dar nu ia Ã®n considerare **ordinea** cuvintelor, deoarece operaÈ›ia de agregare deasupra embedding-urilor eliminÄƒ aceastÄƒ informaÈ›ie din textul original. Deoarece aceste modele nu pot modela ordonarea cuvintelor, ele nu pot rezolva sarcini mai complexe sau ambigue, cum ar fi generarea de text sau rÄƒspunsul la Ã®ntrebÄƒri.

Pentru a capta sensul unei secvenÈ›e de text, trebuie sÄƒ utilizÄƒm o altÄƒ arhitecturÄƒ de reÈ›ea neuronalÄƒ, numitÄƒ **reÈ›ea neuronalÄƒ recurentÄƒ**, sau RNN. Ãn RNN, trecem propoziÈ›ia prin reÈ›ea un simbol la un moment dat, iar reÈ›eaua produce un **statut**, pe care Ã®l trecem din nou prin reÈ›ea Ã®mpreunÄƒ cu urmÄƒtorul simbol.

![RNN](../../../../../translated_images/rnn.27f5c29c53d727b546ad3961637a267f0fe9ec5ab01f2a26a853c92fcefbb574.ro.png)

> Imagine realizatÄƒ de autor

AvÃ¢nd secvenÈ›a de intrare de tokeni X<sub>0</sub>,...,X<sub>n</sub>, RNN creeazÄƒ o secvenÈ›Äƒ de blocuri de reÈ›ea neuronalÄƒ È™i antreneazÄƒ aceastÄƒ secvenÈ›Äƒ cap-coadÄƒ folosind backpropagation. Fiecare bloc de reÈ›ea primeÈ™te o pereche (X<sub>i</sub>,S<sub>i</sub>) ca intrare È™i produce S<sub>i+1</sub> ca rezultat. Statutul final S<sub>n</sub> sau (ieÈ™irea Y<sub>n</sub>) este transmis unui clasificator liniar pentru a produce rezultatul. Toate blocurile de reÈ›ea Ã®mpÄƒrtÄƒÈ™esc aceleaÈ™i greutÄƒÈ›i È™i sunt antrenate cap-coadÄƒ folosind o singurÄƒ trecere de backpropagation.

Deoarece vectorii de statut S<sub>0</sub>,...,S<sub>n</sub> sunt trecuÈ›i prin reÈ›ea, aceasta poate Ã®nvÄƒÈ›a dependenÈ›ele secvenÈ›iale dintre cuvinte. De exemplu, atunci cÃ¢nd cuvÃ¢ntul *nu* apare undeva Ã®n secvenÈ›Äƒ, reÈ›eaua poate Ã®nvÄƒÈ›a sÄƒ nege anumite elemente din vectorul de statut, rezultÃ¢nd o negare.

> âœ… Deoarece greutÄƒÈ›ile tuturor blocurilor RNN din imaginea de mai sus sunt Ã®mpÄƒrtÄƒÈ™ite, aceeaÈ™i imagine poate fi reprezentatÄƒ ca un singur bloc (Ã®n dreapta) cu un circuit de feedback recurent, care transmite statutul de ieÈ™ire al reÈ›elei Ã®napoi la intrare.

## Anatomia unei Celule RNN

SÄƒ vedem cum este organizatÄƒ o celulÄƒ RNN simplÄƒ. Aceasta acceptÄƒ statutul anterior S<sub>i-1</sub> È™i simbolul curent X<sub>i</sub> ca intrÄƒri È™i trebuie sÄƒ producÄƒ statutul de ieÈ™ire S<sub>i</sub> (È™i, uneori, ne intereseazÄƒ È™i o altÄƒ ieÈ™ire Y<sub>i</sub>, ca Ã®n cazul reÈ›elelor generative).

O celulÄƒ RNN simplÄƒ are douÄƒ matrici de greutÄƒÈ›i interne: una transformÄƒ un simbol de intrare (sÄƒ o numim W), iar cealaltÄƒ transformÄƒ un statut de intrare (H). Ãn acest caz, ieÈ™irea reÈ›elei este calculatÄƒ ca Ïƒ(WÃ—X<sub>i</sub>+HÃ—S<sub>i-1</sub>+b), unde Ïƒ este funcÈ›ia de activare È™i b este un bias suplimentar.

<img alt="Anatomia unei celule RNN" src="images/rnn-anatomy.png" width="50%"/>

> Imagine realizatÄƒ de autor

Ãn multe cazuri, tokenii de intrare sunt trecuÈ›i prin stratul de embedding Ã®nainte de a intra Ã®n RNN pentru a reduce dimensionalitatea. Ãn acest caz, dacÄƒ dimensiunea vectorilor de intrare este *emb_size*, iar vectorul de statut este *hid_size* - dimensiunea lui W este *emb_size*Ã—*hid_size*, iar dimensiunea lui H este *hid_size*Ã—*hid_size*.

## Memorie pe Termen Lung È™i Scurt (LSTM)

Una dintre principalele probleme ale RNN-urilor clasice este aÈ™a-numita problemÄƒ a **gradientelor care dispar**. Deoarece RNN-urile sunt antrenate cap-coadÄƒ Ã®ntr-o singurÄƒ trecere de backpropagation, este dificil sÄƒ se propage eroarea cÄƒtre primele straturi ale reÈ›elei, iar astfel reÈ›eaua nu poate Ã®nvÄƒÈ›a relaÈ›ii Ã®ntre tokeni distanÈ›i. Una dintre modalitÄƒÈ›ile de a evita aceastÄƒ problemÄƒ este introducerea **gestionÄƒrii explicite a statutului** prin utilizarea aÈ™a-numitelor **porÈ›i**. ExistÄƒ douÄƒ arhitecturi bine cunoscute de acest tip: **Memorie pe Termen Lung È™i Scurt** (LSTM) È™i **Unitatea de Releu Gated** (GRU).

![Imagine care aratÄƒ un exemplu de celulÄƒ LSTM](../../../../../lessons/5-NLP/16-RNN/images/long-short-term-memory-cell.svg)

> Sursa imaginii TBD

ReÈ›eaua LSTM este organizatÄƒ Ã®ntr-un mod similar cu RNN, dar existÄƒ douÄƒ stÄƒri care sunt transmise de la un strat la altul: statutul actual C È™i vectorul ascuns H. La fiecare unitate, vectorul ascuns H<sub>i</sub> este concatenat cu intrarea X<sub>i</sub>, iar acestea controleazÄƒ ceea ce se Ã®ntÃ¢mplÄƒ cu statutul C prin intermediul **porÈ›ilor**. Fiecare poartÄƒ este o reÈ›ea neuronalÄƒ cu activare sigmoid (ieÈ™ire Ã®n intervalul [0,1]), care poate fi consideratÄƒ ca o mascÄƒ bitwise atunci cÃ¢nd este Ã®nmulÈ›itÄƒ cu vectorul de statut. ExistÄƒ urmÄƒtoarele porÈ›i (de la stÃ¢nga la dreapta Ã®n imaginea de mai sus):

* **Poarta de uitare** ia un vector ascuns È™i determinÄƒ ce componente ale vectorului C trebuie uitate È™i care trebuie transmise mai departe.
* **Poarta de intrare** ia informaÈ›ii din vectorii de intrare È™i ascunÈ™i È™i le insereazÄƒ Ã®n statut.
* **Poarta de ieÈ™ire** transformÄƒ statutul printr-un strat liniar cu activare *tanh*, apoi selecteazÄƒ unele dintre componentele sale folosind vectorul ascuns H<sub>i</sub> pentru a produce un nou statut C<sub>i+1</sub>.

Componentele statutului C pot fi considerate ca niÈ™te semnale care pot fi activate sau dezactivate. De exemplu, atunci cÃ¢nd Ã®ntÃ¢lnim un nume *Alice* Ã®n secvenÈ›Äƒ, putem presupune cÄƒ se referÄƒ la un personaj feminin È™i activÄƒm semnalul Ã®n statut cÄƒ avem un substantiv feminin Ã®n propoziÈ›ie. CÃ¢nd Ã®ntÃ¢lnim ulterior expresia *È™i Tom*, vom activa semnalul cÄƒ avem un substantiv plural. Astfel, prin manipularea statutului, putem presupune cÄƒ urmÄƒrim proprietÄƒÈ›ile gramaticale ale pÄƒrÈ›ilor propoziÈ›iei.

> âœ… O resursÄƒ excelentÄƒ pentru Ã®nÈ›elegerea detaliilor interne ale LSTM este acest articol minunat [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) de Christopher Olah.

## RNN-uri BidirecÈ›ionale È™i Multistrat

Am discutat despre reÈ›elele recurente care opereazÄƒ Ã®ntr-o singurÄƒ direcÈ›ie, de la Ã®nceputul unei secvenÈ›e pÃ¢nÄƒ la sfÃ¢rÈ™it. Pare natural, deoarece seamÄƒnÄƒ cu modul Ã®n care citim È™i ascultÄƒm vorbirea. TotuÈ™i, deoarece Ã®n multe cazuri practice avem acces aleatoriu la secvenÈ›a de intrare, ar putea avea sens sÄƒ rulÄƒm calculul recurent Ã®n ambele direcÈ›ii. Astfel de reÈ›ele se numesc **RNN-uri bidirecÈ›ionale**. CÃ¢nd lucrÄƒm cu o reÈ›ea bidirecÈ›ionalÄƒ, vom avea nevoie de doi vectori de statut ascuns, cÃ¢te unul pentru fiecare direcÈ›ie.

O reÈ›ea recurentÄƒ, fie unidirecÈ›ionalÄƒ, fie bidirecÈ›ionalÄƒ, capteazÄƒ anumite modele dintr-o secvenÈ›Äƒ È™i le poate stoca Ã®ntr-un vector de statut sau le poate transmite ca ieÈ™ire. La fel ca Ã®n cazul reÈ›elelor convoluÈ›ionale, putem construi un alt strat recurent deasupra primului pentru a capta modele de nivel superior È™i a construi din modelele de nivel inferior extrase de primul strat. Acest lucru ne conduce la noÈ›iunea de **RNN multistrat**, care constÄƒ din douÄƒ sau mai multe reÈ›ele recurente, unde ieÈ™irea stratului anterior este transmisÄƒ stratului urmÄƒtor ca intrare.

![Imagine care aratÄƒ un RNN multistrat cu LSTM](../../../../../translated_images/multi-layer-lstm.dd975e29bb2a59fe58b429db833932d734c81f211cad2783797a9608984acb8c.ro.jpg)

*Imagine din [acest articol minunat](https://towardsdatascience.com/from-a-lstm-cell-to-a-multilayer-lstm-network-with-pytorch-2899eb5696f3) de Fernando LÃ³pez*

## âœï¸ ExerciÈ›ii: Embedding-uri

ContinuÄƒ Ã®nvÄƒÈ›area Ã®n urmÄƒtoarele notebook-uri:

* [RNN-uri cu PyTorch](../../../../../lessons/5-NLP/16-RNN/RNNPyTorch.ipynb)
* [RNN-uri cu TensorFlow](../../../../../lessons/5-NLP/16-RNN/RNNTF.ipynb)

## Concluzie

Ãn aceastÄƒ unitate, am vÄƒzut cÄƒ RNN-urile pot fi utilizate pentru clasificarea secvenÈ›elor, dar de fapt, ele pot gestiona multe alte sarcini, cum ar fi generarea de text, traducerea automatÄƒ È™i altele. Vom analiza aceste sarcini Ã®n unitatea urmÄƒtoare.

## ğŸš€ Provocare

CiteÈ™te literaturÄƒ despre LSTM-uri È™i ia Ã®n considerare aplicaÈ›iile lor:

- [Grid Long Short-Term Memory](https://arxiv.org/pdf/1507.01526v1.pdf)
- [Show, Attend and Tell: Neural Image Caption
Generation with Visual Attention](https://arxiv.org/pdf/1502.03044v2.pdf)

## [Chestionar dupÄƒ curs](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/216)

## Recapitulare È™i Studiu Individual

- [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) de Christopher Olah.

## [TemÄƒ: Notebook-uri](assignment.md)

**Declinare de responsabilitate**:  
Acest document a fost tradus folosind serviciul de traducere AI [Co-op Translator](https://github.com/Azure/co-op-translator). DeÈ™i ne strÄƒduim sÄƒ asigurÄƒm acurateÈ›ea, vÄƒ rugÄƒm sÄƒ fiÈ›i conÈ™tienÈ›i cÄƒ traducerile automate pot conÈ›ine erori sau inexactitÄƒÈ›i. Documentul original Ã®n limba sa natalÄƒ ar trebui considerat sursa autoritarÄƒ. Pentru informaÈ›ii critice, se recomandÄƒ traducerea profesionalÄƒ realizatÄƒ de un specialist uman. Nu ne asumÄƒm responsabilitatea pentru eventualele neÃ®nÈ›elegeri sau interpretÄƒri greÈ™ite care pot apÄƒrea din utilizarea acestei traduceri.