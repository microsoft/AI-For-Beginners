# ReÈ›ele Neuronale Recurente

## [Chestionar Ã®nainte de curs](https://ff-quizzes.netlify.app/en/ai/quiz/31)

Ãn secÈ›iunile anterioare, am utilizat reprezentÄƒri semantice bogate ale textului È™i un clasificator liniar simplu deasupra embedding-urilor. AceastÄƒ arhitecturÄƒ capteazÄƒ sensul agregat al cuvintelor dintr-o propoziÈ›ie, dar nu ia Ã®n considerare **ordinea** cuvintelor, deoarece operaÈ›ia de agregare deasupra embedding-urilor eliminÄƒ aceastÄƒ informaÈ›ie din textul original. Deoarece aceste modele nu pot modela ordinea cuvintelor, ele nu pot rezolva sarcini mai complexe sau ambigue, cum ar fi generarea de text sau rÄƒspunsul la Ã®ntrebÄƒri.

Pentru a capta sensul unei secvenÈ›e de text, trebuie sÄƒ utilizÄƒm o altÄƒ arhitecturÄƒ de reÈ›ea neuronalÄƒ, numitÄƒ **reÈ›ea neuronalÄƒ recurentÄƒ**, sau RNN. Ãn RNN, trecem propoziÈ›ia prin reÈ›ea, un simbol la un moment dat, iar reÈ›eaua produce un **stare**, pe care o trecem din nou prin reÈ›ea Ã®mpreunÄƒ cu urmÄƒtorul simbol.

![RNN](../../../../../translated_images/ro/rnn.27f5c29c53d727b5.webp)

> Imagine realizatÄƒ de autor

AvÃ¢nd secvenÈ›a de intrare de tokeni X<sub>0</sub>,...,X<sub>n</sub>, RNN creeazÄƒ o secvenÈ›Äƒ de blocuri de reÈ›ea neuronalÄƒ È™i antreneazÄƒ aceastÄƒ secvenÈ›Äƒ cap-coadÄƒ folosind backpropagation. Fiecare bloc de reÈ›ea primeÈ™te o pereche (X<sub>i</sub>,S<sub>i</sub>) ca intrare È™i produce S<sub>i+1</sub> ca rezultat. Starea finalÄƒ S<sub>n</sub> sau (ieÈ™irea Y<sub>n</sub>) este introdusÄƒ Ã®ntr-un clasificator liniar pentru a produce rezultatul. Toate blocurile de reÈ›ea Ã®mpÄƒrtÄƒÈ™esc aceleaÈ™i greutÄƒÈ›i È™i sunt antrenate cap-coadÄƒ folosind o singurÄƒ trecere de backpropagation.

Deoarece vectorii de stare S<sub>0</sub>,...,S<sub>n</sub> sunt trecuÈ›i prin reÈ›ea, aceasta poate Ã®nvÄƒÈ›a dependenÈ›ele secvenÈ›iale dintre cuvinte. De exemplu, atunci cÃ¢nd cuvÃ¢ntul *nu* apare undeva Ã®n secvenÈ›Äƒ, reÈ›eaua poate Ã®nvÄƒÈ›a sÄƒ nege anumite elemente din vectorul de stare, rezultÃ¢nd o negare.

> âœ… Deoarece greutÄƒÈ›ile tuturor blocurilor RNN din imaginea de mai sus sunt Ã®mpÄƒrtÄƒÈ™ite, aceeaÈ™i imagine poate fi reprezentatÄƒ ca un singur bloc (Ã®n dreapta) cu un circuit de feedback recurent, care trece starea de ieÈ™ire a reÈ›elei Ã®napoi la intrare.

## Anatomia unei Celule RNN

SÄƒ vedem cum este organizatÄƒ o celulÄƒ RNN simplÄƒ. Aceasta acceptÄƒ starea anterioarÄƒ S<sub>i-1</sub> È™i simbolul curent X<sub>i</sub> ca intrÄƒri È™i trebuie sÄƒ producÄƒ starea de ieÈ™ire S<sub>i</sub> (È™i, uneori, ne intereseazÄƒ È™i o altÄƒ ieÈ™ire Y<sub>i</sub>, ca Ã®n cazul reÈ›elelor generative).

O celulÄƒ RNN simplÄƒ are douÄƒ matrice de greutÄƒÈ›i Ã®n interior: una transformÄƒ un simbol de intrare (sÄƒ o numim W), iar cealaltÄƒ transformÄƒ o stare de intrare (H). Ãn acest caz, ieÈ™irea reÈ›elei este calculatÄƒ ca &sigma;(W&times;X<sub>i</sub>+H&times;S<sub>i-1</sub>+b), unde &sigma; este funcÈ›ia de activare È™i b este un bias suplimentar.

<img alt="Anatomia unei celule RNN" src="../../../../../translated_images/ro/rnn-anatomy.79ee3f3920b3294b.webp" width="50%"/>

> Imagine realizatÄƒ de autor

Ãn multe cazuri, tokenii de intrare sunt trecuÈ›i prin stratul de embedding Ã®nainte de a intra Ã®n RNN pentru a reduce dimensionalitatea. Ãn acest caz, dacÄƒ dimensiunea vectorilor de intrare este *emb_size*, iar vectorul de stare este *hid_size* - dimensiunea lui W este *emb_size*&times;*hid_size*, iar dimensiunea lui H este *hid_size*&times;*hid_size*.

## Memorie pe Termen Lung È™i Scurt (LSTM)

Una dintre principalele probleme ale RNN-urilor clasice este problema numitÄƒ **gradienti care dispar**. Deoarece RNN-urile sunt antrenate cap-coadÄƒ Ã®ntr-o singurÄƒ trecere de backpropagation, acestea au dificultÄƒÈ›i Ã®n propagarea erorii cÄƒtre primele straturi ale reÈ›elei È™i, astfel, reÈ›eaua nu poate Ã®nvÄƒÈ›a relaÈ›ii Ã®ntre tokeni distanÈ›i. Una dintre modalitÄƒÈ›ile de a evita aceastÄƒ problemÄƒ este introducerea **gestionÄƒrii explicite a stÄƒrii** prin utilizarea aÈ™a-numitelor **porÈ›i**. ExistÄƒ douÄƒ arhitecturi bine cunoscute de acest tip: **Memorie pe Termen Lung È™i Scurt** (LSTM) È™i **Unitatea de Releu Gated** (GRU).

![Imagine care aratÄƒ un exemplu de celulÄƒ LSTM](../../../../../lessons/5-NLP/16-RNN/images/long-short-term-memory-cell.svg)

> Sursa imaginii TBD

ReÈ›eaua LSTM este organizatÄƒ Ã®ntr-un mod similar cu RNN, dar existÄƒ douÄƒ stÄƒri care sunt transmise de la un strat la altul: starea actualÄƒ C È™i vectorul ascuns H. La fiecare unitate, vectorul ascuns H<sub>i</sub> este concatenat cu intrarea X<sub>i</sub>, iar acestea controleazÄƒ ce se Ã®ntÃ¢mplÄƒ cu starea C prin intermediul **porÈ›ilor**. Fiecare poartÄƒ este o reÈ›ea neuronalÄƒ cu activare sigmoid (ieÈ™ire Ã®n intervalul [0,1]), care poate fi consideratÄƒ ca o mascÄƒ bitwise atunci cÃ¢nd este Ã®nmulÈ›itÄƒ cu vectorul de stare. ExistÄƒ urmÄƒtoarele porÈ›i (de la stÃ¢nga la dreapta Ã®n imaginea de mai sus):

* **Poarta de uitare** ia un vector ascuns È™i determinÄƒ ce componente ale vectorului C trebuie uitate È™i care trebuie transmise mai departe.
* **Poarta de intrare** preia informaÈ›ii din vectorii de intrare È™i ascunÈ™i È™i le insereazÄƒ Ã®n stare.
* **Poarta de ieÈ™ire** transformÄƒ starea printr-un strat liniar cu activare *tanh*, apoi selecteazÄƒ unele dintre componentele sale folosind un vector ascuns H<sub>i</sub> pentru a produce o nouÄƒ stare C<sub>i+1</sub>.

Componentele stÄƒrii C pot fi considerate ca niÈ™te semnale care pot fi activate sau dezactivate. De exemplu, cÃ¢nd Ã®ntÃ¢lnim un nume *Alice* Ã®n secvenÈ›Äƒ, putem presupune cÄƒ se referÄƒ la un personaj feminin È™i activÄƒm semnalul Ã®n stare cÄƒ avem un substantiv feminin Ã®n propoziÈ›ie. CÃ¢nd Ã®ntÃ¢lnim ulterior expresia *È™i Tom*, vom activa semnalul cÄƒ avem un substantiv la plural. Astfel, prin manipularea stÄƒrii, putem pÄƒstra proprietÄƒÈ›ile gramaticale ale pÄƒrÈ›ilor propoziÈ›iei.

> âœ… O resursÄƒ excelentÄƒ pentru Ã®nÈ›elegerea internelor LSTM este acest articol minunat [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) de Christopher Olah.

## RNN-uri BidirecÈ›ionale È™i Multistrat

Am discutat despre reÈ›elele recurente care opereazÄƒ Ã®ntr-o singurÄƒ direcÈ›ie, de la Ã®nceputul unei secvenÈ›e pÃ¢nÄƒ la sfÃ¢rÈ™it. Pare natural, deoarece seamÄƒnÄƒ cu modul Ã®n care citim È™i ascultÄƒm vorbirea. TotuÈ™i, deoarece Ã®n multe cazuri practice avem acces aleatoriu la secvenÈ›a de intrare, ar putea avea sens sÄƒ rulÄƒm calculul recurent Ã®n ambele direcÈ›ii. Astfel de reÈ›ele se numesc **RNN-uri bidirecÈ›ionale**. CÃ¢nd lucrÄƒm cu o reÈ›ea bidirecÈ›ionalÄƒ, vom avea nevoie de doi vectori de stare ascunsÄƒ, cÃ¢te unul pentru fiecare direcÈ›ie.

O reÈ›ea recurentÄƒ, fie unidirecÈ›ionalÄƒ, fie bidirecÈ›ionalÄƒ, capteazÄƒ anumite modele dintr-o secvenÈ›Äƒ È™i le poate stoca Ã®ntr-un vector de stare sau le poate transmite ca ieÈ™ire. La fel ca Ã®n cazul reÈ›elelor convoluÈ›ionale, putem construi un alt strat recurent deasupra primului pentru a capta modele de nivel superior È™i a construi din modelele de nivel inferior extrase de primul strat. Acest lucru ne conduce la noÈ›iunea de **RNN multistrat**, care constÄƒ din douÄƒ sau mai multe reÈ›ele recurente, unde ieÈ™irea stratului anterior este transmisÄƒ stratului urmÄƒtor ca intrare.

![Imagine care aratÄƒ un RNN LSTM multistrat](../../../../../translated_images/ro/multi-layer-lstm.dd975e29bb2a59fe.webp)

*Imagine din [acest articol minunat](https://towardsdatascience.com/from-a-lstm-cell-to-a-multilayer-lstm-network-with-pytorch-2899eb5696f3) de Fernando LÃ³pez*

## âœï¸ ExerciÈ›ii: Embedding-uri

ContinuÄƒ Ã®nvÄƒÈ›area Ã®n urmÄƒtoarele notebook-uri:

* [RNN-uri cu PyTorch](RNNPyTorch.ipynb)
* [RNN-uri cu TensorFlow](RNNTF.ipynb)

## Concluzie

Ãn aceastÄƒ unitate, am vÄƒzut cÄƒ RNN-urile pot fi utilizate pentru clasificarea secvenÈ›elor, dar, de fapt, ele pot gestiona multe alte sarcini, cum ar fi generarea de text, traducerea automatÄƒ È™i altele. Vom analiza aceste sarcini Ã®n unitatea urmÄƒtoare.

## ğŸš€ Provocare

CiteÈ™te literaturÄƒ despre LSTM-uri È™i ia Ã®n considerare aplicaÈ›iile lor:

- [Grid Long Short-Term Memory](https://arxiv.org/pdf/1507.01526v1.pdf)
- [Show, Attend and Tell: Neural Image Caption
Generation with Visual Attention](https://arxiv.org/pdf/1502.03044v2.pdf)

## [Chestionar dupÄƒ curs](https://ff-quizzes.netlify.app/en/ai/quiz/32)

## Recapitulare & Studiu Individual

- [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) de Christopher Olah.

## [TemÄƒ: Notebook-uri](assignment.md)

---

