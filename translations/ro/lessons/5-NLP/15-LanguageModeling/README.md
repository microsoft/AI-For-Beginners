<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "31b46ba1f3aa78578134d4829f88be53",
  "translation_date": "2025-08-25T21:56:03+00:00",
  "source_file": "lessons/5-NLP/15-LanguageModeling/README.md",
  "language_code": "ro"
}
-->
# Modelarea Limbajului

ÃncorporÄƒrile semantice, precum Word2Vec È™i GloVe, reprezintÄƒ de fapt un prim pas cÄƒtre **modelarea limbajului** - crearea unor modele care cumva *Ã®nÈ›eleg* (sau *reprezintÄƒ*) natura limbajului.

## [Chestionar Ã®nainte de lecÈ›ie](https://ff-quizzes.netlify.app/en/ai/quiz/29)

Ideea principalÄƒ din spatele modelÄƒrii limbajului este antrenarea acestora pe seturi de date neetichetate Ã®ntr-un mod nesupravegheat. Acest lucru este important deoarece avem la dispoziÈ›ie cantitÄƒÈ›i uriaÈ™e de text neetichetat, Ã®n timp ce cantitatea de text etichetat va fi Ã®ntotdeauna limitatÄƒ de efortul pe care Ã®l putem depune pentru etichetare. De cele mai multe ori, putem construi modele de limbaj care pot **prezice cuvinte lipsÄƒ** Ã®n text, deoarece este uÈ™or sÄƒ mascÄƒm un cuvÃ¢nt aleatoriu din text È™i sÄƒ-l folosim ca exemplu de antrenament.

## Antrenarea ÃncorporÄƒrilor

Ãn exemplele anterioare, am folosit Ã®ncorporÄƒri semantice pre-antrenate, dar este interesant sÄƒ vedem cum pot fi antrenate aceste Ã®ncorporÄƒri. ExistÄƒ mai multe idei posibile care pot fi utilizate:

* **Modelarea limbajului cu N-Gram**, unde prezicem un token analizÃ¢nd N token-uri anterioare (N-gram).
* **Continuous Bag-of-Words** (CBoW), unde prezicem token-ul din mijloc $W_0$ Ã®ntr-o secvenÈ›Äƒ de token-uri $W_{-N}$, ..., $W_N$.
* **Skip-gram**, unde prezicem un set de token-uri vecine {$W_{-N},\dots, W_{-1}, W_1,\dots, W_N$} pornind de la token-ul din mijloc $W_0$.

![imagine dintr-un articol despre conversia cuvintelor Ã®n vectori](../../../../../translated_images/example-algorithms-for-converting-words-to-vectors.fbe9207a726922f6f0f5de66427e8a6eda63809356114e28fb1fa5f4a83ebda7.ro.png)

> Imagine din [acest articol](https://arxiv.org/pdf/1301.3781.pdf)

## âœï¸ Exemple de Notebook-uri: Antrenarea modelului CBoW

ContinuÄƒ Ã®nvÄƒÈ›area cu urmÄƒtoarele notebook-uri:

* [Antrenarea CBoW Word2Vec cu TensorFlow](../../../../../lessons/5-NLP/15-LanguageModeling/CBoW-TF.ipynb)
* [Antrenarea CBoW Word2Vec cu PyTorch](../../../../../lessons/5-NLP/15-LanguageModeling/CBoW-PyTorch.ipynb)

## Concluzie

Ãn lecÈ›ia anterioarÄƒ am vÄƒzut cÄƒ Ã®ncorporÄƒrile de cuvinte funcÈ›ioneazÄƒ ca prin magie! Acum È™tim cÄƒ antrenarea Ã®ncorporÄƒrilor de cuvinte nu este o sarcinÄƒ foarte complexÄƒ È™i ar trebui sÄƒ putem antrena propriile Ã®ncorporÄƒri pentru texte specifice unui domeniu, dacÄƒ este necesar.

## [Chestionar dupÄƒ lecÈ›ie](https://ff-quizzes.netlify.app/en/ai/quiz/30)

## Recapitulare È™i Studiu Individual

* [Tutorial oficial PyTorch despre Modelarea Limbajului](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html).
* [Tutorial oficial TensorFlow despre antrenarea modelului Word2Vec](https://www.TensorFlow.org/tutorials/text/word2vec).
* Utilizarea framework-ului **gensim** pentru a antrena cele mai utilizate Ã®ncorporÄƒri Ã®n cÃ¢teva linii de cod este descrisÄƒ [Ã®n aceastÄƒ documentaÈ›ie](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html).

## ğŸš€ [TemÄƒ: AntreneazÄƒ Modelul Skip-Gram](lab/README.md)

Ãn laborator, te provocÄƒm sÄƒ modifici codul din aceastÄƒ lecÈ›ie pentru a antrena un model skip-gram Ã®n loc de CBoW. [CiteÈ™te detaliile](lab/README.md)

**Declinare de responsabilitate**:  
Acest document a fost tradus folosind serviciul de traducere AI [Co-op Translator](https://github.com/Azure/co-op-translator). DeÈ™i ne strÄƒduim sÄƒ asigurÄƒm acurateÈ›ea, vÄƒ rugÄƒm sÄƒ fiÈ›i conÈ™tienÈ›i cÄƒ traducerile automate pot conÈ›ine erori sau inexactitÄƒÈ›i. Documentul original Ã®n limba sa natalÄƒ ar trebui considerat sursa autoritarÄƒ. Pentru informaÈ›ii critice, se recomandÄƒ traducerea profesionalÄƒ realizatÄƒ de un specialist uman. Nu ne asumÄƒm responsabilitatea pentru eventualele neÃ®nÈ›elegeri sau interpretÄƒri greÈ™ite care pot apÄƒrea din utilizarea acestei traduceri.