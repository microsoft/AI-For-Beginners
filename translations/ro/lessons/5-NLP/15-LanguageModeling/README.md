# Modelarea Limbajului

칉ncorpor캒rile semantice, precum Word2Vec 탳i GloVe, reprezint캒 de fapt un prim pas c캒tre **modelarea limbajului** - crearea de modele care 칥ntr-un fel *칥n탵eleg* (sau *reprezint캒*) natura limbajului.

## [Chestionar 칥nainte de lec탵ie](https://ff-quizzes.netlify.app/en/ai/quiz/29)

Ideea principal캒 din spatele model캒rii limbajului este antrenarea acestora pe seturi de date neetichetate 칥ntr-un mod nesupravegheat. Acest lucru este important deoarece avem la dispozi탵ie cantit캒탵i uria탳e de text neetichetat, 칥n timp ce cantitatea de text etichetat va fi 칥ntotdeauna limitat캒 de efortul pe care 칥l putem depune pentru etichetare. Cel mai adesea, putem construi modele de limbaj care pot **prezice cuvintele lips캒** din text, deoarece este u탳or s캒 masc캒m un cuv칙nt aleator din text 탳i s캒-l folosim ca exemplu de antrenament.

## Antrenarea 칉ncorpor캒rilor

칉n exemplele anterioare, am folosit 칥ncorpor캒ri semantice pre-antrenate, dar este interesant s캒 vedem cum pot fi antrenate aceste 칥ncorpor캒ri. Exist캒 mai multe idei posibile care pot fi utilizate:

* **Modelarea limbajului N-Gram**, c칙nd prezicem un token analiz칙nd N tokeni anteriori (N-gram).
* **Continuous Bag-of-Words** (CBoW), c칙nd prezicem tokenul din mijloc $W_0$ 칥ntr-o secven탵캒 de tokeni $W_{-N}$, ..., $W_N$.
* **Skip-gram**, unde prezicem un set de tokeni vecini {$W_{-N},\dots, W_{-1}, W_1,\dots, W_N$} pornind de la tokenul din mijloc $W_0$.

![imagine dintr-un articol despre convertirea cuvintelor 칥n vectori](../../../../../translated_images/ro/example-algorithms-for-converting-words-to-vectors.fbe9207a726922f6.webp)

> Imagine din [acest articol](https://arxiv.org/pdf/1301.3781.pdf)

## 九꽲잺 Exemple de Notebook-uri: Antrenarea modelului CBoW

Continu캒 칥nv캒탵area 칥n urm캒toarele notebook-uri:

* [Antrenarea CBoW Word2Vec cu TensorFlow](CBoW-TF.ipynb)
* [Antrenarea CBoW Word2Vec cu PyTorch](CBoW-PyTorch.ipynb)

## Concluzie

칉n lec탵ia anterioar캒 am v캒zut c캒 칥ncorpor캒rile de cuvinte func탵ioneaz캒 ca prin magie! Acum 탳tim c캒 antrenarea 칥ncorpor캒rilor de cuvinte nu este o sarcin캒 foarte complex캒 탳i ar trebui s캒 fim capabili s캒 ne antren캒m propriile 칥ncorpor캒ri de cuvinte pentru texte specifice unui domeniu, dac캒 este necesar.

## [Chestionar dup캒 lec탵ie](https://ff-quizzes.netlify.app/en/ai/quiz/30)

## Recapitulare & Studiu Individual

* [Tutorial oficial PyTorch despre Modelarea Limbajului](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html).
* [Tutorial oficial TensorFlow despre antrenarea modelului Word2Vec](https://www.TensorFlow.org/tutorials/text/word2vec).
* Utilizarea framework-ului **gensim** pentru a antrena cele mai utilizate 칥ncorpor캒ri 칥n c칙teva linii de cod este descris캒 [칥n aceast캒 documenta탵ie](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html).

## 游 [Tem캒: Antreneaz캒 Modelul Skip-Gram](lab/README.md)

칉n laborator, te provoc캒m s캒 modifici codul din aceast캒 lec탵ie pentru a antrena un model skip-gram 칥n loc de CBoW. [Cite탳te detaliile](lab/README.md)

---

