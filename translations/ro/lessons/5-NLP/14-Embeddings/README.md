# ÃncapsulÄƒri

## [Chestionar Ã®nainte de lecÈ›ie](https://ff-quizzes.netlify.app/en/ai/quiz/27)

CÃ¢nd antrenam clasificatori bazÃ¢ndu-ne pe BoW sau TF/IDF, lucram cu vectori bag-of-words de dimensiuni mari, cu lungimea `vocab_size`, È™i transformam explicit vectorii de reprezentare poziÈ›ionalÄƒ de dimensiuni mici Ã®n reprezentÄƒri sparse one-hot. TotuÈ™i, aceastÄƒ reprezentare one-hot nu este eficientÄƒ din punct de vedere al memoriei. Ãn plus, fiecare cuvÃ¢nt este tratat independent de celelalte, adicÄƒ vectorii one-hot codificaÈ›i nu exprimÄƒ nicio similaritate semanticÄƒ Ã®ntre cuvinte.

Ideea de **Ã®ncapsulare** este de a reprezenta cuvintele prin vectori densi de dimensiuni mai mici, care reflectÄƒ Ã®ntr-un fel semnificaÈ›ia semanticÄƒ a unui cuvÃ¢nt. Vom discuta mai tÃ¢rziu cum sÄƒ construim Ã®ncapsulÄƒri semnificative pentru cuvinte, dar pentru moment sÄƒ ne gÃ¢ndim la Ã®ncapsulÄƒri ca la o modalitate de a reduce dimensiunea vectorului unui cuvÃ¢nt.

Astfel, stratul de Ã®ncapsulare ar lua un cuvÃ¢nt ca intrare È™i ar produce un vector de ieÈ™ire cu dimensiunea specificatÄƒ `embedding_size`. Ãntr-un sens, este foarte similar cu un strat `Linear`, dar Ã®n loc sÄƒ ia un vector one-hot codificat, va putea lua un numÄƒr de cuvÃ¢nt ca intrare, permiÈ›Ã¢ndu-ne sÄƒ evitÄƒm crearea unor vectori one-hot codificaÈ›i de dimensiuni mari.

Folosind un strat de Ã®ncapsulare ca prim strat Ã®n reÈ›eaua noastrÄƒ de clasificare, putem trece de la un model bag-of-words la un model **embedding bag**, unde mai Ã®ntÃ¢i convertim fiecare cuvÃ¢nt din textul nostru Ã®n Ã®ncapsularea corespunzÄƒtoare, È™i apoi calculÄƒm o funcÈ›ie agregatÄƒ peste toate aceste Ã®ncapsulÄƒri, cum ar fi `sum`, `average` sau `max`.

![Imagine care aratÄƒ un clasificator bazat pe Ã®ncapsulÄƒri pentru cinci cuvinte dintr-o secvenÈ›Äƒ.](../../../../../translated_images/ro/embedding-classifier-example.b77f021a7ee67eee.webp)

> Imagine realizatÄƒ de autor

## âœï¸ ExerciÈ›ii: ÃncapsulÄƒri

ContinuÄƒ Ã®nvÄƒÈ›area Ã®n urmÄƒtoarele notebook-uri:
* [ÃncapsulÄƒri cu PyTorch](EmbeddingsPyTorch.ipynb)
* [ÃncapsulÄƒri TensorFlow](EmbeddingsTF.ipynb)

## ÃncapsulÄƒri Semantice: Word2Vec

DeÈ™i stratul de Ã®ncapsulare a Ã®nvÄƒÈ›at sÄƒ mapaze cuvintele Ã®n reprezentÄƒri vectoriale, aceastÄƒ reprezentare nu avea neapÄƒrat o semnificaÈ›ie semanticÄƒ profundÄƒ. Ar fi ideal sÄƒ Ã®nvÄƒÈ›Äƒm o reprezentare vectorialÄƒ astfel Ã®ncÃ¢t cuvintele similare sau sinonime sÄƒ corespundÄƒ unor vectori apropiaÈ›i Ã®ntre ei Ã®n termeni de o anumitÄƒ distanÈ›Äƒ vectorialÄƒ (de exemplu, distanÈ›a EuclidianÄƒ).

Pentru a face acest lucru, trebuie sÄƒ pre-antrenÄƒm modelul de Ã®ncapsulare pe o colecÈ›ie mare de texte Ã®ntr-un mod specific. O modalitate de a antrena Ã®ncapsulÄƒri semantice se numeÈ™te [Word2Vec](https://en.wikipedia.org/wiki/Word2vec). Se bazeazÄƒ pe douÄƒ arhitecturi principale utilizate pentru a produce o reprezentare distribuitÄƒ a cuvintelor:

 - **Continuous bag-of-words** (CBoW) â€” Ã®n aceastÄƒ arhitecturÄƒ, antrenÄƒm modelul sÄƒ prezicÄƒ un cuvÃ¢nt din contextul Ã®nconjurÄƒtor. AvÃ¢nd ngram-ul $(W_{-2},W_{-1},W_0,W_1,W_2)$, scopul modelului este sÄƒ prezicÄƒ $W_0$ din $(W_{-2},W_{-1},W_1,W_2)$.
 - **Continuous skip-gram** este opusul CBoW. Modelul foloseÈ™te fereastra de context a cuvintelor Ã®nconjurÄƒtoare pentru a prezice cuvÃ¢ntul curent.

CBoW este mai rapid, Ã®n timp ce skip-gram este mai lent, dar face o treabÄƒ mai bunÄƒ Ã®n reprezentarea cuvintelor rare.

![Imagine care aratÄƒ algoritmii CBoW È™i Skip-Gram pentru conversia cuvintelor Ã®n vectori.](../../../../../translated_images/ro/example-algorithms-for-converting-words-to-vectors.fbe9207a726922f6.webp)

> Imagine din [acest articol](https://arxiv.org/pdf/1301.3781.pdf)

ÃncapsulÄƒrile pre-antrenate Word2Vec (precum È™i alte modele similare, cum ar fi GloVe) pot fi utilizate Ã®n locul stratului de Ã®ncapsulare Ã®n reÈ›elele neuronale. TotuÈ™i, trebuie sÄƒ gestionÄƒm vocabularul, deoarece vocabularul utilizat pentru pre-antrenarea Word2Vec/GloVe este probabil diferit de vocabularul din corpusul nostru de texte. ConsultÄƒ notebook-urile de mai sus pentru a vedea cum poate fi rezolvatÄƒ aceastÄƒ problemÄƒ.

## ÃncapsulÄƒri Contextuale

O limitare cheie a reprezentÄƒrilor tradiÈ›ionale pre-antrenate, cum ar fi Word2Vec, este problema dezambiguizÄƒrii sensului cuvintelor. DeÈ™i Ã®ncapsulÄƒrile pre-antrenate pot captura o parte din semnificaÈ›ia cuvintelor Ã®n context, fiecare posibil sens al unui cuvÃ¢nt este codificat Ã®n aceeaÈ™i Ã®ncapsulare. Acest lucru poate cauza probleme Ã®n modelele ulterioare, deoarece multe cuvinte, cum ar fi cuvÃ¢ntul â€playâ€, au semnificaÈ›ii diferite Ã®n funcÈ›ie de contextul Ã®n care sunt utilizate.

De exemplu, cuvÃ¢ntul â€playâ€ Ã®n aceste douÄƒ propoziÈ›ii are semnificaÈ›ii destul de diferite:

- Am fost la o **piesÄƒ** de teatru.
- John vrea sÄƒ se **joace** cu prietenii sÄƒi.

ÃncapsulÄƒrile pre-antrenate de mai sus reprezintÄƒ ambele sensuri ale cuvÃ¢ntului â€playâ€ Ã®n aceeaÈ™i Ã®ncapsulare. Pentru a depÄƒÈ™i aceastÄƒ limitare, trebuie sÄƒ construim Ã®ncapsulÄƒri bazate pe **modelul lingvistic**, care este antrenat pe un corpus mare de texte È™i *È™tie* cum pot fi utilizate cuvintele Ã®n diferite contexte. Discutarea Ã®ncapsulÄƒrilor contextuale este Ã®n afara scopului acestui tutorial, dar vom reveni la ele cÃ¢nd vom vorbi despre modelele lingvistice mai tÃ¢rziu Ã®n curs.

## Concluzie

Ãn aceastÄƒ lecÈ›ie, ai descoperit cum sÄƒ construieÈ™ti È™i sÄƒ utilizezi straturi de Ã®ncapsulare Ã®n TensorFlow È™i PyTorch pentru a reflecta mai bine semnificaÈ›iile semantice ale cuvintelor.

## ğŸš€ Provocare

Word2Vec a fost utilizat pentru unele aplicaÈ›ii interesante, inclusiv generarea de versuri È™i poezii. AruncÄƒ o privire la [acest articol](https://www.politetype.com/blog/word2vec-color-poems) care explicÄƒ cum autorul a folosit Word2Vec pentru a genera poezie. UrmÄƒreÈ™te È™i [acest videoclip de Dan Shiffmann](https://www.youtube.com/watch?v=LSS_bos_TPI&ab_channel=TheCodingTrain) pentru a descoperi o altÄƒ explicaÈ›ie a acestei tehnici. Apoi Ã®ncearcÄƒ sÄƒ aplici aceste tehnici pe propriul tÄƒu corpus de texte, poate preluat de pe Kaggle.

## [Chestionar dupÄƒ lecÈ›ie](https://ff-quizzes.netlify.app/en/ai/quiz/28)

## Recapitulare È™i Studiu Individual

CiteÈ™te acest articol despre Word2Vec: [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf)

## [TemÄƒ: Notebook-uri](assignment.md)

---

