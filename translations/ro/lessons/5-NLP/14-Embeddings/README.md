<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "e40b47ac3fd48f71304ede1474e66293",
  "translation_date": "2025-08-25T21:39:21+00:00",
  "source_file": "lessons/5-NLP/14-Embeddings/README.md",
  "language_code": "ro"
}
-->
# ÃncapsulÄƒri

## [Chestionar Ã®nainte de curs](https://ff-quizzes.netlify.app/en/ai/quiz/27)

CÃ¢nd antrenam clasificatori bazÃ¢ndu-ne pe BoW sau TF/IDF, lucram cu vectori de cuvinte de dimensiuni mari, avÃ¢nd lungimea `vocab_size`, È™i transformam explicit vectorii de reprezentare poziÈ›ionalÄƒ de dimensiuni mici Ã®n reprezentÄƒri sparse one-hot. TotuÈ™i, aceastÄƒ reprezentare one-hot nu este eficientÄƒ din punct de vedere al memoriei. Ãn plus, fiecare cuvÃ¢nt este tratat independent de celelalte, adicÄƒ vectorii one-hot codificaÈ›i nu exprimÄƒ nicio similaritate semanticÄƒ Ã®ntre cuvinte.

Ideea de **Ã®ncapsulare** este de a reprezenta cuvintele prin vectori densi de dimensiuni mai mici, care reflectÄƒ Ã®ntr-un fel semnificaÈ›ia semanticÄƒ a unui cuvÃ¢nt. Vom discuta mai tÃ¢rziu cum sÄƒ construim Ã®ncapsulÄƒri semnificative pentru cuvinte, dar pentru moment sÄƒ ne gÃ¢ndim la Ã®ncapsulÄƒri ca la o modalitate de a reduce dimensiunea unui vector de cuvinte.

Astfel, stratul de Ã®ncapsulare ar lua un cuvÃ¢nt ca intrare È™i ar produce un vector de ieÈ™ire de dimensiunea specificatÄƒ `embedding_size`. Ãntr-un sens, este foarte similar cu un strat `Linear`, dar Ã®n loc sÄƒ ia un vector one-hot codificat, va putea lua un numÄƒr de cuvÃ¢nt ca intrare, permiÈ›Ã¢ndu-ne sÄƒ evitÄƒm crearea de vectori one-hot codificaÈ›i mari.

Folosind un strat de Ã®ncapsulare ca prim strat Ã®n reÈ›eaua noastrÄƒ de clasificare, putem trece de la un model bag-of-words la un model **embedding bag**, unde mai Ã®ntÃ¢i convertim fiecare cuvÃ¢nt din textul nostru Ã®n Ã®ncapsularea corespunzÄƒtoare, È™i apoi calculÄƒm o funcÈ›ie agregatÄƒ peste toate aceste Ã®ncapsulÄƒri, cum ar fi `sum`, `average` sau `max`.

![Imagine care aratÄƒ un clasificator bazat pe Ã®ncapsulÄƒri pentru cinci cuvinte dintr-o secvenÈ›Äƒ.](../../../../../translated_images/embedding-classifier-example.b77f021a7ee67eeec8e68bfe11636c5b97d6eaa067515a129bfb1d0034b1ac5b.ro.png)

> Imagine realizatÄƒ de autor

## âœï¸ ExerciÈ›ii: ÃncapsulÄƒri

ContinuÄƒ Ã®nvÄƒÈ›area Ã®n urmÄƒtoarele notebook-uri:
* [ÃncapsulÄƒri cu PyTorch](../../../../../lessons/5-NLP/14-Embeddings/EmbeddingsPyTorch.ipynb)
* [ÃncapsulÄƒri TensorFlow](../../../../../lessons/5-NLP/14-Embeddings/EmbeddingsTF.ipynb)

## ÃncapsulÄƒri Semantice: Word2Vec

DeÈ™i stratul de Ã®ncapsulare a Ã®nvÄƒÈ›at sÄƒ mapaze cuvintele la o reprezentare vectorialÄƒ, aceastÄƒ reprezentare nu avea neapÄƒrat o semnificaÈ›ie semanticÄƒ profundÄƒ. Ar fi ideal sÄƒ Ã®nvÄƒÈ›Äƒm o reprezentare vectorialÄƒ astfel Ã®ncÃ¢t cuvintele similare sau sinonime sÄƒ corespundÄƒ unor vectori apropiaÈ›i Ã®ntre ei Ã®n termeni de o anumitÄƒ distanÈ›Äƒ vectorialÄƒ (de exemplu, distanÈ›a EuclidianÄƒ).

Pentru a face acest lucru, trebuie sÄƒ pre-antrenÄƒm modelul de Ã®ncapsulare pe o colecÈ›ie mare de texte Ã®ntr-un mod specific. O modalitate de a antrena Ã®ncapsulÄƒri semantice se numeÈ™te [Word2Vec](https://en.wikipedia.org/wiki/Word2vec). Se bazeazÄƒ pe douÄƒ arhitecturi principale utilizate pentru a produce o reprezentare distribuitÄƒ a cuvintelor:

 - **Bag-of-words continuu** (CBoW) â€” Ã®n aceastÄƒ arhitecturÄƒ, antrenÄƒm modelul sÄƒ prezicÄƒ un cuvÃ¢nt din contextul Ã®nconjurÄƒtor. Dat ngram-ul $(W_{-2},W_{-1},W_0,W_1,W_2)$, scopul modelului este sÄƒ prezicÄƒ $W_0$ din $(W_{-2},W_{-1},W_1,W_2)$.
 - **Skip-gram continuu** este opusul lui CBoW. Modelul foloseÈ™te fereastra de context a cuvintelor Ã®nconjurÄƒtoare pentru a prezice cuvÃ¢ntul curent.

CBoW este mai rapid, Ã®n timp ce skip-gram este mai lent, dar face o treabÄƒ mai bunÄƒ Ã®n reprezentarea cuvintelor rare.

![Imagine care aratÄƒ algoritmii CBoW È™i Skip-Gram pentru conversia cuvintelor Ã®n vectori.](../../../../../translated_images/example-algorithms-for-converting-words-to-vectors.fbe9207a726922f6f0f5de66427e8a6eda63809356114e28fb1fa5f4a83ebda7.ro.png)

> Imagine din [acest articol](https://arxiv.org/pdf/1301.3781.pdf)

ÃncapsulÄƒrile pre-antrenate Word2Vec (precum È™i alte modele similare, cum ar fi GloVe) pot fi utilizate Ã®n locul stratului de Ã®ncapsulare Ã®n reÈ›elele neuronale. TotuÈ™i, trebuie sÄƒ gestionÄƒm vocabularul, deoarece vocabularul utilizat pentru pre-antrenarea Word2Vec/GloVe este probabil diferit de vocabularul din corpusul nostru de texte. ConsultÄƒ notebook-urile de mai sus pentru a vedea cum poate fi rezolvatÄƒ aceastÄƒ problemÄƒ.

## ÃncapsulÄƒri Contextuale

O limitare cheie a reprezentÄƒrilor tradiÈ›ionale de Ã®ncapsulare pre-antrenate, cum ar fi Word2Vec, este problema dezambiguizÄƒrii sensului cuvintelor. DeÈ™i Ã®ncapsulÄƒrile pre-antrenate pot captura o parte din semnificaÈ›ia cuvintelor Ã®n context, fiecare posibil sens al unui cuvÃ¢nt este codificat Ã®n aceeaÈ™i Ã®ncapsulare. Acest lucru poate cauza probleme Ã®n modelele ulterioare, deoarece multe cuvinte, cum ar fi cuvÃ¢ntul â€playâ€, au semnificaÈ›ii diferite Ã®n funcÈ›ie de contextul Ã®n care sunt utilizate.

De exemplu, cuvÃ¢ntul â€playâ€ Ã®n cele douÄƒ propoziÈ›ii de mai jos are semnificaÈ›ii destul de diferite:

- Am fost la o **piesÄƒ** de teatru.
- John vrea sÄƒ **se joace** cu prietenii sÄƒi.

ÃncapsulÄƒrile pre-antrenate de mai sus reprezintÄƒ ambele sensuri ale cuvÃ¢ntului â€playâ€ Ã®n aceeaÈ™i Ã®ncapsulare. Pentru a depÄƒÈ™i aceastÄƒ limitare, trebuie sÄƒ construim Ã®ncapsulÄƒri bazate pe **modelul lingvistic**, care este antrenat pe un corpus mare de texte È™i *È™tie* cum pot fi utilizate cuvintele Ã®n diferite contexte. Discutarea Ã®ncapsulÄƒrilor contextuale este Ã®n afara scopului acestui tutorial, dar vom reveni la ele cÃ¢nd vom vorbi despre modelele lingvistice mai tÃ¢rziu Ã®n curs.

## Concluzie

Ãn aceastÄƒ lecÈ›ie, ai descoperit cum sÄƒ construieÈ™ti È™i sÄƒ utilizezi straturi de Ã®ncapsulare Ã®n TensorFlow È™i PyTorch pentru a reflecta mai bine semnificaÈ›iile semantice ale cuvintelor.

## ğŸš€ Provocare

Word2Vec a fost utilizat pentru unele aplicaÈ›ii interesante, inclusiv generarea de versuri È™i poezii. AruncÄƒ o privire la [acest articol](https://www.politetype.com/blog/word2vec-color-poems) care explicÄƒ cum autorul a folosit Word2Vec pentru a genera poezie. UrmÄƒreÈ™te È™i [acest videoclip de Dan Shiffmann](https://www.youtube.com/watch?v=LSS_bos_TPI&ab_channel=TheCodingTrain) pentru a descoperi o altÄƒ explicaÈ›ie a acestei tehnici. Apoi Ã®ncearcÄƒ sÄƒ aplici aceste tehnici pe propriul tÄƒu corpus de texte, poate obÈ›inut de pe Kaggle.

## [Chestionar dupÄƒ curs](https://ff-quizzes.netlify.app/en/ai/quiz/28)

## Recapitulare È™i Studiu Individual

CiteÈ™te acest articol despre Word2Vec: [Estimarea EficientÄƒ a ReprezentÄƒrilor Cuvintelor Ã®n SpaÈ›iul Vectorial](https://arxiv.org/pdf/1301.3781.pdf)

## [TemÄƒ: Notebook-uri](assignment.md)

**Declinare de responsabilitate**:  
Acest document a fost tradus folosind serviciul de traducere AI [Co-op Translator](https://github.com/Azure/co-op-translator). DeÈ™i ne strÄƒduim sÄƒ asigurÄƒm acurateÈ›ea, vÄƒ rugÄƒm sÄƒ fiÈ›i conÈ™tienÈ›i cÄƒ traducerile automate pot conÈ›ine erori sau inexactitÄƒÈ›i. Documentul original Ã®n limba sa natalÄƒ ar trebui considerat sursa autoritarÄƒ. Pentru informaÈ›ii critice, se recomandÄƒ traducerea profesionalÄƒ realizatÄƒ de un specialist uman. Nu ne asumÄƒm responsabilitatea pentru eventualele neÃ®nÈ›elegeri sau interpretÄƒri greÈ™ite care pot apÄƒrea din utilizarea acestei traduceri.