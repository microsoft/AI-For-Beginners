# Frameworkuri pentru ReÈ›ele Neuronale

AÈ™a cum am Ã®nvÄƒÈ›at deja, pentru a putea antrena reÈ›ele neuronale eficient, trebuie sÄƒ facem douÄƒ lucruri:

* SÄƒ operÄƒm pe tensori, de exemplu sÄƒ multiplicÄƒm, sÄƒ adunÄƒm È™i sÄƒ calculÄƒm funcÈ›ii precum sigmoid sau softmax
* SÄƒ calculÄƒm derivatele tuturor expresiilor, pentru a efectua optimizarea prin gradient descent

## [Chestionar Ã®nainte de lecÈ›ie](https://ff-quizzes.netlify.app/en/ai/quiz/9)

DeÈ™i biblioteca `numpy` poate realiza prima parte, avem nevoie de un mecanism pentru a calcula derivatele. Ãn [frameworkul nostru](../04-OwnFramework/OwnFramework.ipynb) pe care l-am dezvoltat Ã®n secÈ›iunea anterioarÄƒ, a trebuit sÄƒ programÄƒm manual toate funcÈ›iile derivate Ã®n metoda `backward`, care realizeazÄƒ backpropagation. Ideal, un framework ar trebui sÄƒ ne ofere posibilitatea de a calcula derivatele pentru *orice expresie* pe care o putem defini.

Un alt aspect important este sÄƒ putem efectua calcule pe GPU sau pe alte unitÄƒÈ›i de calcul specializate, cum ar fi [TPU](https://en.wikipedia.org/wiki/Tensor_Processing_Unit). Antrenarea reÈ›elelor neuronale profunde necesitÄƒ *foarte multe* calcule, iar posibilitatea de a paraleliza aceste calcule pe GPU-uri este esenÈ›ialÄƒ.

> âœ… Termenul 'paralelizare' Ã®nseamnÄƒ distribuirea calculelor pe mai multe dispozitive.

Ãn prezent, cele mai populare frameworkuri pentru reÈ›ele neuronale sunt: [TensorFlow](http://TensorFlow.org) È™i [PyTorch](https://pytorch.org/). Ambele oferÄƒ o API de nivel scÄƒzut pentru a opera cu tensori atÃ¢t pe CPU, cÃ¢t È™i pe GPU. Pe lÃ¢ngÄƒ API-ul de nivel scÄƒzut, existÄƒ È™i API-uri de nivel Ã®nalt, numite [Keras](https://keras.io/) È™i [PyTorch Lightning](https://pytorchlightning.ai), respectiv.

Low-Level API | [TensorFlow](http://TensorFlow.org) | [PyTorch](https://pytorch.org/)
--------------|-------------------------------------|--------------------------------
High-level API| [Keras](https://keras.io/) | [PyTorch Lightning](https://pytorchlightning.ai/)

**API-urile de nivel scÄƒzut** din ambele frameworkuri permit construirea aÈ™a-numitelor **grafuri computaÈ›ionale**. Acest graf defineÈ™te modul de calcul al rezultatului (de obicei funcÈ›ia de pierdere) cu parametrii de intrare daÈ›i È™i poate fi trimis pentru calcul pe GPU, dacÄƒ este disponibil. ExistÄƒ funcÈ›ii pentru a diferenÈ›ia acest graf computaÈ›ional È™i a calcula derivatele, care pot fi apoi utilizate pentru optimizarea parametrilor modelului.

**API-urile de nivel Ã®nalt** considerÄƒ reÈ›elele neuronale ca o **succesiune de straturi** È™i faciliteazÄƒ construirea majoritÄƒÈ›ii reÈ›elelor neuronale. Antrenarea modelului necesitÄƒ, de obicei, pregÄƒtirea datelor È™i apoi apelarea unei funcÈ›ii `fit` pentru a realiza procesul.

API-ul de nivel Ã®nalt permite construirea rapidÄƒ a reÈ›elelor neuronale tipice fÄƒrÄƒ a fi nevoie sÄƒ ne preocupÄƒm de multe detalii. Ãn acelaÈ™i timp, API-ul de nivel scÄƒzut oferÄƒ mult mai mult control asupra procesului de antrenare È™i, astfel, este utilizat frecvent Ã®n cercetare, atunci cÃ¢nd se lucreazÄƒ cu noi arhitecturi de reÈ›ele neuronale.

Este important de Ã®nÈ›eles cÄƒ ambele API-uri pot fi utilizate Ã®mpreunÄƒ. De exemplu, puteÈ›i dezvolta propria arhitecturÄƒ de strat de reÈ›ea folosind API-ul de nivel scÄƒzut È™i apoi sÄƒ o utilizaÈ›i Ã®ntr-o reÈ›ea mai mare construitÄƒ È™i antrenatÄƒ cu API-ul de nivel Ã®nalt. Sau puteÈ›i defini o reÈ›ea folosind API-ul de nivel Ã®nalt ca o succesiune de straturi È™i apoi sÄƒ folosiÈ›i propriul ciclu de antrenare de nivel scÄƒzut pentru optimizare. Ambele API-uri folosesc aceleaÈ™i concepte de bazÄƒ È™i sunt concepute sÄƒ funcÈ›ioneze bine Ã®mpreunÄƒ.

## ÃnvÄƒÈ›are

Ãn acest curs, oferim majoritatea conÈ›inutului atÃ¢t pentru PyTorch, cÃ¢t È™i pentru TensorFlow. PuteÈ›i alege frameworkul preferat È™i sÄƒ parcurgeÈ›i doar notebook-urile corespunzÄƒtoare. DacÄƒ nu sunteÈ›i sigur ce framework sÄƒ alegeÈ›i, citiÈ›i cÃ¢teva discuÈ›ii pe internet despre **PyTorch vs. TensorFlow**. De asemenea, puteÈ›i arunca o privire asupra ambelor frameworkuri pentru a Ã®nÈ›elege mai bine.

Acolo unde este posibil, vom folosi API-uri de nivel Ã®nalt pentru simplitate. TotuÈ™i, considerÄƒm cÄƒ este important sÄƒ Ã®nÈ›elegem cum funcÈ›ioneazÄƒ reÈ›elele neuronale de la bazÄƒ, astfel Ã®ncÃ¢t, la Ã®nceput, vom lucra cu API-uri de nivel scÄƒzut È™i tensori. Cu toate acestea, dacÄƒ doriÈ›i sÄƒ Ã®ncepeÈ›i rapid È™i nu vreÈ›i sÄƒ petreceÈ›i mult timp Ã®nvÄƒÈ›Ã¢nd aceste detalii, puteÈ›i sÄƒri direct la notebook-urile cu API-uri de nivel Ã®nalt.

## âœï¸ ExerciÈ›ii: Frameworkuri

ContinuaÈ›i Ã®nvÄƒÈ›area Ã®n urmÄƒtoarele notebook-uri:

Low-Level API | [TensorFlow+Keras Notebook](IntroKerasTF.ipynb) | [PyTorch](IntroPyTorch.ipynb)
--------------|-------------------------------------|--------------------------------
High-level API| [Keras](IntroKeras.ipynb) | *PyTorch Lightning*

DupÄƒ ce stÄƒpÃ¢niÈ›i frameworkurile, sÄƒ recapitulÄƒm conceptul de overfitting.

# Overfitting

Overfitting este un concept extrem de important Ã®n Ã®nvÄƒÈ›area automatÄƒ È™i este esenÈ›ial sÄƒ Ã®l Ã®nÈ›elegem corect!

LuaÈ›i Ã®n considerare urmÄƒtoarea problemÄƒ de aproximare a 5 puncte (reprezentate de `x` pe graficele de mai jos):

![linear](../../../../../translated_images/ro/overfit1.f24b71c6f652e59e.webp) | ![overfit](../../../../../translated_images/ro/overfit2.131f5800ae10ca5e.webp)
-------------------------|--------------------------
**Model liniar, 2 parametri** | **Model neliniar, 7 parametri**
Eroare de antrenare = 5.3 | Eroare de antrenare = 0
Eroare de validare = 5.1 | Eroare de validare = 20

* Ãn stÃ¢nga, vedem o aproximare bunÄƒ cu o linie dreaptÄƒ. Deoarece numÄƒrul de parametri este adecvat, modelul Ã®nÈ›elege corect distribuÈ›ia punctelor.
* Ãn dreapta, modelul este prea puternic. Deoarece avem doar 5 puncte È™i modelul are 7 parametri, acesta se poate ajusta astfel Ã®ncÃ¢t sÄƒ treacÄƒ prin toate punctele, fÄƒcÃ¢nd eroarea de antrenare sÄƒ fie 0. TotuÈ™i, acest lucru Ã®mpiedicÄƒ modelul sÄƒ Ã®nÈ›eleagÄƒ corect tiparul datelor, astfel Ã®ncÃ¢t eroarea de validare este foarte mare.

Este foarte important sÄƒ gÄƒsim un echilibru corect Ã®ntre complexitatea modelului (numÄƒrul de parametri) È™i numÄƒrul de exemple de antrenare.

## De ce apare overfitting

  * Date de antrenare insuficiente
  * Model prea puternic
  * Prea mult zgomot Ã®n datele de intrare

## Cum detectÄƒm overfitting

AÈ™a cum se vede din graficul de mai sus, overfitting poate fi detectat printr-o eroare de antrenare foarte micÄƒ È™i o eroare de validare mare. Ãn mod normal, Ã®n timpul antrenÄƒrii, vom vedea atÃ¢t erorile de antrenare, cÃ¢t È™i cele de validare Ã®ncepÃ¢nd sÄƒ scadÄƒ, iar apoi, la un moment dat, eroarea de validare poate Ã®nceta sÄƒ scadÄƒ È™i sÄƒ Ã®nceapÄƒ sÄƒ creascÄƒ. Acesta va fi un semn de overfitting È™i un indicator cÄƒ ar trebui sÄƒ oprim antrenarea Ã®n acel moment (sau cel puÈ›in sÄƒ facem un snapshot al modelului).

![overfitting](../../../../../translated_images/ro/Overfitting.408ad91cd90b4371.webp)

## Cum prevenim overfitting

DacÄƒ observaÈ›i cÄƒ apare overfitting, puteÈ›i face urmÄƒtoarele:

 * CreÈ™teÈ›i cantitatea de date de antrenare
 * ReduceÈ›i complexitatea modelului
 * UtilizaÈ›i o [tehnicÄƒ de regularizare](../../4-ComputerVision/08-TransferLearning/TrainingTricks.md), cum ar fi [Dropout](../../4-ComputerVision/08-TransferLearning/TrainingTricks.md#Dropout), pe care o vom analiza mai tÃ¢rziu.

## Overfitting È™i Compromisul Bias-Variance

Overfitting este, de fapt, un caz al unei probleme mai generale Ã®n statisticÄƒ numitÄƒ [Compromisul Bias-Variance](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff). DacÄƒ luÄƒm Ã®n considerare sursele posibile de eroare Ã®n modelul nostru, putem observa douÄƒ tipuri de erori:

* **Erorile de bias** sunt cauzate de faptul cÄƒ algoritmul nostru nu poate captura corect relaÈ›ia dintre datele de antrenare. Acest lucru poate rezulta din faptul cÄƒ modelul nostru nu este suficient de puternic (**underfitting**).
* **Erorile de variaÈ›ie**, care sunt cauzate de modelul care aproximeazÄƒ zgomotul din datele de intrare Ã®n loc de relaÈ›ia semnificativÄƒ (**overfitting**).

Ãn timpul antrenÄƒrii, eroarea de bias scade (pe mÄƒsurÄƒ ce modelul nostru Ã®nvaÈ›Äƒ sÄƒ aproximeze datele), iar eroarea de variaÈ›ie creÈ™te. Este important sÄƒ oprim antrenarea - fie manual (cÃ¢nd detectÄƒm overfitting), fie automat (prin introducerea regularizÄƒrii) - pentru a preveni overfitting.

## Concluzie

Ãn aceastÄƒ lecÈ›ie, aÈ›i Ã®nvÄƒÈ›at despre diferenÈ›ele dintre diversele API-uri ale celor mai populare frameworkuri AI, TensorFlow È™i PyTorch. Ãn plus, aÈ›i Ã®nvÄƒÈ›at despre un subiect foarte important, overfitting.

## ğŸš€ Provocare

Ãn notebook-urile Ã®nsoÈ›itoare, veÈ›i gÄƒsi 'sarcini' la final; parcurgeÈ›i notebook-urile È™i completaÈ›i sarcinile.

## [Chestionar dupÄƒ lecÈ›ie](https://ff-quizzes.netlify.app/en/ai/quiz/10)

## Recapitulare È™i Studiu Individual

FaceÈ›i cercetÄƒri pe urmÄƒtoarele subiecte:

- TensorFlow
- PyTorch
- Overfitting

ÃntrebaÈ›i-vÄƒ urmÄƒtoarele:

- Care este diferenÈ›a dintre TensorFlow È™i PyTorch?
- Care este diferenÈ›a dintre overfitting È™i underfitting?

## [TemÄƒ](lab/README.md)

Ãn acest laborator, vi se cere sÄƒ rezolvaÈ›i douÄƒ probleme de clasificare folosind reÈ›ele complet conectate cu un singur strat È™i cu mai multe straturi, utilizÃ¢nd PyTorch sau TensorFlow.

* [InstrucÈ›iuni](lab/README.md)
* [Notebook](lab/LabFrameworks.ipynb)

---

