<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "2b544f20b796402507fb05a0df893323",
  "translation_date": "2025-08-25T23:51:20+00:00",
  "source_file": "lessons/3-NeuralNetworks/05-Frameworks/README.md",
  "language_code": "ro"
}
-->
# Frameworkuri pentru ReÈ›ele Neurale

AÈ™a cum am Ã®nvÄƒÈ›at deja, pentru a putea antrena reÈ›ele neurale eficient, trebuie sÄƒ facem douÄƒ lucruri:

* SÄƒ operÄƒm pe tensori, de exemplu sÄƒ Ã®nmulÈ›im, sÄƒ adunÄƒm È™i sÄƒ calculÄƒm funcÈ›ii precum sigmoid sau softmax
* SÄƒ calculÄƒm derivatele tuturor expresiilor, pentru a efectua optimizarea prin gradient descent

## [Chestionar Ã®nainte de lecÈ›ie](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/105)

DeÈ™i biblioteca `numpy` poate face prima parte, avem nevoie de un mecanism pentru a calcula derivatele. Ãn [frameworkul nostru](../../../../../lessons/3-NeuralNetworks/04-OwnFramework/OwnFramework.ipynb) pe care l-am dezvoltat Ã®n secÈ›iunea anterioarÄƒ, a trebuit sÄƒ programÄƒm manual toate funcÈ›iile derivate Ã®n metoda `backward`, care realizeazÄƒ backpropagation. Ideal, un framework ar trebui sÄƒ ne ofere posibilitatea de a calcula derivatele pentru *orice expresie* pe care o putem defini.

Un alt aspect important este sÄƒ putem efectua calcule pe GPU sau pe alte unitÄƒÈ›i de calcul specializate, cum ar fi [TPU](https://en.wikipedia.org/wiki/Tensor_Processing_Unit). Antrenarea reÈ›elelor neurale profunde necesitÄƒ *foarte multe* calcule, iar posibilitatea de a paraleliza aceste calcule pe GPU-uri este esenÈ›ialÄƒ.

> âœ… Termenul 'paralelizare' Ã®nseamnÄƒ distribuirea calculelor pe mai multe dispozitive.

Ãn prezent, cele mai populare frameworkuri pentru reÈ›ele neurale sunt: [TensorFlow](http://TensorFlow.org) È™i [PyTorch](https://pytorch.org/). Ambele oferÄƒ o API de nivel scÄƒzut pentru a opera cu tensori atÃ¢t pe CPU, cÃ¢t È™i pe GPU. Pe lÃ¢ngÄƒ API-ul de nivel scÄƒzut, existÄƒ È™i API-uri de nivel Ã®nalt, numite [Keras](https://keras.io/) È™i [PyTorch Lightning](https://pytorchlightning.ai/) corespunzÄƒtor.

API de nivel scÄƒzut | [TensorFlow](http://TensorFlow.org) | [PyTorch](https://pytorch.org/)
--------------------|-------------------------------------|--------------------------------
API de nivel Ã®nalt  | [Keras](https://keras.io/)         | [PyTorch Lightning](https://pytorchlightning.ai/)

**API-urile de nivel scÄƒzut** din ambele frameworkuri permit construirea aÈ™a-numitelor **grafuri computaÈ›ionale**. Acest graf defineÈ™te cum sÄƒ se calculeze rezultatul (de obicei funcÈ›ia de pierdere) cu parametrii de intrare daÈ›i È™i poate fi trimis pentru calcul pe GPU, dacÄƒ este disponibil. ExistÄƒ funcÈ›ii pentru a diferenÈ›ia acest graf computaÈ›ional È™i a calcula derivatele, care pot fi apoi utilizate pentru optimizarea parametrilor modelului.

**API-urile de nivel Ã®nalt** considerÄƒ reÈ›elele neurale ca o **succesiune de straturi** È™i fac construirea majoritÄƒÈ›ii reÈ›elelor neurale mult mai uÈ™oarÄƒ. Antrenarea modelului necesitÄƒ de obicei pregÄƒtirea datelor È™i apoi apelarea unei funcÈ›ii `fit` pentru a realiza procesul.

API-ul de nivel Ã®nalt permite construirea rapidÄƒ a reÈ›elelor neurale tipice fÄƒrÄƒ a fi nevoie sÄƒ ne preocupÄƒm de multe detalii. Ãn acelaÈ™i timp, API-ul de nivel scÄƒzut oferÄƒ mult mai mult control asupra procesului de antrenare È™i, astfel, este utilizat frecvent Ã®n cercetare, atunci cÃ¢nd se lucreazÄƒ cu noi arhitecturi de reÈ›ele neurale.

Este important de Ã®nÈ›eles cÄƒ se pot folosi ambele API-uri Ã®mpreunÄƒ, de exemplu, se poate dezvolta propria arhitecturÄƒ de strat de reÈ›ea folosind API-ul de nivel scÄƒzut È™i apoi se poate utiliza Ã®n cadrul unei reÈ›ele mai mari construite È™i antrenate cu API-ul de nivel Ã®nalt. Sau se poate defini o reÈ›ea folosind API-ul de nivel Ã®nalt ca o succesiune de straturi È™i apoi se poate utiliza propriul ciclu de antrenare de nivel scÄƒzut pentru a efectua optimizarea. Ambele API-uri folosesc aceleaÈ™i concepte de bazÄƒ È™i sunt concepute sÄƒ funcÈ›ioneze bine Ã®mpreunÄƒ.

## ÃnvÄƒÈ›are

Ãn acest curs, oferim majoritatea conÈ›inutului atÃ¢t pentru PyTorch, cÃ¢t È™i pentru TensorFlow. PuteÈ›i alege frameworkul preferat È™i sÄƒ parcurgeÈ›i doar notebook-urile corespunzÄƒtoare. DacÄƒ nu sunteÈ›i sigur ce framework sÄƒ alegeÈ›i, citiÈ›i cÃ¢teva discuÈ›ii pe internet despre **PyTorch vs. TensorFlow**. De asemenea, puteÈ›i arunca o privire asupra ambelor frameworkuri pentru a Ã®nÈ›elege mai bine.

Acolo unde este posibil, vom folosi API-uri de nivel Ã®nalt pentru simplitate. TotuÈ™i, considerÄƒm cÄƒ este important sÄƒ Ã®nÈ›elegem cum funcÈ›ioneazÄƒ reÈ›elele neurale de la bazÄƒ, astfel Ã®ncÃ¢t la Ã®nceput vom lucra cu API-ul de nivel scÄƒzut È™i tensori. Cu toate acestea, dacÄƒ doriÈ›i sÄƒ Ã®ncepeÈ›i rapid È™i nu vreÈ›i sÄƒ petreceÈ›i mult timp Ã®nvÄƒÈ›Ã¢nd aceste detalii, puteÈ›i sÄƒri direct la notebook-urile cu API de nivel Ã®nalt.

## âœï¸ ExerciÈ›ii: Frameworkuri

ContinuaÈ›i Ã®nvÄƒÈ›area Ã®n urmÄƒtoarele notebook-uri:

API de nivel scÄƒzut | [Notebook TensorFlow+Keras](../../../../../lessons/3-NeuralNetworks/05-Frameworks/IntroKerasTF.ipynb) | [PyTorch](../../../../../lessons/3-NeuralNetworks/05-Frameworks/IntroPyTorch.ipynb)
--------------------|-------------------------------------|--------------------------------
API de nivel Ã®nalt  | [Keras](../../../../../lessons/3-NeuralNetworks/05-Frameworks/IntroKeras.ipynb)          | *PyTorch Lightning*

DupÄƒ ce stÄƒpÃ¢niÈ›i frameworkurile, sÄƒ recapitulÄƒm noÈ›iunea de overfitting.

# Overfitting

Overfitting este un concept extrem de important Ã®n Ã®nvÄƒÈ›area automatÄƒ È™i este esenÈ›ial sÄƒ Ã®l Ã®nÈ›elegem corect!

LuaÈ›i Ã®n considerare urmÄƒtoarea problemÄƒ de aproximare a 5 puncte (reprezentate prin `x` pe graficele de mai jos):

![linear](../../../../../translated_images/overfit1.f24b71c6f652e59e6bed7245ffbeaecc3ba320e16e2221f6832b432052c4da43.ro.jpg) | ![overfit](../../../../../translated_images/overfit2.131f5800ae10ca5e41d12a411f5f705d9ee38b1b10916f284b787028dd55cc1c.ro.jpg)
-------------------------|--------------------------
**Model liniar, 2 parametri** | **Model neliniar, 7 parametri**
Eroare de antrenare = 5.3 | Eroare de antrenare = 0
Eroare de validare = 5.1 | Eroare de validare = 20

* Ãn stÃ¢nga, vedem o aproximare bunÄƒ cu o linie dreaptÄƒ. Deoarece numÄƒrul de parametri este adecvat, modelul Ã®nÈ›elege corect distribuÈ›ia punctelor.
* Ãn dreapta, modelul este prea puternic. Deoarece avem doar 5 puncte È™i modelul are 7 parametri, acesta se poate ajusta astfel Ã®ncÃ¢t sÄƒ treacÄƒ prin toate punctele, fÄƒcÃ¢nd eroarea de antrenare sÄƒ fie 0. TotuÈ™i, acest lucru Ã®mpiedicÄƒ modelul sÄƒ Ã®nÈ›eleagÄƒ corect tiparul datelor, astfel Ã®ncÃ¢t eroarea de validare este foarte mare.

Este foarte important sÄƒ gÄƒsim un echilibru corect Ã®ntre complexitatea modelului (numÄƒrul de parametri) È™i numÄƒrul de exemple de antrenare.

## De ce apare overfitting

  * Date de antrenare insuficiente
  * Model prea puternic
  * Prea mult zgomot Ã®n datele de intrare

## Cum sÄƒ detectÄƒm overfitting

AÈ™a cum se poate vedea din graficul de mai sus, overfitting-ul poate fi detectat printr-o eroare de antrenare foarte micÄƒ È™i o eroare de validare mare. De obicei, Ã®n timpul antrenÄƒrii, vom vedea atÃ¢t erorile de antrenare, cÃ¢t È™i cele de validare Ã®ncepÃ¢nd sÄƒ scadÄƒ, iar apoi, la un moment dat, eroarea de validare poate Ã®nceta sÄƒ scadÄƒ È™i sÄƒ Ã®nceapÄƒ sÄƒ creascÄƒ. Acesta va fi un semn de overfitting È™i un indicator cÄƒ ar trebui sÄƒ oprim antrenarea Ã®n acel moment (sau cel puÈ›in sÄƒ facem un snapshot al modelului).

![overfitting](../../../../../translated_images/Overfitting.408ad91cd90b4371d0a81f4287e1409c359751adeb1ae450332af50e84f08c3e.ro.png)

## Cum sÄƒ prevenim overfitting-ul

DacÄƒ observaÈ›i cÄƒ apare overfitting, puteÈ›i face unul dintre urmÄƒtoarele:

 * CreÈ™teÈ›i cantitatea de date de antrenare
 * ReduceÈ›i complexitatea modelului
 * UtilizaÈ›i o tehnicÄƒ de [regularizare](../../4-ComputerVision/08-TransferLearning/TrainingTricks.md), cum ar fi [Dropout](../../4-ComputerVision/08-TransferLearning/TrainingTricks.md#Dropout), pe care o vom analiza mai tÃ¢rziu.

## Overfitting È™i Compromisul Bias-Variance

Overfitting-ul este de fapt un caz al unei probleme mai generale Ã®n statisticÄƒ numitÄƒ [Compromisul Bias-Variance](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff). DacÄƒ luÄƒm Ã®n considerare sursele posibile de eroare Ã®n modelul nostru, putem observa douÄƒ tipuri de erori:

* **Erorile de bias** sunt cauzate de algoritmul nostru care nu poate captura corect relaÈ›ia dintre datele de antrenare. Acest lucru poate rezulta din faptul cÄƒ modelul nostru nu este suficient de puternic (**underfitting**).
* **Erorile de variaÈ›ie**, care sunt cauzate de modelul care aproximeazÄƒ zgomotul din datele de intrare Ã®n loc de relaÈ›ia semnificativÄƒ (**overfitting**).

Ãn timpul antrenÄƒrii, eroarea de bias scade (pe mÄƒsurÄƒ ce modelul nostru Ã®nvaÈ›Äƒ sÄƒ aproximeze datele), iar eroarea de variaÈ›ie creÈ™te. Este important sÄƒ oprim antrenarea - fie manual (cÃ¢nd detectÄƒm overfitting), fie automat (prin introducerea regularizÄƒrii) - pentru a preveni overfitting-ul.

## Concluzie

Ãn aceastÄƒ lecÈ›ie, aÈ›i Ã®nvÄƒÈ›at despre diferenÈ›ele dintre diversele API-uri pentru cele douÄƒ frameworkuri AI cele mai populare, TensorFlow È™i PyTorch. Ãn plus, aÈ›i Ã®nvÄƒÈ›at despre un subiect foarte important, overfitting-ul.

## ğŸš€ Provocare

Ãn notebook-urile Ã®nsoÈ›itoare, veÈ›i gÄƒsi 'sarcini' la final; parcurgeÈ›i notebook-urile È™i completaÈ›i sarcinile.

## [Chestionar dupÄƒ lecÈ›ie](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/205)

## Recapitulare & Studiu Individual

FaceÈ›i cercetÄƒri pe urmÄƒtoarele subiecte:

- TensorFlow
- PyTorch
- Overfitting

ÃntrebaÈ›i-vÄƒ urmÄƒtoarele:

- Care este diferenÈ›a dintre TensorFlow È™i PyTorch?
- Care este diferenÈ›a dintre overfitting È™i underfitting?

## [TemÄƒ](lab/README.md)

Ãn acest laborator, vi se cere sÄƒ rezolvaÈ›i douÄƒ probleme de clasificare folosind reÈ›ele complet conectate cu unul sau mai multe straturi, utilizÃ¢nd PyTorch sau TensorFlow.

* [InstrucÈ›iuni](lab/README.md)
* [Notebook](../../../../../lessons/3-NeuralNetworks/05-Frameworks/lab/LabFrameworks.ipynb)

**Declinare de responsabilitate**:  
Acest document a fost tradus folosind serviciul de traducere AI [Co-op Translator](https://github.com/Azure/co-op-translator). DeÈ™i ne strÄƒduim sÄƒ asigurÄƒm acurateÈ›ea, vÄƒ rugÄƒm sÄƒ fiÈ›i conÈ™tienÈ›i cÄƒ traducerile automate pot conÈ›ine erori sau inexactitÄƒÈ›i. Documentul original Ã®n limba sa natalÄƒ ar trebui considerat sursa autoritarÄƒ. Pentru informaÈ›ii critice, se recomandÄƒ traducerea profesionalÄƒ realizatÄƒ de un specialist uman. Nu ne asumÄƒm responsabilitatea pentru eventualele neÃ®nÈ›elegeri sau interpretÄƒri greÈ™ite care pot apÄƒrea din utilizarea acestei traduceri.