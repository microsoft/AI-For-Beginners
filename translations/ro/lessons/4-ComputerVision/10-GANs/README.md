# ReÈ›ele Generative Adversariale

Ãn secÈ›iunea anterioarÄƒ, am Ã®nvÄƒÈ›at despre **modele generative**: modele care pot genera imagini noi similare cu cele din setul de date de antrenament. VAE a fost un exemplu bun de model generativ.

## [Chestionar Ã®nainte de lecÈ›ie](https://ff-quizzes.netlify.app/en/ai/quiz/19)

TotuÈ™i, dacÄƒ Ã®ncercÄƒm sÄƒ generÄƒm ceva cu adevÄƒrat semnificativ, cum ar fi o picturÄƒ la o rezoluÈ›ie rezonabilÄƒ, folosind VAE, vom observa cÄƒ antrenamentul nu converge bine. Pentru acest caz, ar trebui sÄƒ Ã®nvÄƒÈ›Äƒm despre o altÄƒ arhitecturÄƒ special conceputÄƒ pentru modele generative - **ReÈ›ele Generative Adversariale**, sau GAN-uri.

Ideea principalÄƒ a unui GAN este sÄƒ avem douÄƒ reÈ›ele neuronale care vor fi antrenate una Ã®mpotriva celeilalte:

<img src="../../../../../translated_images/ro/gan_architecture.8f3a5ab62b8d5d69.webp" width="70%"/>

> Imagine de [Dmitry Soshnikov](http://soshnikov.com)

> âœ… Un pic de vocabular:
> * **Generator** este o reÈ›ea care ia un vector aleator È™i produce o imagine ca rezultat.
> * **Discriminator** este o reÈ›ea care ia o imagine È™i trebuie sÄƒ determine dacÄƒ este o imagine realÄƒ (din setul de date de antrenament) sau dacÄƒ a fost generatÄƒ de un generator. Practic, este un clasificator de imagini.

### Discriminator

Arhitectura discriminatorului nu diferÄƒ de o reÈ›ea obiÈ™nuitÄƒ de clasificare a imaginilor. Ãn cel mai simplu caz, poate fi un clasificator complet conectat, dar cel mai probabil va fi o [reÈ›ea convoluÈ›ionalÄƒ](../07-ConvNets/README.md).

> âœ… Un GAN bazat pe reÈ›ele convoluÈ›ionale se numeÈ™te [DCGAN](https://arxiv.org/pdf/1511.06434.pdf)

Un discriminator CNN constÄƒ din urmÄƒtoarele straturi: mai multe straturi de convoluÈ›ie+pooling (cu dimensiuni spaÈ›iale Ã®n scÄƒdere) È™i unul sau mai multe straturi complet conectate pentru a obÈ›ine "vectorul de caracteristici", urmat de un clasificator binar final.

> âœ… 'Pooling' Ã®n acest context este o tehnicÄƒ care reduce dimensiunea imaginii. "Straturile de pooling reduc dimensiunile datelor prin combinarea ieÈ™irilor grupurilor de neuroni de la un strat Ã®ntr-un singur neuron Ã®n stratul urmÄƒtor." - [sursa](https://wikipedia.org/wiki/Convolutional_neural_network#Pooling_layers)

### Generator

Un Generator este puÈ›in mai complicat. Ãl puteÈ›i considera ca fiind un discriminator inversat. Pornind de la un vector latent (Ã®n locul unui vector de caracteristici), acesta are un strat complet conectat pentru a-l converti Ã®n dimensiunea/forma necesarÄƒ, urmat de deconvoluÈ›ii+scalare. Acest lucru este similar cu partea de *decoder* a unui [autoencoder](../09-Autoencoders/README.md).

> âœ… Deoarece stratul de convoluÈ›ie este implementat ca un filtru liniar care traverseazÄƒ imaginea, deconvoluÈ›ia este esenÈ›ial similarÄƒ cu convoluÈ›ia È™i poate fi implementatÄƒ folosind aceeaÈ™i logicÄƒ de strat.

<img src="../../../../../translated_images/ro/gan_arch_detail.46b95fd366f8e543.webp" width="70%"/>

> Imagine de [Dmitry Soshnikov](http://soshnikov.com)

### Antrenarea GAN-ului

GAN-urile sunt numite **adversariale** deoarece existÄƒ o competiÈ›ie constantÄƒ Ã®ntre generator È™i discriminator. Ãn timpul acestei competiÈ›ii, atÃ¢t generatorul, cÃ¢t È™i discriminatorul se Ã®mbunÄƒtÄƒÈ›esc, astfel Ã®ncÃ¢t reÈ›eaua Ã®nvaÈ›Äƒ sÄƒ producÄƒ imagini din ce Ã®n ce mai bune.

Antrenamentul se desfÄƒÈ™oarÄƒ Ã®n douÄƒ etape:

* **Antrenarea discriminatorului**. AceastÄƒ sarcinÄƒ este destul de simplÄƒ: generÄƒm un lot de imagini cu ajutorul generatorului, etichetÃ¢ndu-le cu 0, ceea ce Ã®nseamnÄƒ imagine falsÄƒ, È™i luÄƒm un lot de imagini din setul de date de intrare (cu eticheta 1, imagine realÄƒ). ObÈ›inem o *pierdere discriminatorie* È™i efectuÄƒm backpropagation.
* **Antrenarea generatorului**. Acest lucru este puÈ›in mai complicat, deoarece nu È™tim direct rezultatul aÈ™teptat pentru generator. LuÄƒm Ã®ntreaga reÈ›ea GAN, formatÄƒ dintr-un generator urmat de un discriminator, o alimentÄƒm cu niÈ™te vectori aleatori È™i ne aÈ™teptÄƒm ca rezultatul sÄƒ fie 1 (corespunzÄƒtor imaginilor reale). Apoi Ã®ngheÈ›Äƒm parametrii discriminatorului (nu dorim sÄƒ fie antrenat Ã®n acest pas) È™i efectuÄƒm backpropagation.

Ãn timpul acestui proces, pierderile generatorului È™i discriminatorului nu scad semnificativ. Ãn situaÈ›ia idealÄƒ, acestea ar trebui sÄƒ oscileze, corespunzÃ¢nd Ã®mbunÄƒtÄƒÈ›irii performanÈ›ei ambelor reÈ›ele.

## âœï¸ ExerciÈ›ii: GAN-uri

* [Notebook GAN Ã®n TensorFlow/Keras](GANTF.ipynb)
* [Notebook GAN Ã®n PyTorch](GANPyTorch.ipynb)

### Probleme cu antrenarea GAN-urilor

GAN-urile sunt cunoscute pentru faptul cÄƒ sunt deosebit de dificile de antrenat. IatÄƒ cÃ¢teva probleme:

* **Colapsul modului**. Prin acest termen, ne referim la faptul cÄƒ generatorul Ã®nvaÈ›Äƒ sÄƒ producÄƒ o singurÄƒ imagine de succes care pÄƒcÄƒleÈ™te discriminatorul, È™i nu o varietate de imagini diferite.
* **Sensibilitate la hiperparametri**. Adesea, se poate observa cÄƒ un GAN nu converge deloc, iar apoi o scÄƒdere bruscÄƒ a ratei de Ã®nvÄƒÈ›are duce la convergenÈ›Äƒ.
* MenÈ›inerea unui **echilibru** Ã®ntre generator È™i discriminator. Ãn multe cazuri, pierderea discriminatorului poate scÄƒdea la zero relativ rapid, ceea ce face ca generatorul sÄƒ nu mai poatÄƒ fi antrenat. Pentru a depÄƒÈ™i acest lucru, putem Ã®ncerca sÄƒ setÄƒm rate de Ã®nvÄƒÈ›are diferite pentru generator È™i discriminator sau sÄƒ sÄƒrim peste antrenarea discriminatorului dacÄƒ pierderea este deja prea micÄƒ.
* Antrenarea pentru **rezoluÈ›ie Ã®naltÄƒ**. ReflectÃ¢nd aceeaÈ™i problemÄƒ ca Ã®n cazul autoencoderelor, aceastÄƒ problemÄƒ apare deoarece reconstruirea prea multor straturi ale reÈ›elei convoluÈ›ionale duce la artefacte. AceastÄƒ problemÄƒ este de obicei rezolvatÄƒ prin aÈ™a-numita **creÈ™tere progresivÄƒ**, cÃ¢nd mai Ã®ntÃ¢i cÃ¢teva straturi sunt antrenate pe imagini de rezoluÈ›ie micÄƒ, iar apoi straturile sunt "deblocate" sau adÄƒugate. O altÄƒ soluÈ›ie ar fi adÄƒugarea de conexiuni suplimentare Ã®ntre straturi È™i antrenarea mai multor rezoluÈ›ii simultan - vedeÈ›i acest [articol despre GAN-uri cu gradient multi-scalÄƒ](https://arxiv.org/abs/1903.06048) pentru detalii.

## Transfer de Stil

GAN-urile sunt o modalitate excelentÄƒ de a genera imagini artistice. O altÄƒ tehnicÄƒ interesantÄƒ este aÈ™a-numitul **transfer de stil**, care ia o **imagine de conÈ›inut** È™i o redeseneazÄƒ Ã®ntr-un stil diferit, aplicÃ¢nd filtre din **imaginea de stil**.

Cum funcÈ›ioneazÄƒ:
* Ãncepem cu o imagine de zgomot aleator (sau cu o imagine de conÈ›inut, dar pentru Ã®nÈ›elegere este mai uÈ™or sÄƒ Ã®ncepem cu zgomot aleator).
* Scopul nostru este sÄƒ creÄƒm o imagine care sÄƒ fie apropiatÄƒ atÃ¢t de imaginea de conÈ›inut, cÃ¢t È™i de imaginea de stil. Acest lucru va fi determinat de douÄƒ funcÈ›ii de pierdere:
   - **Pierdere de conÈ›inut** este calculatÄƒ pe baza caracteristicilor extrase de CNN la unele straturi din imaginea curentÄƒ È™i imaginea de conÈ›inut.
   - **Pierdere de stil** este calculatÄƒ Ã®ntre imaginea curentÄƒ È™i imaginea de stil Ã®ntr-un mod ingenios folosind matrici Gram (mai multe detalii Ã®n [notebook-ul de exemplu](StyleTransfer.ipynb)).
* Pentru a face imaginea mai netedÄƒ È™i a elimina zgomotul, introducem È™i **Pierdere de variaÈ›ie**, care calculeazÄƒ distanÈ›a medie Ã®ntre pixeli vecini.
* Bucla principalÄƒ de optimizare ajusteazÄƒ imaginea curentÄƒ folosind gradient descent (sau un alt algoritm de optimizare) pentru a minimiza pierderea totalÄƒ, care este o sumÄƒ ponderatÄƒ a tuturor celor trei pierderi.

## âœï¸ Exemplu: [Transfer de Stil](StyleTransfer.ipynb)

## [Chestionar dupÄƒ lecÈ›ie](https://ff-quizzes.netlify.app/en/ai/quiz/20)

## Concluzie

Ãn aceastÄƒ lecÈ›ie, aÈ›i Ã®nvÄƒÈ›at despre GAN-uri È™i cum sÄƒ le antrenaÈ›i. De asemenea, aÈ›i Ã®nvÄƒÈ›at despre provocÄƒrile speciale pe care acest tip de reÈ›ea neuronalÄƒ le poate Ã®ntÃ¢mpina È™i cÃ¢teva strategii pentru a le depÄƒÈ™i.

## ğŸš€ Provocare

ParcurgeÈ›i [notebook-ul Transfer de Stil](StyleTransfer.ipynb) folosind propriile imagini.

## Recapitulare È™i Studiu Individual

Pentru referinÈ›Äƒ, citiÈ›i mai multe despre GAN-uri Ã®n aceste resurse:

* Marco Pasini, [10 lecÈ›ii pe care le-am Ã®nvÄƒÈ›at antrenÃ¢nd GAN-uri timp de un an](https://towardsdatascience.com/10-lessons-i-learned-training-generative-adversarial-networks-gans-for-a-year-c9071159628)
* [StyleGAN](https://en.wikipedia.org/wiki/StyleGAN), o arhitecturÄƒ GAN *de facto* de luat Ã®n considerare
* [Crearea artei generative folosind GAN-uri pe Azure ML](https://soshnikov.com/scienceart/creating-generative-art-using-gan-on-azureml/)

## TemÄƒ

RevedeÈ›i unul dintre cele douÄƒ notebook-uri asociate acestei lecÈ›ii È™i reantrenaÈ›i GAN-ul pe propriile imagini. Ce puteÈ›i crea?

---

