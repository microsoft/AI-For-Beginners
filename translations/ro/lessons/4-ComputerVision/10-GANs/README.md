<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "f07c85bbf05a1f67505da98f4ecc124c",
  "translation_date": "2025-08-25T22:39:42+00:00",
  "source_file": "lessons/4-ComputerVision/10-GANs/README.md",
  "language_code": "ro"
}
-->
# ReÈ›ele Generative Adversariale

Ãn secÈ›iunea anterioarÄƒ, am Ã®nvÄƒÈ›at despre **modelele generative**: modele care pot genera imagini noi similare cu cele din setul de date de antrenament. VAE a fost un exemplu bun de model generativ.

## [Chestionar Ã®nainte de lecÈ›ie](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/110)

TotuÈ™i, dacÄƒ Ã®ncercÄƒm sÄƒ generÄƒm ceva cu adevÄƒrat semnificativ, cum ar fi o picturÄƒ la o rezoluÈ›ie rezonabilÄƒ, folosind VAE, vom observa cÄƒ antrenamentul nu converge bine. Pentru acest caz de utilizare, ar trebui sÄƒ Ã®nvÄƒÈ›Äƒm despre o altÄƒ arhitecturÄƒ specificÄƒ modelelor generative - **ReÈ›ele Generative Adversariale**, sau GAN-uri.

Ideea principalÄƒ a unui GAN este sÄƒ aibÄƒ douÄƒ reÈ›ele neuronale care sunt antrenate una Ã®mpotriva celeilalte:

<img src="images/gan_architecture.png" width="70%"/>

> Imagine de [Dmitry Soshnikov](http://soshnikov.com)

> âœ… Un pic de vocabular:
> * **Generatorul** este o reÈ›ea care primeÈ™te un vector aleator È™i produce o imagine ca rezultat.
> * **Discriminatorul** este o reÈ›ea care primeÈ™te o imagine È™i trebuie sÄƒ determine dacÄƒ este o imagine realÄƒ (din setul de date de antrenament) sau dacÄƒ a fost generatÄƒ de un generator. Practic, este un clasificator de imagini.

### Discriminatorul

Arhitectura discriminatorului nu diferÄƒ de o reÈ›ea obiÈ™nuitÄƒ de clasificare a imaginilor. Ãn cel mai simplu caz, poate fi un clasificator complet conectat, dar cel mai probabil va fi o [reÈ›ea convoluÈ›ionalÄƒ](../07-ConvNets/README.md).

> âœ… Un GAN bazat pe reÈ›ele convoluÈ›ionale se numeÈ™te [DCGAN](https://arxiv.org/pdf/1511.06434.pdf)

Un discriminator CNN constÄƒ din urmÄƒtoarele straturi: mai multe convoluÈ›ii + pooling-uri (cu dimensiuni spaÈ›iale Ã®n scÄƒdere) È™i unul sau mai multe straturi complet conectate pentru a obÈ›ine "vectorul de caracteristici", urmate de un clasificator binar final.

> âœ… Un 'pooling' Ã®n acest context este o tehnicÄƒ ce reduce dimensiunea imaginii. "Straturile de pooling reduc dimensiunile datelor prin combinarea ieÈ™irilor unor clustere de neuroni dintr-un strat Ã®ntr-un singur neuron Ã®n stratul urmÄƒtor." - [sursa](https://wikipedia.org/wiki/Convolutional_neural_network#Pooling_layers)

### Generatorul

Un generator este puÈ›in mai complicat. Ãl puteÈ›i considera ca fiind un discriminator inversat. Pornind de la un vector latent (Ã®n locul unui vector de caracteristici), are un strat complet conectat pentru a-l converti Ã®n dimensiunea/forma necesarÄƒ, urmat de deconvoluÈ›ii + upscaling. Acest lucru este similar cu partea de *decodor* a unui [autoencoder](../09-Autoencoders/README.md).

> âœ… Deoarece stratul de convoluÈ›ie este implementat ca un filtru liniar care parcurge imaginea, deconvoluÈ›ia este esenÈ›ial similarÄƒ cu convoluÈ›ia È™i poate fi implementatÄƒ folosind aceeaÈ™i logicÄƒ de strat.

<img src="images/gan_arch_detail.png" width="70%"/>

> Imagine de [Dmitry Soshnikov](http://soshnikov.com)

### Antrenarea GAN-ului

GAN-urile sunt numite **adversariale** deoarece existÄƒ o competiÈ›ie constantÄƒ Ã®ntre generator È™i discriminator. Ãn timpul acestei competiÈ›ii, atÃ¢t generatorul, cÃ¢t È™i discriminatorul se Ã®mbunÄƒtÄƒÈ›esc, astfel Ã®ncÃ¢t reÈ›eaua Ã®nvaÈ›Äƒ sÄƒ producÄƒ imagini din ce Ã®n ce mai bune.

Antrenamentul are loc Ã®n douÄƒ etape:

* **Antrenarea discriminatorului**. AceastÄƒ sarcinÄƒ este destul de simplÄƒ: generÄƒm un lot de imagini cu ajutorul generatorului, le etichetÄƒm cu 0, ceea ce Ã®nseamnÄƒ imagine falsÄƒ, È™i luÄƒm un lot de imagini din setul de date de intrare (cu eticheta 1, imagine realÄƒ). ObÈ›inem o *pierdere a discriminatorului* È™i efectuÄƒm backpropagation.
* **Antrenarea generatorului**. Aceasta este puÈ›in mai complicatÄƒ, deoarece nu cunoaÈ™tem direct ieÈ™irea aÈ™teptatÄƒ pentru generator. LuÄƒm Ã®ntreaga reÈ›ea GAN formatÄƒ dintr-un generator urmat de un discriminator, o alimentÄƒm cu niÈ™te vectori aleatori È™i ne aÈ™teptÄƒm ca rezultatul sÄƒ fie 1 (corespunzÄƒtor imaginilor reale). Apoi Ã®ngheÈ›Äƒm parametrii discriminatorului (nu dorim sÄƒ fie antrenat Ã®n acest pas) È™i efectuÄƒm backpropagation.

Ãn timpul acestui proces, pierderile generatorului È™i discriminatorului nu scad semnificativ. Ãn situaÈ›ia idealÄƒ, acestea ar trebui sÄƒ oscileze, corespunzÃ¢nd Ã®mbunÄƒtÄƒÈ›irii performanÈ›ei ambelor reÈ›ele.

## âœï¸ ExerciÈ›ii: GAN-uri

* [Carnet GAN Ã®n TensorFlow/Keras](../../../../../lessons/4-ComputerVision/10-GANs/GANTF.ipynb)
* [Carnet GAN Ã®n PyTorch](../../../../../lessons/4-ComputerVision/10-GANs/GANPyTorch.ipynb)

### Probleme cu antrenarea GAN-urilor

GAN-urile sunt cunoscute pentru dificultatea lor de antrenare. IatÄƒ cÃ¢teva probleme:

* **Colapsul modului**. Prin acest termen Ã®nÈ›elegem cÄƒ generatorul Ã®nvaÈ›Äƒ sÄƒ producÄƒ o singurÄƒ imagine de succes care pÄƒcÄƒleÈ™te discriminatorul, fÄƒrÄƒ a genera o varietate de imagini diferite.
* **Sensibilitate la hiperparametri**. Adesea se poate observa cÄƒ un GAN nu converge deloc, iar apoi o scÄƒdere bruscÄƒ a ratei de Ã®nvÄƒÈ›are duce la convergenÈ›Äƒ.
* MenÈ›inerea unui **echilibru** Ã®ntre generator È™i discriminator. Ãn multe cazuri, pierderea discriminatorului poate scÄƒdea la zero relativ repede, ceea ce face ca generatorul sÄƒ nu mai poatÄƒ fi antrenat. Pentru a depÄƒÈ™i acest lucru, putem Ã®ncerca sÄƒ setÄƒm rate de Ã®nvÄƒÈ›are diferite pentru generator È™i discriminator sau sÄƒ sÄƒrim peste antrenamentul discriminatorului dacÄƒ pierderea este deja prea micÄƒ.
* Antrenarea pentru **rezoluÈ›ie Ã®naltÄƒ**. ReflectÃ¢nd aceeaÈ™i problemÄƒ ca Ã®n cazul autoencoderelor, aceastÄƒ problemÄƒ apare deoarece reconstruirea prea multor straturi ale unei reÈ›ele convoluÈ›ionale duce la artefacte. AceastÄƒ problemÄƒ este de obicei rezolvatÄƒ prin aÈ™a-numita **creÈ™tere progresivÄƒ**, Ã®n care mai Ã®ntÃ¢i cÃ¢teva straturi sunt antrenate pe imagini de rezoluÈ›ie micÄƒ, iar apoi straturile sunt "deblocate" sau adÄƒugate. O altÄƒ soluÈ›ie ar fi adÄƒugarea de conexiuni suplimentare Ã®ntre straturi È™i antrenarea mai multor rezoluÈ›ii simultan - vedeÈ›i acest [articol despre GAN-uri cu gradient multi-scalÄƒ](https://arxiv.org/abs/1903.06048) pentru detalii.

## Transfer de stil

GAN-urile sunt o metodÄƒ excelentÄƒ pentru a genera imagini artistice. O altÄƒ tehnicÄƒ interesantÄƒ este aÈ™a-numitul **transfer de stil**, care ia o **imagine de conÈ›inut** È™i o re-desenazÄƒ Ã®ntr-un stil diferit, aplicÃ¢nd filtre dintr-o **imagine de stil**.

Cum funcÈ›ioneazÄƒ:
* Ãncepem cu o imagine de zgomot aleator (sau cu o imagine de conÈ›inut, dar pentru Ã®nÈ›elegere este mai uÈ™or sÄƒ Ã®ncepem cu zgomot aleator).
* Scopul nostru este sÄƒ creÄƒm o imagine care sÄƒ fie apropiatÄƒ atÃ¢t de imaginea de conÈ›inut, cÃ¢t È™i de imaginea de stil. Acest lucru este determinat de douÄƒ funcÈ›ii de pierdere:
   - **Pierderea de conÈ›inut** este calculatÄƒ pe baza caracteristicilor extrase de CNN la anumite straturi din imaginea curentÄƒ È™i imaginea de conÈ›inut.
   - **Pierderea de stil** este calculatÄƒ Ã®ntre imaginea curentÄƒ È™i imaginea de stil Ã®ntr-un mod inteligent folosind matrici Gram (mai multe detalii Ã®n [carnetul de exemplu](../../../../../lessons/4-ComputerVision/10-GANs/StyleTransfer.ipynb)).
* Pentru a face imaginea mai netedÄƒ È™i a elimina zgomotul, introducem È™i **Pierderea de variaÈ›ie**, care calculeazÄƒ distanÈ›a medie Ã®ntre pixelii vecini.
* Bucla principalÄƒ de optimizare ajusteazÄƒ imaginea curentÄƒ folosind gradient descent (sau un alt algoritm de optimizare) pentru a minimiza pierderea totalÄƒ, care este o sumÄƒ ponderatÄƒ a tuturor celor trei pierderi.

## âœï¸ Exemplu: [Transfer de stil](../../../../../lessons/4-ComputerVision/10-GANs/StyleTransfer.ipynb)

## [Chestionar dupÄƒ lecÈ›ie](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/210)

## Concluzie

Ãn aceastÄƒ lecÈ›ie, aÈ›i Ã®nvÄƒÈ›at despre GAN-uri È™i cum sÄƒ le antrenaÈ›i. De asemenea, aÈ›i Ã®nvÄƒÈ›at despre provocÄƒrile speciale pe care acest tip de reÈ›ea neuronalÄƒ le poate Ã®ntÃ¢mpina È™i cÃ¢teva strategii pentru a le depÄƒÈ™i.

## ğŸš€ Provocare

ParcurgeÈ›i [carnetul de Transfer de Stil](../../../../../lessons/4-ComputerVision/10-GANs/StyleTransfer.ipynb) folosind propriile imagini.

## Recapitulare È™i studiu individual

Pentru referinÈ›Äƒ, citiÈ›i mai multe despre GAN-uri Ã®n aceste resurse:

* Marco Pasini, [10 lecÈ›ii Ã®nvÄƒÈ›ate antrenÃ¢nd GAN-uri timp de un an](https://towardsdatascience.com/10-lessons-i-learned-training-generative-adversarial-networks-gans-for-a-year-c9071159628)
* [StyleGAN](https://en.wikipedia.org/wiki/StyleGAN), o arhitecturÄƒ GAN *de facto* de luat Ã®n considerare
* [Crearea artei generative folosind GAN-uri pe Azure ML](https://soshnikov.com/scienceart/creating-generative-art-using-gan-on-azureml/)

## TemÄƒ

RevedeÈ›i unul dintre cele douÄƒ carnete asociate acestei lecÈ›ii È™i reantrenaÈ›i GAN-ul pe imaginile proprii. Ce puteÈ›i crea?

**Declinare de responsabilitate**:  
Acest document a fost tradus folosind serviciul de traducere AI [Co-op Translator](https://github.com/Azure/co-op-translator). DeÈ™i ne strÄƒduim sÄƒ asigurÄƒm acurateÈ›ea, vÄƒ rugÄƒm sÄƒ fiÈ›i conÈ™tienÈ›i cÄƒ traducerile automate pot conÈ›ine erori sau inexactitÄƒÈ›i. Documentul original Ã®n limba sa natalÄƒ ar trebui considerat sursa autoritarÄƒ. Pentru informaÈ›ii critice, se recomandÄƒ traducerea profesionalÄƒ realizatÄƒ de un specialist uman. Nu ne asumÄƒm responsabilitatea pentru eventualele neÃ®nÈ›elegeri sau interpretÄƒri greÈ™ite care pot apÄƒrea din utilizarea acestei traduceri.