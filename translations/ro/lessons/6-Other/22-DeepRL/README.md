# 칉nv캒탵are prin Recompens캒 Profund캒

칉nv캒탵area prin recompens캒 (RL) este considerat캒 unul dintre paradigmele de baz캒 ale 칥nv캒탵캒rii automate, al캒turi de 칥nv캒탵area supravegheat캒 탳i nesupravegheat캒. 칉n timp ce 칥n 칥nv캒탵area supravegheat캒 ne baz캒m pe un set de date cu rezultate cunoscute, RL se bazeaz캒 pe **칥nv캒탵area prin ac탵iune**. De exemplu, c칙nd vedem pentru prima dat캒 un joc pe calculator, 칥ncepem s캒 juc캒m, chiar dac캒 nu cunoa탳tem regulile, 탳i 칥n scurt timp ne putem 칥mbun캒t캒탵i abilit캒탵ile doar prin procesul de joc 탳i ajustarea comportamentului.

## [Chestionar 칥nainte de lec탵ie](https://ff-quizzes.netlify.app/en/ai/quiz/43)

Pentru a realiza RL, avem nevoie de:

* Un **mediu** sau **simulator** care stabile탳te regulile jocului. Trebuie s캒 putem rula experimente 칥n simulator 탳i s캒 observ캒m rezultatele.
* O **func탵ie de recompens캒**, care indic캒 c칙t de reu탳it a fost experimentul nostru. 칉n cazul 칥nv캒탵캒rii s캒 juc캒m un joc pe calculator, recompensa ar fi scorul final.

Pe baza func탵iei de recompens캒, ar trebui s캒 putem ajusta comportamentul nostru 탳i s캒 ne 칥mbun캒t캒탵im abilit캒탵ile, astfel 칥nc칙t data viitoare s캒 juc캒m mai bine. Principala diferen탵캒 칥ntre alte tipuri de 칥nv캒탵are automat캒 탳i RL este c캒 칥n RL, de obicei, nu 탳tim dac캒 c칙탳tig캒m sau pierdem p칙n캒 nu termin캒m jocul. Astfel, nu putem spune dac캒 o anumit캒 mi탳care este bun캒 sau nu - primim recompensa doar la sf칙r탳itul jocului.

칉n timpul RL, de obicei realiz캒m multe experimente. 칉n fiecare experiment, trebuie s캒 echilibr캒m 칥ntre urmarea strategiei optime pe care am 칥nv캒탵at-o p칙n캒 acum (**exploatare**) 탳i explorarea unor st캒ri noi posibile (**explorare**).

## OpenAI Gym

Un instrument excelent pentru RL este [OpenAI Gym](https://gym.openai.com/) - un **mediu de simulare**, care poate simula multe medii diferite, de la jocuri Atari p칙n캒 la fizica din spatele echilibr캒rii unui st칙lp. Este unul dintre cele mai populare medii de simulare pentru antrenarea algoritmilor de 칥nv캒탵are prin recompens캒 탳i este 칥ntre탵inut de [OpenAI](https://openai.com/).

> **Not캒**: Pute탵i vedea toate mediile disponibile 칥n OpenAI Gym [aici](https://gym.openai.com/envs/#classic_control).

## Echilibrarea CartPole

Probabil a탵i v캒zut cu to탵ii dispozitive moderne de echilibrare, cum ar fi *Segway* sau *Gyroscooters*. Acestea sunt capabile s캒 se echilibreze automat ajust칙ndu-탳i ro탵ile ca r캒spuns la un semnal de la un accelerometru sau giroscop. 칉n aceast캒 sec탵iune, vom 칥nv캒탵a cum s캒 rezolv캒m o problem캒 similar캒 - echilibrarea unui st칙lp. Este similar cu situa탵ia 칥n care un artist de circ trebuie s캒 echilibreze un st칙lp pe m칙na sa - dar aceast캒 echilibrare a st칙lpului are loc doar 칥n 1D.

O versiune simplificat캒 a echilibr캒rii este cunoscut캒 sub numele de problema **CartPole**. 칉n lumea CartPole, avem un slider orizontal care se poate mi탳ca la st칙nga sau la dreapta, iar scopul este s캒 echilibr캒m un st칙lp vertical deasupra sliderului 칥n timp ce acesta se mi탳c캒.

<img alt="un cartpole" src="../../../../../translated_images/ro/cartpole.f52a67f27e058170.webp" width="200"/>

Pentru a crea 탳i utiliza acest mediu, avem nevoie de c칙teva linii de cod Python:

```python
import gym
env = gym.make("CartPole-v1")

env.reset()
done = False
total_reward = 0
while not done:
   env.render()
   action = env.action_space.sample()
   observaton, reward, done, info = env.step(action)
   total_reward += reward

print(f"Total reward: {total_reward}")
```

Fiecare mediu poate fi accesat exact 칥n acela탳i mod:
* `env.reset` 칥ncepe un nou experiment
* `env.step` efectueaz캒 un pas de simulare. Prime탳te o **ac탵iune** din **spa탵iul de ac탵iuni** 탳i returneaz캒 o **observa탵ie** (din spa탵iul de observa탵ii), precum 탳i o recompens캒 탳i un indicator de terminare.

칉n exemplul de mai sus, efectu캒m o ac탵iune aleatorie la fiecare pas, motiv pentru care durata experimentului este foarte scurt캒:

![cartpole f캒r캒 echilibrare](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-nobalance.gif)

Scopul unui algoritm RL este s캒 antreneze un model - a탳a-numita **politic캒** &pi; - care va returna ac탵iunea ca r캒spuns la o stare dat캒. De asemenea, putem considera politica ca fiind probabilistic캒, de exemplu, pentru orice stare *s* 탳i ac탵iune *a*, va returna probabilitatea &pi;(*a*|*s*) c캒 ar trebui s캒 lu캒m *a* 칥n starea *s*.

## Algoritmul Policy Gradients

Cel mai evident mod de a modela o politic캒 este prin crearea unei re탵ele neuronale care va lua st캒rile ca intrare 탳i va returna ac탵iunile corespunz캒toare (sau mai degrab캒 probabilit캒탵ile tuturor ac탵iunilor). 칉ntr-un sens, ar fi similar cu o sarcin캒 normal캒 de clasificare, cu o diferen탵캒 major캒 - nu 탳tim 칥n avans ce ac탵iuni ar trebui s캒 lu캒m la fiecare pas.

Ideea aici este s캒 estim캒m aceste probabilit캒탵i. Construim un vector de **recompense cumulative** care arat캒 recompensa total캒 la fiecare pas al experimentului. De asemenea, aplic캒m **discounting-ul recompenselor** prin multiplicarea recompenselor anterioare cu un coeficient &gamma;=0.99, pentru a diminua rolul recompenselor anterioare. Apoi, 칥nt캒rim acei pa탳i de-a lungul traseului experimentului care genereaz캒 recompense mai mari.

> Afla탵i mai multe despre algoritmul Policy Gradient 탳i vede탵i-l 칥n ac탵iune 칥n [notebook-ul de exemplu](CartPole-RL-TF.ipynb).

## Algoritmul Actor-Critic

O versiune 칥mbun캒t캒탵it캒 a abord캒rii Policy Gradients se nume탳te **Actor-Critic**. Ideea principal캒 din spatele acestuia este c캒 re탵eaua neuronal캒 ar fi antrenat캒 s캒 returneze dou캒 lucruri:

* Politica, care determin캒 ce ac탵iune s캒 lu캒m. Aceast캒 parte se nume탳te **actor**.
* Estimarea recompensei totale pe care ne putem a탳tepta s캒 o ob탵inem 칥n aceast캒 stare - aceast캒 parte se nume탳te **critic**.

칉ntr-un sens, aceast캒 arhitectur캒 seam캒n캒 cu un [GAN](../../4-ComputerVision/10-GANs/README.md), unde avem dou캒 re탵ele care sunt antrenate una 칥mpotriva celeilalte. 칉n modelul actor-critic, actorul propune ac탵iunea pe care trebuie s캒 o lu캒m, iar criticul 칥ncearc캒 s캒 fie critic 탳i s캒 estimeze rezultatul. Totu탳i, scopul nostru este s캒 antren캒m aceste re탵ele 칥n armonie.

Pentru c캒 탳tim at칙t recompensele cumulative reale, c칙t 탳i rezultatele returnate de critic 칥n timpul experimentului, este relativ u탳or s캒 construim o func탵ie de pierdere care va minimiza diferen탵a dintre ele. Aceasta ne-ar oferi **critic loss**. Putem calcula **actor loss** folosind aceea탳i abordare ca 칥n algoritmul Policy Gradient.

Dup캒 rularea unuia dintre aceste algoritme, ne putem a탳tepta ca CartPole-ul nostru s캒 se comporte astfel:

![cartpole echilibrat](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-balance.gif)

## 九꽲잺 Exerci탵ii: Policy Gradients 탳i Actor-Critic RL

Continua탵i 칥nv캒탵area 칥n urm캒toarele notebook-uri:

* [RL 칥n TensorFlow](CartPole-RL-TF.ipynb)
* [RL 칥n PyTorch](CartPole-RL-PyTorch.ipynb)

## Alte Sarcini RL

칉nv캒탵area prin recompens캒 este 칥n prezent un domeniu de cercetare 칥n plin캒 expansiune. Unele exemple interesante de 칥nv캒탵are prin recompens캒 sunt:

* 칉nv캒탵area unui computer s캒 joace **jocuri Atari**. Partea provocatoare 칥n aceast캒 problem캒 este c캒 nu avem o stare simpl캒 reprezentat캒 ca un vector, ci mai degrab캒 un screenshot - 탳i trebuie s캒 folosim CNN pentru a converti aceast캒 imagine 칥ntr-un vector de caracteristici sau pentru a extrage informa탵ii despre recompens캒. Jocurile Atari sunt disponibile 칥n Gym.
* 칉nv캒탵area unui computer s캒 joace jocuri de mas캒, cum ar fi 탲ah 탳i Go. Recent, programe de ultim캒 genera탵ie precum **Alpha Zero** au fost antrenate de la zero prin doi agen탵i care joac캒 unul 칥mpotriva celuilalt 탳i se 칥mbun캒t캒탵esc la fiecare pas.
* 칉n industrie, RL este utilizat pentru a crea sisteme de control din simulare. Un serviciu numit [Bonsai](https://azure.microsoft.com/services/project-bonsai/?WT.mc_id=academic-77998-cacaste) este special conceput pentru acest lucru.

## Concluzie

Am 칥nv캒탵at acum cum s캒 antren캒m agen탵i pentru a ob탵ine rezultate bune doar oferindu-le o func탵ie de recompens캒 care define탳te starea dorit캒 a jocului 탳i oferindu-le oportunitatea de a explora inteligent spa탵iul de c캒utare. Am 칥ncercat cu succes dou캒 algoritmi 탳i am ob탵inut un rezultat bun 칥ntr-o perioad캒 relativ scurt캒 de timp. Totu탳i, acesta este doar 칥nceputul c캒l캒toriei voastre 칥n RL, 탳i ar trebui s캒 lua탵i 칥n considerare un curs separat dac캒 dori탵i s캒 aprofunda탵i.

## 游 Provocare

Explora탵i aplica탵iile enumerate 칥n sec탵iunea 'Alte Sarcini RL' 탳i 칥ncerca탵i s캒 implementa탵i una!

## [Chestionar dup캒 lec탵ie](https://ff-quizzes.netlify.app/en/ai/quiz/44)

## Recapitulare & Studiu Individual

Afla탵i mai multe despre 칥nv캒탵area prin recompens캒 clasic캒 칥n [Curriculum-ul nostru de 칉nv캒탵are Automat캒 pentru 칉ncep캒tori](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/README.md).

Urm캒ri탵i [acest video excelent](https://www.youtube.com/watch?v=qv6UVOQ0F44) despre cum un computer poate 칥nv캒탵a s캒 joace Super Mario.

## Tem캒: [Antreneaz캒 o Ma탳in캒 de Munte](lab/README.md)

Scopul vostru 칥n aceast캒 tem캒 va fi s캒 antrena탵i un mediu diferit din Gym - [Mountain Car](https://www.gymlibrary.ml/environments/classic_control/mountain_car/).

---

