# Generativne nasprotujoÄe si mreÅ¾e

V prejÅ¡njem poglavju smo spoznali **generativne modele**: modele, ki lahko ustvarijo nove slike, podobne tistim v uÄnem naboru podatkov. VAE je bil dober primer generativnega modela.

## [Predhodni kviz](https://ff-quizzes.netlify.app/en/ai/quiz/19)

ÄŒe pa poskuÅ¡amo ustvariti nekaj resniÄno smiselnega, na primer sliko z razumno loÄljivostjo, z VAE, bomo opazili, da se uÄenje ne konvergira dobro. Za ta primer uporabe se moramo nauÄiti o drugi arhitekturi, ki je posebej namenjena generativnim modelom - **Generativne nasprotujoÄe si mreÅ¾e**, ali GAN-i.

Glavna ideja GAN-a je, da imamo dve nevronski mreÅ¾i, ki se uÄita ena proti drugi:

<img src="../../../../../translated_images/sl/gan_architecture.8f3a5ab62b8d5d69.webp" width="70%"/>

> Slika avtorja [Dmitry Soshnikov](http://soshnikov.com)

> âœ… Malo besediÅ¡Äa:
> * **Generator** je mreÅ¾a, ki vzame nakljuÄni vektor in kot rezultat ustvari sliko.
> * **Diskriminator** je mreÅ¾a, ki vzame sliko in mora ugotoviti, ali je to prava slika (iz uÄnega nabora podatkov) ali pa jo je ustvaril generator. V bistvu gre za klasifikator slik.

### Diskriminator

Arhitektura diskriminatorja se ne razlikuje od obiÄajne mreÅ¾e za klasifikacijo slik. V najpreprostejÅ¡em primeru je lahko popolnoma povezan klasifikator, najverjetneje pa bo to [konvolucijska mreÅ¾a](../07-ConvNets/README.md).

> âœ… GAN, ki temelji na konvolucijskih mreÅ¾ah, se imenuje [DCGAN](https://arxiv.org/pdf/1511.06434.pdf)

CNN diskriminator je sestavljen iz naslednjih slojev: veÄ konvolucij+poolingov (z zmanjÅ¡anjem prostorske velikosti) in enega ali veÄ popolnoma povezanih slojev za pridobitev "vektorskih znaÄilnosti", konÄnega binarnega klasifikatorja.

> âœ… 'Pooling' v tem kontekstu je tehnika, ki zmanjÅ¡a velikost slike. "Pooling sloji zmanjÅ¡ajo dimenzije podatkov tako, da zdruÅ¾ijo izhode skupin nevronov na enem sloju v en nevron na naslednjem sloju." - [vir](https://wikipedia.org/wiki/Convolutional_neural_network#Pooling_layers)

### Generator

Generator je nekoliko bolj zahteven. Lahko si ga predstavljate kot obrnjenega diskriminatorja. ZaÄne se z latentnim vektorjem (namesto vektorskih znaÄilnosti), ima popolnoma povezan sloj, ki ga pretvori v zahtevano velikost/obliko, nato pa sledi dekonvolucija+poveÄevanje. To je podobno *dekoderju* v [avtoenkoderju](../09-Autoencoders/README.md).

> âœ… Ker je konvolucijski sloj implementiran kot linearni filter, ki prehaja skozi sliko, je dekonvolucija v bistvu podobna konvoluciji in jo je mogoÄe implementirati z isto logiko sloja.

<img src="../../../../../translated_images/sl/gan_arch_detail.46b95fd366f8e543.webp" width="70%"/>

> Slika avtorja [Dmitry Soshnikov](http://soshnikov.com)

### UÄenje GAN-a

GAN-i se imenujejo **nasprotujoÄi si**, ker med generatorjem in diskriminatorjem poteka stalno tekmovanje. Med tem tekmovanjem se oba, generator in diskriminator, izboljÅ¡ujeta, kar omogoÄa mreÅ¾i, da se nauÄi ustvarjati vedno boljÅ¡e slike.

UÄenje poteka v dveh fazah:

* **UÄenje diskriminatorja**. Ta naloga je precej preprosta: ustvarimo serijo slik z generatorjem, jih oznaÄimo z 0, kar pomeni laÅ¾na slika, in vzamemo serijo slik iz vhodnega nabora podatkov (z oznako 1, prava slika). Dobimo *izgubo diskriminatorja* in izvedemo povratno propagacijo.
* **UÄenje generatorja**. To je nekoliko bolj zapleteno, ker neposredno ne poznamo priÄakovanega izhoda za generator. Vzamemo celotno GAN mreÅ¾o, ki jo sestavljata generator in diskriminator, jo napolnimo z nakljuÄnimi vektorji in priÄakujemo, da bo rezultat 1 (kar ustreza pravim slikam). Nato zamrznemo parametre diskriminatorja (ne Å¾elimo, da se uÄi v tem koraku) in izvedemo povratno propagacijo.

Med tem procesom izgube generatorja in diskriminatorja ne upadata bistveno. V idealnem primeru bi se morale oscilirati, kar ustreza izboljÅ¡anju zmogljivosti obeh mreÅ¾.

## âœï¸ Vaje: GAN-i

* [GAN zvezek v TensorFlow/Keras](GANTF.ipynb)
* [GAN zvezek v PyTorch](GANPyTorch.ipynb)

### TeÅ¾ave pri uÄenju GAN-ov

GAN-i so znani po tem, da jih je Å¡e posebej teÅ¾ko uÄiti. Tukaj je nekaj teÅ¾av:

* **Kolaps naÄina**. S tem izrazom mislimo, da se generator nauÄi ustvariti eno uspeÅ¡no sliko, ki zavaja diskriminator, in ne raznolikosti razliÄnih slik.
* **ObÄutljivost na hiperparametre**. Pogosto lahko opazimo, da GAN sploh ne konvergira, nato pa nenadna sprememba hitrosti uÄenja vodi v konvergenco.
* Ohranjanje **ravnoteÅ¾ja** med generatorjem in diskriminatorjem. V mnogih primerih lahko izguba diskriminatorja relativno hitro pade na niÄ, kar povzroÄi, da generator ne more nadalje uÄiti. Da bi to premagali, lahko poskusimo nastaviti razliÄne hitrosti uÄenja za generator in diskriminator ali preskoÄimo uÄenje diskriminatorja, Äe je izguba Å¾e prenizka.
* UÄenje za **visoko loÄljivost**. OdraÅ¾a isti problem kot pri avtoenkoderjih, ta teÅ¾ava se pojavi, ker rekonstrukcija preveÄ slojev konvolucijske mreÅ¾e vodi do artefaktov. Ta teÅ¾ava se obiÄajno reÅ¡i s tako imenovano **progresivno rastjo**, ko se najprej nekaj slojev uÄi na slikah z nizko loÄljivostjo, nato pa se sloji "odblokirajo" ali dodajo. Druga reÅ¡itev bi bila dodajanje dodatnih povezav med sloji in uÄenje veÄ loÄljivosti hkrati - podrobnosti si oglejte v tem [Multi-Scale Gradient GANs Älanku](https://arxiv.org/abs/1903.06048).

## Prenos sloga

GAN-i so odliÄen naÄin za ustvarjanje umetniÅ¡kih slik. Druga zanimiva tehnika je tako imenovani **prenos sloga**, ki vzame eno **vsebinsko sliko** in jo ponovno nariÅ¡e v drugaÄnem slogu, pri Äemer uporabi filtre iz **slogovne slike**.

Kako deluje:
* ZaÄnemo z nakljuÄno Å¡umno sliko (ali z vsebinsko sliko, vendar je za razumevanje laÅ¾je zaÄeti z nakljuÄnim Å¡umom).
* NaÅ¡ cilj je ustvariti takÅ¡no sliko, ki bo blizu tako vsebinski sliki kot slogovni sliki. To doloÄata dve funkciji izgube:
   - **Izguba vsebine** se izraÄuna na podlagi znaÄilnosti, ki jih CNN izluÅ¡Äi na nekaterih slojih iz trenutne slike in vsebinske slike.
   - **Izguba sloga** se izraÄuna med trenutno sliko in slogovno sliko na pameten naÄin z uporabo Gramovih matrik (veÄ podrobnosti v [primeru zvezka](StyleTransfer.ipynb)).
* Da bi sliko naredili bolj gladko in odstranili Å¡um, uvedemo tudi **izgubo variacije**, ki izraÄuna povpreÄno razdaljo med sosednjimi piksli.
* Glavna optimizacijska zanka prilagaja trenutno sliko z uporabo gradientnega spusta (ali kakÅ¡nega drugega optimizacijskega algoritma), da minimizira skupno izgubo, ki je uteÅ¾ena vsota vseh treh izgub.

## âœï¸ Primer: [Prenos sloga](StyleTransfer.ipynb)

## [Naknadni kviz](https://ff-quizzes.netlify.app/en/ai/quiz/20)

## ZakljuÄek

V tej lekciji ste se nauÄili o GAN-ih in kako jih uÄiti. Prav tako ste spoznali posebne izzive, s katerimi se lahko sooÄa ta vrsta nevronske mreÅ¾e, ter nekatere strategije, kako jih premagati.

## ğŸš€ Izziv

Preizkusite [zvezek za prenos sloga](StyleTransfer.ipynb) z uporabo svojih slik.

## Pregled in samostojno uÄenje

Za referenco preberite veÄ o GAN-ih v teh virih:

* Marco Pasini, [10 lekcij, ki sem se jih nauÄil pri uÄenju GAN-ov eno leto](https://towardsdatascience.com/10-lessons-i-learned-training-generative-adversarial-networks-gans-for-a-year-c9071159628)
* [StyleGAN](https://en.wikipedia.org/wiki/StyleGAN), *de facto* GAN arhitektura, ki jo je vredno upoÅ¡tevati
* [Ustvarjanje generativne umetnosti z uporabo GAN-ov na Azure ML](https://soshnikov.com/scienceart/creating-generative-art-using-gan-on-azureml/)

## Naloga

Ponovno preglejte enega od dveh zvezkov, povezanih s to lekcijo, in ponovno nauÄite GAN na svojih slikah. Kaj lahko ustvarite?

---

