<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "31b46ba1f3aa78578134d4829f88be53",
  "translation_date": "2025-08-25T21:57:00+00:00",
  "source_file": "lessons/5-NLP/15-LanguageModeling/README.md",
  "language_code": "sl"
}
-->
# Modeliranje jezika

SemantiÄne vektorske predstavitve, kot sta Word2Vec in GloVe, so pravzaprav prvi korak k **modeliranju jezika** â€“ ustvarjanju modelov, ki na nek naÄin *razumejo* (ali *predstavljajo*) naravo jezika.

## [Predhodni kviz](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/115)

Glavna ideja modeliranja jezika je, da jih uÄimo na nenalepÄenih podatkovnih zbirkah na nenadzorovan naÄin. To je pomembno, ker imamo na voljo ogromne koliÄine nenalepÄenega besedila, medtem ko je koliÄina nalepÄenega besedila vedno omejena z vloÅ¾enim trudom za oznaÄevanje. Najpogosteje lahko zgradimo jezikovne modele, ki lahko **napovedujejo manjkajoÄe besede** v besedilu, saj je enostavno nakljuÄno besedo v besedilu zakriti in jo uporabiti kot uÄni vzorec.

## UÄenje vektorskih predstavitev

V prejÅ¡njih primerih smo uporabljali vnaprej nauÄene semantiÄne vektorske predstavitve, vendar je zanimivo videti, kako lahko te predstavitve sami nauÄimo. Obstaja veÄ moÅ¾nih pristopov:

* **N-gram** modeliranje jezika, kjer napovedujemo token na podlagi N prejÅ¡njih tokenov (N-gram).
* **Neprekinjena vreÄa besed** (CBoW), kjer napovedujemo sredinski token $W_0$ v zaporedju tokenov $W_{-N}$, ..., $W_N$.
* **Skip-gram**, kjer iz sredinskega tokena $W_0$ napovedujemo mnoÅ¾ico sosednjih tokenov {$W_{-N},\dots, W_{-1}, W_1,\dots, W_N$}.

![slika iz Älanka o pretvorbi besed v vektorje](../../../../../translated_images/example-algorithms-for-converting-words-to-vectors.fbe9207a726922f6f0f5de66427e8a6eda63809356114e28fb1fa5f4a83ebda7.sl.png)

> Slika iz [tega Älanka](https://arxiv.org/pdf/1301.3781.pdf)

## âœï¸ Primeri zvezkov: UÄenje CBoW modela

Nadaljujte z uÄenjem v naslednjih zvezkih:

* [UÄenje CBoW Word2Vec z uporabo TensorFlow](../../../../../lessons/5-NLP/15-LanguageModeling/CBoW-TF.ipynb)
* [UÄenje CBoW Word2Vec z uporabo PyTorch](../../../../../lessons/5-NLP/15-LanguageModeling/CBoW-PyTorch.ipynb)

## ZakljuÄek

V prejÅ¡nji lekciji smo videli, da vektorske predstavitve besed delujejo skoraj Äarobno! Zdaj vemo, da uÄenje vektorskih predstavitev besed ni zelo zapletena naloga, in Äe je potrebno, lahko sami nauÄimo svoje vektorske predstavitve za besedila iz specifiÄnih domen.

## [Naknadni kviz](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/215)

## Pregled in samostojno uÄenje

* [Uradni PyTorch vodiÄ za modeliranje jezika](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html).
* [Uradni TensorFlow vodiÄ za uÄenje Word2Vec modela](https://www.TensorFlow.org/tutorials/text/word2vec).
* Uporaba ogrodja **gensim** za uÄenje najpogosteje uporabljenih vektorskih predstavitev v nekaj vrsticah kode je opisana [v tej dokumentaciji](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html).

## ğŸš€ [Naloga: NauÄite Skip-Gram model](lab/README.md)

V laboratorijski nalogi vas izzivamo, da spremenite kodo iz te lekcije in nauÄite Skip-Gram model namesto CBoW. [Preberite podrobnosti](lab/README.md)

**Omejitev odgovornosti**:  
Ta dokument je bil preveden z uporabo storitve za prevajanje z umetno inteligenco [Co-op Translator](https://github.com/Azure/co-op-translator). ÄŒeprav si prizadevamo za natanÄnost, vas prosimo, da upoÅ¡tevate, da lahko avtomatizirani prevodi vsebujejo napake ali netoÄnosti. Izvirni dokument v njegovem maternem jeziku je treba obravnavati kot avtoritativni vir. Za kljuÄne informacije priporoÄamo profesionalni prevod s strani Äloveka. Ne prevzemamo odgovornosti za morebitna nesporazumevanja ali napaÄne razlage, ki bi nastale zaradi uporabe tega prevoda.