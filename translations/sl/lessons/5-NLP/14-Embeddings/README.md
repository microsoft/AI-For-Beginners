# Vdelave

## [Predhodni kviz](https://ff-quizzes.netlify.app/en/ai/quiz/27)

Pri treniranju klasifikatorjev na osnovi BoW ali TF/IDF smo delali z visoko-dimenzionalnimi vektorji vreÄe besed dolÅ¾ine `vocab_size`, pri Äemer smo izrecno pretvarjali nizko-dimenzionalne vektorje pozicijske reprezentacije v redke enovroÄne reprezentacije. Ta enovroÄna reprezentacija pa ni uÄinkovita glede porabe pomnilnika. Poleg tega se vsaka beseda obravnava neodvisno od drugih, tj. enovroÄno kodirani vektorji ne izraÅ¾ajo nobene semantiÄne podobnosti med besedami.

Ideja **vdelave** je, da besede predstavimo z nizko-dimenzionalnimi gostimi vektorji, ki nekako odraÅ¾ajo semantiÄni pomen besede. Kasneje bomo razpravljali o tem, kako zgraditi smiselne vdelave besed, za zdaj pa si vdelave predstavljajmo kot naÄin za zmanjÅ¡anje dimenzionalnosti vektorja besede.

Tako bi plast vdelave sprejela besedo kot vhod in ustvarila izhodni vektor doloÄene velikosti `embedding_size`. Na nek naÄin je to zelo podobno plasti `Linear`, vendar namesto da bi sprejela enovroÄno kodiran vektor, lahko sprejme Å¡tevilko besede kot vhod, kar nam omogoÄa, da se izognemo ustvarjanju velikih enovroÄno kodiranih vektorjev.

Z uporabo plasti vdelave kot prve plasti v naÅ¡em klasifikacijskem omreÅ¾ju lahko preklopimo iz modela vreÄe besed na model **vreÄe vdelav**, kjer najprej vsako besedo v naÅ¡em besedilu pretvorimo v ustrezno vdelavo, nato pa izraÄunamo neko agregatno funkcijo nad vsemi temi vdelavami, kot so `sum`, `average` ali `max`.  

![Slika prikazuje klasifikator vdelav za pet besed v zaporedju.](../../../../../translated_images/sl/embedding-classifier-example.b77f021a7ee67eee.webp)

> Slika avtorja

## âœï¸ Vaje: Vdelave

Nadaljujte z uÄenjem v naslednjih zvezkih:
* [Vdelave s PyTorch](EmbeddingsPyTorch.ipynb)
* [Vdelave s TensorFlow](EmbeddingsTF.ipynb)

## SemantiÄne vdelave: Word2Vec

Medtem ko se je plast vdelave nauÄila preslikati besede v vektorsko reprezentacijo, ta reprezentacija ni nujno imela veliko semantiÄnega pomena. Lepo bi bilo nauÄiti vektorsko reprezentacijo, pri kateri so podobne besede ali sopomenke predstavljene z vektorji, ki so si blizu glede na neko vektorsko razdaljo (npr. Evklidsko razdaljo).

Da bi to dosegli, moramo naÅ¡ model vdelave predhodno trenirati na veliki zbirki besedil na specifiÄen naÄin. Eden od naÄinov za treniranje semantiÄnih vdelav se imenuje [Word2Vec](https://en.wikipedia.org/wiki/Word2vec). Temelji na dveh glavnih arhitekturah, ki se uporabljata za ustvarjanje porazdeljene reprezentacije besed:

 - **Neprekinjena vreÄa besed** (CBoW) â€” pri tej arhitekturi treniramo model, da napove besedo iz okoliÅ¡kega konteksta. Glede na ngram $(W_{-2},W_{-1},W_0,W_1,W_2)$ je cilj modela napovedati $W_0$ iz $(W_{-2},W_{-1},W_1,W_2)$.
 - **Neprekinjeni preskok-gram** je nasproten CBoW. Model uporablja okoliÅ¡ko okno kontekstnih besed za napoved trenutne besede.

CBoW je hitrejÅ¡i, medtem ko je preskok-gram poÄasnejÅ¡i, vendar bolje predstavlja redke besede.

![Slika prikazuje algoritma CBoW in Skip-Gram za pretvorbo besed v vektorje.](../../../../../translated_images/sl/example-algorithms-for-converting-words-to-vectors.fbe9207a726922f6.webp)

> Slika iz [tega Älanka](https://arxiv.org/pdf/1301.3781.pdf)

Predhodno trenirane vdelave Word2Vec (kot tudi drugi podobni modeli, kot je GloVe) se lahko uporabljajo namesto plasti vdelave v nevronskih omreÅ¾jih. Vendar pa se moramo ukvarjati z besediÅ¡Äi, saj se besediÅ¡Äe, uporabljeno za predhodno treniranje Word2Vec/GloVe, verjetno razlikuje od besediÅ¡Äa v naÅ¡i zbirki besedil. Oglejte si zgornje zvezke, da vidite, kako se ta problem lahko reÅ¡i.

## Kontekstualne vdelave

Ena kljuÄna omejitev tradicionalnih predhodno treniranih vdelav, kot je Word2Vec, je problem razloÄevanja pomenov besed. Medtem ko predhodno trenirane vdelave lahko zajamejo nekaj pomena besed v kontekstu, je vsak moÅ¾en pomen besede kodiran v isto vdelavo. To lahko povzroÄi teÅ¾ave v nadaljnjih modelih, saj imajo mnoge besede, kot je beseda 'play', razliÄne pomene glede na kontekst, v katerem se uporabljajo.

Na primer, beseda 'play' v teh dveh stavkih ima precej razliÄne pomene:

- Å el sem na **igro** v gledaliÅ¡Äe.
- Janez Å¾eli **igrati** s svojimi prijatelji.

Predhodno trenirane vdelave zgoraj predstavljajo oba pomena besede 'play' v isti vdelavi. Da bi premagali to omejitev, moramo zgraditi vdelave na osnovi **jezikovnega modela**, ki je treniran na veliki zbirki besedil in *ve*, kako se besede lahko povezujejo v razliÄnih kontekstih. Razprava o kontekstualnih vdelavah presega obseg tega teÄaja, vendar se bomo k njim vrnili, ko bomo govorili o jezikovnih modelih kasneje v teÄaju.

## ZakljuÄek

V tej lekciji ste odkrili, kako zgraditi in uporabljati plasti vdelave v TensorFlow in Pytorch za boljÅ¡e odraÅ¾anje semantiÄnih pomenov besed.

## ğŸš€ Izziv

Word2Vec je bil uporabljen za nekaj zanimivih aplikacij, vkljuÄno z generiranjem besedil pesmi in poezije. Oglejte si [ta Älanek](https://www.politetype.com/blog/word2vec-color-poems), ki opisuje, kako je avtor uporabil Word2Vec za generiranje poezije. Oglejte si tudi [ta video Dana Shiffmanna](https://www.youtube.com/watch?v=LSS_bos_TPI&ab_channel=TheCodingTrain), da odkrijete drugaÄno razlago te tehnike. Nato poskusite te tehnike uporabiti na svoji zbirki besedil, morda pridobljeni iz Kaggle.

## [Naknadni kviz](https://ff-quizzes.netlify.app/en/ai/quiz/28)

## Pregled in samostojno uÄenje

Preberite ta Älanek o Word2Vec: [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf)

## [Naloga: Zvezki](assignment.md)

---

