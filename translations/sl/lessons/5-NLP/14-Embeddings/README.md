<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "e40b47ac3fd48f71304ede1474e66293",
  "translation_date": "2025-08-25T21:40:51+00:00",
  "source_file": "lessons/5-NLP/14-Embeddings/README.md",
  "language_code": "sl"
}
-->
# Vdelave

## [Predavanje kviz](https://ff-quizzes.netlify.app/en/ai/quiz/27)

Pri treniranju klasifikatorjev, ki temeljijo na BoW ali TF/IDF, smo delali z visoko-dimenzionalnimi vektorji vreÄe besed dolÅ¾ine `vocab_size`, pri Äemer smo izrecno pretvarjali nizko-dimenzionalne vektorje pozicijske reprezentacije v redke eno-vroÄe (one-hot) reprezentacije. Ta eno-vroÄa reprezentacija pa ni uÄinkovita glede porabe pomnilnika. Poleg tega se vsaka beseda obravnava neodvisno od drugih, tj. eno-vroÄe kodirani vektorji ne izraÅ¾ajo nobene semantiÄne podobnosti med besedami.

Ideja **vdelave** (embedding) je predstaviti besede z nizko-dimenzionalnimi gostimi vektorji, ki na nek naÄin odraÅ¾ajo semantiÄni pomen besede. Kasneje bomo razpravljali o tem, kako zgraditi smiselne vdelave besed, za zdaj pa si vdelave predstavljajmo kot naÄin za zmanjÅ¡anje dimenzionalnosti vektorja besede.

Tako bi plast za vdelavo kot vhod vzela besedo in ustvarila izhodni vektor doloÄene velikosti `embedding_size`. Na nek naÄin je to zelo podobno plasti `Linear`, vendar namesto da bi vzela eno-vroÄe kodiran vektor, lahko kot vhod vzame Å¡tevilko besede, kar nam omogoÄa, da se izognemo ustvarjanju velikih eno-vroÄe kodiranih vektorjev.

Z uporabo plasti za vdelavo kot prve plasti v naÅ¡em omreÅ¾ju klasifikatorja lahko preklopimo iz modela vreÄe besed na model **vreÄe vdelav** (embedding bag), kjer najprej vsako besedo v naÅ¡em besedilu pretvorimo v ustrezno vdelavo, nato pa izraÄunamo neko agregatno funkcijo nad vsemi temi vdelavami, kot so `sum`, `average` ali `max`.

![Slika, ki prikazuje klasifikator z vdelavami za pet zaporednih besed.](../../../../../translated_images/embedding-classifier-example.b77f021a7ee67eeec8e68bfe11636c5b97d6eaa067515a129bfb1d0034b1ac5b.sl.png)

> Slika avtorja

## âœï¸ Vaje: Vdelave

Nadaljujte z uÄenjem v naslednjih beleÅ¾nicah:
* [Vdelave s PyTorch](../../../../../lessons/5-NLP/14-Embeddings/EmbeddingsPyTorch.ipynb)
* [Vdelave TensorFlow](../../../../../lessons/5-NLP/14-Embeddings/EmbeddingsTF.ipynb)

## SemantiÄne vdelave: Word2Vec

Medtem ko se je plast za vdelavo nauÄila preslikati besede v vektorsko predstavitev, ta predstavitev ni nujno imela veliko semantiÄnega pomena. Bilo bi koristno nauÄiti se vektorske predstavitve, kjer so podobne besede ali sopomenke predstavljene z vektorji, ki so si blizu glede na neko vektorsko razdaljo (npr. Evklidska razdalja).

Za to moramo naÅ¡ model za vdelavo predhodno trenirati na veliki zbirki besedil na specifiÄen naÄin. Eden od naÄinov za treniranje semantiÄnih vdelav se imenuje [Word2Vec](https://en.wikipedia.org/wiki/Word2vec). Temelji na dveh glavnih arhitekturah, ki se uporabljata za ustvarjanje porazdeljene predstavitve besed:

 - **Neprekinjena vreÄa besed** (CBoW) â€” v tej arhitekturi treniramo model, da napove besedo iz okoliÅ¡kega konteksta. Glede na n-gram $(W_{-2},W_{-1},W_0,W_1,W_2)$ je cilj modela napovedati $W_0$ iz $(W_{-2},W_{-1},W_1,W_2)$.
 - **Neprekinjeni preskok-gram** (skip-gram) je nasproten CBoW. Model uporablja okoliÅ¡ko okno kontekstnih besed za napoved trenutne besede.

CBoW je hitrejÅ¡i, medtem ko je skip-gram poÄasnejÅ¡i, vendar bolje predstavlja redke besede.

![Slika, ki prikazuje algoritma CBoW in Skip-Gram za pretvorbo besed v vektorje.](../../../../../translated_images/example-algorithms-for-converting-words-to-vectors.fbe9207a726922f6f0f5de66427e8a6eda63809356114e28fb1fa5f4a83ebda7.sl.png)

> Slika iz [tega Älanka](https://arxiv.org/pdf/1301.3781.pdf)

Predhodno trenirane vdelave Word2Vec (kot tudi drugi podobni modeli, kot je GloVe) se lahko uporabijo namesto plasti za vdelavo v nevronskih omreÅ¾jih. Vendar pa se moramo ukvarjati z besediÅ¡Äi, saj se besediÅ¡Äe, uporabljeno za predhodno treniranje Word2Vec/GloVe, verjetno razlikuje od besediÅ¡Äa v naÅ¡em besedilnem korpusu. Oglejte si zgornje beleÅ¾nice, da vidite, kako reÅ¡iti ta problem.

## Kontekstualne vdelave

Ena kljuÄna omejitev tradicionalnih predhodno treniranih predstavitev vdelav, kot je Word2Vec, je problem razloÄevanja pomenov besed. Medtem ko predhodno trenirane vdelave lahko zajamejo nekaj pomena besed v kontekstu, je vsak moÅ¾en pomen besede kodiran v isto vdelavo. To lahko povzroÄi teÅ¾ave v nadaljnjih modelih, saj imajo Å¡tevilne besede, kot je beseda 'play', razliÄne pomene glede na kontekst, v katerem so uporabljene.

Na primer, beseda 'play' v teh dveh razliÄnih stavkih ima precej drugaÄen pomen:

- Å el sem na **igro** v gledaliÅ¡Äe.
- Janez se Å¾eli **igrati** s prijatelji.

Predhodno trenirane vdelave zgoraj predstavljajo oba pomena besede 'play' v isti vdelavi. Da bi premagali to omejitev, moramo zgraditi vdelave na podlagi **jezikovnega modela**, ki je treniran na velikem korpusu besedil in *ve*, kako se besede lahko sestavijo v razliÄnih kontekstih. Razprava o kontekstualnih vdelavah presega obseg tega teÄaja, vendar se bomo k njim vrnili, ko bomo kasneje govorili o jezikovnih modelih.

## ZakljuÄek

V tej lekciji ste odkrili, kako zgraditi in uporabljati plasti za vdelavo v TensorFlow in PyTorch, da bolje odraÅ¾ajo semantiÄne pomene besed.

## ğŸš€ Izziv

Word2Vec je bil uporabljen za nekatere zanimive aplikacije, vkljuÄno z ustvarjanjem pesmi in poezije. Oglejte si [ta Älanek](https://www.politetype.com/blog/word2vec-color-poems), ki opisuje, kako je avtor uporabil Word2Vec za ustvarjanje poezije. Oglejte si tudi [ta video Dana Shiffmanna](https://www.youtube.com/watch?v=LSS_bos_TPI&ab_channel=TheCodingTrain), da odkrijete drugaÄno razlago te tehnike. Nato poskusite te tehnike uporabiti na svojem besedilnem korpusu, morda pridobljenem s Kaggle.

## [Kviz po predavanju](https://ff-quizzes.netlify.app/en/ai/quiz/28)

## Pregled in samostojno uÄenje

Preberite ta Älanek o Word2Vec: [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf)

## [Naloga: BeleÅ¾nice](assignment.md)

**Omejitev odgovornosti**:  
Ta dokument je bil preveden z uporabo storitve za strojno prevajanje [Co-op Translator](https://github.com/Azure/co-op-translator). ÄŒeprav si prizadevamo za natanÄnost, vas prosimo, da upoÅ¡tevate, da lahko avtomatizirani prevodi vsebujejo napake ali netoÄnosti. Izvirni dokument v njegovem izvirnem jeziku je treba obravnavati kot avtoritativni vir. Za kljuÄne informacije priporoÄamo profesionalni ÄloveÅ¡ki prevod. Ne prevzemamo odgovornosti za morebitne nesporazume ali napaÄne razlage, ki izhajajo iz uporabe tega prevoda.