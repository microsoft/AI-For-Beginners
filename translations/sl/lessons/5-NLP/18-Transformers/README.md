<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "7e617f0b8de85a43957a853aba09bfeb",
  "translation_date": "2025-08-25T22:03:30+00:00",
  "source_file": "lessons/5-NLP/18-Transformers/README.md",
  "language_code": "sl"
}
-->
# Mehanizmi pozornosti in transformatorji

## [Predavanje kviz](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/118)

Eden najpomembnej코ih problemov na podro캜ju NLP je **strojno prevajanje**, klju캜na naloga, ki je osnova za orodja, kot je Google Translate. V tem poglavju se bomo osredoto캜ili na strojno prevajanje oziroma bolj splo코no na katerokoli nalogo *zaporedje-v-zaporedje* (imenovano tudi **pretvorba stavkov**).

Pri RNN-jih je zaporedje-v-zaporedje implementirano z dvema rekurzivnima omre쬵ema, kjer eno omre쬵e, **enkoder**, stisne vhodno zaporedje v skrito stanje, drugo omre쬵e, **dekoder**, pa to skrito stanje raz코iri v preveden rezultat. Ta pristop ima nekaj te쬬v:

* Kon캜no stanje enkoderskega omre쬵a te쬶o ohrani spomin na za캜etek stavka, kar povzro캜i slab코o kakovost modela za dolge stavke.
* Vse besede v zaporedju imajo enak vpliv na rezultat. V resnici pa imajo dolo캜ene besede v vhodnem zaporedju pogosto ve캜ji vpliv na izhod kot druge.

**Mehanizmi pozornosti** omogo캜ajo tehtanje kontekstualnega vpliva vsakega vhodnega vektorja na vsako napoved izhoda RNN-ja. To se implementira z ustvarjanjem bli쬹jic med vmesnimi stanji vhodnega RNN-ja in izhodnega RNN-ja. Na ta na캜in pri generiranju izhodnega simbola y<sub>t</sub> upo코tevamo vsa vhodna skrita stanja h<sub>i</sub> z razli캜nimi ute쬹imi koeficienti 풤<sub>t,i</sub>.

![Slika, ki prikazuje enkoder/dekoder model z aditivno plastjo pozornosti](../../../../../translated_images/encoder-decoder-attention.7a726296894fb567aa2898c94b17b3289087f6705c11907df8301df9e5eeb3de.sl.png)

> Enkoder-dekoder model z aditivnim mehanizmom pozornosti v [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf), citirano iz [tega bloga](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)

Matrika pozornosti {풤<sub>i,j</sub>} predstavlja stopnjo, do katere dolo캜ene vhodne besede vplivajo na generacijo dolo캜ene besede v izhodnem zaporedju. Spodaj je primer tak코ne matrike:

![Slika, ki prikazuje vzor캜no poravnavo, najdeno z RNNsearch-50, vzeto iz Bahdanau - arviz.org](../../../../../translated_images/bahdanau-fig3.09ba2d37f202a6af11de6c82d2d197830ba5f4528d9ea430eb65fd3a75065973.sl.png)

> Slika iz [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) (Slika 3)

Mehanizmi pozornosti so odgovorni za velik del trenutnega ali skoraj trenutnega stanja umetnosti na podro캜ju NLP. Dodajanje pozornosti pa mo캜no pove캜a 코tevilo parametrov modela, kar je povzro캜ilo te쬬ve pri skaliranju RNN-jev. Klju캜na omejitev skaliranja RNN-jev je, da rekurzivna narava modelov ote쬿je zdru쬰vanje in paralelizacijo u캜enja. V RNN-ju je treba vsak element zaporedja obdelati po vrstnem redu, kar pomeni, da ga ni mogo캜e enostavno paralelizirati.

![Enkoder Dekoder s Pozornostjo](../../../../../lessons/5-NLP/18-Transformers/images/EncDecAttention.gif)

> Slika iz [Googlovega bloga](https://research.googleblog.com/2016/09/a-neural-network-for-machine.html)

Uporaba mehanizmov pozornosti v kombinaciji s to omejitvijo je privedla do nastanka zdaj코njih vrhunskih modelov transformatorjev, kot so BERT in Open-GPT3.

## Modeli transformatorjev

Ena glavnih idej transformatorjev je izogniti se zaporedni naravi RNN-jev in ustvariti model, ki je paraleliziran med u캜enjem. To se dose쬰 z implementacijo dveh idej:

* kodiranje polo쬬ja
* uporaba mehanizma samopozornosti za zajemanje vzorcev namesto RNN-jev (ali CNN-jev) (zato se 캜lanek, ki uvaja transformatorje, imenuje *[Attention is all you need](https://arxiv.org/abs/1706.03762)*)

### Kodiranje/vdelava polo쬬ja

Ideja kodiranja polo쬬ja je naslednja.  
1. Pri uporabi RNN-jev je relativni polo쬬j tokenov predstavljen s 코tevilom korakov, zato ga ni treba izrecno predstaviti.  
2. Ko pa preklopimo na pozornost, moramo poznati relativne polo쬬je tokenov znotraj zaporedja.  
3. Za pridobitev kodiranja polo쬬ja dopolnimo na코e zaporedje tokenov z zaporedjem polo쬬jev tokenov v zaporedju (tj. zaporedje 코tevil 0, 1, ...).  
4. Nato zme코amo polo쬬j tokena z vektorsko vdelavo tokena. Za pretvorbo polo쬬ja (cela 코tevila) v vektor lahko uporabimo razli캜ne pristope:

* U캜ljiva vdelava, podobno kot vdelava tokenov. To je pristop, ki ga obravnavamo tukaj. Na vrhu tako tokenov kot njihovih polo쬬jev uporabimo sloje za vdelavo, kar rezultira v vektorskih vdelavah enakih dimenzij, ki jih nato se코tejemo.
* Fiksna funkcija kodiranja polo쬬ja, kot je predlagano v izvirnem 캜lanku.

<img src="images/pos-embedding.png" width="50%"/>

> Slika avtorja

Rezultat, ki ga dobimo z vdelavo polo쬬ja, vdeluje tako izvirni token kot njegov polo쬬j znotraj zaporedja.

### Samopozornost z ve캜 glavami

Nato moramo zajeti nekatere vzorce znotraj na코ega zaporedja. Da bi to dosegli, transformatorji uporabljajo mehanizem **samopozornosti**, ki je v bistvu pozornost, uporabljena na istem zaporedju kot vhod in izhod. Uporaba samopozornosti nam omogo캜a, da upo코tevamo **kontekst** znotraj stavka in vidimo, katere besede so medsebojno povezane. Na primer, omogo캜a nam, da vidimo, na katere besede se nana코ajo zaimki, kot je *to*, in tudi upo코tevamo kontekst:

![](../../../../../translated_images/CoreferenceResolution.861924d6d384a7d68d8d0039d06a71a151f18a796b8b1330239d3590bd4947eb.sl.png)

> Slika iz [Googlovega bloga](https://research.googleblog.com/2017/08/transformer-novel-neural-network.html)

V transformatorjih uporabljamo **pozornost z ve캜 glavami**, da damo omre쬵u mo캜 za zajemanje ve캜 razli캜nih vrst odvisnosti, npr. dolgoro캜nih proti kratkoro캜nim odnosom med besedami, soodvisnosti proti ne캜emu drugemu itd.

[TensorFlow Notebook](../../../../../lessons/5-NLP/18-Transformers/TransformersTF.ipynb) vsebuje ve캜 podrobnosti o implementaciji slojev transformatorjev.

### Pozornost enkoder-dekoder

V transformatorjih se pozornost uporablja na dveh mestih:

* Za zajemanje vzorcev znotraj vhodnega besedila z uporabo samopozornosti
* Za izvajanje prevajanja zaporedij - to je plast pozornosti med enkoderjem in dekoderjem.

Pozornost enkoder-dekoder je zelo podobna mehanizmu pozornosti, uporabljenemu v RNN-jih, kot je opisano na za캜etku tega poglavja. Ta animirani diagram pojasnjuje vlogo pozornosti enkoder-dekoder.

![Animirani GIF, ki prikazuje, kako se izvajajo ocene v modelih transformatorjev.](../../../../../lessons/5-NLP/18-Transformers/images/transformer-animated-explanation.gif)

Ker je vsak vhodni polo쬬j neodvisno preslikan na vsak izhodni polo쬬j, lahko transformatorji bolje paralelizirajo kot RNN-ji, kar omogo캜a veliko ve캜je in izrazitej코e jezikovne modele. Vsaka glava pozornosti se lahko uporablja za u캜enje razli캜nih odnosov med besedami, kar izbolj코a naloge obdelave naravnega jezika.

## BERT

**BERT** (Bidirectional Encoder Representations from Transformers) je zelo veliko ve캜plastno omre쬵e transformatorjev z 12 sloji za *BERT-base* in 24 za *BERT-large*. Model je najprej predhodno nau캜en na velikem korpusu besedilnih podatkov (WikiPedia + knjige) z uporabo nenadzorovanega u캜enja (napovedovanje zamaskiranih besed v stavku). Med predhodnim u캜enjem model absorbira pomembne ravni razumevanja jezika, ki jih je nato mogo캜e uporabiti z drugimi nabori podatkov z uporabo finega prilagajanja. Ta proces se imenuje **prenosno u캜enje**.

![slika iz http://jalammar.github.io/illustrated-bert/](../../../../../translated_images/jalammarBERT-language-modeling-masked-lm.34f113ea5fec4362e39ee4381aab7cad06b5465a0b5f053a0f2aa05fbe14e746.sl.png)

> Slika [vir](http://jalammar.github.io/illustrated-bert/)

## 九꽲잺 Vaje: Transformatorji

Nadaljujte z u캜enjem v naslednjih bele쬹icah:

* [Transformatorji v PyTorch](../../../../../lessons/5-NLP/18-Transformers/TransformersPyTorch.ipynb)
* [Transformatorji v TensorFlow](../../../../../lessons/5-NLP/18-Transformers/TransformersTF.ipynb)

## Zaklju캜ek

V tej lekciji ste se nau캜ili o transformatorjih in mehanizmih pozornosti, ki so klju캜na orodja v NLP. Obstaja veliko razli캜ic arhitektur transformatorjev, vklju캜no z BERT, DistilBERT, BigBird, OpenGPT3 in drugimi, ki jih je mogo캜e fino prilagoditi. Paket [HuggingFace](https://github.com/huggingface/) ponuja repozitorij za u캜enje mnogih teh arhitektur z uporabo PyTorch in TensorFlow.

## 游 Izziv

## [Kviz po predavanju](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/218)

## Pregled in samostojno u캜enje

* [Blog objava](https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/), ki pojasnjuje klasi캜ni 캜lanek [Attention is all you need](https://arxiv.org/abs/1706.03762) o transformatorjih.
* [Serija blog objav](https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452) o transformatorjih, ki podrobno pojasnjuje arhitekturo.

## [Naloga](assignment.md)

**Omejitev odgovornosti**:  
Ta dokument je bil preveden z uporabo storitve AI za prevajanje [Co-op Translator](https://github.com/Azure/co-op-translator). 캛eprav si prizadevamo za natan캜nost, vas prosimo, da upo코tevate, da lahko avtomatizirani prevodi vsebujejo napake ali neto캜nosti. Izvirni dokument v njegovem maternem jeziku je treba obravnavati kot avtoritativni vir. Za klju캜ne informacije priporo캜amo profesionalni 캜love코ki prevod. Ne prevzemamo odgovornosti za morebitne nesporazume ali napa캜ne razlage, ki bi nastale zaradi uporabe tega prevoda.