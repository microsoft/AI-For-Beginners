# Mehanizmi pozornosti in transformatorji

## [Predavanje kviz](https://ff-quizzes.netlify.app/en/ai/quiz/35)

Eden najpomembnej코ih problemov na podro캜ju NLP je **strojno prevajanje**, klju캜na naloga, ki je osnova za orodja, kot je Google Translate. V tem poglavju se bomo osredoto캜ili na strojno prevajanje oziroma bolj splo코no na katerokoli nalogo *zaporedje-v-zaporedje* (imenovano tudi **pretvorba stavkov**).

Pri RNN-jih je zaporedje-v-zaporedje implementirano z dvema rekurzivnima mre쬬ma, kjer ena mre쬬, **enkoder**, stisne vhodno zaporedje v skrito stanje, medtem ko druga mre쬬, **dekoder**, to skrito stanje raz코iri v preveden rezultat. Ta pristop ima nekaj te쬬v:

* Kon캜no stanje enkoderske mre쬰 te쬶o ohranja spomin na za캜etek stavka, kar povzro캜a slab코o kakovost modela za dolge stavke.
* Vse besede v zaporedju imajo enak vpliv na rezultat. V resnici pa imajo dolo캜ene besede v vhodnem zaporedju pogosto ve캜ji vpliv na izhodne rezultate kot druge.

**Mehanizmi pozornosti** omogo캜ajo tehtanje kontekstualnega vpliva vsakega vhodnega vektorja na vsako napoved izhoda RNN. To se implementira z ustvarjanjem bli쬹jic med vmesnimi stanji vhodnega RNN in izhodnega RNN. Na ta na캜in bomo pri generiranju izhodnega simbola y<sub>t</sub> upo코tevali vsa vhodna skrita stanja h<sub>i</sub>, z razli캜nimi ute쬹imi koeficienti &alpha;<sub>t,i</sub>.

![Slika, ki prikazuje model enkoder/dekoder z aditivno plastjo pozornosti](../../../../../translated_images/sl/encoder-decoder-attention.7a726296894fb567.webp)

> Model enkoder-dekoder z aditivnim mehanizmom pozornosti v [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf), povzeto iz [tega bloga](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)

Matrika pozornosti {&alpha;<sub>i,j</sub>} predstavlja stopnjo, do katere dolo캜ene vhodne besede vplivajo na generiranje dolo캜ene besede v izhodnem zaporedju. Spodaj je primer tak코ne matrike:

![Slika, ki prikazuje vzor캜no poravnavo, najdeno z RNNsearch-50, povzeto iz Bahdanau - arviz.org](../../../../../translated_images/sl/bahdanau-fig3.09ba2d37f202a6af.webp)

> Slika iz [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) (Slika 3)

Mehanizmi pozornosti so odgovorni za velik del trenutnega ali skoraj trenutnega stanja umetnosti v NLP. Dodajanje pozornosti pa mo캜no pove캜a 코tevilo parametrov modela, kar je povzro캜ilo te쬬ve pri skaliranju RNN-jev. Klju캜na omejitev pri skaliranju RNN-jev je, da rekurzivna narava modelov ote쬿je zdru쬰vanje in paralelizacijo u캜enja. V RNN je treba vsak element zaporedja obdelati v zaporednem vrstnem redu, kar pomeni, da ga ni mogo캜e enostavno paralelizirati.

![Enkoder Dekoder s Pozornostjo](../../../../../lessons/5-NLP/18-Transformers/images/EncDecAttention.gif)

> Slika iz [Googlovega bloga](https://research.googleblog.com/2016/09/a-neural-network-for-machine.html)

Uporaba mehanizmov pozornosti v kombinaciji s to omejitvijo je privedla do nastanka zdaj코njih vrhunskih modelov transformatorjev, kot so BERT in Open-GPT3.

## Modeli transformatorjev

Ena glavnih idej za transformatorji je izogniti se zaporedni naravi RNN-jev in ustvariti model, ki je paraleliziran med u캜enjem. To se dose쬰 z implementacijo dveh idej:

* kodiranje polo쬬ja
* uporaba mehanizma samopozornosti za zajemanje vzorcev namesto RNN-jev (ali CNN-jev) (zato se 캜lanek, ki uvaja transformatorje, imenuje *[Attention is all you need](https://arxiv.org/abs/1706.03762)*)

### Kodiranje/ugnezdenje polo쬬ja

Ideja kodiranja polo쬬ja je naslednja. 
1. Pri uporabi RNN-jev je relativni polo쬬j tokenov predstavljen s 코tevilom korakov in zato ni treba, da je eksplicitno predstavljen. 
2. Ko pa preklopimo na pozornost, moramo poznati relativne polo쬬je tokenov znotraj zaporedja. 
3. Za pridobitev kodiranja polo쬬ja dopolnimo na코e zaporedje tokenov z zaporedjem polo쬬jev tokenov v zaporedju (tj. zaporedje 코tevil 0,1, ...).
4. Nato zme코amo polo쬬j tokena z vektorjem ugnezdenja tokena. Za pretvorbo polo쬬ja (cela 코tevila) v vektor lahko uporabimo razli캜ne pristope:

* U캜ljivo ugnezdenje, podobno ugnezdenju tokenov. To je pristop, ki ga obravnavamo tukaj. Na vrhu tako tokenov kot njihovih polo쬬jev uporabimo plasti ugnezdenja, kar rezultira v vektorjih ugnezdenja enakih dimenzij, ki jih nato se코tejemo.
* Fiksna funkcija kodiranja polo쬬ja, kot je predlagano v izvirnem 캜lanku.

<img src="../../../../../translated_images/sl/pos-embedding.e41ce9b6cf6078af.webp" width="50%"/>

> Slika avtorja

Rezultat, ki ga dobimo s kodiranjem polo쬬ja, ugnezdi tako izvirni token kot njegov polo쬬j znotraj zaporedja.

### Ve캜glava samopozornost

Nato moramo zajeti nekatere vzorce znotraj na코ega zaporedja. Da bi to dosegli, transformatorji uporabljajo mehanizem **samopozornosti**, ki je v bistvu pozornost, uporabljena na istem zaporedju kot vhod in izhod. Uporaba samopozornosti nam omogo캜a, da upo코tevamo **kontekst** znotraj stavka in vidimo, katere besede so medsebojno povezane. Na primer, omogo캜a nam, da vidimo, na katere besede se nana코ajo koreference, kot je *to*, in tudi upo코tevamo kontekst:

![](../../../../../translated_images/sl/CoreferenceResolution.861924d6d384a7d6.webp)

> Slika iz [Googlovega bloga](https://research.googleblog.com/2017/08/transformer-novel-neural-network.html)

V transformatorjih uporabljamo **ve캜glavo pozornost**, da damo mre쬴 mo캜 za zajemanje ve캜 razli캜nih vrst odvisnosti, npr. dolgoro캜nih proti kratkoro캜nim odnosom med besedami, koreferenc proti ne캜emu drugemu itd.

[TensorFlow Notebook](TransformersTF.ipynb) vsebuje ve캜 podrobnosti o implementaciji plasti transformatorjev.

### Pozornost enkoder-dekoder

V transformatorjih se pozornost uporablja na dveh mestih:

* Za zajemanje vzorcev znotraj vhodnega besedila z uporabo samopozornosti
* Za izvajanje prevajanja zaporedij - to je plast pozornosti med enkoderjem in dekoderjem.

Pozornost enkoder-dekoder je zelo podobna mehanizmu pozornosti, uporabljenemu v RNN-jih, kot je opisano na za캜etku tega poglavja. Ta animirani diagram pojasnjuje vlogo pozornosti enkoder-dekoder.

![Animirani GIF, ki prikazuje, kako se izvajajo ocene v modelih transformatorjev.](../../../../../lessons/5-NLP/18-Transformers/images/transformer-animated-explanation.gif)

Ker je vsak vhodni polo쬬j neodvisno preslikan na vsak izhodni polo쬬j, lahko transformatorji bolje paralelizirajo kot RNN-ji, kar omogo캜a veliko ve캜je in bolj izrazne jezikovne modele. Vsaka glava pozornosti se lahko uporablja za u캜enje razli캜nih odnosov med besedami, kar izbolj코a naloge obdelave naravnega jezika.

## BERT

**BERT** (Bidirectional Encoder Representations from Transformers) je zelo velika ve캜plastna mre쬬 transformatorjev z 12 plastmi za *BERT-base* in 24 za *BERT-large*. Model je najprej predhodno nau캜en na velikem korpusu besedilnih podatkov (WikiPedia + knjige) z uporabo nenadzorovanega u캜enja (napovedovanje zamaskiranih besed v stavku). Med predhodnim u캜enjem model absorbira pomembne ravni razumevanja jezika, ki jih je nato mogo캜e uporabiti z drugimi nabori podatkov z uporabo finega ugla코evanja. Ta proces se imenuje **prenosno u캜enje**.

![slika iz http://jalammar.github.io/illustrated-bert/](../../../../../translated_images/sl/jalammarBERT-language-modeling-masked-lm.34f113ea5fec4362.webp)

> Slika [vir](http://jalammar.github.io/illustrated-bert/)

## 九꽲잺 Vaje: Transformatorji

Nadaljujte z u캜enjem v naslednjih bele쬹icah:

* [Transformatorji v PyTorch](TransformersPyTorch.ipynb)
* [Transformatorji v TensorFlow](TransformersTF.ipynb)

## Zaklju캜ek

V tej lekciji ste se nau캜ili o transformatorjih in mehanizmih pozornosti, ki so klju캜na orodja v NLP. Obstaja veliko razli캜ic arhitektur transformatorjev, vklju캜no z BERT, DistilBERT, BigBird, OpenGPT3 in drugimi, ki jih je mogo캜e fino ugla코evati. Paket [HuggingFace](https://github.com/huggingface/) ponuja repozitorij za u캜enje mnogih teh arhitektur z uporabo PyTorch in TensorFlow.

## 游 Izziv

## [Kviz po predavanju](https://ff-quizzes.netlify.app/en/ai/quiz/36)

## Pregled in samostojno u캜enje

* [Blog objava](https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/), ki pojasnjuje klasi캜ni 캜lanek [Attention is all you need](https://arxiv.org/abs/1706.03762) o transformatorjih.
* [Serija blog objav](https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452) o transformatorjih, ki podrobno pojasnjuje arhitekturo.

## [Naloga](assignment.md)

---

