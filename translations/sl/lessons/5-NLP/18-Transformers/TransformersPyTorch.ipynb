{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mehanizmi pozornosti in transformatorji\n",
    "\n",
    "Ena glavnih pomanjkljivosti rekurentnih mrež je, da imajo vse besede v zaporedju enak vpliv na rezultat. To povzroča suboptimalno delovanje pri standardnih LSTM modelih kodirnik-dekodirnik za naloge, kot sta prepoznavanje imenovanih entitet in strojno prevajanje. V resnici imajo določene besede v vhodnem zaporedju pogosto večji vpliv na izhodne rezultate kot druge.\n",
    "\n",
    "Razmislimo o modelu zaporedje-v-zaporedje, kot je strojno prevajanje. Ta model je implementiran z dvema rekurentnima mrežama, kjer ena mreža (**kodirnik**) stisne vhodno zaporedje v skrito stanje, druga mreža (**dekodirnik**) pa to skrito stanje razširi v preveden rezultat. Težava pri tem pristopu je, da ima končno stanje mreže težave z zapomnitvijo začetka stavka, kar povzroča slabo kakovost modela pri dolgih stavkih.\n",
    "\n",
    "**Mehanizmi pozornosti** omogočajo tehtanje kontekstualnega vpliva vsakega vhodnega vektorja na vsako izhodno napoved RNN. To se implementira z ustvarjanjem bližnjic med vmesnimi stanji vhodne RNN in izhodne RNN. Na ta način pri generiranju izhodnega simbola $y_t$ upoštevamo vsa vhodna skrita stanja $h_i$ z različnimi utežnimi koeficienti $\\alpha_{t,i}$.\n",
    "\n",
    "![Slika, ki prikazuje model kodirnik/dekodirnik z aditivno plastjo pozornosti](../../../../../translated_images/sl/encoder-decoder-attention.7a726296894fb567.webp)\n",
    "*Model kodirnik-dekodirnik z mehanizmom aditivne pozornosti v [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf), povzeto iz [tega bloga](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)*\n",
    "\n",
    "Matrika pozornosti $\\{\\alpha_{i,j}\\}$ predstavlja stopnjo, do katere določene vhodne besede vplivajo na generacijo določene besede v izhodnem zaporedju. Spodaj je primer takšne matrike:\n",
    "\n",
    "![Slika, ki prikazuje vzorčno poravnavo, najdeno z RNNsearch-50, povzeto iz Bahdanau - arviz.org](../../../../../translated_images/sl/bahdanau-fig3.09ba2d37f202a6af.webp)\n",
    "\n",
    "*Slika povzeta iz [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) (Slika 3)*\n",
    "\n",
    "Mehanizmi pozornosti so odgovorni za velik del trenutnega ali skoraj trenutnega stanja umetnosti na področju obdelave naravnega jezika. Dodajanje pozornosti pa močno poveča število parametrov modela, kar je povzročilo težave pri skaliranju RNN. Ključna omejitev pri skaliranju RNN je, da rekurentna narava modelov otežuje združevanje in paralelizacijo učenja. V RNN je treba vsak element zaporedja obdelati v zaporednem vrstnem redu, kar pomeni, da ga ni mogoče enostavno paralelizirati.\n",
    "\n",
    "Sprejetje mehanizmov pozornosti v kombinaciji s to omejitvijo je privedlo do nastanka zdajšnjih transformatorjev, ki predstavljajo stanje umetnosti, kot so BERT, OpenGPT3 in drugi.\n",
    "\n",
    "## Transformatorji\n",
    "\n",
    "Namesto da bi kontekst vsake prejšnje napovedi posredovali v naslednji korak ocenjevanja, **transformatorji** uporabljajo **pozicijske kodiranja** in pozornost za zajemanje konteksta določenega vhoda znotraj določenega okna besedila. Spodnja slika prikazuje, kako pozicijska kodiranja s pozornostjo zajamejo kontekst znotraj določenega okna.\n",
    "\n",
    "![Animiran GIF, ki prikazuje, kako se izvajajo ocene v transformatorjih.](../../../../../lessons/5-NLP/18-Transformers/images/transformer-animated-explanation.gif)\n",
    "\n",
    "Ker je vsak vhodni položaj neodvisno preslikan na vsak izhodni položaj, lahko transformatorji bolje paralelizirajo kot RNN, kar omogoča veliko večje in izrazitejše jezikovne modele. Vsaka glava pozornosti se lahko uporablja za učenje različnih odnosov med besedami, kar izboljša naloge obdelave naravnega jezika.\n",
    "\n",
    "**BERT** (Bidirectional Encoder Representations from Transformers) je zelo velik večplastni transformator z 12 plastmi za *BERT-base* in 24 za *BERT-large*. Model je najprej predhodno usposobljen na velikem korpusu besedilnih podatkov (Wikipedia + knjige) z uporabo nenadzorovanega učenja (napovedovanje zamaskiranih besed v stavku). Med predhodnim učenjem model pridobi pomembno raven razumevanja jezika, ki jo je nato mogoče uporabiti z drugimi nabori podatkov z uporabo prilagoditve. Ta proces se imenuje **prenosno učenje**.\n",
    "\n",
    "![Slika s http://jalammar.github.io/illustrated-bert/](../../../../../translated_images/sl/jalammarBERT-language-modeling-masked-lm.34f113ea5fec4362.webp)\n",
    "\n",
    "Obstaja veliko različic arhitektur transformatorjev, vključno z BERT, DistilBERT, BigBird, OpenGPT3 in drugimi, ki jih je mogoče prilagoditi. Paket [HuggingFace](https://github.com/huggingface/) ponuja repozitorij za učenje mnogih teh arhitektur s PyTorch.\n",
    "\n",
    "## Uporaba BERT za klasifikacijo besedila\n",
    "\n",
    "Poglejmo, kako lahko uporabimo predhodno usposobljen model BERT za reševanje naše tradicionalne naloge: klasifikacije zaporedij. Klasificirali bomo naš izvirni nabor podatkov AG News.\n",
    "\n",
    "Najprej naložimo knjižnico HuggingFace in naš nabor podatkov:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Building vocab...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "from torchnlp import *\n",
    "import transformers\n",
    "train_dataset, test_dataset, classes, vocab = load_dataset()\n",
    "vocab_len = len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ker bomo uporabljali vnaprej naučen BERT model, bomo morali uporabiti določen tokenizer. Najprej bomo naložili tokenizer, povezan z vnaprej naučenim BERT modelom.\n",
    "\n",
    "Knjižnica HuggingFace vsebuje repozitorij vnaprej naučenih modelov, ki jih lahko uporabite tako, da njihove imena podate kot argumente funkcijam `from_pretrained`. Vsi potrebni binarni datoteki za model bodo samodejno preneseni.\n",
    "\n",
    "V določenih primerih pa boste morali naložiti svoje modele. V tem primeru lahko določite imenik, ki vsebuje vse ustrezne datoteke, vključno s parametri za tokenizer, datoteko `config.json` s parametri modela, binarnimi utežmi itd.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load the model from Internet repository using model name. \n",
    "# Use this if you are running from your own copy of the notebooks\n",
    "bert_model = 'bert-base-uncased' \n",
    "\n",
    "# To load the model from the directory on disk. Use this for Microsoft Learn module, because we have\n",
    "# prepared all required files for you.\n",
    "bert_model = './bert'\n",
    "\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(bert_model)\n",
    "\n",
    "MAX_SEQ_LEN = 128\n",
    "PAD_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "UNK_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tokenizer` objekt vsebuje funkcijo `encode`, ki jo je mogoče neposredno uporabiti za kodiranje besedila:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 1052, 22123, 2953, 2818, 2003, 1037, 2307, 7705, 2005, 17953, 2361, 102]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('PyTorch is a great framework for NLP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Potem ustvarimo iteratorje, ki jih bomo uporabljali med treningom za dostop do podatkov. Ker BERT uporablja svojo lastno funkcijo kodiranja, bi morali definirati funkcijo za dodajanje presledkov, podobno kot `padify`, ki smo jo definirali prej:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_bert(b):\n",
    "    # b is the list of tuples of length batch_size\n",
    "    #   - first element of a tuple = label, \n",
    "    #   - second = feature (text sequence)\n",
    "    # build vectorized sequence\n",
    "    v = [tokenizer.encode(x[1]) for x in b]\n",
    "    # compute max length of a sequence in this minibatch\n",
    "    l = max(map(len,v))\n",
    "    return ( # tuple of two tensors - labels and features\n",
    "        torch.LongTensor([t[0] for t in b]),\n",
    "        torch.stack([torch.nn.functional.pad(torch.tensor(t),(0,l-len(t)),mode='constant',value=0) for t in v])\n",
    "    )\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=8, collate_fn=pad_bert, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=8, collate_fn=pad_bert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "V našem primeru bomo uporabili vnaprej naučen BERT model, imenovan `bert-base-uncased`. Naložimo model z uporabo paketa `BertForSequenceClassification`. To zagotavlja, da ima naš model že zahtevano arhitekturo za klasifikacijo, vključno s končnim klasifikatorjem. Prikazalo se bo opozorilno sporočilo, ki navaja, da uteži končnega klasifikatorja niso inicializirane in da model potrebuje predhodno učenje - to je povsem v redu, saj je točno to tisto, kar bomo storili!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./bert were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./bert and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = transformers.BertForSequenceClassification.from_pretrained(bert_model,num_labels=4).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zdaj smo pripravljeni na začetek učenja! Ker je BERT že predhodno naučen, želimo začeti z razmeroma majhno hitrostjo učenja, da ne bi uničili začetnih uteži.\n",
    "\n",
    "Vso zahtevno delo opravi model `BertForSequenceClassification`. Ko model pokličemo na učnih podatkih, vrne tako izgubo kot izhod omrežja za vhodni minibatch. Izgubo uporabimo za optimizacijo parametrov (`loss.backward()` izvede povratno propagacijo), medtem ko `out` uporabimo za izračun učne natančnosti, tako da primerjamo dobljene oznake `labs` (izračunane z uporabo `argmax`) s pričakovanimi `labels`.\n",
    "\n",
    "Za nadzor procesa akumuliramo izgubo in natančnost skozi več iteracij ter ju izpišemo vsakih `report_freq` učnih ciklov.\n",
    "\n",
    "To učenje bo verjetno trajalo kar nekaj časa, zato omejimo število iteracij.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 1.1254194641113282, Accuracy = 0.585\n",
      "Loss = 0.6194715118408203, Accuracy = 0.83\n",
      "Loss = 0.46665248870849607, Accuracy = 0.8475\n",
      "Loss = 0.4309701919555664, Accuracy = 0.8575\n",
      "Loss = 0.35427074432373046, Accuracy = 0.8825\n",
      "Loss = 0.3306886291503906, Accuracy = 0.8975\n",
      "Loss = 0.30340143203735354, Accuracy = 0.8975\n",
      "Loss = 0.26139299392700194, Accuracy = 0.915\n",
      "Loss = 0.26708646774291994, Accuracy = 0.9225\n",
      "Loss = 0.3667240524291992, Accuracy = 0.8675\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
    "\n",
    "report_freq = 50\n",
    "iterations = 500 # make this larger to train for longer time!\n",
    "\n",
    "model.train()\n",
    "\n",
    "i,c = 0,0\n",
    "acc_loss = 0\n",
    "acc_acc = 0\n",
    "\n",
    "for labels,texts in train_loader:\n",
    "    labels = labels.to(device)-1 # get labels in the range 0-3         \n",
    "    texts = texts.to(device)\n",
    "    loss, out = model(texts, labels=labels)[:2]\n",
    "    labs = out.argmax(dim=1)\n",
    "    acc = torch.mean((labs==labels).type(torch.float32))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    acc_loss += loss\n",
    "    acc_acc += acc\n",
    "    i+=1\n",
    "    c+=1\n",
    "    if i%report_freq==0:\n",
    "        print(f\"Loss = {acc_loss.item()/c}, Accuracy = {acc_acc.item()/c}\")\n",
    "        c = 0\n",
    "        acc_loss = 0\n",
    "        acc_acc = 0\n",
    "    iterations-=1\n",
    "    if not iterations:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vidite lahko (še posebej, če povečate število iteracij in počakate dovolj dolgo), da nam klasifikacija z BERT daje precej dobro natančnost! To je zato, ker BERT že zelo dobro razume strukturo jezika, mi pa moramo le prilagoditi končni klasifikator. Vendar pa, ker je BERT velik model, celoten proces učenja traja dolgo in zahteva veliko računske moči! (GPU, in po možnosti več kot enega).\n",
    "\n",
    "> **Opomba:** V našem primeru uporabljamo enega najmanjših vnaprej naučenih BERT modelov. Obstajajo večji modeli, ki bodo verjetno dali boljše rezultate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ocena zmogljivosti modela\n",
    "\n",
    "Zdaj lahko ocenimo zmogljivost našega modela na testnem naboru podatkov. Zanka za ocenjevanje je precej podobna zanki za učenje, vendar ne smemo pozabiti preklopiti modela v način ocenjevanja z uporabo `model.eval()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy: 0.9047029702970297\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "iterations = 100\n",
    "acc = 0\n",
    "i = 0\n",
    "for labels,texts in test_loader:\n",
    "    labels = labels.to(device)-1      \n",
    "    texts = texts.to(device)\n",
    "    _, out = model(texts, labels=labels)[:2]\n",
    "    labs = out.argmax(dim=1)\n",
    "    acc += torch.mean((labs==labels).type(torch.float32))\n",
    "    i+=1\n",
    "    if i>iterations: break\n",
    "        \n",
    "print(f\"Final accuracy: {acc.item()/i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ključne točke\n",
    "\n",
    "V tej enoti smo videli, kako enostavno je vzeti vnaprej naučen jezikovni model iz knjižnice **transformers** in ga prilagoditi naši nalogi razvrščanja besedil. Podobno lahko modele BERT uporabimo za ekstrakcijo entitet, odgovarjanje na vprašanja in druge naloge obdelave naravnega jezika.\n",
    "\n",
    "Transformers modeli predstavljajo trenutno najsodobnejši pristop v obdelavi naravnega jezika (NLP), in v večini primerov bi morali biti prva rešitev, s katero začnete eksperimentirati pri implementaciji prilagojenih NLP rešitev. Vendar pa je razumevanje osnovnih principov rekurentnih nevronskih mrež, o katerih smo govorili v tem modulu, izjemno pomembno, če želite graditi napredne nevronske modele.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Omejitev odgovornosti**:  \nTa dokument je bil preveden z uporabo storitve za strojno prevajanje [Co-op Translator](https://github.com/Azure/co-op-translator). Čeprav si prizadevamo za natančnost, vas prosimo, da se zavedate, da lahko avtomatizirani prevodi vsebujejo napake ali netočnosti. Izvirni dokument v njegovem izvirnem jeziku je treba obravnavati kot avtoritativni vir. Za ključne informacije priporočamo strokovno človeško prevajanje. Ne prevzemamo odgovornosti za morebitna nesporazumevanja ali napačne razlage, ki izhajajo iz uporabe tega prevoda.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37_pytorch",
   "language": "python",
   "name": "conda-env-py37_pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "coopTranslator": {
   "original_hash": "753865967678a92dbce7d7efbd36d980",
   "translation_date": "2025-08-30T08:04:27+00:00",
   "source_file": "lessons/5-NLP/18-Transformers/TransformersPyTorch.ipynb",
   "language_code": "sl"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}