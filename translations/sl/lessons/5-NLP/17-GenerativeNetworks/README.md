# Generativne mreÅ¾e

## [Predhodni kviz](https://ff-quizzes.netlify.app/en/ai/quiz/33)

Rekurentne nevronske mreÅ¾e (RNN) in njihove razliÄice z zapornimi celicami, kot so celice dolgega kratkoroÄnega spomina (LSTM) in zaporne rekurentne enote (GRU), omogoÄajo modeliranje jezika, saj lahko uÄijo zaporedje besed in napovedujejo naslednjo besedo v zaporedju. To nam omogoÄa uporabo RNN za **generativne naloge**, kot so obiÄajno generiranje besedila, strojno prevajanje in celo opisovanje slik.

> âœ… Pomislite na vse primere, ko ste imeli koristi od generativnih nalog, kot je dopolnjevanje besedila med tipkanjem. RaziÅ¡Äite svoje najljubÅ¡e aplikacije in preverite, ali uporabljajo RNN.

V arhitekturi RNN, ki smo jo obravnavali v prejÅ¡nji enoti, je vsaka enota RNN proizvedla naslednje skrito stanje kot izhod. Vendar pa lahko dodamo Å¡e en izhod vsaki rekurentni enoti, kar nam omogoÄa, da ustvarimo **zaporedje** (ki je enako dolÅ¾ini izvirnega zaporedja). Poleg tega lahko uporabimo RNN enote, ki ne sprejemajo vhodnih podatkov na vsakem koraku, temveÄ le zaÄetni vektorski stanje, nato pa proizvedejo zaporedje izhodov.

To omogoÄa razliÄne nevronske arhitekture, prikazane na spodnji sliki:

![Slika, ki prikazuje pogoste vzorce rekurentnih nevronskih mreÅ¾.](../../../../../translated_images/sl/unreasonable-effectiveness-of-rnn.541ead816778f42d.webp)

> Slika iz blog objave [Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) avtorja [Andrej Karpaty](http://karpathy.github.io/)

* **Ena-na-ena** je tradicionalna nevronska mreÅ¾a z enim vhodom in enim izhodom
* **Ena-na-veÄ** je generativna arhitektura, ki sprejme eno vhodno vrednost in ustvari zaporedje izhodnih vrednosti. Na primer, Äe Å¾elimo trenirati mreÅ¾o za **opisovanje slik**, ki bi ustvarila tekstovni opis slike, lahko sliko uporabimo kot vhod, jo prenesemo skozi CNN za pridobitev skritega stanja, nato pa rekurentna veriga generira opis besedo za besedo
* **VeÄ-na-ena** ustreza arhitekturi RNN, ki smo jo opisali v prejÅ¡nji enoti, kot je klasifikacija besedila
* **VeÄ-na-veÄ**, ali **zaporedje-na-zaporedje**, ustreza nalogam, kot je **strojno prevajanje**, kjer prva RNN zbere vse informacije iz vhodnega zaporedja v skrito stanje, druga RNN veriga pa to stanje razÅ¡iri v izhodno zaporedje.

V tej enoti se bomo osredotoÄili na preproste generativne modele, ki nam pomagajo generirati besedilo. Zaradi enostavnosti bomo uporabili tokenizacijo na ravni znakov.

To RNN bomo trenirali za generiranje besedila korak za korakom. Na vsakem koraku bomo vzeli zaporedje znakov dolÅ¾ine `nchars` in mreÅ¾i naroÄili, naj za vsak vhodni znak ustvari naslednji izhodni znak:

![Slika, ki prikazuje primer generiranja besede 'HELLO' z RNN.](../../../../../translated_images/sl/rnn-generate.56c54afb52f9781d.webp)

Pri generiranju besedila (med inferenco) zaÄnemo z nekim **pozivom**, ki ga prenesemo skozi RNN celice za generiranje vmesnega stanja, nato pa se zaÄne generiranje. Generiramo en znak naenkrat, stanje in generirani znak pa prenesemo v drugo RNN celico za generiranje naslednjega, dokler ne generiramo dovolj znakov.

<img src="../../../../../translated_images/sl/rnn-generate-inf.5168dc65e0370eea.webp" width="60%"/>

> Slika avtorja

## âœï¸ Vaje: Generativne mreÅ¾e

Nadaljujte z uÄenjem v naslednjih zvezkih:

* [Generativne mreÅ¾e s PyTorch](GenerativePyTorch.ipynb)
* [Generativne mreÅ¾e s TensorFlow](GenerativeTF.ipynb)

## Mehko generiranje besedila in temperatura

Izhod vsake RNN celice je porazdelitev verjetnosti znakov. ÄŒe vedno izberemo znak z najviÅ¡jo verjetnostjo kot naslednji znak v generiranem besedilu, se besedilo pogosto lahko "zacikli" med istimi zaporedji znakov znova in znova, kot v tem primeru:

```
today of the second the company and a second the company ...
```

Vendar pa, Äe pogledamo porazdelitev verjetnosti za naslednji znak, je lahko razlika med nekaj najviÅ¡jimi verjetnostmi majhna, npr. en znak ima verjetnost 0.2, drugi pa 0.19 itd. Na primer, pri iskanju naslednjega znaka v zaporedju '*play*' je lahko naslednji znak enako verjetno presledek ali **e** (kot v besedi *player*).

To nas pripelje do zakljuÄka, da ni vedno "poÅ¡teno" izbrati znaka z viÅ¡jo verjetnostjo, saj lahko izbira drugega najviÅ¡jega Å¡e vedno vodi do smiselnega besedila. Bolj smiselno je **vzorec** znakov vzeti iz porazdelitve verjetnosti, ki jo poda izhod mreÅ¾e. Uporabimo lahko tudi parameter, **temperaturo**, ki bo zgladila porazdelitev verjetnosti, Äe Å¾elimo dodati veÄ nakljuÄnosti, ali jo naredila bolj strmo, Äe Å¾elimo bolj slediti znakom z najviÅ¡jo verjetnostjo.

Raziskujte, kako je to mehko generiranje besedila implementirano v zgoraj povezanih zvezkih.

## ZakljuÄek

ÄŒeprav je generiranje besedila lahko koristno samo po sebi, so glavne prednosti v sposobnosti generiranja besedila z uporabo RNN iz nekega zaÄetnega vektorskega stanja. Na primer, generiranje besedila se uporablja kot del strojnega prevajanja (zaporedje-na-zaporedje, v tem primeru se stanje iz *kodirnika* uporablja za generiranje ali *dekodiranje* prevedenega sporoÄila) ali generiranje tekstovnega opisa slike (v tem primeru bi vektorsko stanje priÅ¡lo iz CNN ekstraktorja).

## ğŸš€ Izziv

Opravite nekaj lekcij na Microsoft Learn na to temo

* Generiranje besedila s [PyTorch](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-pytorch/6-generative-networks/?WT.mc_id=academic-77998-cacaste)/[TensorFlow](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-tensorflow/5-generative-networks/?WT.mc_id=academic-77998-cacaste)

## [Naknadni kviz](https://ff-quizzes.netlify.app/en/ai/quiz/34)

## Pregled in samostojno uÄenje

Tukaj je nekaj Älankov za razÅ¡iritev vaÅ¡ega znanja

* RazliÄni pristopi k generiranju besedila z Markov Chain, LSTM in GPT-2: [blog objava](https://towardsdatascience.com/text-generation-gpt-2-lstm-markov-chain-9ea371820e1e)
* Primer generiranja besedila v [Keras dokumentaciji](https://keras.io/examples/generative/lstm_character_level_text_generation/)

## [Naloga](lab/README.md)

Videli smo, kako generirati besedilo znak za znakom. V laboratoriju boste raziskovali generiranje besedila na ravni besed.

---

