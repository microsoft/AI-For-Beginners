# Predstavljanje besedila kot tenzorjev

## [Predhodni kviz](https://ff-quizzes.netlify.app/en/ai/quiz/25)

## Razvr코캜anje besedila

V prvem delu tega poglavja se bomo osredoto캜ili na nalogo **razvr코캜anja besedila**. Uporabili bomo [AG News](https://www.kaggle.com/amananandrai/ag-news-classification-dataset) podatkovni niz, ki vsebuje novice, kot je naslednja:

* Kategorija: Znanost/Tehnologija
* Naslov: Ky. podjetje prejme subvencijo za raziskovanje peptidov (AP)
* Telo: AP - Podjetje, ki ga je ustanovil raziskovalec kemije na Univerzi v Louisvilleu, je prejelo subvencijo za razvoj...

Na코 cilj bo razvrstiti novico v eno od kategorij na podlagi besedila.

## Predstavljanje besedila

캛e 쬰limo re코evati naloge obdelave naravnega jezika (NLP) z nevronskimi mre쬬mi, potrebujemo na캜in za predstavljanje besedila kot tenzorjev. Ra캜unalniki 쬰 predstavljajo besedilne znake kot 코tevilke, ki se preslikajo v pisave na va코em zaslonu z uporabo kodiranj, kot sta ASCII ali UTF-8.

<img alt="Slika prikazuje diagram, ki preslika znak v ASCII in binarno predstavitev" src="../../../../../translated_images/sl/ascii-character-map.18ed6aa7f3b0a7ff.webp" width="50%"/>

> [Vir slike](https://www.seobility.net/en/wiki/ASCII)

Kot ljudje razumemo, kaj vsak posamezen znak **predstavlja** in kako se vsi znaki zdru쬴jo v besede stavka. Ra캜unalniki pa sami po sebi nimajo tak코nega razumevanja, zato mora nevronska mre쬬 pomen nau캜iti med treningom.

Zato lahko uporabimo razli캜ne pristope pri predstavljanju besedila:

* **Predstavitev na ravni znakov**, kjer besedilo predstavljamo tako, da vsak znak obravnavamo kot 코tevilko. 캛e imamo *C* razli캜nih znakov v na코em korpusu besedila, bi bila beseda *Hello* predstavljena kot 5x*C* tenzor. Vsaka 캜rka bi ustrezala stolpcu tenzorja v enovrsti캜ni kodiranju.
* **Predstavitev na ravni besed**, kjer ustvarimo **besednjak** vseh besed v na코em besedilu in nato besede predstavimo z enovrsti캜nim kodiranjem. Ta pristop je nekoliko bolj코i, saj posamezna 캜rka sama po sebi nima velikega pomena, zato z uporabo vi코jih semanti캜nih konceptov - besed - poenostavimo nalogo za nevronsko mre쬺. Vendar pa moramo zaradi velikega slovarja obravnavati visoko-dimenzionalne redke tenzorje.

Ne glede na predstavitev moramo najprej pretvoriti besedilo v zaporedje **tokenov**, pri 캜emer je en token lahko znak, beseda ali v캜asih celo del besede. Nato token pretvorimo v 코tevilko, obi캜ajno z uporabo **besednjaka**, in to 코tevilko lahko vnesemo v nevronsko mre쬺 z enovrsti캜nim kodiranjem.

## N-Grami

V naravnem jeziku je natan캜en pomen besed mogo캜e dolo캜iti le v kontekstu. Na primer, pomena *nevronska mre쬬* in *ribi코ka mre쬬* sta popolnoma razli캜na. Eden od na캜inov, kako to upo코tevati, je, da na코 model zgradimo na parih besed in obravnavamo pare besed kot lo캜ene token-e besednjaka. Na ta na캜in bo stavek *Rad grem na ribolov* predstavljen z naslednjim zaporedjem tokenov: *Rad grem*, *grem na*, *na ribolov*. Te쬬va pri tem pristopu je, da se velikost slovarja znatno pove캜a, kombinacije, kot sta *na ribolov* in *na nakupovanje*, pa so predstavljene z razli캜nimi tokeni, ki ne delijo nobene semanti캜ne podobnosti kljub istemu glagolu.

V nekaterih primerih lahko razmislimo o uporabi tri-gramov -- kombinacij treh besed -- prav tako. Zato se tak코en pristop pogosto imenuje **n-grami**. Smiselno je tudi uporabljati n-grame pri predstavitvi na ravni znakov, pri 캜emer n-grame pribli쬹o ustrezajo razli캜nim zlogom.

## Vre캜a besed in TF/IDF

Pri re코evanju nalog, kot je razvr코캜anje besedila, moramo biti sposobni predstaviti besedilo z enim vektorjem fiksne velikosti, ki ga bomo uporabili kot vhod za kon캜ni gosti klasifikator. Eden najpreprostej코ih na캜inov za to je zdru쬴ti vse posamezne predstavitve besed, npr. z njihovim se코tevanjem. 캛e se코tejemo enovrsti캜ne kodiranja vsake besede, bomo dobili vektor frekvenc, ki prikazuje, kolikokrat se vsaka beseda pojavi v besedilu. Tak코na predstavitev besedila se imenuje **vre캜a besed** (BoW).

<img src="../../../../../translated_images/sl/bow.3811869cff59368d.webp" width="90%"/>

> Slika avtorja

BoW v bistvu predstavlja, katere besede se pojavijo v besedilu in v kak코nih koli캜inah, kar je lahko dober pokazatelj, o 캜em govori besedilo. Na primer, novica o politiki bo verjetno vsebovala besede, kot sta *predsednik* in *dr쬬va*, medtem ko bo znanstvena publikacija imela nekaj, kot sta *trkalnik*, *odkritje* itd. Tako so frekvence besed v mnogih primerih lahko dober pokazatelj vsebine besedila.

Te쬬va pri BoW je, da se dolo캜ene pogoste besede, kot sta *in*, *je* itd., pojavljajo v ve캜ini besedil in imajo najvi코je frekvence, kar zakrije besede, ki so resni캜no pomembne. Pomembnost teh besed lahko zmanj코amo tako, da upo코tevamo frekvenco, s katero se besede pojavljajo v celotni zbirki dokumentov. To je glavna ideja pristopa TF/IDF, ki je podrobneje obravnavan v prilo쬰nih zvezkih k tej lekciji.

Vendar pa noben od teh pristopov ne more v celoti upo코tevati **semantike** besedila. Za to potrebujemo mo캜nej코e modele nevronskih mre, o katerih bomo razpravljali kasneje v tem poglavju.

## 九꽲잺 Vaje: Predstavitev besedila

Nadaljujte z u캜enjem v naslednjih zvezkih:

* [Predstavitev besedila s PyTorch](TextRepresentationPyTorch.ipynb)
* [Predstavitev besedila s TensorFlow](TextRepresentationTF.ipynb)

## Zaklju캜ek

Doslej smo preu캜ili tehnike, ki lahko dodajo te쬺 frekvencam razli캜nih besed. Vendar pa te tehnike ne morejo predstaviti pomena ali vrstnega reda. Kot je slavni lingvist J. R. Firth dejal leta 1935: "Popolni pomen besede je vedno kontekstualen, in nobena 코tudija pomena brez konteksta ne more biti resna." Kasneje v te캜aju se bomo nau캜ili, kako zajeti kontekstualne informacije iz besedila z uporabo jezikovnega modeliranja.

## 游 Izziv

Preizkusite nekaj drugih vaj z uporabo vre캜e besed in razli캜nih podatkovnih modelov. Navdih lahko najdete v tem [tekmovanju na Kaggle](https://www.kaggle.com/competitions/word2vec-nlp-tutorial/overview/part-1-for-beginners-bag-of-words)

## [Kviz po predavanju](https://ff-quizzes.netlify.app/en/ai/quiz/26)

## Pregled in samostojno u캜enje

Vadite svoje ve코캜ine z vgrajevanjem besedila in tehnikami vre캜e besed na [Microsoft Learn](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-pytorch/?WT.mc_id=academic-77998-cacaste)

## [Naloga: Zvezki](assignment.md)

---

