# Rekurentne nevronske mreÅ¾e

## [Predhodni kviz](https://ff-quizzes.netlify.app/en/ai/quiz/31)

V prejÅ¡njih poglavjih smo uporabljali bogate semantiÄne reprezentacije besedila in preprost linearni klasifikator na vrhu vgrajenih predstavitev. Ta arhitektura zajame zdruÅ¾eni pomen besed v stavku, vendar ne upoÅ¡teva **vrstnega reda** besed, saj operacija zdruÅ¾evanja na vrhu vgrajenih predstavitev odstrani to informacijo iz izvirnega besedila. Ker ti modeli ne morejo modelirati vrstnega reda besed, ne morejo reÅ¡iti bolj zapletenih ali dvoumnih nalog, kot so generiranje besedila ali odgovarjanje na vpraÅ¡anja.

Da bi zajeli pomen zaporedja besedila, moramo uporabiti drugo arhitekturo nevronske mreÅ¾e, imenovano **rekurentna nevronska mreÅ¾a** ali RNN. Pri RNN stavke poÅ¡iljamo skozi mreÅ¾o en simbol naenkrat, mreÅ¾a pa ustvari neko **stanje**, ki ga nato ponovno poÅ¡ljemo v mreÅ¾o skupaj z naslednjim simbolom.

![RNN](../../../../../translated_images/sl/rnn.27f5c29c53d727b5.webp)

> Slika avtorja

Glede na vhodno zaporedje simbolov X<sub>0</sub>,...,X<sub>n</sub>, RNN ustvari zaporedje blokov nevronske mreÅ¾e in to zaporedje trenira od zaÄetka do konca z uporabo povratnega razÅ¡irjanja napake. Vsak blok mreÅ¾e sprejme par (X<sub>i</sub>,S<sub>i</sub>) kot vhod in ustvari S<sub>i+1</sub> kot rezultat. KonÄno stanje S<sub>n</sub> ali (izhod Y<sub>n</sub>) gre v linearni klasifikator, da ustvari rezultat. Vsi bloki mreÅ¾e delijo iste uteÅ¾i in se trenirajo od zaÄetka do konca z enim prehodom povratnega razÅ¡irjanja napake.

Ker se vektorska stanja S<sub>0</sub>,...,S<sub>n</sub> prenaÅ¡ajo skozi mreÅ¾o, lahko ta model uÄi zaporedne odvisnosti med besedami. Na primer, ko se beseda *ne* pojavi nekje v zaporedju, se lahko nauÄi negirati doloÄene elemente znotraj vektorskega stanja, kar vodi do negacije.

> âœ… Ker so uteÅ¾i vseh blokov RNN na zgornji sliki enake, lahko isto sliko predstavimo kot en blok (na desni) z rekurenÄno povratno zanko, ki prenaÅ¡a izhodno stanje mreÅ¾e nazaj na vhod.

## Anatomija RNN celice

Poglejmo, kako je organizirana preprosta RNN celica. Sprejme prejÅ¡nje stanje S<sub>i-1</sub> in trenutni simbol X<sub>i</sub> kot vhoda ter mora ustvariti izhodno stanje S<sub>i</sub> (vÄasih nas zanima tudi kakÅ¡en drug izhod Y<sub>i</sub>, kot v primeru generativnih mreÅ¾).

Preprosta RNN celica ima znotraj dve matriki uteÅ¾i: ena transformira vhodni simbol (imenujmo jo W), druga pa transformira vhodno stanje (H). V tem primeru se izhod mreÅ¾e izraÄuna kot &sigma;(W&times;X<sub>i</sub>+H&times;S<sub>i-1</sub>+b), kjer je &sigma; aktivacijska funkcija, b pa dodatna pristranskost.

<img alt="Anatomija RNN celice" src="../../../../../translated_images/sl/rnn-anatomy.79ee3f3920b3294b.webp" width="50%"/>

> Slika avtorja

V mnogih primerih se vhodni simboli poÅ¡ljejo skozi plast vgrajenih predstavitev, preden vstopijo v RNN, da se zmanjÅ¡a dimenzionalnost. V tem primeru, Äe je dimenzija vhodnih vektorjev *emb_size*, dimenzija vektorskega stanja pa *hid_size*, je velikost W *emb_size*&times;*hid_size*, velikost H pa *hid_size*&times;*hid_size*.

## DolgoroÄni kratkoroÄni spomin (LSTM)

Ena glavnih teÅ¾av klasiÄnih RNN je tako imenovana teÅ¾ava **izginjajoÄih gradientov**. Ker se RNN trenirajo od zaÄetka do konca z enim prehodom povratnega razÅ¡irjanja napake, imajo teÅ¾ave pri prenaÅ¡anju napake do prvih slojev mreÅ¾e, zato mreÅ¾a ne more uÄiti odnosov med oddaljenimi simboli. Eden od naÄinov za izogibanje tej teÅ¾avi je uvedba **eksplicitnega upravljanja stanja** z uporabo tako imenovanih **vrat**. Obstajata dve dobro znani arhitekturi te vrste: **DolgoroÄni kratkoroÄni spomin** (LSTM) in **Enota z vrati za prenos** (GRU).

![Slika, ki prikazuje primer celice dolgoroÄnega kratkoroÄnega spomina](../../../../../lessons/5-NLP/16-RNN/images/long-short-term-memory-cell.svg)

> Vir slike TBD

LSTM mreÅ¾a je organizirana podobno kot RNN, vendar se iz sloja v sloj prenaÅ¡ata dve stanji: dejansko stanje C in skriti vektor H. Pri vsaki enoti se skriti vektor H<sub>i</sub> zdruÅ¾i z vhodom X<sub>i</sub>, in skupaj nadzorujeta, kaj se zgodi s stanjem C prek **vrat**. Vsaka vrata so nevronska mreÅ¾a s sigmoidno aktivacijo (izhod v obmoÄju [0,1]), ki jih lahko obravnavamo kot bitno masko, ko jih pomnoÅ¾imo z vektorskim stanjem. Na sliki zgoraj so naslednja vrata (od leve proti desni):

* **Vrata za pozabo** sprejmejo skriti vektor in doloÄijo, katere komponente vektorja C moramo pozabiti in katere prenesti naprej.
* **Vrata za vnos** vzamejo nekaj informacij iz vhodnih in skritih vektorjev ter jih vstavijo v stanje.
* **Vrata za izhod** transformirajo stanje prek linearne plasti z aktivacijo *tanh*, nato pa izberejo nekatere njegove komponente z uporabo skritega vektorja H<sub>i</sub>, da ustvarijo novo stanje C<sub>i+1</sub>.

Komponente stanja C lahko obravnavamo kot neke vrste zastavice, ki jih lahko vklopimo ali izklopimo. Na primer, ko v zaporedju naletimo na ime *Alice*, lahko domnevamo, da gre za Å¾enski lik, in dvignemo zastavico v stanju, da imamo v stavku Å¾enski samostalnik. Ko kasneje naletimo na frazo *in Tom*, dvignemo zastavico, da imamo mnoÅ¾inski samostalnik. Tako lahko z manipulacijo stanja sledimo slovniÄnim lastnostim delov stavka.

> âœ… OdliÄen vir za razumevanje notranjosti LSTM je ta odliÄen Älanek [Razumevanje LSTM mreÅ¾](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) avtorja Christopherja Olaha.

## Dvosmerne in veÄslojne RNN

Razpravljali smo o rekurentnih mreÅ¾ah, ki delujejo v eno smer, od zaÄetka zaporedja do konca. To se zdi naravno, saj spominja na naÄin, kako beremo in posluÅ¡amo govor. Vendar pa, ker imamo v mnogih praktiÄnih primerih nakljuÄni dostop do vhodnega zaporedja, bi bilo smiselno izvajati rekurenÄno raÄunanje v obeh smereh. TakÅ¡ne mreÅ¾e imenujemo **dvosmerne** RNN. Pri delu z dvosmerno mreÅ¾o bi potrebovali dva skrita vektorska stanja, po enega za vsako smer.

Rekurentna mreÅ¾a, bodisi enosmerna ali dvosmerna, zajame doloÄene vzorce znotraj zaporedja in jih lahko shrani v vektorsko stanje ali prenese v izhod. Tako kot pri konvolucijskih mreÅ¾ah lahko na prvo plast zgradimo drugo rekurentno plast, da zajamemo vzorce viÅ¡je ravni in gradimo na nizkoroÄnih vzorcih, ki jih je zajela prva plast. To nas pripelje do pojma **veÄslojne RNN**, ki je sestavljena iz dveh ali veÄ rekurentnih mreÅ¾, kjer se izhod prejÅ¡nje plasti prenese v naslednjo plast kot vhod.

![Slika, ki prikazuje veÄslojno LSTM RNN](../../../../../translated_images/sl/multi-layer-lstm.dd975e29bb2a59fe.webp)

*Slika iz [tega Äudovitega prispevka](https://towardsdatascience.com/from-a-lstm-cell-to-a-multilayer-lstm-network-with-pytorch-2899eb5696f3) avtorja Fernanda LÃ³peza*

## âœï¸ Vaje: Vgrajene predstavitve

Nadaljujte z uÄenjem v naslednjih zvezkih:

* [RNN z uporabo PyTorch](RNNPyTorch.ipynb)
* [RNN z uporabo TensorFlow](RNNTF.ipynb)

## ZakljuÄek

V tej enoti smo videli, da lahko RNN uporabimo za klasifikacijo zaporedij, vendar lahko dejansko obravnavajo Å¡e veliko veÄ nalog, kot so generiranje besedila, strojno prevajanje in drugo. Te naloge bomo obravnavali v naslednji enoti.

## ğŸš€ Izziv

Preberite nekaj literature o LSTM in razmislite o njihovih aplikacijah:

- [Grid Long Short-Term Memory](https://arxiv.org/pdf/1507.01526v1.pdf)
- [Show, Attend and Tell: Neural Image Caption
Generation with Visual Attention](https://arxiv.org/pdf/1502.03044v2.pdf)

## [Kviz po predavanju](https://ff-quizzes.netlify.app/en/ai/quiz/32)

## Pregled in samostojno uÄenje

- [Razumevanje LSTM mreÅ¾](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) avtorja Christopherja Olaha.

## [Naloga: Zvezki](assignment.md)

---

