<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "58bf4adb210aab53e8f78c8082040e7c",
  "translation_date": "2025-08-25T21:35:06+00:00",
  "source_file": "lessons/5-NLP/16-RNN/README.md",
  "language_code": "sl"
}
-->
# Rekurentne nevronske mreÅ¾e

## [Predhodni kviz](https://ff-quizzes.netlify.app/en/ai/quiz/31)

V prejÅ¡njih poglavjih smo uporabljali bogate semantiÄne reprezentacije besedila in preprost linearni klasifikator na vrhu vgrajenih predstavitev. Ta arhitektura zajame zdruÅ¾eni pomen besed v stavku, vendar ne upoÅ¡teva **zaporedja** besed, saj operacija zdruÅ¾evanja na vrhu vgrajenih predstavitev odstrani te informacije iz izvirnega besedila. Ker ti modeli ne morejo modelirati vrstnega reda besed, ne morejo reÅ¡evati bolj zapletenih ali dvoumnih nalog, kot so generiranje besedila ali odgovarjanje na vpraÅ¡anja.

Za zajemanje pomena zaporedja besedila moramo uporabiti drugo arhitekturo nevronske mreÅ¾e, imenovano **rekurentna nevronska mreÅ¾a** ali RNN. Pri RNN stavek poÅ¡iljamo skozi mreÅ¾o en simbol naenkrat, mreÅ¾a pa ustvari neko **stanje**, ki ga nato skupaj z naslednjim simbolom ponovno poÅ¡ljemo v mreÅ¾o.

![RNN](../../../../../translated_images/rnn.27f5c29c53d727b546ad3961637a267f0fe9ec5ab01f2a26a853c92fcefbb574.sl.png)

> Slika avtorja

Glede na vhodno zaporedje simbolov X<sub>0</sub>,...,X<sub>n</sub>, RNN ustvari zaporedje blokov nevronske mreÅ¾e in to zaporedje trenira od zaÄetka do konca z uporabo povratnega razÅ¡irjanja napake. Vsak blok mreÅ¾e sprejme par (X<sub>i</sub>,S<sub>i</sub>) kot vhod in ustvari S<sub>i+1</sub> kot rezultat. KonÄno stanje S<sub>n</sub> (ali izhod Y<sub>n</sub>) gre v linearni klasifikator za ustvarjanje rezultata. Vsi bloki mreÅ¾e delijo iste uteÅ¾i in se trenirajo od zaÄetka do konca z enim prehodom povratnega razÅ¡irjanja napake.

Ker se vektorska stanja S<sub>0</sub>,...,S<sub>n</sub> prenaÅ¡ajo skozi mreÅ¾o, lahko ta modelira zaporedne odvisnosti med besedami. Na primer, ko se v zaporedju pojavi beseda *ne*, se lahko nauÄi negirati doloÄene elemente v vektorskem stanju, kar povzroÄi negacijo.

> âœ… Ker so uteÅ¾i vseh RNN blokov na zgornji sliki enake, lahko isto sliko predstavimo kot en blok (na desni) z rekurzivno povratno zanko, ki izhodno stanje mreÅ¾e poÅ¡lje nazaj na vhod.

## Anatomija RNN celice

Poglejmo, kako je organizirana preprosta RNN celica. Sprejme prejÅ¡nje stanje S<sub>i-1</sub> in trenutni simbol X<sub>i</sub> kot vhoda ter mora ustvariti izhodno stanje S<sub>i</sub> (vÄasih nas zanima tudi kakÅ¡en drug izhod Y<sub>i</sub>, kot v primeru generativnih mreÅ¾).

Preprosta RNN celica ima znotraj dve matriki uteÅ¾i: ena transformira vhodni simbol (poimenujmo jo W), druga pa transformira vhodno stanje (H). V tem primeru se izhod mreÅ¾e izraÄuna kot Ïƒ(WÃ—X<sub>i</sub>+HÃ—S<sub>i-1</sub>+b), kjer je Ïƒ aktivacijska funkcija, b pa dodatni pristranski Älen.

<img alt="Anatomija RNN celice" src="images/rnn-anatomy.png" width="50%"/>

> Slika avtorja

V mnogih primerih se vhodni simboli pred vstopom v RNN poÅ¡ljejo skozi plast vgrajenih predstavitev, da se zmanjÅ¡a dimenzionalnost. V tem primeru, Äe je dimenzija vhodnih vektorjev *emb_size* in dimenzija stanja *hid_size*, je velikost W *emb_size*Ã—*hid_size*, velikost H pa *hid_size*Ã—*hid_size*.

## DolgoroÄni kratkoroÄni spomin (LSTM)

Ena glavnih teÅ¾av klasiÄnih RNN je tako imenovana teÅ¾ava **izginjajoÄih gradientov**. Ker se RNN trenirajo od zaÄetka do konca z enim prehodom povratnega razÅ¡irjanja napake, imajo teÅ¾ave s prenaÅ¡anjem napake na prve plasti mreÅ¾e, zato se mreÅ¾a ne more nauÄiti odnosov med oddaljenimi simboli. Ena od reÅ¡itev te teÅ¾ave je uvedba **eksplicitnega upravljanja stanja** z uporabo tako imenovanih **vrat**. Obstajata dve dobro znani arhitekturi te vrste: **dolgoroÄni kratkoroÄni spomin** (LSTM) in **enota z vrati za posredovanje** (GRU).

![Slika, ki prikazuje primer celice dolgoroÄnega kratkoroÄnega spomina](../../../../../lessons/5-NLP/16-RNN/images/long-short-term-memory-cell.svg)

> Vir slike TBD

LSTM mreÅ¾a je organizirana podobno kot RNN, vendar se iz plasti v plast prenaÅ¡ata dve stanji: dejansko stanje C in skriti vektor H. Pri vsaki enoti se skriti vektor H<sub>i</sub> zdruÅ¾i z vhodom X<sub>i</sub>, nato pa nadzorujeta, kaj se zgodi s stanjem C prek **vrat**. Vsaka vrata so nevronska mreÅ¾a s sigmoidno aktivacijo (izhod v obmoÄju [0,1]), ki jih lahko razumemo kot bitno masko, ko jih pomnoÅ¾imo z vektorskim stanjem. Na sliki zgoraj so naslednja vrata (od leve proti desni):

* **Vrata za pozabo** sprejmejo skriti vektor in doloÄijo, katere komponente vektorja C je treba pozabiti in katere prenesti naprej.
* **Vhodna vrata** vzamejo nekaj informacij iz vhodnega in skritega vektorja ter jih vstavijo v stanje.
* **Izhodna vrata** transformirajo stanje prek linearne plasti s *tanh* aktivacijo, nato pa izberejo nekatere njegove komponente z uporabo skritega vektorja H<sub>i</sub>, da ustvarijo novo stanje C<sub>i+1</sub>.

Komponente stanja C lahko razumemo kot zastavice, ki jih lahko vklopimo in izklopimo. Na primer, ko v zaporedju naletimo na ime *Alice*, lahko predpostavimo, da se nanaÅ¡a na Å¾enski lik, in dvignemo zastavico v stanju, da imamo v stavku Å¾enski samostalnik. Ko kasneje naletimo na frazo *in Tom*, dvignemo zastavico, da imamo mnoÅ¾inski samostalnik. Tako lahko z manipulacijo stanja sledimo slovniÄnim lastnostim delov stavka.

> âœ… OdliÄen vir za razumevanje notranjega delovanja LSTM je ta odliÄen Älanek [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) avtorja Christopherja Olaha.

## Dvosmerni in veÄplastni RNN

Razpravljali smo o rekurzivnih mreÅ¾ah, ki delujejo v eno smer, od zaÄetka zaporedja do konca. To se zdi naravno, saj posnema naÄin, kako beremo in posluÅ¡amo govor. Vendar pa ima v mnogih praktiÄnih primerih smisel izvajati rekurzivno raÄunanje v obe smeri. TakÅ¡ne mreÅ¾e imenujemo **dvosmerne** RNN. Pri delu z dvosmerno mreÅ¾o potrebujemo dva skrita vektorska stanja, enega za vsako smer.

Rekurzivna mreÅ¾a, bodisi enosmerna bodisi dvosmerna, zajame doloÄene vzorce znotraj zaporedja in jih lahko shrani v vektorsko stanje ali prenese v izhod. Tako kot pri konvolucijskih mreÅ¾ah lahko na prvo plast zgradimo drugo rekurzivno plast, da zajamemo vzorce viÅ¡je ravni in gradimo na nizkonivojskih vzorcih, ki jih je izloÄila prva plast. To nas pripelje do pojma **veÄplastnega RNN**, ki je sestavljen iz dveh ali veÄ rekurzivnih mreÅ¾, kjer se izhod prejÅ¡nje plasti prenese v naslednjo plast kot vhod.

![Slika, ki prikazuje veÄplastni dolgoroÄni kratkoroÄni spomin RNN](../../../../../translated_images/multi-layer-lstm.dd975e29bb2a59fe58b429db833932d734c81f211cad2783797a9608984acb8c.sl.jpg)

*Slika iz [tega Äudovitega prispevka](https://towardsdatascience.com/from-a-lstm-cell-to-a-multilayer-lstm-network-with-pytorch-2899eb5696f3) avtorja Fernanda LÃ³peza*

## âœï¸ Vaje: Vgrajene predstavitve

Nadaljujte z uÄenjem v naslednjih zvezkih:

* [RNN-ji s PyTorch](../../../../../lessons/5-NLP/16-RNN/RNNPyTorch.ipynb)
* [RNN-ji s TensorFlow](../../../../../lessons/5-NLP/16-RNN/RNNTF.ipynb)

## ZakljuÄek

V tej enoti smo videli, da lahko RNN uporabljamo za klasifikacijo zaporedij, vendar lahko v resnici obravnavajo Å¡e veliko veÄ nalog, kot so generiranje besedila, strojno prevajanje in drugo. Te naloge bomo obravnavali v naslednji enoti.

## ğŸš€ Izziv

Preberite nekaj literature o LSTM-jih in razmislite o njihovih aplikacijah:

- [Grid Long Short-Term Memory](https://arxiv.org/pdf/1507.01526v1.pdf)
- [Show, Attend and Tell: Neural Image Caption
Generation with Visual Attention](https://arxiv.org/pdf/1502.03044v2.pdf)

## [Kviz po predavanju](https://ff-quizzes.netlify.app/en/ai/quiz/32)

## Pregled in samostojno uÄenje

- [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) avtorja Christopherja Olaha.

## [Naloga: Zvezki](assignment.md)

**Omejitev odgovornosti**:  
Ta dokument je bil preveden z uporabo storitve AI za prevajanje [Co-op Translator](https://github.com/Azure/co-op-translator). ÄŒeprav si prizadevamo za natanÄnost, vas prosimo, da upoÅ¡tevate, da lahko avtomatizirani prevodi vsebujejo napake ali netoÄnosti. Izvirni dokument v njegovem izvirnem jeziku je treba obravnavati kot avtoritativni vir. Za kljuÄne informacije priporoÄamo profesionalni ÄloveÅ¡ki prevod. Ne prevzemamo odgovornosti za morebitna nesporazumevanja ali napaÄne razlage, ki bi nastale zaradi uporabe tega prevoda.