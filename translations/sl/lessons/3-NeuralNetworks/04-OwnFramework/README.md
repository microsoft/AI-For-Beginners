<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "186bf7eeab776b36f557357ea56d4751",
  "translation_date": "2025-08-25T23:47:26+00:00",
  "source_file": "lessons/3-NeuralNetworks/04-OwnFramework/README.md",
  "language_code": "sl"
}
-->
# Uvod v nevronske mreÅ¾e. VeÄplastni perceptron

V prejÅ¡njem poglavju ste spoznali najpreprostejÅ¡i model nevronske mreÅ¾e - enoplastni perceptron, linearen model za klasifikacijo dveh razredov.

V tem poglavju bomo ta model razÅ¡irili v bolj prilagodljiv okvir, ki nam omogoÄa:

* izvajanje **klasifikacije veÄ razredov** poleg klasifikacije dveh razredov
* reÅ¡evanje **regresijskih problemov** poleg klasifikacije
* loÄevanje razredov, ki niso linearno loÄljivi

Prav tako bomo razvili svoj modularni okvir v Pythonu, ki nam bo omogoÄil sestavljanje razliÄnih arhitektur nevronskih mreÅ¾.

## [Predhodni kviz](https://ff-quizzes.netlify.app/en/ai/quiz/7)

## Formalizacija strojnega uÄenja

ZaÄnimo s formalizacijo problema strojnega uÄenja. Predpostavimo, da imamo uÄni podatkovni niz **X** z oznakami **Y**, in moramo zgraditi model *f*, ki bo podajal najbolj natanÄne napovedi. Kakovost napovedi merimo z **funkcijo izgube** â„’. Pogosto uporabljene funkcije izgube so:

* Pri regresijskih problemih, ko moramo napovedati Å¡tevilo, lahko uporabimo **absolutno napako** âˆ‘<sub>i</sub>|f(x<sup>(i)</sup>)-y<sup>(i)</sup>| ali **kvadratno napako** âˆ‘<sub>i</sub>(f(x<sup>(i)</sup>)-y<sup>(i)</sup>)<sup>2</sup>
* Pri klasifikaciji uporabljamo **0-1 izgubo** (ki je v bistvu enaka **natanÄnosti** modela) ali **logistiÄno izgubo**.

Pri enoplastnem perceptronu je bila funkcija *f* definirana kot linearna funkcija *f(x)=wx+b* (kjer je *w* matrika uteÅ¾i, *x* vektor vhodnih znaÄilnosti, in *b* vektor pristranskosti). Pri razliÄnih arhitekturah nevronskih mreÅ¾ lahko ta funkcija prevzame bolj zapleteno obliko.

> Pri klasifikaciji je pogosto zaÅ¾eleno, da dobimo verjetnosti ustreznih razredov kot izhod mreÅ¾e. Za pretvorbo poljubnih Å¡tevil v verjetnosti (npr. za normalizacijo izhoda) pogosto uporabljamo funkcijo **softmax** Ïƒ, in funkcija *f* postane *f(x)=Ïƒ(wx+b)*.

V zgornji definiciji *f* sta *w* in *b* imenovana **parametra** Î¸=âŸ¨*w,b*âŸ©. Glede na podatkovni niz âŸ¨**X**,**Y**âŸ© lahko izraÄunamo skupno napako na celotnem podatkovnem nizu kot funkcijo parametrov Î¸.

> âœ… **Cilj uÄenja nevronske mreÅ¾e je zmanjÅ¡ati napako z spreminjanjem parametrov Î¸**

## Optimizacija z gradientnim spustom

Obstaja dobro poznana metoda optimizacije funkcij, imenovana **gradientni spust**. Ideja je, da lahko izraÄunamo odvod (v veÄdimenzionalnem primeru imenovan **gradient**) funkcije izgube glede na parametre in spreminjamo parametre tako, da se napaka zmanjÅ¡a. To lahko formaliziramo na naslednji naÄin:

* Inicializiramo parametre z nakljuÄnimi vrednostmi w<sup>(0)</sup>, b<sup>(0)</sup>
* VeÄkrat ponovimo naslednji korak:
    - w<sup>(i+1)</sup> = w<sup>(i)</sup>-Î·âˆ‚â„’/âˆ‚w
    - b<sup>(i+1)</sup> = b<sup>(i)</sup>-Î·âˆ‚â„’/âˆ‚b

Med uÄenjem naj bi se koraki optimizacije izraÄunavali glede na celoten podatkovni niz (spomnite se, da se izguba izraÄuna kot vsota skozi vse uÄne primere). V praksi pa vzamemo majhne dele podatkovnega niza, imenovane **miniserije**, in izraÄunamo gradient na podlagi podniza podatkov. Ker se podniz vsakiÄ izbere nakljuÄno, se takÅ¡na metoda imenuje **stohastiÄni gradientni spust** (SGD).

## VeÄplastni perceptroni in povratno razÅ¡irjanje

Enoplastna mreÅ¾a, kot smo videli zgoraj, je sposobna klasificirati linearno loÄljive razrede. Za gradnjo bogatejÅ¡ega modela lahko zdruÅ¾imo veÄ plasti mreÅ¾e. MatematiÄno to pomeni, da bo funkcija *f* imela bolj zapleteno obliko in bo izraÄunana v veÄ korakih:
* z<sub>1</sub>=w<sub>1</sub>x+b<sub>1</sub>
* z<sub>2</sub>=w<sub>2</sub>Î±(z<sub>1</sub>)+b<sub>2</sub>
* f = Ïƒ(z<sub>2</sub>)

Tukaj je Î± **nelinearna aktivacijska funkcija**, Ïƒ funkcija softmax, in parametri Î¸=<*w<sub>1</sub>,b<sub>1</sub>,w<sub>2</sub>,b<sub>2</sub>*>.

Algoritem gradientnega spusta ostane enak, vendar je izraÄun gradientov bolj zahteven. Glede na pravilo veriÅ¾nega diferenciiranja lahko izraÄunamo odvode kot:

* âˆ‚â„’/âˆ‚w<sub>2</sub> = (âˆ‚â„’/âˆ‚Ïƒ)(âˆ‚Ïƒ/âˆ‚z<sub>2</sub>)(âˆ‚z<sub>2</sub>/âˆ‚w<sub>2</sub>)
* âˆ‚â„’/âˆ‚w<sub>1</sub> = (âˆ‚â„’/âˆ‚Ïƒ)(âˆ‚Ïƒ/âˆ‚z<sub>2</sub>)(âˆ‚z<sub>2</sub>/âˆ‚Î±)(âˆ‚Î±/âˆ‚z<sub>1</sub>)(âˆ‚z<sub>1</sub>/âˆ‚w<sub>1</sub>)

> âœ… Pravilo veriÅ¾nega diferenciiranja se uporablja za izraÄun odvoda funkcije izgube glede na parametre.

Opazimo, da je levi del vseh teh izrazov enak, zato lahko uÄinkovito izraÄunamo odvode, zaÄenÅ¡i s funkcijo izgube in gremo "nazaj" skozi raÄunski graf. Zato se metoda uÄenja veÄplastnega perceptrona imenuje **povratno razÅ¡irjanje** ali 'backprop'.

<img alt="raÄunski graf" src="images/ComputeGraphGrad.png"/>

> TODO: navedba vira slike

> âœ… Povratno razÅ¡irjanje bomo podrobneje obravnavali v naÅ¡em zvezku z zgledom.

## ZakljuÄek

V tej lekciji smo zgradili svojo knjiÅ¾nico nevronskih mreÅ¾ in jo uporabili za preprosto dvodimenzionalno klasifikacijsko nalogo.

## ğŸš€ Izziv

V priloÅ¾enem zvezku boste implementirali svoj okvir za gradnjo in uÄenje veÄplastnih perceptronov. Podrobno boste lahko videli, kako delujejo sodobne nevronske mreÅ¾e.

Nadaljujte z zvezkom [OwnFramework](../../../../../lessons/3-NeuralNetworks/04-OwnFramework/OwnFramework.ipynb) in ga preuÄite.

## [Kviz po predavanju](https://ff-quizzes.netlify.app/en/ai/quiz/8)

## Pregled in samostojno uÄenje

Povratno razÅ¡irjanje je pogost algoritem, ki se uporablja v umetni inteligenci in strojnem uÄenju, zato je vredno [ga podrobneje preuÄiti](https://wikipedia.org/wiki/Backpropagation).

## [Naloga](lab/README.md)

V tej laboratorijski nalogi boste uporabili okvir, ki ste ga zgradili v tej lekciji, za reÅ¡evanje klasifikacije roÄno napisanih Å¡tevilk MNIST.

* [Navodila](lab/README.md)
* [Zvezek](../../../../../lessons/3-NeuralNetworks/04-OwnFramework/lab/MyFW_MNIST.ipynb)

**Omejitev odgovornosti**:  
Ta dokument je bil preveden z uporabo storitve AI za prevajanje [Co-op Translator](https://github.com/Azure/co-op-translator). ÄŒeprav si prizadevamo za natanÄnost, vas prosimo, da upoÅ¡tevate, da lahko avtomatizirani prevodi vsebujejo napake ali netoÄnosti. Izvirni dokument v njegovem maternem jeziku je treba obravnavati kot avtoritativni vir. Za kljuÄne informacije priporoÄamo profesionalni ÄloveÅ¡ki prevod. Ne prevzemamo odgovornosti za morebitna nesporazumevanja ali napaÄne razlage, ki izhajajo iz uporabe tega prevoda.