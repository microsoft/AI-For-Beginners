# Uvod v nevronske mreÅ¾e. VeÄplastni perceptron

V prejÅ¡njem poglavju ste spoznali najpreprostejÅ¡i model nevronske mreÅ¾e - enoplastni perceptron, linearen model za klasifikacijo dveh razredov.

V tem poglavju bomo ta model razÅ¡irili v bolj prilagodljiv okvir, ki nam omogoÄa:

* izvajanje **klasifikacije veÄ razredov** poleg klasifikacije dveh razredov
* reÅ¡evanje **regresijskih problemov** poleg klasifikacije
* loÄevanje razredov, ki niso linearno loÄljivi

Prav tako bomo razvili svoj modularni okvir v Pythonu, ki nam bo omogoÄil sestavljanje razliÄnih arhitektur nevronskih mreÅ¾.

## [Predavanje kviz](https://ff-quizzes.netlify.app/en/ai/quiz/7)

## Formalizacija strojnega uÄenja

ZaÄnimo s formalizacijo problema strojnega uÄenja. Predpostavimo, da imamo uÄni podatkovni niz **X** z oznakami **Y**, in moramo zgraditi model *f*, ki bo zagotavljal najbolj natanÄne napovedi. Kakovost napovedi merimo z **funkcijo izgube** &lagran;. Pogosto uporabljene funkcije izgube so:

* Pri regresijskem problemu, ko moramo napovedati Å¡tevilo, lahko uporabimo **absolutno napako** &sum;<sub>i</sub>|f(x<sup>(i)</sup>)-y<sup>(i)</sup>| ali **kvadratno napako** &sum;<sub>i</sub>(f(x<sup>(i)</sup>)-y<sup>(i)</sup>)<sup>2</sup>
* Pri klasifikaciji uporabljamo **0-1 izgubo** (ki je v bistvu enaka **natanÄnosti** modela) ali **logistiÄno izgubo**.

Pri enoplastnem perceptronu je bila funkcija *f* definirana kot linearna funkcija *f(x)=wx+b* (kjer je *w* matrika uteÅ¾i, *x* je vektor vhodnih znaÄilnosti, in *b* je vektor pristranskosti). Pri razliÄnih arhitekturah nevronskih mreÅ¾ lahko ta funkcija prevzame bolj zapleteno obliko.

> Pri klasifikaciji je pogosto zaÅ¾eleno, da kot izhod mreÅ¾e dobimo verjetnosti ustreznih razredov. Za pretvorbo poljubnih Å¡tevil v verjetnosti (npr. za normalizacijo izhoda) pogosto uporabljamo funkcijo **softmax** &sigma;, in funkcija *f* postane *f(x)=&sigma;(wx+b)*.

V zgornji definiciji *f* sta *w* in *b* imenovana **parametra** &theta;=âŸ¨*w,b*âŸ©. Glede na podatkovni niz âŸ¨**X**,**Y**âŸ© lahko izraÄunamo skupno napako na celotnem podatkovnem nizu kot funkcijo parametrov &theta;.

> âœ… **Cilj uÄenja nevronske mreÅ¾e je zmanjÅ¡ati napako z variiranjem parametrov &theta;**

## Optimizacija z gradientnim spustom

Obstaja dobro poznana metoda optimizacije funkcij, imenovana **gradientni spust**. Ideja je, da lahko izraÄunamo odvod (v veÄdimenzionalnem primeru imenovan **gradient**) funkcije izgube glede na parametre in spreminjamo parametre tako, da se napaka zmanjÅ¡a. To lahko formaliziramo na naslednji naÄin:

* Inicializiramo parametre z nakljuÄnimi vrednostmi w<sup>(0)</sup>, b<sup>(0)</sup>
* VeÄkrat ponovimo naslednji korak:
    - w<sup>(i+1)</sup> = w<sup>(i)</sup>-&eta;&part;&lagran;/&part;w
    - b<sup>(i+1)</sup> = b<sup>(i)</sup>-&eta;&part;&lagran;/&part;b

Med uÄenjem naj bi se koraki optimizacije izraÄunavali glede na celoten podatkovni niz (spomnite se, da se izguba izraÄuna kot vsota skozi vse uÄne vzorce). V praksi pa vzamemo majhne dele podatkovnega niza, imenovane **minibatchi**, in izraÄunamo gradiente na podlagi podmnoÅ¾ice podatkov. Ker je podmnoÅ¾ica vsakiÄ izbrana nakljuÄno, se takÅ¡na metoda imenuje **stohastiÄni gradientni spust** (SGD).

## VeÄplastni perceptroni in povratno razÅ¡irjanje

Enoplastna mreÅ¾a, kot smo videli zgoraj, je sposobna klasificirati linearno loÄljive razrede. Za gradnjo bogatejÅ¡ega modela lahko zdruÅ¾imo veÄ plasti mreÅ¾e. MatematiÄno to pomeni, da bo funkcija *f* imela bolj zapleteno obliko in bo izraÄunana v veÄ korakih:
* z<sub>1</sub>=w<sub>1</sub>x+b<sub>1</sub>
* z<sub>2</sub>=w<sub>2</sub>&alpha;(z<sub>1</sub>)+b<sub>2</sub>
* f = &sigma;(z<sub>2</sub>)

Tu je &alpha; **nelinearna aktivacijska funkcija**, &sigma; je funkcija softmax, in parametri &theta;=<*w<sub>1</sub>,b<sub>1</sub>,w<sub>2</sub>,b<sub>2</sub>*>.

Algoritem gradientnega spusta ostane enak, vendar je izraÄun gradientov bolj zahteven. Glede na pravilo veriÅ¾nega diferenciiranja lahko izraÄunamo odvode kot:

* &part;&lagran;/&part;w<sub>2</sub> = (&part;&lagran;/&part;&sigma;)(&part;&sigma;/&part;z<sub>2</sub>)(&part;z<sub>2</sub>/&part;w<sub>2</sub>)
* &part;&lagran;/&part;w<sub>1</sub> = (&part;&lagran;/&part;&sigma;)(&part;&sigma;/&part;z<sub>2</sub>)(&part;z<sub>2</sub>/&part;&alpha;)(&part;&alpha;/&part;z<sub>1</sub>)(&part;z<sub>1</sub>/&part;w<sub>1</sub>)

> âœ… Pravilo veriÅ¾nega diferenciiranja se uporablja za izraÄun odvoda funkcije izgube glede na parametre.

Opazite, da je skrajno levi del vseh teh izrazov enak, zato lahko uÄinkovito izraÄunamo odvode, zaÄenÅ¡i s funkcijo izgube in gremo "nazaj" skozi raÄunski graf. Zato se metoda uÄenja veÄplastnega perceptrona imenuje **povratno razÅ¡irjanje** ali 'backprop'.

<img alt="raÄunski graf" src="../../../../../translated_images/sl/ComputeGraphGrad.4626252c0de03507.webp"/>

> TODO: navedba vira slike

> âœ… Povratno razÅ¡irjanje bomo podrobneje obravnavali v naÅ¡em zvezku z zgledom.  

## ZakljuÄek

V tej lekciji smo zgradili svojo knjiÅ¾nico nevronskih mreÅ¾ in jo uporabili za preprosto nalogo klasifikacije v dveh dimenzijah.

## ğŸš€ Izziv

V priloÅ¾enem zvezku boste implementirali svoj okvir za gradnjo in uÄenje veÄplastnih perceptronov. Podrobno boste lahko videli, kako delujejo sodobne nevronske mreÅ¾e.

Nadaljujte z zvezkom [OwnFramework](OwnFramework.ipynb) in ga preuÄite.

## [Kviz po predavanju](https://ff-quizzes.netlify.app/en/ai/quiz/8)

## Pregled in samostojno uÄenje

Povratno razÅ¡irjanje je pogost algoritem, ki se uporablja v AI in ML, vredno ga je [podrobneje preuÄiti](https://wikipedia.org/wiki/Backpropagation).

## [Naloga](lab/README.md)

V tej laboratorijski nalogi boste uporabili okvir, ki ste ga zgradili v tej lekciji, za reÅ¡evanje klasifikacije roÄno napisanih Å¡tevilk MNIST.

* [Navodila](lab/README.md)
* [Zvezek](lab/MyFW_MNIST.ipynb)

---

