# Okvirji za nevronske mreÅ¾e

Kot smo Å¾e spoznali, za uÄinkovito uÄenje nevronskih mreÅ¾ moramo narediti dve stvari:

* Operirati na tenzorjih, npr. mnoÅ¾iti, seÅ¡tevati in izraÄunavati funkcije, kot sta sigmoid ali softmax
* IzraÄunati gradient vseh izrazov, da lahko izvedemo optimizacijo z gradientnim spustom

## [Predhodni kviz](https://ff-quizzes.netlify.app/en/ai/quiz/9)

Medtem ko knjiÅ¾nica `numpy` omogoÄa prvo nalogo, potrebujemo mehanizem za izraÄun gradientov. V [naÅ¡em okviru](../04-OwnFramework/OwnFramework.ipynb), ki smo ga razvili v prejÅ¡njem poglavju, smo morali roÄno programirati vse funkcije za odvode znotraj metode `backward`, ki izvaja povratno propagacijo. Idealno bi bilo, da nam okvir omogoÄa izraÄun gradientov *kateregakoli izraza*, ki ga lahko definiramo.

Druga pomembna stvar je moÅ¾nost izvajanja izraÄunov na GPU ali drugih specializiranih enotah, kot je [TPU](https://en.wikipedia.org/wiki/Tensor_Processing_Unit). UÄenje globokih nevronskih mreÅ¾ zahteva *zelo veliko* izraÄunov, zato je sposobnost paralelizacije teh izraÄunov na GPU-jih zelo pomembna.

> âœ… Izraz 'paralelizacija' pomeni razdelitev izraÄunov na veÄ naprav.

Trenutno sta najbolj priljubljena okvira za nevronske mreÅ¾e: [TensorFlow](http://TensorFlow.org) in [PyTorch](https://pytorch.org/). Oba ponujata nizkonivojski API za delo s tenzorji na CPU in GPU. Poleg nizkonivojskega API-ja obstaja tudi visok nivojski API, imenovan [Keras](https://keras.io/) in [PyTorch Lightning](https://pytorchlightning.ai/).

Nizkonivojski API | [TensorFlow](http://TensorFlow.org) | [PyTorch](https://pytorch.org/)
------------------|-------------------------------------|--------------------------------
Visokonivojski API| [Keras](https://keras.io/) | [PyTorch Lightning](https://pytorchlightning.ai/)

**Nizkonivojski API-ji** v obeh okvirjih omogoÄajo gradnjo tako imenovanih **raÄunalniÅ¡kih grafov**. Ta graf definira, kako izraÄunati rezultat (obiÄajno funkcijo izgube) z danimi vhodnimi parametri, in ga je mogoÄe prenesti na GPU za izraÄun, Äe je na voljo. Obstajajo funkcije za diferenciacijo tega raÄunalniÅ¡kega grafa in izraÄun gradientov, ki jih je nato mogoÄe uporabiti za optimizacijo parametrov modela.

**Visokonivojski API-ji** obravnavajo nevronske mreÅ¾e kot **zaporedje slojev** in omogoÄajo enostavnejÅ¡o konstrukcijo veÄine nevronskih mreÅ¾. UÄenje modela obiÄajno zahteva pripravo podatkov in nato klic funkcije `fit`, ki opravi delo.

Visokonivojski API omogoÄa hitro konstrukcijo tipiÄnih nevronskih mreÅ¾ brez skrbi za Å¡tevilne podrobnosti. Hkrati pa nizkonivojski API ponuja veliko veÄ nadzora nad procesom uÄenja, zato se pogosto uporablja v raziskavah, ko se ukvarjamo z novimi arhitekturami nevronskih mreÅ¾.

Pomembno je tudi razumeti, da lahko oba API-ja uporabljamo skupaj, npr. lahko razvijemo svojo arhitekturo sloja mreÅ¾e z nizkonivojskim API-jem in jo nato uporabimo znotraj veÄje mreÅ¾e, ki je bila zgrajena in nauÄena z visokonivojskim API-jem. Lahko pa definiramo mreÅ¾o z visokonivojskim API-jem kot zaporedje slojev in nato uporabimo svoj nizkonivojski uÄni zanki za optimizacijo. Oba API-ja uporabljata iste osnovne koncepte in sta zasnovana tako, da dobro sodelujeta.

## UÄenje

V tem teÄaju ponujamo veÄino vsebine tako za PyTorch kot za TensorFlow. Izberete lahko svoj najljubÅ¡i okvir in se osredotoÄite le na ustrezne zvezke. ÄŒe niste prepriÄani, kateri okvir izbrati, preberite nekaj razprav na internetu o **PyTorch vs. TensorFlow**. Lahko si tudi ogledate oba okvira, da pridobite boljÅ¡e razumevanje.

Kjer je mogoÄe, bomo za enostavnost uporabljali visokonivojske API-je. Vendar pa verjamemo, da je pomembno razumeti, kako nevronske mreÅ¾e delujejo od temeljev naprej, zato na zaÄetku zaÄnemo z nizkonivojskim API-jem in tenzorji. ÄŒe pa Å¾elite hitro zaÄeti in ne Å¾elite porabiti veliko Äasa za uÄenje teh podrobnosti, lahko te preskoÄite in se takoj osredotoÄite na zvezke z visokonivojskim API-jem.

## âœï¸ Naloge: Okvirji

Nadaljujte z uÄenjem v naslednjih zvezkih:

Nizkonivojski API | [TensorFlow+Keras Notebook](IntroKerasTF.ipynb) | [PyTorch](IntroPyTorch.ipynb)
------------------|-------------------------------------|--------------------------------
Visokonivojski API| [Keras](IntroKeras.ipynb) | *PyTorch Lightning*

Ko obvladate okvirje, si poglejmo koncept prenauÄenja.

# PrenauÄenje

PrenauÄenje je izjemno pomemben koncept v strojnem uÄenju, zato je zelo pomembno, da ga pravilno razumemo!

Razmislimo o naslednjem problemu pribliÅ¾evanja 5 toÄk (predstavljenih z `x` na spodnjih grafih):

![linear](../../../../../translated_images/sl/overfit1.f24b71c6f652e59e.webp) | ![overfit](../../../../../translated_images/sl/overfit2.131f5800ae10ca5e.webp)
-------------------------|--------------------------
**Linearen model, 2 parametra** | **Nelinearen model, 7 parametrov**
Napaka pri uÄenju = 5.3 | Napaka pri uÄenju = 0
Napaka pri validaciji = 5.1 | Napaka pri validaciji = 20

* Na levi vidimo dobro pribliÅ¾anje s premico. Ker je Å¡tevilo parametrov ustrezno, model pravilno razume razporeditev toÄk.
* Na desni je model preveÄ zmogljiv. Ker imamo le 5 toÄk in model ima 7 parametrov, se lahko prilagodi tako, da gre skozi vse toÄke, kar povzroÄi, da je napaka pri uÄenju 0. Vendar pa to prepreÄuje modelu, da bi razumel pravilni vzorec podatkov, zato je napaka pri validaciji zelo visoka.

Zelo pomembno je najti pravo ravnovesje med kompleksnostjo modela (Å¡tevilom parametrov) in Å¡tevilom uÄnih vzorcev.

## Zakaj pride do prenauÄenja

  * Premalo uÄnih podatkov
  * PreveÄ zmogljiv model
  * PreveÄ Å¡uma v vhodnih podatkih

## Kako zaznati prenauÄenje

Kot lahko vidite na zgornjem grafu, lahko prenauÄenje zaznamo z zelo nizko napako pri uÄenju in visoko napako pri validaciji. ObiÄajno med uÄenjem vidimo, da se napake pri uÄenju in validaciji zmanjÅ¡ujejo, nato pa se v nekem trenutku napaka pri validaciji preneha zmanjÅ¡evati in zaÄne naraÅ¡Äati. To bo znak prenauÄenja in indikator, da bi morali verjetno ustaviti uÄenje (ali vsaj narediti posnetek modela).

![prenauÄenje](../../../../../translated_images/sl/Overfitting.408ad91cd90b4371.webp)

## Kako prepreÄiti prenauÄenje

ÄŒe opazite, da pride do prenauÄenja, lahko storite naslednje:

 * PoveÄajte koliÄino uÄnih podatkov
 * ZmanjÅ¡ajte kompleksnost modela
 * Uporabite kakÅ¡no [tehniko regularizacije](../../4-ComputerVision/08-TransferLearning/TrainingTricks.md), kot je [Dropout](../../4-ComputerVision/08-TransferLearning/TrainingTricks.md#Dropout), ki jo bomo obravnavali kasneje.

## PrenauÄenje in kompromis med pristranskostjo in varianco

PrenauÄenje je pravzaprav primer bolj sploÅ¡nega problema v statistiki, imenovanega [kompromis med pristranskostjo in varianco](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff). ÄŒe razmislimo o moÅ¾nih virih napak v naÅ¡em modelu, lahko vidimo dva tipa napak:

* **Napake zaradi pristranskosti** nastanejo, ker naÅ¡ algoritem ne more pravilno zajeti odnosa med uÄnimi podatki. To je lahko posledica dejstva, da naÅ¡ model ni dovolj zmogljiv (**podnauÄenje**).
* **Napake zaradi variance**, ki nastanejo, ker model pribliÅ¾uje Å¡um v vhodnih podatkih namesto smiselnega odnosa (**prenauÄenje**).

Med uÄenjem se napake zaradi pristranskosti zmanjÅ¡ujejo (ker se naÅ¡ model uÄi pribliÅ¾evati podatke), medtem ko se napake zaradi variance poveÄujejo. Pomembno je ustaviti uÄenje - bodisi roÄno (ko zaznamo prenauÄenje) bodisi samodejno (z uvedbo regularizacije) - da prepreÄimo prenauÄenje.

## ZakljuÄek

V tej lekciji ste spoznali razlike med razliÄnimi API-ji za dva najbolj priljubljena AI okvirja, TensorFlow in PyTorch. Poleg tega ste spoznali zelo pomembno temo, prenauÄenje.

## ğŸš€ Izziv

V priloÅ¾enih zvezkih boste naÅ¡li 'naloge' na dnu; preglejte zvezke in dokonÄajte naloge.

## [Kviz po predavanju](https://ff-quizzes.netlify.app/en/ai/quiz/10)

## Pregled in samostojno uÄenje

Raziskujte naslednje teme:

- TensorFlow
- PyTorch
- PrenauÄenje

VpraÅ¡ajte se naslednja vpraÅ¡anja:

- KakÅ¡na je razlika med TensorFlow in PyTorch?
- KakÅ¡na je razlika med prenauÄenjem in podnauÄenjem?

## [Naloga](lab/README.md)

V tej nalogi boste reÅ¡evali dve klasifikacijski nalogi z uporabo enoslojnih in veÄslojnih popolnoma povezanih mreÅ¾ z uporabo PyTorch ali TensorFlow.

* [Navodila](lab/README.md)
* [Zvezek](lab/LabFrameworks.ipynb)

---

