<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "2b544f20b796402507fb05a0df893323",
  "translation_date": "2025-08-25T23:53:25+00:00",
  "source_file": "lessons/3-NeuralNetworks/05-Frameworks/README.md",
  "language_code": "sl"
}
-->
# Okvirji za nevronske mreÅ¾e

Kot smo Å¾e spoznali, za uÄinkovito uÄenje nevronskih mreÅ¾ moramo narediti dve stvari:

* Operirati na tensorjih, npr. mnoÅ¾iti, seÅ¡tevati in izraÄunavati funkcije, kot sta sigmoid ali softmax
* IzraÄunati gradient vseh izrazov, da lahko izvedemo optimizacijo z gradientnim spustom

## [Pre-uÄni kviz](https://ff-quizzes.netlify.app/en/ai/quiz/9)

Medtem ko knjiÅ¾nica `numpy` omogoÄa prvo nalogo, potrebujemo mehanizem za izraÄun gradientov. V [naÅ¡em okviru](../../../../../lessons/3-NeuralNetworks/04-OwnFramework/OwnFramework.ipynb), ki smo ga razvili v prejÅ¡njem razdelku, smo morali roÄno programirati vse funkcije za odvode znotraj metode `backward`, ki izvaja povratno propagacijo. Idealno bi bilo, da nam okvir omogoÄa izraÄun gradientov *kateregakoli izraza*, ki ga lahko definiramo.

Druga pomembna stvar je moÅ¾nost izvajanja izraÄunov na GPU ali drugih specializiranih enotah, kot je [TPU](https://en.wikipedia.org/wiki/Tensor_Processing_Unit). UÄenje globokih nevronskih mreÅ¾ zahteva *zelo veliko* izraÄunov, zato je kljuÄnega pomena, da lahko te izraÄune paraleliziramo na GPU-jih.

> âœ… Izraz 'paralelizirati' pomeni razdeliti izraÄune med veÄ naprav.

Trenutno sta najbolj priljubljena okvira za nevronske mreÅ¾e: [TensorFlow](http://TensorFlow.org) in [PyTorch](https://pytorch.org/). Oba ponujata nizkonivojski API za delo s tensorji na CPU in GPU. Poleg nizkonivojskega API-ja obstaja tudi viÅ¡jenivojski API, imenovan [Keras](https://keras.io/) in [PyTorch Lightning](https://pytorchlightning.ai/).

Nizkonivojski API | [TensorFlow](http://TensorFlow.org) | [PyTorch](https://pytorch.org/)
------------------|-------------------------------------|--------------------------------
ViÅ¡jenivojski API | [Keras](https://keras.io/) | [PyTorch Lightning](https://pytorchlightning.ai/)

**Nizkonivojski API-ji** v obeh okvirjih omogoÄajo gradnjo tako imenovanih **raÄunalniÅ¡kih grafov**. Ta graf definira, kako izraÄunati rezultat (obiÄajno funkcijo izgube) z danimi vhodnimi parametri, in ga je mogoÄe prenesti na GPU za izraÄun, Äe je na voljo. Obstajajo funkcije za diferenciacijo tega raÄunalniÅ¡kega grafa in izraÄun gradientov, ki jih nato lahko uporabimo za optimizacijo parametrov modela.

**ViÅ¡jenivojski API-ji** obravnavajo nevronske mreÅ¾e kot **zaporedje slojev** in omogoÄajo enostavnejÅ¡o konstrukcijo veÄine nevronskih mreÅ¾. UÄenje modela obiÄajno zahteva pripravo podatkov in nato klic funkcije `fit`, ki opravi delo.

ViÅ¡jenivojski API omogoÄa hitro konstrukcijo tipiÄnih nevronskih mreÅ¾ brez skrbi za Å¡tevilne podrobnosti. Hkrati pa nizkonivojski API ponuja veliko veÄ nadzora nad procesom uÄenja, zato se pogosto uporablja v raziskavah, ko se ukvarjamo z novimi arhitekturami nevronskih mreÅ¾.

Pomembno je tudi razumeti, da lahko oba API-ja uporabljamo skupaj, npr. lahko razvijemo svojo arhitekturo sloja mreÅ¾e z nizkonivojskim API-jem in jo nato uporabimo znotraj veÄje mreÅ¾e, ki je bila konstruirana in nauÄena z viÅ¡jenivojskim API-jem. Lahko pa definiramo mreÅ¾o z viÅ¡jenivojskim API-jem kot zaporedje slojev in nato uporabimo svoj nizkonivojski uÄni zanki za izvedbo optimizacije. Oba API-ja uporabljata iste osnovne koncepte in sta zasnovana tako, da dobro delujeta skupaj.

## UÄenje

V tem teÄaju ponujamo veÄino vsebine tako za PyTorch kot za TensorFlow. Izberete lahko svoj najljubÅ¡i okvir in se osredotoÄite le na ustrezne zvezke. ÄŒe niste prepriÄani, kateri okvir izbrati, preberite nekaj razprav na internetu o **PyTorch vs. TensorFlow**. Lahko si tudi ogledate oba okvira, da pridobite boljÅ¡e razumevanje.

Kjer je mogoÄe, bomo za enostavnost uporabljali viÅ¡jenivojske API-je. Vendar pa verjamemo, da je pomembno razumeti, kako nevronske mreÅ¾e delujejo od temeljev naprej, zato na zaÄetku zaÄnemo z delom z nizkonivojskim API-jem in tensorji. ÄŒe pa Å¾elite hitro zaÄeti in ne Å¾elite porabiti veliko Äasa za uÄenje teh podrobnosti, lahko te preskoÄite in se takoj osredotoÄite na zvezke z viÅ¡jenivojskim API-jem.

## âœï¸ Naloge: Okvirji

Nadaljujte z uÄenjem v naslednjih zvezkih:

Nizkonivojski API | [TensorFlow+Keras Zvezek](../../../../../lessons/3-NeuralNetworks/05-Frameworks/IntroKerasTF.ipynb) | [PyTorch](../../../../../lessons/3-NeuralNetworks/05-Frameworks/IntroPyTorch.ipynb)
------------------|-------------------------------------|--------------------------------
ViÅ¡jenivojski API | [Keras](../../../../../lessons/3-NeuralNetworks/05-Frameworks/IntroKeras.ipynb) | *PyTorch Lightning*

Ko obvladate okvirje, si poglejmo koncept prenauÄenja.

# PrenauÄenje

PrenauÄenje je izjemno pomemben koncept v strojnem uÄenju, zato je zelo pomembno, da ga pravilno razumemo!

Razmislimo o naslednjem problemu pribliÅ¾evanja 5 toÄk (predstavljenih z `x` na spodnjih grafih):

![linearno](../../../../../translated_images/overfit1.f24b71c6f652e59e6bed7245ffbeaecc3ba320e16e2221f6832b432052c4da43.sl.jpg) | ![prenauÄeno](../../../../../translated_images/overfit2.131f5800ae10ca5e41d12a411f5f705d9ee38b1b10916f284b787028dd55cc1c.sl.jpg)
-------------------------|--------------------------
**Linearen model, 2 parametra** | **Nelinearen model, 7 parametrov**
Napaka pri uÄenju = 5.3 | Napaka pri uÄenju = 0
Napaka pri validaciji = 5.1 | Napaka pri validaciji = 20

* Na levi vidimo dobro pribliÅ¾anje s premico. Ker je Å¡tevilo parametrov ustrezno, model pravilno razume razporeditev toÄk.
* Na desni je model premoÄan. Ker imamo le 5 toÄk in model ima 7 parametrov, se lahko prilagodi tako, da gre skozi vse toÄke, kar povzroÄi, da je napaka pri uÄenju 0. Vendar pa to prepreÄuje modelu, da bi razumel pravilni vzorec podatkov, zato je napaka pri validaciji zelo visoka.

Zelo pomembno je najti pravo ravnovesje med kompleksnostjo modela (Å¡tevilom parametrov) in Å¡tevilom uÄnih vzorcev.

## Zakaj pride do prenauÄenja

  * Premalo uÄnih podatkov
  * PreveÄ zmogljiv model
  * PreveÄ Å¡uma v vhodnih podatkih

## Kako zaznati prenauÄenje

Kot lahko vidite na zgornjem grafu, lahko prenauÄenje zaznamo z zelo nizko napako pri uÄenju in visoko napako pri validaciji. ObiÄajno med uÄenjem vidimo, da se napake pri uÄenju in validaciji zmanjÅ¡ujejo, nato pa se pri neki toÄki napaka pri validaciji preneha zmanjÅ¡evati in zaÄne naraÅ¡Äati. To je znak prenauÄenja in indikator, da bi morali verjetno prenehati z uÄenjem (ali vsaj narediti posnetek modela).

![prenauÄenje](../../../../../translated_images/Overfitting.408ad91cd90b4371d0a81f4287e1409c359751adeb1ae450332af50e84f08c3e.sl.png)

## Kako prepreÄiti prenauÄenje

ÄŒe opazite, da pride do prenauÄenja, lahko storite naslednje:

 * PoveÄate koliÄino uÄnih podatkov
 * ZmanjÅ¡ate kompleksnost modela
 * Uporabite kakÅ¡no [tehniko regularizacije](../../4-ComputerVision/08-TransferLearning/TrainingTricks.md), kot je [Dropout](../../4-ComputerVision/08-TransferLearning/TrainingTricks.md#Dropout), ki jo bomo obravnavali kasneje.

## PrenauÄenje in kompromis med pristranskostjo in varianco

PrenauÄenje je pravzaprav primer bolj sploÅ¡nega problema v statistiki, imenovanega [kompromis med pristranskostjo in varianco](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff). ÄŒe razmislimo o moÅ¾nih virih napak v naÅ¡em modelu, lahko vidimo dva tipa napak:

* **Napake zaradi pristranskosti** nastanejo, ker naÅ¡ algoritem ne more pravilno zajeti odnosa med uÄnimi podatki. To je lahko posledica dejstva, da naÅ¡ model ni dovolj zmogljiv (**podnauÄenje**).
* **Napake zaradi variance**, ki nastanejo, ker model pribliÅ¾uje Å¡um v vhodnih podatkih namesto smiselnega odnosa (**prenauÄenje**).

Med uÄenjem se napake zaradi pristranskosti zmanjÅ¡ujejo (ker se naÅ¡ model uÄi pribliÅ¾evati podatke), medtem ko se napake zaradi variance poveÄujejo. Pomembno je, da prenehamo z uÄenjem - bodisi roÄno (ko zaznamo prenauÄenje) bodisi samodejno (z uvedbo regularizacije) - da prepreÄimo prenauÄenje.

## ZakljuÄek

V tej lekciji ste spoznali razlike med razliÄnimi API-ji za dva najbolj priljubljena AI okvirja, TensorFlow in PyTorch. Poleg tega ste se nauÄili o zelo pomembni temi, prenauÄenju.

## ğŸš€ Izziv

V priloÅ¾enih zvezkih boste naÅ¡li 'naloge' na dnu; preglejte zvezke in dokonÄajte naloge.

## [Po-uÄni kviz](https://ff-quizzes.netlify.app/en/ai/quiz/10)

## Pregled & Samostojno uÄenje

Raziskujte naslednje teme:

- TensorFlow
- PyTorch
- PrenauÄenje

VpraÅ¡ajte se naslednja vpraÅ¡anja:

- KakÅ¡na je razlika med TensorFlow in PyTorch?
- KakÅ¡na je razlika med prenauÄenjem in podnauÄenjem?

## [Naloga](lab/README.md)

V tej nalogi boste reÅ¡evali dve klasifikacijski teÅ¾avi z uporabo enoslojnih in veÄslojnih popolnoma povezanih mreÅ¾ z uporabo PyTorch ali TensorFlow.

* [Navodila](lab/README.md)
* [Zvezek](../../../../../lessons/3-NeuralNetworks/05-Frameworks/lab/LabFrameworks.ipynb)

**Omejitev odgovornosti**:  
Ta dokument je bil preveden z uporabo storitve AI za prevajanje [Co-op Translator](https://github.com/Azure/co-op-translator). ÄŒeprav si prizadevamo za natanÄnost, vas prosimo, da upoÅ¡tevate, da lahko avtomatizirani prevodi vsebujejo napake ali netoÄnosti. Izvirni dokument v njegovem izvirnem jeziku je treba obravnavati kot avtoritativni vir. Za kljuÄne informacije priporoÄamo profesionalni ÄloveÅ¡ki prevod. Ne prevzemamo odgovornosti za morebitna nesporazumevanja ali napaÄne razlage, ki bi nastale zaradi uporabe tega prevoda.