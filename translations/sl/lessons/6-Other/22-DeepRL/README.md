# Globoko okrepljeno u캜enje

Okrepljeno u캜enje (RL) velja za enega osnovnih paradigm strojnega u캜enja, poleg nadzorovanega in nenadzorovanega u캜enja. Medtem ko se pri nadzorovanem u캜enju zana코amo na podatkovne nabore z znanimi rezultati, je RL osnovano na **u캜enju skozi izku코nje**. Na primer, ko prvi캜 vidimo ra캜unalni코ko igro, za캜nemo igrati, 캜eprav ne poznamo pravil, in kmalu izbolj코amo svoje spretnosti zgolj z igranjem in prilagajanjem svojega vedenja.

## [Predhodni kviz](https://ff-quizzes.netlify.app/en/ai/quiz/43)

Za izvajanje RL potrebujemo:

* **Okolje** ali **simulator**, ki dolo캜a pravila igre. V simulatorju moramo biti sposobni izvajati poskuse in opazovati rezultate.
* Nekak코no **funkcijo nagrajevanja**, ki ka쬰, kako uspe코en je bil na코 poskus. Pri u캜enju igranja ra캜unalni코ke igre bi bila nagrada na코 kon캜ni rezultat.

Na podlagi funkcije nagrajevanja moramo prilagoditi svoje vedenje in izbolj코ati svoje spretnosti, da bomo naslednji캜 igrali bolje. Glavna razlika med drugimi vrstami strojnega u캜enja in RL je, da pri RL obi캜ajno ne vemo, ali smo zmagali ali izgubili, dokler igre ne kon캜amo. Zato ne moremo re캜i, ali je dolo캜ena poteza sama po sebi dobra ali ne - nagrado prejmemo 코ele na koncu igre.

Med RL obi캜ajno izvedemo veliko poskusov. Pri vsakem poskusu moramo uravnote쬴ti med sledenjem optimalni strategiji, ki smo jo doslej osvojili (**izkori코캜anje**), in raziskovanjem novih mo쬹ih stanj (**raziskovanje**).

## OpenAI Gym

Odli캜no orodje za RL je [OpenAI Gym](https://gym.openai.com/) - **simulacijsko okolje**, ki lahko simulira 코tevilna razli캜na okolja, od Atari iger do fizike ravnote쬵a droga. To je eno najbolj priljubljenih simulacijskih okolij za treniranje algoritmov okrepljenega u캜enja, ki ga vzdr쬿je [OpenAI](https://openai.com/).

> **Note**: Vsa okolja, ki jih ponuja OpenAI Gym, si lahko ogledate [tukaj](https://gym.openai.com/envs/#classic_control).

## Uravnote쬰nje CartPole

Verjetno ste 쬰 videli sodobne naprave za uravnote쬰nje, kot so *Segway* ali *Gyroscooterji*. Te naprave se samodejno uravnote쬴jo z nastavljanjem svojih koles glede na signal iz pospe코komera ali 쬴roskopa. V tem razdelku se bomo nau캜ili re코iti podoben problem - uravnote쬰nje droga. To je podobno situaciji, ko cirkusant uravnava drog na svoji roki - vendar se to uravnote쬰nje droga zgodi le v 1D.

Poenostavljena razli캜ica uravnote쬰nja je znana kot problem **CartPole**. V svetu CartPole imamo horizontalni drsnik, ki se lahko premika levo ali desno, cilj pa je uravnote쬴ti navpi캜ni drog na vrhu drsnika med njegovim premikanjem.

<img alt="cartpole" src="../../../../../translated_images/sl/cartpole.f52a67f27e058170.webp" width="200"/>

Za ustvarjanje in uporabo tega okolja potrebujemo nekaj vrstic kode v Pythonu:

```python
import gym
env = gym.make("CartPole-v1")

env.reset()
done = False
total_reward = 0
while not done:
   env.render()
   action = env.action_space.sample()
   observaton, reward, done, info = env.step(action)
   total_reward += reward

print(f"Total reward: {total_reward}")
```

Vsako okolje je dostopno na enak na캜in:
* `env.reset` za캜ne nov poskus
* `env.step` izvede simulacijski korak. Prejme **akcijo** iz **akcijskega prostora** in vrne **opazovanje** (iz opazovalnega prostora), kot tudi nagrado in zastavico za zaklju캜ek.

V zgornjem primeru izvajamo naklju캜ne akcije pri vsakem koraku, zato je 쬴vljenjska doba poskusa zelo kratka:

![neuravnote쬰n cartpole](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-nobalance.gif)

Cilj algoritma RL je trenirati model - tako imenovano **politiko** &pi; - ki bo vrnil akcijo kot odgovor na dano stanje. Politiko lahko obravnavamo tudi kot verjetnostno, npr. za katero koli stanje *s* in akcijo *a* bo vrnila verjetnost &pi;(*a*|*s*), da bi morali izvesti *a* v stanju *s*.

## Algoritem Policy Gradients

Najbolj o캜iten na캜in za modeliranje politike je ustvarjanje nevronske mre쬰, ki bo kot vhod prejela stanja in vrnila ustrezne akcije (ali bolje re캜eno verjetnosti vseh akcij). Na nek na캜in bi bilo to podobno obi캜ajni nalogi klasifikacije, z eno veliko razliko - vnaprej ne vemo, katere akcije bi morali izvesti pri vsakem koraku.

Ideja tukaj je oceniti te verjetnosti. Zgradimo vektor **kumulativnih nagrad**, ki prikazuje na코o skupno nagrado pri vsakem koraku poskusa. Prav tako uporabimo **diskontiranje nagrad** z mno쬰njem zgodnej코ih nagrad z nekim koeficientom &gamma;=0.99, da zmanj코amo vlogo zgodnej코ih nagrad. Nato okrepimo tiste korake vzdol poti poskusa, ki prina코ajo ve캜je nagrade.

> Ve캜 o algoritmu Policy Gradient in njegovem delovanju si oglejte v [primeru zvezka](CartPole-RL-TF.ipynb).

## Algoritem Actor-Critic

Izbolj코ana razli캜ica pristopa Policy Gradients se imenuje **Actor-Critic**. Glavna ideja je, da bi nevronska mre쬬 vrnila dve stvari:

* Politiko, ki dolo캜a, katero akcijo izvesti. Ta del se imenuje **actor**.
* Oceno skupne nagrade, ki jo lahko pri캜akujemo v tem stanju - ta del se imenuje **critic**.

Na nek na캜in ta arhitektura spominja na [GAN](../../4-ComputerVision/10-GANs/README.md), kjer imamo dve mre쬴, ki se trenirata ena proti drugi. V modelu actor-critic actor predlaga akcijo, ki jo moramo izvesti, critic pa posku코a biti kriti캜en in oceniti rezultat. Vendar je na코 cilj trenirati ti mre쬴 v harmoniji.

Ker poznamo tako resni캜ne kumulativne nagrade kot rezultate, ki jih med poskusom vrne critic, je razmeroma enostavno zgraditi funkcijo izgube, ki bo minimizirala razliko med njima. To nam daje **critic loss**. **Actor loss** lahko izra캜unamo z uporabo istega pristopa kot pri algoritmu Policy Gradient.

Po izvedbi enega od teh algoritmov lahko pri캜akujemo, da se bo na코 CartPole obna코al takole:

![uravnote쬰n cartpole](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-balance.gif)

## 九꽲잺 Vaje: Policy Gradients in Actor-Critic RL

Nadaljujte z u캜enjem v naslednjih zvezkih:

* [RL v TensorFlow](CartPole-RL-TF.ipynb)
* [RL v PyTorch](CartPole-RL-PyTorch.ipynb)

## Drugi RL nalogi

Okrepljeno u캜enje je danes hitro rasto캜e podro캜je raziskav. Nekateri zanimivi primeri okrepljenega u캜enja so:

* U캜enje ra캜unalnika igranja **Atari iger**. Izziv pri tem problemu je, da nimamo preprostega stanja, predstavljenega kot vektor, temve캜 posnetek zaslona - in moramo uporabiti CNN za pretvorbo slike zaslona v vektorsko zna캜ilnost ali za pridobivanje informacij o nagradi. Atari igre so na voljo v Gym.
* U캜enje ra캜unalnika igranja namiznih iger, kot sta 코ah in Go. Nedavno so vrhunski programi, kot je **Alpha Zero**, trenirani od za캜etka z dvema agentoma, ki igrata drug proti drugemu in se izbolj코ujeta pri vsakem koraku.
* V industriji se RL uporablja za ustvarjanje sistemov za nadzor iz simulacij. Storitev, imenovana [Bonsai](https://azure.microsoft.com/services/project-bonsai/?WT.mc_id=academic-77998-cacaste), je posebej zasnovana za to.

## Zaklju캜ek

Sedaj smo se nau캜ili, kako trenirati agente, da dose쬰jo dobre rezultate zgolj z zagotavljanjem funkcije nagrajevanja, ki definira 쬰leno stanje igre, in z omogo캜anjem inteligentnega raziskovanja iskalnega prostora. Uspe코no smo preizkusili dva algoritma in dosegli dober rezultat v razmeroma kratkem 캜asu. Vendar je to 코ele za캜etek va코e poti v RL, zato bi morali razmisliti o lo캜enem te캜aju, 캜e 쬰lite raziskati globlje.

## 游 Izziv

Raziskujte aplikacije, navedene v razdelku 'Drugi RL nalogi', in poskusite implementirati eno!

## [Naknadni kviz](https://ff-quizzes.netlify.app/en/ai/quiz/44)

## Pregled in samostojno u캜enje

Ve캜 o klasi캜nem okrepljenem u캜enju si preberite v na코em [u캜nem na캜rtu za za캜etnike strojnega u캜enja](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/README.md).

Oglejte si [odli캜en video](https://www.youtube.com/watch?v=qv6UVOQ0F44), ki govori o tem, kako se ra캜unalnik lahko nau캜i igrati Super Mario.

## Naloga: [Trenirajte Mountain Car](lab/README.md)

Va코 cilj pri tej nalogi bo trenirati drugo Gym okolje - [Mountain Car](https://www.gymlibrary.ml/environments/classic_control/mountain_car/).

---

