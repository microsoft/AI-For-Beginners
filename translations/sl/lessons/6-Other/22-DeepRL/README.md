<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "dbacf9b1915612981d76059678e563e5",
  "translation_date": "2025-08-25T23:33:25+00:00",
  "source_file": "lessons/6-Other/22-DeepRL/README.md",
  "language_code": "sl"
}
-->
# Globoko okrepitevno uÄenje

Okrepitevno uÄenje (RL) velja za enega osnovnih paradigm strojnega uÄenja, poleg nadzorovanega in nenadzorovanega uÄenja. Medtem ko se pri nadzorovanem uÄenju zanaÅ¡amo na podatkovne nabore z znanimi rezultati, je RL osnovano na **uÄenju skozi izkuÅ¡nje**. Na primer, ko prviÄ vidimo raÄunalniÅ¡ko igro, zaÄnemo igrati, Äetudi ne poznamo pravil, in kmalu izboljÅ¡amo svoje sposobnosti zgolj z igranjem in prilagajanjem svojega vedenja.

## [Predhodni kviz](https://ff-quizzes.netlify.app/en/ai/quiz/43)

Za izvajanje RL potrebujemo:

* **Okolje** ali **simulator**, ki doloÄa pravila igre. V simulatorju moramo biti sposobni izvajati poskuse in opazovati rezultate.
* **Funkcijo nagrajevanja**, ki kaÅ¾e, kako uspeÅ¡en je bil naÅ¡ poskus. V primeru uÄenja igranja raÄunalniÅ¡ke igre bi bila nagrada naÅ¡ konÄni rezultat.

Na podlagi funkcije nagrajevanja moramo prilagoditi svoje vedenje in izboljÅ¡ati svoje sposobnosti, da bomo naslednjiÄ igrali bolje. Glavna razlika med drugimi vrstami strojnega uÄenja in RL je v tem, da pri RL obiÄajno ne vemo, ali smo zmagali ali izgubili, dokler igre ne konÄamo. Zato ne moremo reÄi, ali je bila doloÄena poteza sama po sebi dobra ali ne â€“ nagrado prejmemo Å¡ele na koncu igre.

Med RL obiÄajno izvedemo veliko poskusov. Pri vsakem poskusu moramo uravnoteÅ¾iti med sledenjem optimalni strategiji, ki smo jo do sedaj osvojili (**izkoriÅ¡Äanje**), in raziskovanjem novih moÅ¾nih stanj (**raziskovanje**).

## OpenAI Gym

OdliÄno orodje za RL je [OpenAI Gym](https://gym.openai.com/) - **simulacijsko okolje**, ki lahko simulira Å¡tevilna razliÄna okolja, od Atari iger do fizike ravnoteÅ¾ja droga. To je eno najbolj priljubljenih simulacijskih okolij za treniranje algoritmov okrepitevnega uÄenja, ki ga vzdrÅ¾uje [OpenAI](https://openai.com/).

> **Note**: Vsa okolja, ki jih ponuja OpenAI Gym, si lahko ogledate [tukaj](https://gym.openai.com/envs/#classic_control).

## UravnoteÅ¾enje CartPole

Verjetno ste Å¾e videli sodobne naprave za ravnoteÅ¾je, kot so *Segway* ali *Gyroscooterji*. Te naprave se lahko samodejno uravnoteÅ¾ijo z nastavljanjem koles glede na signal iz pospeÅ¡komera ali Å¾iroskopa. V tem poglavju se bomo nauÄili reÅ¡iti podoben problem â€“ uravnoteÅ¾enje droga. To je podobno situaciji, ko cirkuÅ¡ki artist uravnava drog na svoji roki â€“ vendar to uravnoteÅ¾enje poteka le v eni dimenziji.

Poenostavljena razliÄica uravnoteÅ¾enja je znana kot problem **CartPole**. V svetu CartPole imamo vodoravno drsno ploÅ¡Äo, ki se lahko premika levo ali desno, cilj pa je uravnoteÅ¾iti navpiÄni drog na vrhu drsne ploÅ¡Äe med njenim premikanjem.

<img alt="a cartpole" src="images/cartpole.png" width="200"/>

Za ustvarjanje in uporabo tega okolja potrebujemo nekaj vrstic kode v Pythonu:

```python
import gym
env = gym.make("CartPole-v1")

env.reset()
done = False
total_reward = 0
while not done:
   env.render()
   action = env.action_space.sample()
   observaton, reward, done, info = env.step(action)
   total_reward += reward

print(f"Total reward: {total_reward}")
```

Vsako okolje je dostopno na enak naÄin:
* `env.reset` zaÄne nov poskus
* `env.step` izvede simulacijski korak. Prejme **akcijo** iz **akcijskega prostora** in vrne **opazovanje** (iz opazovalnega prostora), kot tudi nagrado in zastavico za zakljuÄek.

V zgornjem primeru izvajamo nakljuÄne akcije pri vsakem koraku, zato je Å¾ivljenjska doba poskusa zelo kratka:

![non-balancing cartpole](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-nobalance.gif)

Cilj RL algoritma je trenirati model â€“ tako imenovano **politiko** Ï€ â€“ ki bo vrnila akcijo kot odgovor na dano stanje. Politiko lahko obravnavamo tudi kot verjetnostno, npr. za katero koli stanje *s* in akcijo *a* bo vrnila verjetnost Ï€(*a*|*s*), da izvedemo *a* v stanju *s*.

## Algoritem Policy Gradients

Najbolj oÄiten naÄin za modeliranje politike je ustvarjanje nevronske mreÅ¾e, ki bo kot vhod prejela stanja in vrnila ustrezne akcije (oziroma verjetnosti vseh akcij). Na nek naÄin je to podobno obiÄajni nalogi klasifikacije, z eno pomembno razliko â€“ vnaprej ne vemo, katere akcije bi morali izvesti v vsakem koraku.

Ideja tukaj je oceniti te verjetnosti. Zgradimo vektor **kumulativnih nagrad**, ki prikazuje naÅ¡o skupno nagrado v vsakem koraku poskusa. Prav tako uporabimo **diskontiranje nagrad** z mnoÅ¾enjem zgodnejÅ¡ih nagrad z nekim koeficientom Î³=0.99, da zmanjÅ¡amo vpliv zgodnejÅ¡ih nagrad. Nato okrepimo tiste korake na poti poskusa, ki prinaÅ¡ajo veÄje nagrade.

> VeÄ o algoritmu Policy Gradient in njegovem delovanju si lahko ogledate v [primeru v beleÅ¾nici](../../../../../lessons/6-Other/22-DeepRL/CartPole-RL-TF.ipynb).

## Algoritem Actor-Critic

IzboljÅ¡ana razliÄica pristopa Policy Gradients se imenuje **Actor-Critic**. Glavna ideja je, da bi nevronska mreÅ¾a vrnila dve stvari:

* Politiko, ki doloÄa, katero akcijo izvesti. Ta del se imenuje **actor**.
* Oceno skupne nagrade, ki jo lahko priÄakujemo v tem stanju â€“ ta del se imenuje **critic**.

Na nek naÄin ta arhitektura spominja na [GAN](../../4-ComputerVision/10-GANs/README.md), kjer imamo dve mreÅ¾i, ki se trenirata druga proti drugi. V modelu actor-critic actor predlaga akcijo, ki jo moramo izvesti, critic pa poskuÅ¡a biti kritiÄen in oceniti rezultat. NaÅ¡ cilj pa je, da obe mreÅ¾i treniramo usklajeno.

Ker poznamo tako realne kumulativne nagrade kot rezultate, ki jih med poskusom vrne critic, je relativno enostavno zgraditi funkcijo izgube, ki bo minimizirala razliko med njima. To nam daje **izgubo critica**. **Izgubo actorja** lahko izraÄunamo z enakim pristopom kot pri algoritmu Policy Gradient.

Po zagonu enega od teh algoritmov lahko priÄakujemo, da se bo naÅ¡ CartPole obnaÅ¡al takole:

![a balancing cartpole](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-balance.gif)

## âœï¸ Vaje: Policy Gradients in Actor-Critic RL

Nadaljujte z uÄenjem v naslednjih beleÅ¾nicah:

* [RL v TensorFlow](../../../../../lessons/6-Other/22-DeepRL/CartPole-RL-TF.ipynb)
* [RL v PyTorch](../../../../../lessons/6-Other/22-DeepRL/CartPole-RL-PyTorch.ipynb)

## Drugi RL nalogi

Okrepitevno uÄenje je danes hitro rastoÄe podroÄje raziskav. Nekateri zanimivi primeri uporabe okrepitevnega uÄenja so:

* UÄenje raÄunalnika igranja **Atari iger**. Izziv pri tem problemu je, da nimamo preprostega stanja, predstavljenega kot vektor, temveÄ posnetek zaslona â€“ in moramo uporabiti CNN za pretvorbo slike zaslona v vektorsko predstavitev ali za pridobivanje informacij o nagradi. Atari igre so na voljo v Gym.
* UÄenje raÄunalnika igranja druÅ¾abnih iger, kot sta Å¡ah in go. Nedavno so bili programi, kot je **Alpha Zero**, trenirani od zaÄetka z dvema agentoma, ki igrata drug proti drugemu in se izboljÅ¡ujeta pri vsakem koraku.
* V industriji se RL uporablja za ustvarjanje sistemov za nadzor iz simulacij. Storitev, imenovana [Bonsai](https://azure.microsoft.com/services/project-bonsai/?WT.mc_id=academic-77998-cacaste), je posebej zasnovana za to.

## ZakljuÄek

Zdaj smo se nauÄili, kako trenirati agente, da doseÅ¾ejo dobre rezultate zgolj z zagotavljanjem funkcije nagrajevanja, ki definira Å¾eleno stanje igre, in z omogoÄanjem inteligentnega raziskovanja iskalnega prostora. UspeÅ¡no smo preizkusili dva algoritma in dosegli dober rezultat v relativno kratkem Äasu. Vendar je to Å¡ele zaÄetek vaÅ¡e poti v RL, zato bi morali razmisliti o udeleÅ¾bi na loÄenem teÄaju, Äe Å¾elite raziskati to podroÄje bolj poglobljeno.

## ğŸš€ Izziv

Raziskujte aplikacije, navedene v razdelku 'Drugi RL nalogi', in poskusite implementirati eno!

## [Kviz po predavanju](https://ff-quizzes.netlify.app/en/ai/quiz/44)

## Pregled in samostojno uÄenje

VeÄ o klasiÄnem okrepitevnem uÄenju se nauÄite v naÅ¡em [uÄnem naÄrtu za zaÄetnike strojnega uÄenja](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/README.md).

Oglejte si [ta odliÄen video](https://www.youtube.com/watch?v=qv6UVOQ0F44), ki govori o tem, kako se raÄunalnik lahko nauÄi igrati Super Mario.

## Naloga: [Trenirajte Mountain Car](lab/README.md)

VaÅ¡ cilj pri tej nalogi je trenirati drugo Gym okolje â€“ [Mountain Car](https://www.gymlibrary.ml/environments/classic_control/mountain_car/).

**Omejitev odgovornosti**:  
Ta dokument je bil preveden z uporabo storitve AI za prevajanje [Co-op Translator](https://github.com/Azure/co-op-translator). ÄŒeprav si prizadevamo za natanÄnost, vas prosimo, da upoÅ¡tevate, da lahko avtomatizirani prevodi vsebujejo napake ali netoÄnosti. Izvirni dokument v njegovem maternem jeziku je treba obravnavati kot avtoritativni vir. Za kljuÄne informacije priporoÄamo profesionalni ÄloveÅ¡ki prevod. Ne prevzemamo odgovornosti za morebitna nesporazumevanja ali napaÄne razlage, ki izhajajo iz uporabe tega prevoda.