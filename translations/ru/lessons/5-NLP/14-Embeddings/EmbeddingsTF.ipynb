{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Встраивания\n",
    "\n",
    "В предыдущем примере мы работали с высокоразмерными векторами мешка слов длиной `vocab_size` и явно преобразовывали низкоразмерные вектора позиционного представления в разреженное представление с одним активным элементом. Такое представление с одним активным элементом неэффективно с точки зрения памяти. Кроме того, каждое слово рассматривается независимо от других, поэтому закодированные таким образом вектора не отражают семантические сходства между словами.\n",
    "\n",
    "В этом разделе мы продолжим изучать набор данных **News AG**. Для начала загрузим данные и возьмем некоторые определения из предыдущего раздела.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "\n",
    "ds_train, ds_test = tfds.load('ag_news_subset').values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Что такое эмбеддинг?\n",
    "\n",
    "Идея **эмбеддинга** заключается в представлении слов с помощью низкоразмерных плотных векторов, которые отражают семантическое значение слова. Позже мы обсудим, как создавать осмысленные эмбеддинги слов, но пока просто будем рассматривать эмбеддинги как способ уменьшения размерности вектора слова.\n",
    "\n",
    "Таким образом, слой эмбеддинга принимает слово на вход и выдает выходной вектор заданного размера `embedding_size`. В некотором смысле, он очень похож на слой `Dense`, но вместо того, чтобы принимать вектор в формате one-hot encoding, он может принимать номер слова.\n",
    "\n",
    "Используя слой эмбеддинга в качестве первого слоя нашей сети, мы можем перейти от модели \"мешка слов\" к модели **мешка эмбеддингов**, где сначала каждое слово в тексте преобразуется в соответствующий эмбеддинг, а затем вычисляется некоторая агрегирующая функция для всех этих эмбеддингов, например, `sum`, `average` или `max`.\n",
    "\n",
    "![Изображение, показывающее классификатор с использованием эмбеддингов для пяти слов последовательности.](../../../../../translated_images/ru/embedding-classifier-example.b77f021a7ee67eee.webp)\n",
    "\n",
    "Наша нейронная сеть-классификатор состоит из следующих слоев:\n",
    "\n",
    "* Слой `TextVectorization`, который принимает строку на вход и преобразует ее в тензор с номерами токенов. Мы зададим разумный размер словаря `vocab_size` и будем игнорировать менее часто используемые слова. Входная форма будет равна 1, а выходная форма — $n$, так как мы получим $n$ токенов в результате, каждый из которых содержит числа от 0 до `vocab_size`.\n",
    "* Слой `Embedding`, который принимает $n$ чисел и преобразует каждое число в плотный вектор заданной длины (в нашем примере 100). Таким образом, входной тензор формы $n$ будет преобразован в тензор формы $n\\times 100$.\n",
    "* Агрегирующий слой, который вычисляет среднее значение этого тензора вдоль первой оси, то есть вычисляет среднее всех $n$ входных тензоров, соответствующих разным словам. Для реализации этого слоя мы будем использовать слой `Lambda` и передадим в него функцию для вычисления среднего. Выходная форма будет равна 100, и это будет численное представление всей входной последовательности.\n",
    "* Финальный линейный классификатор `Dense`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization (TextVec  (None, None)             0         \n",
      " torization)                                                     \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, None, 100)         3000000   \n",
      "                                                                 \n",
      " lambda (Lambda)             (None, 100)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4)                 404       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,000,404\n",
      "Trainable params: 3,000,404\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 30000\n",
    "batch_size = 128\n",
    "\n",
    "vectorizer = keras.layers.experimental.preprocessing.TextVectorization(max_tokens=vocab_size,input_shape=(1,))\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    vectorizer,    \n",
    "    keras.layers.Embedding(vocab_size,100),\n",
    "    keras.layers.Lambda(lambda x: tf.reduce_mean(x,axis=1)),\n",
    "    keras.layers.Dense(4, activation='softmax')\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В выводе `summary`, в колонке **output shape**, первое измерение тензора `None` соответствует размеру минибатча, а второе — длине последовательности токенов. Все последовательности токенов в минибатче имеют разные длины. О том, как с этим справляться, мы поговорим в следующем разделе.\n",
    "\n",
    "Теперь давайте обучим сеть:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training vectorizer\n",
      "938/938 [==============================] - 20s 20ms/step - loss: 0.7891 - acc: 0.8155 - val_loss: 0.4470 - val_acc: 0.8642\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22255515100>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_text(x):\n",
    "    return x['title']+' '+x['description']\n",
    "\n",
    "def tupelize(x):\n",
    "    return (extract_text(x),x['label'])\n",
    "\n",
    "print(\"Training vectorizer\")\n",
    "vectorizer.adapt(ds_train.take(500).map(extract_text))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'])\n",
    "model.fit(ds_train.map(tupelize).batch(batch_size),validation_data=ds_test.map(tupelize).batch(batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "> **Примечание**: мы создаем векторизатор на основе подмножества данных. Это делается для ускорения процесса, и это может привести к ситуации, когда не все токены из нашего текста присутствуют в словаре. В этом случае эти токены будут игнорироваться, что может немного снизить точность. Однако в реальной жизни подмножество текста часто дает хорошую оценку словаря.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Работа с переменными размерами последовательностей\n",
    "\n",
    "Давайте разберемся, как происходит обучение на минибатчах. В приведенном выше примере входной тензор имеет размерность 1, и мы используем минибатчи длиной 128, так что фактический размер тензора составляет $128 \\times 1$. Однако количество токенов в каждом предложении различается. Если мы применим слой `TextVectorization` к одному входу, количество возвращаемых токенов будет различным, в зависимости от того, как текст был токенизирован:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 1 45], shape=(2,), dtype=int64)\n",
      "tf.Tensor([ 112 1271    1    3 1747  158], shape=(6,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer('Hello, world!'))\n",
    "print(vectorizer('I am glad to meet you!'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Однако, когда мы применяем векторизатор к нескольким последовательностям, он должен создавать тензор прямоугольной формы, поэтому он заполняет неиспользуемые элементы токеном PAD (который в нашем случае равен нулю):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 6), dtype=int64, numpy=\n",
       "array([[   1,   45,    0,    0,    0,    0],\n",
       "       [ 112, 1271,    1,    3, 1747,  158]], dtype=int64)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer(['Hello, world!','I am glad to meet you!'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь мы можем видеть встраивания:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 1.53059261e-02,  6.80514947e-02,  3.14026810e-02, ...,\n",
       "         -8.92002955e-02,  1.52911525e-04, -5.65562584e-02],\n",
       "        [ 2.57456154e-01,  2.79364467e-01, -2.03605562e-01, ...,\n",
       "         -2.07474351e-01,  8.31158683e-02, -2.03911960e-01],\n",
       "        [ 3.98201384e-02, -8.03454965e-03,  2.39790026e-02, ...,\n",
       "         -7.18549127e-04,  2.66963355e-02, -4.30646613e-02],\n",
       "        [ 3.98201384e-02, -8.03454965e-03,  2.39790026e-02, ...,\n",
       "         -7.18549127e-04,  2.66963355e-02, -4.30646613e-02],\n",
       "        [ 3.98201384e-02, -8.03454965e-03,  2.39790026e-02, ...,\n",
       "         -7.18549127e-04,  2.66963355e-02, -4.30646613e-02],\n",
       "        [ 3.98201384e-02, -8.03454965e-03,  2.39790026e-02, ...,\n",
       "         -7.18549127e-04,  2.66963355e-02, -4.30646613e-02]],\n",
       "\n",
       "       [[ 1.89674050e-01,  2.61548996e-01, -3.67433839e-02, ...,\n",
       "         -2.07366899e-01, -1.05442435e-01, -2.36952081e-01],\n",
       "        [ 6.16133213e-02,  1.80511594e-01,  9.77298319e-02, ...,\n",
       "         -5.46628237e-02, -1.07340455e-01, -1.06589928e-01],\n",
       "        [ 1.53059261e-02,  6.80514947e-02,  3.14026810e-02, ...,\n",
       "         -8.92002955e-02,  1.52911525e-04, -5.65562584e-02],\n",
       "        [-4.84890305e-02, -8.41715634e-02,  1.51529670e-01, ...,\n",
       "          1.28192469e-01, -7.77286515e-02,  1.26041949e-01],\n",
       "        [-4.17212099e-02, -5.60694858e-02,  4.08860669e-02, ...,\n",
       "          8.70475471e-02,  8.92383084e-02,  1.67974353e-01],\n",
       "        [ 2.85779923e-01,  4.57767487e-01,  4.52292450e-02, ...,\n",
       "         -1.97419018e-01, -2.04659685e-01, -2.79758364e-01]]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[1](vectorizer(['Hello, world!','I am glad to meet you!'])).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Примечание**: Чтобы минимизировать количество заполнения, в некоторых случаях имеет смысл отсортировать все последовательности в наборе данных в порядке увеличения длины (или, точнее, количества токенов). Это обеспечит, что каждая минипартия будет содержать последовательности схожей длины.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Семантические эмбеддинги: Word2Vec\n",
    "\n",
    "В нашем предыдущем примере слой эмбеддинга обучался отображать слова в векторные представления, однако эти представления не имели семантического значения. Было бы здорово обучить векторное представление таким образом, чтобы похожие слова или синонимы соответствовали векторам, которые находятся близко друг к другу с точки зрения некоторого векторного расстояния (например, евклидова расстояния).\n",
    "\n",
    "Для этого нам нужно предварительно обучить модель эмбеддинга на большом наборе текстов, используя такую технику, как [Word2Vec](https://en.wikipedia.org/wiki/Word2vec). Она основана на двух основных архитектурах, которые используются для создания распределенного представления слов:\n",
    "\n",
    " - **Непрерывная модель \"мешок слов\"** (Continuous bag-of-words, CBoW), где мы обучаем модель предсказывать слово по окружающему контексту. Учитывая n-грамму $(W_{-2},W_{-1},W_0,W_1,W_2)$, цель модели — предсказать $W_0$ на основе $(W_{-2},W_{-1},W_1,W_2)$.\n",
    " - **Непрерывная модель \"пропускающий грамм\"** (Continuous skip-gram) является противоположностью CBoW. Модель использует окружающее окно контекстных слов для предсказания текущего слова.\n",
    "\n",
    "CBoW работает быстрее, а skip-gram, хотя и медленнее, лучше справляется с представлением редких слов.\n",
    "\n",
    "![Изображение, показывающее алгоритмы CBoW и Skip-Gram для преобразования слов в векторы.](../../../../../translated_images/ru/example-algorithms-for-converting-words-to-vectors.fbe9207a726922f6.webp)\n",
    "\n",
    "Чтобы поэкспериментировать с эмбеддингом Word2Vec, предварительно обученным на наборе данных Google News, мы можем использовать библиотеку **gensim**. Ниже мы находим слова, наиболее похожие на 'neural'.\n",
    "\n",
    "> **Note:** Когда вы впервые создаете векторные представления слов, их загрузка может занять некоторое время!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "w2v = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neuronal -> 0.7804799675941467\n",
      "neurons -> 0.7326500415802002\n",
      "neural_circuits -> 0.7252851724624634\n",
      "neuron -> 0.7174385190010071\n",
      "cortical -> 0.6941086649894714\n",
      "brain_circuitry -> 0.6923246383666992\n",
      "synaptic -> 0.6699118614196777\n",
      "neural_circuitry -> 0.6638563275337219\n",
      "neurochemical -> 0.6555314064025879\n",
      "neuronal_activity -> 0.6531826257705688\n"
     ]
    }
   ],
   "source": [
    "for w,p in w2v.most_similar('neural'):\n",
    "    print(f\"{w} -> {p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы также можем извлечь векторное представление из слова, чтобы использовать его при обучении модели классификации. Представление имеет 300 компонентов, но здесь мы показываем только первые 20 компонентов вектора для ясности:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01226807,  0.06225586,  0.10693359,  0.05810547,  0.23828125,\n",
       "        0.03686523,  0.05151367, -0.20703125,  0.01989746,  0.10058594,\n",
       "       -0.03759766, -0.1015625 , -0.15820312, -0.08105469, -0.0390625 ,\n",
       "       -0.05053711,  0.16015625,  0.2578125 ,  0.10058594, -0.25976562],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v['play'][:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Замечательная особенность семантических встраиваний заключается в том, что можно манипулировать векторным кодированием на основе семантики. Например, мы можем попросить найти слово, чье векторное представление максимально близко к словам *король* и *женщина*, и максимально далеко от слова *мужчина*:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('queen', 0.7118192911148071)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar(positive=['king','woman'],negative=['man'])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Пример выше использует некоторую внутреннюю магию GenSym, но основная логика на самом деле довольно проста. Интересная вещь в эмбеддингах заключается в том, что вы можете выполнять обычные операции с векторами на векторах эмбеддинга, и это будет отражать операции с **значениями** слов. Пример выше можно выразить в терминах операций с векторами: мы вычисляем вектор, соответствующий **KING-MAN+WOMAN** (операции `+` и `-` выполняются на векторных представлениях соответствующих слов), а затем находим ближайшее слово в словаре к этому вектору:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'queen'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the vector corresponding to kind-man+woman\n",
    "qvec = w2v['king']-1.7*w2v['man']+1.7*w2v['woman']\n",
    "# find the index of the closest embedding vector \n",
    "d = np.sum((w2v.vectors-qvec)**2,axis=1)\n",
    "min_idx = np.argmin(d)\n",
    "# find the corresponding word\n",
    "w2v.index_to_key[min_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NOTE**: Мы добавили небольшие коэффициенты к векторам *man* и *woman* — попробуйте убрать их, чтобы увидеть, что произойдет.\n",
    "\n",
    "Чтобы найти ближайший вектор, мы используем инструменты TensorFlow для вычисления вектора расстояний между нашим вектором и всеми векторами в словаре, а затем находим индекс минимального слова с помощью `argmin`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Хотя Word2Vec кажется отличным способом выражения семантики слов, у него есть множество недостатков, включая следующие:\n",
    "\n",
    "* Модели CBoW и skip-gram являются **предсказательными векторами**, и они учитывают только локальный контекст. Word2Vec не использует глобальный контекст.\n",
    "* Word2Vec не учитывает **морфологию** слов, то есть тот факт, что значение слова может зависеть от различных частей слова, таких как корень.\n",
    "\n",
    "**FastText** пытается преодолеть второе ограничение и расширяет Word2Vec, обучая векторные представления для каждого слова и n-грамм символов, найденных внутри каждого слова. Значения этих представлений затем усредняются в один вектор на каждом этапе обучения. Хотя это добавляет значительное количество вычислений на этапе предварительного обучения, это позволяет векторным представлениям слов кодировать информацию о частях слова.\n",
    "\n",
    "Другой метод, **GloVe**, использует иной подход к созданию векторных представлений слов, основанный на факторизации матрицы \"слово-контекст\". Сначала он строит большую матрицу, которая подсчитывает количество появлений слов в различных контекстах, а затем пытается представить эту матрицу в меньших измерениях таким образом, чтобы минимизировать потери при восстановлении.\n",
    "\n",
    "Библиотека gensim поддерживает эти методы создания векторных представлений слов, и вы можете экспериментировать с ними, изменяя код загрузки модели выше.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Использование предварительно обученных эмбеддингов в Keras\n",
    "\n",
    "Мы можем изменить приведённый выше пример, чтобы заполнить матрицу в нашем слое эмбеддинга семантическими эмбеддингами, такими как Word2Vec. Словари предварительно обученного эмбеддинга и текстового корпуса, скорее всего, не будут совпадать, поэтому нам нужно выбрать один из них. Здесь мы рассмотрим два возможных варианта: использование словаря токенизатора и использование словаря из эмбеддингов Word2Vec.\n",
    "\n",
    "### Использование словаря токенизатора\n",
    "\n",
    "При использовании словаря токенизатора некоторые слова из словаря будут иметь соответствующие эмбеддинги Word2Vec, а некоторые будут отсутствовать. Учитывая, что размер нашего словаря равен `vocab_size`, а длина вектора эмбеддинга Word2Vec равна `embed_size`, слой эмбеддинга будет представлен весовой матрицей формы `vocab_size`$\\times$`embed_size`. Мы заполним эту матрицу, проходя по словарю:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding size: 300\n",
      "Populating matrix, this will take some time...Done, found 4551 words, 784 words missing\n"
     ]
    }
   ],
   "source": [
    "embed_size = len(w2v.get_vector('hello'))\n",
    "print(f'Embedding size: {embed_size}')\n",
    "\n",
    "vocab = vectorizer.get_vocabulary()\n",
    "W = np.zeros((vocab_size,embed_size))\n",
    "print('Populating matrix, this will take some time...',end='')\n",
    "found, not_found = 0,0\n",
    "for i,w in enumerate(vocab):\n",
    "    try:\n",
    "        W[i] = w2v.get_vector(w)\n",
    "        found+=1\n",
    "    except:\n",
    "        # W[i] = np.random.normal(0.0,0.3,size=(embed_size,))\n",
    "        not_found+=1\n",
    "\n",
    "print(f\"Done, found {found} words, {not_found} words missing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для слов, которые отсутствуют в словаре Word2Vec, мы можем либо оставить их как нули, либо сгенерировать случайный вектор.\n",
    "\n",
    "Теперь мы можем определить слой эмбеддинга с предварительно обученными весами:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = keras.layers.Embedding(vocab_size,embed_size,weights=[W],trainable=False)\n",
    "model = keras.models.Sequential([\n",
    "    vectorizer, emb,\n",
    "    keras.layers.Lambda(lambda x: tf.reduce_mean(x,axis=1)),\n",
    "    keras.layers.Dense(4, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 10s 10ms/step - loss: 1.1075 - acc: 0.7822 - val_loss: 0.9134 - val_acc: 0.8175\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2220226ef10>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'])\n",
    "model.fit(ds_train.map(tupelize).batch(batch_size),\n",
    "          validation_data=ds_test.map(tupelize).batch(batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note**: Обратите внимание, что мы устанавливаем `trainable=False` при создании `Embedding`, что означает, что слой Embedding не будет переобучаться. Это может немного снизить точность, но ускоряет процесс обучения.\n",
    "\n",
    "### Использование словаря для эмбеддингов\n",
    "\n",
    "Одна из проблем предыдущего подхода заключается в том, что словари, используемые в TextVectorization и Embedding, отличаются. Чтобы решить эту проблему, мы можем воспользоваться одним из следующих решений:\n",
    "* Переобучить модель Word2Vec на нашем словаре.\n",
    "* Загрузить наш набор данных, используя словарь из предварительно обученной модели Word2Vec. Словари, используемые для загрузки набора данных, можно указать во время загрузки.\n",
    "\n",
    "Второй подход кажется проще, поэтому давайте его реализуем. Прежде всего, мы создадим слой `TextVectorization` с указанным словарем, взятым из эмбеддингов Word2Vec:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(w2v.vocab.keys())\n",
    "vectorizer = keras.layers.experimental.preprocessing.TextVectorization(input_shape=(1,))\n",
    "vectorizer.set_vocabulary(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Библиотека word embeddings gensim содержит удобную функцию `get_keras_embeddings`, которая автоматически создаст соответствующий слой embeddings для Keras.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "938/938 [==============================] - 20s 14ms/step - loss: 1.3377 - acc: 0.4978 - val_loss: 1.2995 - val_acc: 0.5647\n",
      "Epoch 2/5\n",
      "938/938 [==============================] - 10s 10ms/step - loss: 1.2587 - acc: 0.5722 - val_loss: 1.2339 - val_acc: 0.5842\n",
      "Epoch 3/5\n",
      "938/938 [==============================] - 10s 10ms/step - loss: 1.1980 - acc: 0.5884 - val_loss: 1.1826 - val_acc: 0.5954\n",
      "Epoch 4/5\n",
      "938/938 [==============================] - 12s 13ms/step - loss: 1.1503 - acc: 0.6002 - val_loss: 1.1417 - val_acc: 0.6018\n",
      "Epoch 5/5\n",
      "938/938 [==============================] - 11s 12ms/step - loss: 1.1120 - acc: 0.6097 - val_loss: 1.1083 - val_acc: 0.6104\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2220ccb81c0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    vectorizer, \n",
    "    w2v.get_keras_embedding(train_embeddings=False),\n",
    "    keras.layers.Lambda(lambda x: tf.reduce_mean(x,axis=1)),\n",
    "    keras.layers.Dense(4, activation='softmax')\n",
    "])\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'])\n",
    "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128),epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Одна из причин, по которой мы не наблюдаем более высокой точности, заключается в том, что некоторые слова из нашего набора данных отсутствуют в предварительно обученном словаре GloVe, и поэтому они фактически игнорируются. Чтобы преодолеть это, мы можем обучить собственные эмбеддинги на основе нашего набора данных.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Контекстуальные эмбеддинги\n",
    "\n",
    "Одним из ключевых ограничений традиционных предварительно обученных представлений эмбеддингов, таких как Word2Vec, является то, что, хотя они и могут захватывать некоторое значение слова, они не способны различать его разные значения. Это может вызывать проблемы в последующих моделях.\n",
    "\n",
    "Например, слово «play» имеет разные значения в следующих предложениях:\n",
    "- Я сходил на **пьесу** в театр.\n",
    "- Джон хочет **поиграть** со своими друзьями.\n",
    "\n",
    "Предварительно обученные эмбеддинги, о которых мы говорили, представляют оба значения слова «play» одним и тем же эмбеддингом. Чтобы преодолеть это ограничение, необходимо строить эмбеддинги на основе **языковой модели**, которая обучена на большом корпусе текста и *знает*, как слова могут сочетаться в разных контекстах. Обсуждение контекстуальных эмбеддингов выходит за рамки данного урока, но мы вернемся к ним, когда будем говорить о языковых моделях в следующем разделе.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Отказ от ответственности**:  \nЭтот документ был переведен с использованием сервиса автоматического перевода [Co-op Translator](https://github.com/Azure/co-op-translator). Несмотря на наши усилия обеспечить точность, автоматические переводы могут содержать ошибки или неточности. Оригинальный документ на его исходном языке следует считать авторитетным источником. Для получения критически важной информации рекомендуется профессиональный перевод человеком. Мы не несем ответственности за любые недоразумения или неправильные интерпретации, возникшие в результате использования данного перевода.\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0cb620c6d4b9f7a635928804c26cf22403d89d98d79684e4529119355ee6d5a5"
  },
  "kernel_info": {
   "name": "conda-env-py37_tensorflow-py"
  },
  "kernelspec": {
   "display_name": "py37_tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "coopTranslator": {
   "original_hash": "b859482be7f61d1eadc2c6a2720a37e4",
   "translation_date": "2025-08-28T12:25:11+00:00",
   "source_file": "lessons/5-NLP/14-Embeddings/EmbeddingsTF.ipynb",
   "language_code": "ru"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}