# Обучение модели Skip-Gram

Лабораторная работа из [AI for Beginners Curriculum](https://github.com/microsoft/ai-for-beginners).

## Задание

В этой лабораторной работе вам предлагается обучить модель Word2Vec, используя технику Skip-Gram. Обучите сеть с встраиванием (embedding), чтобы она предсказывала соседние слова в окне Skip-Gram шириной $N$ токенов. Вы можете использовать [код из этого урока](../../../../../../lessons/5-NLP/15-LanguageModeling/CBoW-TF.ipynb) и немного его модифицировать.

## Датасет

Вы можете использовать любую книгу. Множество бесплатных текстов можно найти на [Project Gutenberg](https://www.gutenberg.org/), например, вот прямая ссылка на [«Приключения Алисы в Стране чудес»](https://www.gutenberg.org/files/11/11-0.txt) Льюиса Кэрролла. Или вы можете использовать пьесы Шекспира, которые можно получить с помощью следующего кода:

```python
path_to_file = tf.keras.utils.get_file(
   'shakespeare.txt', 
   'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')
text = open(path_to_file, 'rb').read().decode(encoding='utf-8')
```

## Исследуйте!

Если у вас есть время и желание углубиться в тему, попробуйте исследовать несколько вопросов:

* Как размер встраивания (embedding size) влияет на результаты?
* Как разные стили текста влияют на результат?
* Возьмите несколько очень разных типов слов и их синонимов, получите их векторные представления, примените PCA для уменьшения размерности до 2 и изобразите их в 2D-пространстве. Видите ли вы какие-либо закономерности?

**Отказ от ответственности**:  
Этот документ был переведен с использованием сервиса автоматического перевода [Co-op Translator](https://github.com/Azure/co-op-translator). Хотя мы стремимся к точности, пожалуйста, учитывайте, что автоматические переводы могут содержать ошибки или неточности. Оригинальный документ на его родном языке следует считать авторитетным источником. Для получения критически важной информации рекомендуется профессиональный перевод человеком. Мы не несем ответственности за любые недоразумения или неправильные интерпретации, возникающие в результате использования данного перевода.