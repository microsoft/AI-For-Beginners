# рдзреНрдпрд╛рди рдореЗрдХрд╛рдирд┐рдЬреНрдо рд░ рдЯреНрд░рд╛рдиреНрд╕рдлрд░реНрдорд░рд╣рд░реВ

## [рдкрд╛рда рдЕрдШрд┐ рдХреНрд╡рд┐рдЬ](https://ff-quizzes.netlify.app/en/ai/quiz/35)

NLP рдХреНрд╖реЗрддреНрд░рдорд╛ рд╕рдмреИрднрдиреНрджрд╛ рдорд╣рддреНрддреНрд╡рдкреВрд░реНрдг рд╕рдорд╕реНрдпрд╛рд╣рд░реВ рдордзреНрдпреЗ рдПрдХ рд╣реЛ **рдореЗрд╕рд┐рди рдЕрдиреБрд╡рд╛рдж**, рдЬреБрди Google Translate рдЬрд╕реНрддрд╛ рдЙрдкрдХрд░рдгрд╣рд░реВрдХреЛ рдЖрдзрд╛рд░рднреВрдд рдХрд╛рд░реНрдп рд╣реЛред рдпрд╕ рдЦрдгреНрдбрдорд╛, рд╣рд╛рдореА рдореЗрд╕рд┐рди рдЕрдиреБрд╡рд╛рджрдорд╛ рдХреЗрдиреНрджреНрд░рд┐рдд рд╣реБрдиреЗрдЫреМрдВ, рд╡рд╛ рд╕рд╛рдорд╛рдиреНрдп рд░реВрдкрдорд╛, рдХреБрдиреИ рдкрдирд┐ *sequence-to-sequence* рдХрд╛рд░реНрдпрдорд╛ (рдЬрд╕рд▓рд╛рдИ **sentence transduction** рдкрдирд┐ рднрдирд┐рдиреНрдЫ)ред

RNNs рдкреНрд░рдпреЛрдЧ рдЧрд░реНрджрд╛, sequence-to-sequence рджреБрдИ рдкреБрдирд░рд╛рд╡рд░реНрддреА рдиреЗрдЯрд╡рд░реНрдХрд╣рд░реВрджреНрд╡рд╛рд░рд╛ рдХрд╛рд░реНрдпрд╛рдиреНрд╡рдпрди рдЧрд░рд┐рдиреНрдЫ, рдЬрд╣рд╛рдБ рдПрдЙрдЯрд╛ рдиреЗрдЯрд╡рд░реНрдХ, **encoder**, рдЗрдирдкреБрдЯ рдЕрдиреБрдХреНрд░рдорд▓рд╛рдИ рд▓реБрдХрд╛рдЗрдПрдХреЛ рдЕрд╡рд╕реНрдерд╛рдорд╛ рд╕рдВрдХреБрдЪрд┐рдд рдЧрд░реНрдЫ, рдЬрдмрдХрд┐ рдЕрд░реНрдХреЛ рдиреЗрдЯрд╡рд░реНрдХ, **decoder**, рдпрд╕ рд▓реБрдХрд╛рдЗрдПрдХреЛ рдЕрд╡рд╕реНрдерд╛рд▓рд╛рдИ рдЕрдиреБрд╡рд╛рджрд┐рдд рдкрд░рд┐рдгрд╛рдордорд╛ рдЕрдирд░реЛрд▓ рдЧрд░реНрдЫред рдпрд╕ рджреГрд╖реНрдЯрд┐рдХреЛрдгрд╕рдБрдЧ рдХреЗрд╣реА рд╕рдорд╕реНрдпрд╛рд╣рд░реВ рдЫрдиреН:

* encoder рдиреЗрдЯрд╡рд░реНрдХрдХреЛ рдЕрдиреНрддрд┐рдо рдЕрд╡рд╕реНрдерд╛рд▓реЗ рд╡рд╛рдХреНрдпрдХреЛ рд╕реБрд░реБрд╡рд╛рдд рд╕рдореНрдЭрди рдЧрд╛рд╣реНрд░реЛ рд╣реБрдиреНрдЫ, рдЬрд╕рд▓реЗ рд▓рд╛рдореЛ рд╡рд╛рдХреНрдпрд╣рд░реВрдХреЛ рд▓рд╛рдЧрд┐ рдореЛрдбреЗрд▓рдХреЛ рдЧреБрдгрд╕реНрддрд░ рдХрдордЬреЛрд░ рдмрдирд╛рдЙрдБрдЫред
* рдЕрдиреБрдХреНрд░рдордХрд╛ рд╕рдмреИ рд╢рдмреНрджрд╣рд░реВрд▓реЗ рдирддрд┐рдЬрд╛рдорд╛ рд╕рдорд╛рди рдкреНрд░рднрд╛рд╡ рдкрд╛рд░реНрдЫрдиреНред рддрд░ рд╡рд╛рд╕реНрддрд╡рд┐рдХрддрд╛рдорд╛, рдЗрдирдкреБрдЯ рдЕрдиреБрдХреНрд░рдордХрд╛ рд╡рд┐рд╢рд┐рд╖реНрдЯ рд╢рдмреНрджрд╣рд░реВрд▓реЗ рдЕрдиреНрдп рд╢рдмреНрджрд╣рд░реВрдХреЛ рддреБрд▓рдирд╛рдорд╛ рдЕрдиреБрдХреНрд░рдорд┐рдХ рдЖрдЙрдЯрдкреБрдЯрдорд╛ рдмрдвреА рдкреНрд░рднрд╛рд╡ рдкрд╛рд░реНрдЫрдиреНред

**рдзреНрдпрд╛рди рдореЗрдХрд╛рдирд┐рдЬреНрдорд╣рд░реВ** рдкреНрд░рддреНрдпреЗрдХ рдЗрдирдкреБрдЯ рднреЗрдХреНрдЯрд░рдХреЛ рд╕рдиреНрджрд░реНрднрд╛рддреНрдордХ рдкреНрд░рднрд╛рд╡рд▓рд╛рдИ рдкреНрд░рддреНрдпреЗрдХ RNN рдХреЛ рдЖрдЙрдЯрдкреБрдЯ рднрд╡рд┐рд╖реНрдпрд╡рд╛рдгреАрдорд╛ рддреМрд▓ рджрд┐рдиреЗ рдорд╛рдзреНрдпрдо рдкреНрд░рджрд╛рди рдЧрд░реНрдЫред рдпрд╕рд▓рд╛рдИ рдХрд╛рд░реНрдпрд╛рдиреНрд╡рдпрди рдЧрд░реНрдиреЗ рддрд░рд┐рдХрд╛ рднрдиреЗрдХреЛ рдЗрдирдкреБрдЯ RNN рд░ рдЖрдЙрдЯрдкреБрдЯ RNN рдХрд╛ рдмреАрдЪрдорд╛ рдЫреЛрдЯреЛ рдорд╛рд░реНрдЧрд╣рд░реВ рд╕рд┐рд░реНрдЬрдирд╛ рдЧрд░реНрдиреБ рд╣реЛред рдпрд╕ рдкреНрд░рдХрд╛рд░, рдЖрдЙрдЯрдкреБрдЯ рдкреНрд░рддреАрдХ y<sub>t</sub> рдЙрддреНрдкрдиреНрди рдЧрд░реНрджрд╛, рд╣рд╛рдореА рд╕рдмреИ рдЗрдирдкреБрдЯ рд▓реБрдХрд╛рдЗрдПрдХреЛ рдЕрд╡рд╕реНрдерд╛рд╣рд░реВ h<sub>i</sub> рд▓рд╛рдИ рд╡рд┐рднрд┐рдиреНрди рддреМрд▓ рдЧреБрдгрд╛рдВрдХрд╣рд░реВ &alpha;<sub>t,i</sub> рд╕рд╣рд┐рдд рд╡рд┐рдЪрд╛рд░ рдЧрд░реНрдиреЗрдЫреМрдВред

![Image showing an encoder/decoder model with an additive attention layer](../../../../../translated_images/ne/encoder-decoder-attention.7a726296894fb567.webp)

> [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) рдорд╛ additive attention рдореЗрдХрд╛рдирд┐рдЬреНрдорд╕рд╣рд┐рддрдХреЛ encoder-decoder рдореЛрдбреЗрд▓, [рдпреЛ рдмреНрд▓рдЧ рдкреЛрд╕реНрдЯ](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html) рдмрд╛рдЯ рдЙрджреНрдзреГрддред

рдзреНрдпрд╛рди рдореНрдпрд╛рдЯреНрд░рд┐рдХреНрд╕ {&alpha;<sub>i,j</sub>} рд▓реЗ рдЗрдирдкреБрдЯ рдЕрдиреБрдХреНрд░рдордХрд╛ рдирд┐рд╢реНрдЪрд┐рдд рд╢рдмреНрджрд╣рд░реВрд▓реЗ рдЖрдЙрдЯрдкреБрдЯ рдЕрдиреБрдХреНрд░рдордХреЛ рдХреБрдиреИ рд╢рдмреНрджрдХреЛ рдЙрддреНрдкрддреНрддрд┐рдорд╛ рдХрддрд┐ рднреВрдорд┐рдХрд╛ рдЦреЗрд▓реНрдЫрдиреН рднрдиреНрдиреЗ рдкреНрд░рддрд┐рдирд┐рдзрд┐рддреНрд╡ рдЧрд░реНрджрдЫред рддрд▓ рдпрд╕реНрддреЛ рдореНрдпрд╛рдЯреНрд░рд┐рдХреНрд╕рдХреЛ рдЙрджрд╛рд╣рд░рдг рдЫ:

![Image showing a sample alignment found by RNNsearch-50, taken from Bahdanau - arviz.org](../../../../../translated_images/ne/bahdanau-fig3.09ba2d37f202a6af.webp)

> [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) рдмрд╛рдЯ рдЪрд┐рддреНрд░ (Fig.3)

рдзреНрдпрд╛рди рдореЗрдХрд╛рдирд┐рдЬреНрдорд╣рд░реВ NLP рдорд╛ рд╣рд╛рд▓рдХреЛ рд╡рд╛ рдирд┐рдХрдЯ рднрд╡рд┐рд╖реНрдпрдХреЛ рдЙрддреНрдХреГрд╖реНрдЯ рдЕрд╡рд╕реНрдерд╛рдХреЛ рд▓рд╛рдЧрд┐ рдЬрд┐рдореНрдореЗрд╡рд╛рд░ рдЫрдиреНред рдзреНрдпрд╛рди рдердкреНрджрд╛ рдореЛрдбреЗрд▓рдХреЛ рдкреНрдпрд╛рд░рд╛рдорд┐рдЯрд░рд╣рд░реВрдХреЛ рд╕рдВрдЦреНрдпрд╛ рдзреЗрд░реИ рдмрдвреНрдЫ, рдЬрд╕рд▓реЗ RNNs рд╕рдБрдЧ рд╕реНрдХреЗрд▓рд┐рдЩ рд╕рдорд╕реНрдпрд╛рд╣рд░реВ рдирд┐рдореНрддреНрдпрд╛рдпреЛред RNNs рдХреЛ рд╕реНрдХреЗрд▓рд┐рдЩрдХреЛ рдореБрдЦреНрдп рдмрд╛рдзрд╛ рднрдиреЗрдХреЛ рдореЛрдбреЗрд▓рд╣рд░реВрдХреЛ рдкреБрдирд░рд╛рд╡рд░реНрддреА рдкреНрд░рдХреГрддрд┐рд▓реЗ рдкреНрд░рд╢рд┐рдХреНрд╖рдгрд▓рд╛рдИ рдмреНрдпрд╛рдЪ рд░ рд╕рдорд╛рдирд╛рдиреНрддрд░ рдмрдирд╛рдЙрди рдЪреБрдиреМрддреАрдкреВрд░реНрдг рдмрдирд╛рдЙрдБрдЫред RNN рдорд╛ рдЕрдиреБрдХреНрд░рдордХреЛ рдкреНрд░рддреНрдпреЗрдХ рддрддреНрд╡рд▓рд╛рдИ рдХреНрд░рдорд┐рдХ рд░реВрдкрдорд╛ рдкреНрд░рдХреНрд░рд┐рдпрд╛ рдЧрд░реНрди рдЖрд╡рд╢реНрдпрдХ рд╣реБрдиреНрдЫ, рдЬрд╕рд▓реЗ рдпрд╕рд▓рд╛рдИ рд╕рдЬрд┐рд▓реИ рд╕рдорд╛рдирд╛рдиреНрддрд░ рдмрдирд╛рдЙрди рдЕрд╕рдореНрднрд╡ рдмрдирд╛рдЙрдБрдЫред

![Encoder Decoder with Attention](../../../../../lessons/5-NLP/18-Transformers/images/EncDecAttention.gif)

> [Google's Blog](https://research.googleblog.com/2016/09/a-neural-network-for-machine.html) рдмрд╛рдЯ рдЪрд┐рддреНрд░ред

рдзреНрдпрд╛рди рдореЗрдХрд╛рдирд┐рдЬреНрдорд╣рд░реВрдХреЛ рдЕрдкрдирддреНрд╡ рд░ рдпрд╕ рдмрд╛рдзрд╛рд▓реЗ рдЖрдЬ рд╣рд╛рдореАрд▓реЗ рдЪрд┐рдиреЗрдХрд╛ рд░ рдкреНрд░рдпреЛрдЧ рдЧрд░реНрдиреЗ рдЙрддреНрдХреГрд╖реНрдЯ рдЯреНрд░рд╛рдиреНрд╕рдлрд░реНрдорд░ рдореЛрдбреЗрд▓рд╣рд░реВрдХреЛ рд╕рд┐рд░реНрдЬрдирд╛ рдЧрд░реНтАНрдпреЛ, рдЬрд╕реНрддреИ BERT рджреЗрдЦрд┐ Open-GPT3ред

## рдЯреНрд░рд╛рдиреНрд╕рдлрд░реНрдорд░ рдореЛрдбреЗрд▓рд╣рд░реВ

рдЯреНрд░рд╛рдиреНрд╕рдлрд░реНрдорд░рд╣рд░реВрдХреЛ рдкрдЫрд╛рдбрд┐ рдореБрдЦреНрдп рд╡рд┐рдЪрд╛рд░ рднрдиреЗрдХреЛ RNNs рдХреЛ рдХреНрд░рдорд┐рдХ рдкреНрд░рдХреГрддрд┐рд▓рд╛рдИ рдЯрд╛рдврд╛ рд░рд╛рдЦреНрдиреБ рд░ рдкреНрд░рд╢рд┐рдХреНрд╖рдгрдХреЛ рдХреНрд░рдордорд╛ рд╕рдорд╛рдирд╛рдиреНрддрд░ рдмрдирд╛рдЙрди рд╕рдХрд┐рдиреЗ рдореЛрдбреЗрд▓ рд╕рд┐рд░реНрдЬрдирд╛ рдЧрд░реНрдиреБ рд╣реЛред рдпреЛ рджреБрдИ рд╡рд┐рдЪрд╛рд░рд╣рд░реВ рдХрд╛рд░реНрдпрд╛рдиреНрд╡рдпрди рдЧрд░реЗрд░ рдкреНрд░рд╛рдкреНрдд рдЧрд░рд┐рдиреНрдЫ:

* positional encoding
* RNNs (рд╡рд╛ CNNs) рдХреЛ рд╕рдЯреНрдЯрд╛ рдврд╛рдБрдЪрд╛рд╣рд░реВ рдХрдмреНрдЬрд╛ рдЧрд░реНрди self-attention рдореЗрдХрд╛рдирд┐рдЬреНрдо рдкреНрд░рдпреЛрдЧ рдЧрд░реНрдиреБ (рддреНрдпрд╕реИрд▓реЗ рдЯреНрд░рд╛рдиреНрд╕рдлрд░реНрдорд░рд╣рд░реВ рдкреНрд░рд╕реНрддреБрдд рдЧрд░реНрдиреЗ рдкреЗрдкрд░рд▓рд╛рдИ *[Attention is all you need](https://arxiv.org/abs/1706.03762)* рднрдирд┐рдПрдХреЛ рд╣реЛ)

### Positional Encoding/Embedding

Positional encoding рдХреЛ рд╡рд┐рдЪрд╛рд░ рдирд┐рдореНрдирд╛рдиреБрд╕рд╛рд░ рдЫред 
1. RNNs рдкреНрд░рдпреЛрдЧ рдЧрд░реНрджрд╛, рдЯреЛрдХрдирд╣рд░реВрдХреЛ рд╕рд╛рдкреЗрдХреНрд╖ рд╕реНрдерд┐рддрд┐ рдЪрд░рдгрд╣рд░реВрдХреЛ рд╕рдВрдЦреНрдпрд╛рд▓реЗ рдкреНрд░рддрд┐рдирд┐рдзрд┐рддреНрд╡ рдЧрд░рд┐рдиреНрдЫ, рд░ рдпрд╕рд▓рд╛рдИ рд╕реНрдкрд╖реНрдЯ рд░реВрдкрдорд╛ рдкреНрд░рддрд┐рдирд┐рдзрд┐рддреНрд╡ рдЧрд░реНрди рдЖрд╡рд╢реНрдпрдХ рдЫреИрдиред 
2. рддрд░, рдзреНрдпрд╛рдирдорд╛ рд╕реНрд╡рд┐рдЪ рдЧрд░реЗрдкрдЫрд┐, рд╣рд╛рдореАрд▓рд╛рдИ рдЕрдиреБрдХреНрд░рдорднрд┐рддреНрд░ рдЯреЛрдХрдирд╣рд░реВрдХреЛ рд╕рд╛рдкреЗрдХреНрд╖ рд╕реНрдерд┐рддрд┐ рдерд╛рд╣рд╛ рд╣реБрди рдЖрд╡рд╢реНрдпрдХ рдЫред 
3. Positional encoding рдкреНрд░рд╛рдкреНрдд рдЧрд░реНрди, рд╣рд╛рдореА рдЯреЛрдХрдирд╣рд░реВрдХреЛ рдЕрдиреБрдХреНрд░рдорд▓рд╛рдИ рдЕрдиреБрдХреНрд░рдорднрд┐рддреНрд░ рдЯреЛрдХрди рд╕реНрдерд┐рддрд┐рд╣рд░реВрдХреЛ рдЕрдиреБрдХреНрд░рдо (рдЬрд╕реНрддреИ, 0,1, ...) рд╕рдВрдЧ рдмрдврд╛рдЙрдБрдЫреМрдВред 
4. рддреНрдпрд╕рдкрдЫрд┐ рд╣рд╛рдореА рдЯреЛрдХрди рд╕реНрдерд┐рддрд┐ рд░ рдЯреЛрдХрди embedding рднреЗрдХреНрдЯрд░рд▓рд╛рдИ рдорд┐рд╕рд╛рдЙрдБрдЫреМрдВред рд╕реНрдерд┐рддрд┐ (integer) рд▓рд╛рдИ рднреЗрдХреНрдЯрд░рдорд╛ рд░реВрдкрд╛рдиреНрддрд░рдг рдЧрд░реНрди, рд╣рд╛рдореА рд╡рд┐рднрд┐рдиреНрди рджреГрд╖реНрдЯрд┐рдХреЛрдгрд╣рд░реВ рдкреНрд░рдпреЛрдЧ рдЧрд░реНрди рд╕рдХреНрдЫреМрдВ:

* рдЯреЛрдХрди embedding рдЬрд╕реНрддреИ trainable embeddingред рдпреЛ рдпрд╣рд╛рдБ рд╣рд╛рдореАрд▓реЗ рд╡рд┐рдЪрд╛рд░ рдЧрд░реЗрдХреЛ рджреГрд╖реНрдЯрд┐рдХреЛрдг рд╣реЛред рд╣рд╛рдореА рдЯреЛрдХрдирд╣рд░реВ рд░ рддрд┐рдирдХрд╛ рд╕реНрдерд┐рддрд┐рд╣рд░реВрдорд╛ embedding рд▓реЗрдпрд░рд╣рд░реВ рд▓рд╛рдЧреВ рдЧрд░реНрдЫреМрдВ, рдЬрд╕рд▓реЗ рд╕рдорд╛рди рдЖрдпрд╛рдордХрд╛ embedding рднреЗрдХреНрдЯрд░рд╣рд░реВ рдЙрддреНрдкрд╛рджрди рдЧрд░реНрдЫ, рдЬреБрди рд╣рд╛рдореАрд▓реЗ рдПрдХрд╕рд╛рде рдердкреНрдЫреМрдВред
* рдореВрд▓ рдкреЗрдкрд░рдорд╛ рдкреНрд░рд╕реНрддрд╛рд╡ рдЧрд░рд┐рдПрдХреЛ fixed position encoding functionред

<img src="../../../../../translated_images/ne/pos-embedding.e41ce9b6cf6078af.webp" width="50%"/>

> рд▓реЗрдЦрдХрджреНрд╡рд╛рд░рд╛ рдЪрд┐рддреНрд░ред

Positional embedding рдХреЛ рдкрд░рд┐рдгрд╛рдорд▓реЗ рдореВрд▓ рдЯреЛрдХрди рд░ рдпрд╕рдХреЛ рдЕрдиреБрдХреНрд░рдорднрд┐рддреНрд░рдХреЛ рд╕реНрдерд┐рддрд┐ рджреБрд╡реИрд▓рд╛рдИ embedding рдЧрд░реНрдЫред

### Multi-Head Self-Attention

рдЕрдм, рд╣рд╛рдореАрд▓реЗ рд╣рд╛рдореНрд░реЛ рдЕрдиреБрдХреНрд░рдорднрд┐рддреНрд░ рдХреЗрд╣реА рдврд╛рдБрдЪрд╛рд╣рд░реВ рдХрдмреНрдЬрд╛ рдЧрд░реНрди рдЖрд╡рд╢реНрдпрдХ рдЫред рдпреЛ рдЧрд░реНрди рдЯреНрд░рд╛рдиреНрд╕рдлрд░реНрдорд░рд╣рд░реВрд▓реЗ **self-attention** рдореЗрдХрд╛рдирд┐рдЬреНрдо рдкреНрд░рдпреЛрдЧ рдЧрд░реНрдЫрдиреН, рдЬреБрди рдЗрдирдкреБрдЯ рд░ рдЖрдЙрдЯрдкреБрдЯрдХреЛ рд░реВрдкрдорд╛ рд╕рдорд╛рди рдЕрдиреБрдХреНрд░рдордорд╛ рд▓рд╛рдЧреВ рдЧрд░рд┐рдПрдХреЛ рдзреНрдпрд╛рди рд╣реЛред Self-attention рд▓рд╛рдЧреВ рдЧрд░реНрджрд╛ рд╣рд╛рдореА рд╡рд╛рдХреНрдпрднрд┐рддреНрд░рдХреЛ **рд╕рдиреНрджрд░реНрдн**рд▓рд╛рдИ рд╡рд┐рдЪрд╛рд░ рдЧрд░реНрди рд╕рдХреНрдЫреМрдВ, рд░ рдХреБрди рд╢рдмреНрджрд╣рд░реВ рдкрд░рд╕реНрдкрд░ рд╕рдореНрдмрдиреНрдзрд┐рдд рдЫрдиреН рд╣реЗрд░реНрди рд╕рдХреНрдЫреМрдВред рдЙрджрд╛рд╣рд░рдгрдХрд╛ рд▓рд╛рдЧрд┐, рдпрд╕рд▓реЗ *it* рдЬрд╕реНрддрд╛ coreferences рджреНрд╡рд╛рд░рд╛ рдЙрд▓реНрд▓реЗрдЦ рдЧрд░рд┐рдПрдХрд╛ рд╢рдмреНрджрд╣рд░реВ рд╣реЗрд░реНрди рд░ рд╕рдиреНрджрд░реНрднрд▓рд╛рдИ рд╡рд┐рдЪрд╛рд░ рдЧрд░реНрди рдЕрдиреБрдорддрд┐ рджрд┐рдиреНрдЫ:

![](../../../../../translated_images/ne/CoreferenceResolution.861924d6d384a7d6.webp)

> [Google Blog](https://research.googleblog.com/2017/08/transformer-novel-neural-network.html) рдмрд╛рдЯ рдЪрд┐рддреНрд░ред

рдЯреНрд░рд╛рдиреНрд╕рдлрд░реНрдорд░рд╣рд░реВрдорд╛, рд╣рд╛рдореА **Multi-Head Attention** рдкреНрд░рдпреЛрдЧ рдЧрд░реНрдЫреМрдВ рддрд╛рдХрд┐ рдиреЗрдЯрд╡рд░реНрдХрд▓реЗ рд╡рд┐рднрд┐рдиреНрди рдкреНрд░рдХрд╛рд░рдХрд╛ рдирд┐рд░реНрднрд░рддрд╛ рдХрдмреНрдЬрд╛ рдЧрд░реНрди рд╕рдХреНрдиреЗ рд╢рдХреНрддрд┐ рдкреНрд░рд╛рдкреНрдд рдЧрд░реЛрд╕реН, рдЬрд╕реНрддреИ рд▓рд╛рдореЛ-рдЕрд╡рдзрд┐ рдмрдирд╛рдо рдЫреЛрдЯреЛ-рдЕрд╡рдзрд┐ рд╢рдмреНрдж рд╕рдореНрдмрдиреНрдзрд╣рд░реВ, co-reference рдмрдирд╛рдо рдЕрдиреНрдп рдХреЗрд╣реАред

[TensorFlow Notebook](TransformersTF.ipynb) рдорд╛ рдЯреНрд░рд╛рдиреНрд╕рдлрд░реНрдорд░ рд▓реЗрдпрд░рд╣рд░реВрдХреЛ рдХрд╛рд░реНрдпрд╛рдиреНрд╡рдпрдирдХреЛ рдердк рд╡рд┐рд╡рд░рдгрд╣рд░реВ рдЫрдиреНред

### Encoder-Decoder Attention

рдЯреНрд░рд╛рдиреНрд╕рдлрд░реНрдорд░рд╣рд░реВрдорд╛, рдзреНрдпрд╛рди рджреБрдИ рд╕реНрдерд╛рдирдорд╛ рдкреНрд░рдпреЛрдЧ рдЧрд░рд┐рдиреНрдЫ:

* рдЗрдирдкреБрдЯ рдкрд╛рдарднрд┐рддреНрд░ рдврд╛рдБрдЪрд╛рд╣рд░реВ рдХрдмреНрдЬрд╛ рдЧрд░реНрди self-attention рдкреНрд░рдпреЛрдЧ рдЧрд░реНрди
* рдЕрдиреБрдХреНрд░рдо рдЕрдиреБрд╡рд╛рдж рдЧрд░реНрди - рдпреЛ encoder рд░ decoder рдмреАрдЪрдХреЛ рдзреНрдпрд╛рди рд▓реЗрдпрд░ рд╣реЛред

Encoder-decoder attention RNNs рдорд╛ рдкреНрд░рдпреЛрдЧ рдЧрд░рд┐рдПрдХреЛ рдзреНрдпрд╛рди рдореЗрдХрд╛рдирд┐рдЬреНрдорд╕рдБрдЧ рдзреЗрд░реИ рд╕рдорд╛рди рдЫ, рдЬрд╕рдХреЛ рд╡рд░реНрдгрди рдпрд╕ рдЦрдгреНрдбрдХреЛ рд╕реБрд░реБрд╡рд╛рддрдорд╛ рдЧрд░рд┐рдПрдХреЛ рдерд┐рдпреЛред рдпреЛ рдПрдирд┐рдореЗрдЯреЗрдб рдбрд╛рдпрдЧреНрд░рд╛рдорд▓реЗ encoder-decoder attention рдХреЛ рднреВрдорд┐рдХрд╛рд▓рд╛рдИ рд╕реНрдкрд╖реНрдЯ рдЧрд░реНрджрдЫред

![Animated GIF showing how the evaluations are performed in transformer models.](../../../../../lessons/5-NLP/18-Transformers/images/transformer-animated-explanation.gif)

рдХрд┐рдирдХрд┐ рдкреНрд░рддреНрдпреЗрдХ рдЗрдирдкреБрдЯ рд╕реНрдерд┐рддрд┐ рд╕реНрд╡рддрдиреНрддреНрд░ рд░реВрдкрдорд╛ рдкреНрд░рддреНрдпреЗрдХ рдЖрдЙрдЯрдкреБрдЯ рд╕реНрдерд┐рддрд┐рдорд╛ рдореНрдпрд╛рдк рдЧрд░рд┐рдиреНрдЫ, рдЯреНрд░рд╛рдиреНрд╕рдлрд░реНрдорд░рд╣рд░реВрд▓реЗ RNNs рднрдиреНрджрд╛ рд░рд╛рдореНрд░реЛ рд╕рдорд╛рдирд╛рдиреНрддрд░ рдЧрд░реНрди рд╕рдХреНрдЫрдиреН, рдЬрд╕рд▓реЗ рдзреЗрд░реИ рдареВрд▓реЛ рд░ рдЕрдзрд┐рдХ рдЕрднрд┐рд╡реНрдпрдХреНрддрд┐рдкреВрд░реНрдг рднрд╛рд╖рд╛ рдореЛрдбреЗрд▓рд╣рд░реВ рд╕рдХреНрд╖рдо рдмрдирд╛рдЙрдБрдЫред рдкреНрд░рддреНрдпреЗрдХ attention head рд╡рд┐рднрд┐рдиреНрди рд╢рдмреНрджрд╣рд░реВрдХреЛ рд╕рдореНрдмрдиреНрдзрд╣рд░реВ рд╕рд┐рдХреНрди рдкреНрд░рдпреЛрдЧ рдЧрд░реНрди рд╕рдХрд┐рдиреНрдЫ, рдЬрд╕рд▓реЗ Natural Language Processing рдХрд╛рд░реНрдпрд╣рд░реВрдорд╛ рд╕реБрдзрд╛рд░ рд▓реНрдпрд╛рдЙрдБрдЫред

## BERT

**BERT** (Bidirectional Encoder Representations from Transformers) рдПрдХ рдзреЗрд░реИ рдареВрд▓реЛ рдмрд╣реБ-рд▓реЗрдпрд░ рдЯреНрд░рд╛рдиреНрд╕рдлрд░реНрдорд░ рдиреЗрдЯрд╡рд░реНрдХ рд╣реЛ, *BERT-base* рдХреЛ рд▓рд╛рдЧрд┐ 12 рд▓реЗрдпрд░рд╣рд░реВ, рд░ *BERT-large* рдХреЛ рд▓рд╛рдЧрд┐ 24ред рдореЛрдбреЗрд▓рд▓рд╛рдИ рдкрд╣рд┐рд▓реЛ рдкрдЯрдХ рдареВрд▓реЛ рдкрд╛рда рдбрд╛рдЯрд╛рдХреЛ рд╕рдВрдЧреНрд░рд╣ (WikiPedia + рдХрд┐рддрд╛рдмрд╣рд░реВ) рдорд╛ unsupervised рдкреНрд░рд╢рд┐рдХреНрд╖рдг (рд╡рд╛рдХреНрдпрдорд╛ masked рд╢рдмреНрджрд╣рд░реВрдХреЛ рднрд╡рд┐рд╖реНрдпрд╡рд╛рдгреА рдЧрд░реНрджреИ) рдкреНрд░рдпреЛрдЧ рдЧрд░реЗрд░ рдкреНрд░рд┐-рдЯреНрд░реЗрди рдЧрд░рд┐рдиреНрдЫред рдкреНрд░рд┐-рдЯреНрд░реЗрдирд┐рдЩрдХреЛ рдХреНрд░рдордорд╛ рдореЛрдбреЗрд▓рд▓реЗ рднрд╛рд╖рд╛ рдмреБрдЭреНрдиреЗ рдорд╣рддреНрддреНрд╡рдкреВрд░реНрдг рд╕реНрддрд░рд╣рд░реВ рдЕрд╡рд╢реЛрд╖рд┐рдд рдЧрд░реНрдЫ, рдЬрд╕рд▓рд╛рдИ рдЕрдиреНрдп рдбрд╛рдЯрд╛рд╕реЗрдЯрд╣рд░реВрд╕рдБрдЧ fine tuning рдЧрд░реЗрд░ рдЙрдкрдпреЛрдЧ рдЧрд░реНрди рд╕рдХрд┐рдиреНрдЫред рдпрд╕ рдкреНрд░рдХреНрд░рд┐рдпрд╛рд▓рд╛рдИ **transfer learning** рднрдирд┐рдиреНрдЫред

![picture from http://jalammar.github.io/illustrated-bert/](../../../../../translated_images/ne/jalammarBERT-language-modeling-masked-lm.34f113ea5fec4362.webp)

> [рд╕реНрд░реЛрдд](http://jalammar.github.io/illustrated-bert/)

## тЬНя╕П рдЕрднреНрдпрд╛рд╕рд╣рд░реВ: рдЯреНрд░рд╛рдиреНрд╕рдлрд░реНрдорд░рд╣рд░реВ

рддрдкрд╛рдИрдВрдХреЛ рд╕рд┐рдХрд╛рдЗрд▓рд╛рдИ рдирд┐рдореНрди рдиреЛрдЯрдмреБрдХрд╣рд░реВрдорд╛ рдЬрд╛рд░реА рд░рд╛рдЦреНрдиреБрд╣реЛрд╕реН:

* [Transformers in PyTorch](TransformersPyTorch.ipynb)
* [Transformers in TensorFlow](TransformersTF.ipynb)

## рдирд┐рд╖реНрдХрд░реНрд╖

рдпрд╕ рдкрд╛рдардорд╛ рддрдкрд╛рдИрдВрд▓реЗ рдЯреНрд░рд╛рдиреНрд╕рдлрд░реНрдорд░рд╣рд░реВ рд░ рдзреНрдпрд╛рди рдореЗрдХрд╛рдирд┐рдЬреНрдорд╣рд░реВрдмрд╛рд░реЗ рд╕рд┐рдХреНрдиреБрднрдпреЛ, рдЬреБрди NLP рдЯреВрд▓рдмрдХреНрд╕рдХрд╛ рд╕рдмреИ рдЖрд╡рд╢реНрдпрдХ рдЙрдкрдХрд░рдгрд╣рд░реВ рд╣реБрдиреНред рдЯреНрд░рд╛рдиреНрд╕рдлрд░реНрдорд░ рдЖрд░реНрдХрд┐рдЯреЗрдХреНрдЪрд░рдХрд╛ рдзреЗрд░реИ рднреЗрд░рд┐рдПрд╕рдирд╣рд░реВ рдЫрдиреН, рдЬрд╕реНрддреИ BERT, DistilBERT, BigBird, OpenGPT3 рд░ рдЕрдиреНрдп, рдЬрд╕рд▓рд╛рдИ fine tune рдЧрд░реНрди рд╕рдХрд┐рдиреНрдЫред [HuggingFace package](https://github.com/huggingface/) рд▓реЗ PyTorch рд░ TensorFlow рджреБрд╡реИрд╕рдБрдЧ рдзреЗрд░реИ рдЖрд░реНрдХрд┐рдЯреЗрдХреНрдЪрд░рд╣рд░реВ рдкреНрд░рд╢рд┐рдХреНрд╖рдг рдЧрд░реНрдирдХреЛ рд▓рд╛рдЧрд┐ рд░рд┐рдкреЛрдЬрд┐рдЯрд░реА рдкреНрд░рджрд╛рди рдЧрд░реНрджрдЫред

## ЁЯЪА рдЪреБрдиреМрддреА

## [рдкрд╛рда рдкрдЫрд┐ рдХреНрд╡рд┐рдЬ](https://ff-quizzes.netlify.app/en/ai/quiz/36)

## рд╕рдореАрдХреНрд╖рд╛ рд░ рдЖрддреНрдо-рдЕрдзреНрдпрдпрди

* [рдмреНрд▓рдЧ рдкреЛрд╕реНрдЯ](https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/), рдЯреНрд░рд╛рдиреНрд╕рдлрд░реНрдорд░рд╣рд░реВрдорд╛ [Attention is all you need](https://arxiv.org/abs/1706.03762) рдкреЗрдкрд░рд▓рд╛рдИ рд╡реНрдпрд╛рдЦреНрдпрд╛ рдЧрд░реНрджреИред
* [рдмреНрд▓рдЧ рдкреЛрд╕реНрдЯрд╣рд░реВрдХреЛ рд╢реНрд░реГрдВрдЦрд▓рд╛](https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452), рдЯреНрд░рд╛рдиреНрд╕рдлрд░реНрдорд░рд╣рд░реВрд▓рд╛рдИ рд╡рд┐рд╕реНрддреГрдд рд░реВрдкрдорд╛ рд╡реНрдпрд╛рдЦреНрдпрд╛ рдЧрд░реНрджреИред

## [рдЕрд╕рд╛рдЗрдирдореЗрдиреНрдЯ](assignment.md)

---

