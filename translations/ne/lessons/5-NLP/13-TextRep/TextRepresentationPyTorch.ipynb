{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# पाठ वर्गीकरण कार्य\n",
    "\n",
    "जसरी हामीले उल्लेख गरेका छौं, हामी **AG_NEWS** डेटासेटमा आधारित साधारण पाठ वर्गीकरण कार्यमा केन्द्रित हुनेछौं, जसले समाचार शीर्षकहरूलाई चार श्रेणीहरूमा वर्गीकृत गर्नुपर्छ: विश्व, खेलकुद, व्यापार, र विज्ञान/प्रविधि।\n",
    "\n",
    "## डेटासेट\n",
    "\n",
    "यो डेटासेट [`torchtext`](https://github.com/pytorch/text) मोड्युलमा समावेश गरिएको छ, त्यसैले हामी यसलाई सजिलै पहुँच गर्न सक्छौं।\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "import os\n",
    "import collections\n",
    "os.makedirs('./data',exist_ok=True)\n",
    "train_dataset, test_dataset = torchtext.datasets.AG_NEWS(root='./data')\n",
    "classes = ['World', 'Sports', 'Business', 'Sci/Tech']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "यहाँ, `train_dataset` र `test_dataset` ले क्रमशः वर्गको संख्या (लेबल) र पाठको जोडीहरू फर्काउने संग्रहहरू समावेश गर्दछ, उदाहरणका लागि:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,\n",
       " \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(train_dataset)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "त्यसैले, हाम्रो डाटासेटबाट पहिलो १० नयाँ शीर्षकहरू प्रिन्ट गरौं:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Sci/Tech** -> Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\band of ultra-cynics, are seeing green again.\n",
      "**Sci/Tech** -> Carlyle Looks Toward Commercial Aerospace (Reuters) Reuters - Private investment firm Carlyle Group,\\which has a reputation for making well-timed and occasionally\\controversial plays in the defense industry, has quietly placed\\its bets on another part of the market.\n",
      "**Sci/Tech** -> Oil and Economy Cloud Stocks' Outlook (Reuters) Reuters - Soaring crude prices plus worries\\about the economy and the outlook for earnings are expected to\\hang over the stock market next week during the depth of the\\summer doldrums.\n",
      "**Sci/Tech** -> Iraq Halts Oil Exports from Main Southern Pipeline (Reuters) Reuters - Authorities have halted oil export\\flows from the main pipeline in southern Iraq after\\intelligence showed a rebel militia could strike\\infrastructure, an oil official said on Saturday.\n",
      "**Sci/Tech** -> Oil prices soar to all-time record, posing new menace to US economy (AFP) AFP - Tearaway world oil prices, toppling records and straining wallets, present a new economic menace barely three months before the US presidential elections.\n"
     ]
    }
   ],
   "source": [
    "for i,x in zip(range(5),train_dataset):\n",
    "    print(f\"**{classes[x[0]]}** -> {x[1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "किनभने डेटासेटहरू इटरेटरहरू हुन्, यदि हामीले डाटा धेरै पटक प्रयोग गर्न चाहन्छौं भने हामीले यसलाई सूचीमा रूपान्तरण गर्न आवश्यक छ:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = torchtext.datasets.AG_NEWS(root='./data')\n",
    "train_dataset = list(train_dataset)\n",
    "test_dataset = list(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## टोकनाइजेशन\n",
    "\n",
    "अब हामीले पाठलाई **संख्या** मा रूपान्तरण गर्न आवश्यक छ जसलाई टेन्सरको रूपमा प्रस्तुत गर्न सकिन्छ। यदि हामी शब्द-स्तरको प्रतिनिधित्व चाहन्छौं भने, हामीले दुई कुरा गर्नुपर्छ:\n",
    "* पाठलाई **टोकन** मा विभाजन गर्न **टोकनाइजर** प्रयोग गर्नुहोस्\n",
    "* ती टोकनहरूको **शब्दकोश** निर्माण गर्नुहोस्।\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['he', 'said', 'hello']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = torchtext.data.utils.get_tokenizer('basic_english')\n",
    "tokenizer('He said: hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = collections.Counter()\n",
    "for (label, line) in train_dataset:\n",
    "    counter.update(tokenizer(line))\n",
    "vocab = torchtext.vocab.vocab(counter, min_freq=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "शब्दावली प्रयोग गरेर, हामी सजिलैसँग टोकन गरिएको स्ट्रिङलाई संख्याको सेटमा एन्कोड गर्न सक्छौं:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size if 95810\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[599, 3279, 97, 1220, 329, 225, 7368]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(vocab)\n",
    "print(f\"Vocab size if {vocab_size}\")\n",
    "\n",
    "stoi = vocab.get_stoi() # dict to convert tokens to indices\n",
    "\n",
    "def encode(x):\n",
    "    return [stoi[s] for s in tokenizer(x)]\n",
    "\n",
    "encode('I love to play with my words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## शब्दहरूको झोला (Bag of Words) पाठ प्रतिनिधित्व\n",
    "\n",
    "किनभने शब्दहरूले अर्थ व्यक्त गर्छन्, कहिलेकाहीँ हामी वाक्यको क्रमको पर्वाह नगरी, व्यक्तिगत शब्दहरू हेरेरै पाठको अर्थ पत्ता लगाउन सक्छौं। उदाहरणका लागि, समाचार वर्गीकरण गर्दा, *मौसम*, *हिउँ* जस्ता शब्दहरूले *मौसम पूर्वानुमान* जनाउन सक्छन्, जबकि *स्टक*, *डलर* जस्ता शब्दहरूले *आर्थिक समाचार* तर्फ सङ्केत गर्न सक्छन्।\n",
    "\n",
    "**शब्दहरूको झोला** (BoW) भेक्टर प्रतिनिधित्व परम्परागत भेक्टर प्रतिनिधित्वमा सबैभन्दा धेरै प्रयोग गरिने विधि हो। प्रत्येक शब्दलाई भेक्टरको सूचकसँग जोडिन्छ, र भेक्टरको तत्त्वले कुनै विशेष दस्तावेजमा शब्दको उपस्थितिको सङ्ख्या समावेश गर्दछ।\n",
    "\n",
    "![शब्दहरूको झोला भेक्टर प्रतिनिधित्व मेमोरीमा कसरी देखिन्छ भन्ने देखाउने छवि।](../../../../../translated_images/ne/bag-of-words-example.606fc1738f1d7ba9.webp)\n",
    "\n",
    "> **Note**: तपाईं BoW लाई पाठका व्यक्तिगत शब्दहरूको लागि सबै एक-तर्फ-कोड गरिएको भेक्टरहरूको योगको रूपमा पनि सोच्न सक्नुहुन्छ।\n",
    "\n",
    "तल Scikit Learn पायथन पुस्तकालय प्रयोग गरेर शब्दहरूको झोला प्रतिनिधित्व कसरी निर्माण गर्ने उदाहरण दिइएको छ:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 2, 0, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "corpus = [\n",
    "        'I like hot dogs.',\n",
    "        'The dog ran fast.',\n",
    "        'Its hot outside.',\n",
    "    ]\n",
    "vectorizer.fit_transform(corpus)\n",
    "vectorizer.transform(['My dog likes hot dogs on a hot day.']).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AG_NEWS डेटासेटको भेक्टर प्रतिनिधित्वबाट bag-of-words भेक्टर गणना गर्न, हामी निम्न फङ्सन प्रयोग गर्न सक्छौं:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 1., 2.,  ..., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(vocab)\n",
    "\n",
    "def to_bow(text,bow_vocab_size=vocab_size):\n",
    "    res = torch.zeros(bow_vocab_size,dtype=torch.float32)\n",
    "    for i in encode(text):\n",
    "        if i<bow_vocab_size:\n",
    "            res[i] += 1\n",
    "    return res\n",
    "\n",
    "print(to_bow(train_dataset[0][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **नोट:** यहाँ हामीले डिफल्ट शब्दकोशको आकार निर्दिष्ट गर्न ग्लोबल `vocab_size` भेरिएबल प्रयोग गरेका छौं। किनभने प्रायः शब्दकोशको आकार धेरै ठूलो हुन्छ, हामी शब्दकोशको आकारलाई सबैभन्दा बारम्बार प्रयोग हुने शब्दहरूमा सीमित गर्न सक्छौं। `vocab_size` को मान घटाएर तलको कोड चलाउने प्रयास गर्नुहोस्, र यसले शुद्धतामा कसरी प्रभाव पार्छ हेर्नुहोस्। तपाईंले केही शुद्धता घट्ने अपेक्षा गर्नुपर्छ, तर प्रदर्शन सुधारको सट्टामा नाटकीय रूपमा होइन।\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BoW वर्गीकरणकर्ता प्रशिक्षण\n",
    "\n",
    "अब हामीले हाम्रो पाठको Bag-of-Words प्रतिनिधित्व कसरी निर्माण गर्ने भनेर सिकिसकेपछि, यसमा आधारित भएर एक वर्गीकरणकर्ता प्रशिक्षण गरौं। पहिलो चरणमा, हामीले हाम्रो डेटासेटलाई यस्तो तरिकाले प्रशिक्षणका लागि रूपान्तरण गर्नुपर्छ, जसले गर्दा सबै स्थानगत भेक्टर प्रतिनिधित्वहरूलाई Bag-of-Words प्रतिनिधित्वमा परिवर्तन गरिन्छ। यो `bowify` फंक्शनलाई मानक torch `DataLoader` मा `collate_fn` प्यारामिटरको रूपमा पास गरेर हासिल गर्न सकिन्छ:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import numpy as np \n",
    "\n",
    "# this collate function gets list of batch_size tuples, and needs to \n",
    "# return a pair of label-feature tensors for the whole minibatch\n",
    "def bowify(b):\n",
    "    return (\n",
    "            torch.LongTensor([t[0]-1 for t in b]),\n",
    "            torch.stack([to_bow(t[1]) for t in b])\n",
    "    )\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, collate_fn=bowify, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, collate_fn=bowify, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "अब हामी एउटा साधारण वर्गीकरण न्युरल नेटवर्क परिभाषित गरौं जसमा एउटा मात्र रेखीय तह हुन्छ। इनपुट भेक्टरको आकार `vocab_size` बराबर हुन्छ, र आउटपुट आकार कक्षाहरूको संख्या (4) सँग मेल खान्छ। किनभने हामी वर्गीकरण कार्य समाधान गर्दैछौं, अन्तिम सक्रियता प्रकार्य `LogSoftmax()` हो।\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = torch.nn.Sequential(torch.nn.Linear(vocab_size,4),torch.nn.LogSoftmax(dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "अब हामी मानक PyTorch प्रशिक्षण लूप परिभाषित गर्नेछौं। किनभने हाम्रो डेटासेट धेरै ठूलो छ, हाम्रो शिक्षण उद्देश्यको लागि हामी केवल एक इपोकको लागि प्रशिक्षण गर्नेछौं, र कहिलेकाहीँ इपोकभन्दा पनि कम (प्रशिक्षण सीमित गर्न `epoch_size` प्यारामिटर निर्दिष्ट गर्न सकिन्छ)। हामीले प्रशिक्षणको क्रममा संचित प्रशिक्षण शुद्धता पनि रिपोर्ट गर्नेछौं; रिपोर्ट गर्ने आवृत्ति `report_freq` प्यारामिटर प्रयोग गरेर निर्दिष्ट गरिन्छ।\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(net,dataloader,lr=0.01,optimizer=None,loss_fn = torch.nn.NLLLoss(),epoch_size=None, report_freq=200):\n",
    "    optimizer = optimizer or torch.optim.Adam(net.parameters(),lr=lr)\n",
    "    net.train()\n",
    "    total_loss,acc,count,i = 0,0,0,0\n",
    "    for labels,features in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        out = net(features)\n",
    "        loss = loss_fn(out,labels) #cross_entropy(out,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss+=loss\n",
    "        _,predicted = torch.max(out,1)\n",
    "        acc+=(predicted==labels).sum()\n",
    "        count+=len(labels)\n",
    "        i+=1\n",
    "        if i%report_freq==0:\n",
    "            print(f\"{count}: acc={acc.item()/count}\")\n",
    "        if epoch_size and count>epoch_size:\n",
    "            break\n",
    "    return total_loss.item()/count, acc.item()/count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200: acc=0.8028125\n",
      "6400: acc=0.8371875\n",
      "9600: acc=0.8534375\n",
      "12800: acc=0.85765625\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.026090790722161722, 0.8620069296375267)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_epoch(net,train_loader,epoch_size=15000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BiGrams, TriGrams र N-Grams\n",
    "\n",
    "Bag of words विधिको एउटा सीमितता भनेको केही शब्दहरू बहु-शब्द अभिव्यक्तिहरूको हिस्सा हुन्छन्। उदाहरणका लागि, 'hot dog' शब्दको अर्थ 'hot' र 'dog' शब्दहरूको अन्य सन्दर्भमा भन्दा पूर्ण रूपमा फरक हुन्छ। यदि हामी 'hot' र 'dog' शब्दहरूलाई सधैं एउटै भेक्टरद्वारा प्रतिनिधित्व गर्छौं भने, यसले हाम्रो मोडेललाई भ्रमित गर्न सक्छ।\n",
    "\n",
    "यस समस्यालाई समाधान गर्न, **N-gram प्रतिनिधित्वहरू** प्रायः कागजात वर्गीकरणका विधिहरूमा प्रयोग गरिन्छ, जहाँ प्रत्येक शब्द, दुई-शब्द (bi-word) वा तीन-शब्द (tri-word) को आवृत्ति classifiers प्रशिक्षण गर्नका लागि उपयोगी विशेषता हुन्छ। उदाहरणका लागि, bigram प्रतिनिधित्वमा, हामी मूल शब्दहरूको अतिरिक्त सबै शब्द जोडीहरूलाई पनि शब्दकोशमा थप्नेछौं।\n",
    "\n",
    "तल Scikit Learn प्रयोग गरेर bigram bag of word प्रतिनिधित्व कसरी निर्माण गर्ने भन्ने उदाहरण छ:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:\n",
      " {'i': 7, 'like': 11, 'hot': 4, 'dogs': 2, 'i like': 8, 'like hot': 12, 'hot dogs': 5, 'the': 16, 'dog': 0, 'ran': 14, 'fast': 3, 'the dog': 17, 'dog ran': 1, 'ran fast': 15, 'its': 9, 'outside': 13, 'its hot': 10, 'hot outside': 6}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 1, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_vectorizer = CountVectorizer(ngram_range=(1, 2), token_pattern=r'\\b\\w+\\b', min_df=1)\n",
    "corpus = [\n",
    "        'I like hot dogs.',\n",
    "        'The dog ran fast.',\n",
    "        'Its hot outside.',\n",
    "    ]\n",
    "bigram_vectorizer.fit_transform(corpus)\n",
    "print(\"Vocabulary:\\n\",bigram_vectorizer.vocabulary_)\n",
    "bigram_vectorizer.transform(['My dog likes hot dogs on a hot day.']).toarray()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N-gram विधिको मुख्य कमजोरी भनेको शब्दकोशको आकार अत्यन्त छिटो बढ्न थाल्छ। व्यवहारमा, हामीलाई N-gram प्रतिनिधित्वलाई केही आयाम घटाउने प्रविधिहरूसँग संयोजन गर्न आवश्यक पर्छ, जस्तै *embeddings*, जसको बारेमा हामी अर्को इकाईमा छलफल गर्नेछौं।\n",
    "\n",
    "हाम्रो **AG News** डेटासेटमा N-gram प्रतिनिधित्व प्रयोग गर्न, हामीले विशेष ngram शब्दकोश निर्माण गर्न आवश्यक छ:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram vocabulary length =  1308842\n"
     ]
    }
   ],
   "source": [
    "counter = collections.Counter()\n",
    "for (label, line) in train_dataset:\n",
    "    l = tokenizer(line)\n",
    "    counter.update(torchtext.data.utils.ngrams_iterator(l,ngrams=2))\n",
    "    \n",
    "bi_vocab = torchtext.vocab.vocab(counter, min_freq=1)\n",
    "\n",
    "print(\"Bigram vocabulary length = \",len(bi_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "हामी माथिको जस्तै कोड प्रयोग गरेर classifier प्रशिक्षण गर्न सक्थ्यौं, तर यो धेरै मेमोरी-अक्षम हुने थियो। अर्को इकाईमा, हामी embeddings प्रयोग गरेर bigram classifier प्रशिक्षण गर्नेछौं।\n",
    "\n",
    "> **Note:** तपाईंले केवल ती ngrams छोड्न सक्नुहुन्छ जुन पाठमा निर्दिष्ट गरिएको संख्याभन्दा बढी पटक देखा पर्छ। यसले सुनिश्चित गर्नेछ कि कम बारम्बार देखा पर्ने bigrams हटाइनेछ, र dimensionality उल्लेखनीय रूपमा घट्नेछ। यसका लागि, `min_freq` parameter लाई उच्च मानमा सेट गर्नुहोस्, र vocabulary को लम्बाइ परिवर्तनलाई अवलोकन गर्नुहोस्।\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## टर्म फ्रिक्वेन्सी इनभर्स डकुमेन्ट फ्रिक्वेन्सी TF-IDF\n",
    "\n",
    "BoW प्रतिनिधित्वमा, शब्दको उपस्थितिलाई समान तौल दिइन्छ, चाहे शब्द कस्तो भए पनि। तर, यो स्पष्ट छ कि बारम्बार प्रयोग हुने शब्दहरू, जस्तै *a*, *in*, आदि वर्गीकरणका लागि विशेष शब्दहरूको तुलनामा धेरै कम महत्त्वपूर्ण हुन्छन्। वास्तवमा, अधिकांश NLP कार्यहरूमा केही शब्दहरू अरूभन्दा बढी सान्दर्भिक हुन्छन्।\n",
    "\n",
    "**TF-IDF** को अर्थ **टर्म फ्रिक्वेन्सी–इनभर्स डकुमेन्ट फ्रिक्वेन्सी** हो। यो बाग अफ वर्ड्सको एक भिन्नता हो, जहाँ कुनै शब्दको उपस्थितिलाई संकेत गर्ने 0/1 बाइनरी मानको सट्टा, फ्लोटिङ-पोइन्ट मान प्रयोग गरिन्छ, जुन शब्दको कर्पसमा उपस्थितिको फ्रिक्वेन्सीसँग सम्बन्धित हुन्छ।\n",
    "\n",
    "औपचारिक रूपमा, शब्द $i$ को डकुमेन्ट $j$ मा तौल $w_{ij}$ यसरी परिभाषित गरिन्छ:\n",
    "$$\n",
    "w_{ij} = tf_{ij}\\times\\log({N\\over df_i})\n",
    "$$\n",
    "जहाँ\n",
    "* $tf_{ij}$ भनेको $j$ मा $i$ को उपस्थितिको संख्या हो, अर्थात् हामीले पहिले देखेको BoW मान\n",
    "* $N$ भनेको संग्रहमा रहेका डकुमेन्टहरूको संख्या हो\n",
    "* $df_i$ भनेको सम्पूर्ण संग्रहमा शब्द $i$ समावेश भएका डकुमेन्टहरूको संख्या हो\n",
    "\n",
    "TF-IDF मान $w_{ij}$ शब्दको डकुमेन्टमा उपस्थितिको संख्याको अनुपातमा बढ्छ र कर्पसमा शब्द समावेश भएका डकुमेन्टहरूको संख्याले समायोजन गरिन्छ, जसले केही शब्द बारम्बार देखा पर्ने तथ्यलाई सम्हाल्न मद्दत गर्छ। उदाहरणका लागि, यदि शब्द *प्रत्येक* डकुमेन्टमा देखा पर्छ भने, $df_i=N$, र $w_{ij}=0$, र ती शब्दहरू पूर्ण रूपमा बेवास्ता गरिन्छन्।\n",
    "\n",
    "तपाईं Scikit Learn प्रयोग गरेर सजिलै TF-IDF टेक्स्टको भेक्टराइजेसन बनाउन सक्नुहुन्छ:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.43381609, 0.        , 0.43381609, 0.        , 0.65985664,\n",
       "        0.43381609, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,2))\n",
    "vectorizer.fit_transform(corpus)\n",
    "vectorizer.transform(['My dog likes hot dogs on a hot day.']).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## निष्कर्ष\n",
    "\n",
    "यद्यपि TF-IDF प्रतिनिधित्वहरूले विभिन्न शब्दहरूलाई आवृत्तिको तौल प्रदान गर्छन्, तिनीहरूले अर्थ वा क्रमलाई प्रतिनिधित्व गर्न असमर्थ छन्। प्रसिद्ध भाषाविद् जे. आर. फर्थले १९३५ मा भनेका थिए, \"शब्दको पूर्ण अर्थ सधैं सन्दर्भमा आधारित हुन्छ, र सन्दर्भबाहेकको अर्थको कुनै पनि अध्ययनलाई गम्भीरतापूर्वक लिन सकिँदैन।\"। हामी यस पाठ्यक्रममा पछि पाठबाट भाषागत मोडेलिङ प्रयोग गरी कसरी पाठबाट सन्दर्भगत जानकारी कब्जा गर्ने भन्ने कुरा सिक्नेछौं।\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**अस्वीकरण**:  \nयो दस्तावेज़ AI अनुवाद सेवा [Co-op Translator](https://github.com/Azure/co-op-translator) प्रयोग गरेर अनुवाद गरिएको छ। हामी यथासम्भव शुद्धता सुनिश्चित गर्न प्रयास गर्छौं, तर कृपया ध्यान दिनुहोस् कि स्वचालित अनुवादमा त्रुटिहरू वा अशुद्धताहरू हुन सक्छ। यसको मूल भाषामा रहेको मूल दस्तावेज़लाई आधिकारिक स्रोत मानिनुपर्छ। महत्वपूर्ण जानकारीको लागि, व्यावसायिक मानव अनुवाद सिफारिस गरिन्छ। यस अनुवादको प्रयोगबाट उत्पन्न हुने कुनै पनि गलतफहमी वा गलत व्याख्याको लागि हामी जिम्मेवार हुने छैनौं।\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "16af2a8bbb083ea23e5e41c7f5787656b2ce26968575d8763f2c4b17f9cd711f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('py38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "coopTranslator": {
   "original_hash": "7b9040985e748e4e2d4c689892456ad7",
   "translation_date": "2025-08-28T09:51:29+00:00",
   "source_file": "lessons/5-NLP/13-TextRep/TextRepresentationPyTorch.ipynb",
   "language_code": "ne"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}