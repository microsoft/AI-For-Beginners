{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ਰਿਕਰੰਟ ਨਿਊਰਲ ਨੈਟਵਰਕਸ\n",
    "\n",
    "ਪਿਛਲੇ ਮੋਡੀਊਲ ਵਿੱਚ, ਅਸੀਂ ਪਾਠ ਦੇ ਸਮਰੱਥ ਅਰਥਪੂਰਨ ਪ੍ਰਸਤੁਤੀਆਂ ਬਾਰੇ ਗੱਲ ਕੀਤੀ ਸੀ। ਅਸੀਂ ਜੋ ਆਰਕੀਟੈਕਚਰ ਵਰਤ ਰਹੇ ਹਾਂ, ਉਹ ਵਾਕ ਦੇ ਸ਼ਬਦਾਂ ਦੇ ਸਮੁੱਚਿਤ ਅਰਥ ਨੂੰ ਕੈਪਚਰ ਕਰਦਾ ਹੈ, ਪਰ ਇਹ ਸ਼ਬਦਾਂ ਦੇ **ਕ੍ਰਮ** ਨੂੰ ਧਿਆਨ ਵਿੱਚ ਨਹੀਂ ਰੱਖਦਾ, ਕਿਉਂਕਿ ਐਮਬੈਡਿੰਗ ਦੇ ਬਾਅਦ ਹੋਣ ਵਾਲੀ ਐਗਰੀਗੇਸ਼ਨ ਪ੍ਰਕਿਰਿਆ ਮੂਲ ਪਾਠ ਤੋਂ ਇਸ ਜਾਣਕਾਰੀ ਨੂੰ ਹਟਾ ਦਿੰਦੀ ਹੈ। ਕਿਉਂਕਿ ਇਹ ਮਾਡਲ ਸ਼ਬਦਾਂ ਦੇ ਕ੍ਰਮ ਨੂੰ ਪ੍ਰਸਤੁਤ ਕਰਨ ਵਿੱਚ ਅਸਮਰੱਥ ਹਨ, ਇਸ ਲਈ ਇਹ ਪਾਠ ਜਨਰੇਸ਼ਨ ਜਾਂ ਪ੍ਰਸ਼ਨ ਉੱਤਰ ਦੇਣ ਵਰਗੇ ਜਟਿਲ ਜਾਂ ਅਸਪਸ਼ਟ ਕੰਮਾਂ ਨੂੰ ਹੱਲ ਨਹੀਂ ਕਰ ਸਕਦੇ।\n",
    "\n",
    "ਪਾਠ ਕ੍ਰਮ ਦੇ ਅਰਥ ਨੂੰ ਕੈਪਚਰ ਕਰਨ ਲਈ, ਅਸੀਂ ਇੱਕ ਨਿਊਰਲ ਨੈਟਵਰਕ ਆਰਕੀਟੈਕਚਰ ਵਰਤਾਂਗੇ ਜਿਸਨੂੰ **ਰਿਕਰੰਟ ਨਿਊਰਲ ਨੈਟਵਰਕ** ਜਾਂ RNN ਕਿਹਾ ਜਾਂਦਾ ਹੈ। RNN ਵਰਤਦੇ ਸਮੇਂ, ਅਸੀਂ ਆਪਣੀ ਵਾਕ ਨੂੰ ਨੈਟਵਰਕ ਵਿੱਚ ਇੱਕ ਟੋਕਨ ਇੱਕ ਵਾਰ ਵਿੱਚ ਪਾਸ ਕਰਦੇ ਹਾਂ, ਅਤੇ ਨੈਟਵਰਕ ਕੁਝ **ਸਟੇਟ** ਤਿਆਰ ਕਰਦਾ ਹੈ, ਜਿਸਨੂੰ ਅਸੀਂ ਅਗਲੇ ਟੋਕਨ ਦੇ ਨਾਲ ਫਿਰ ਨੈਟਵਰਕ ਵਿੱਚ ਪਾਸ ਕਰਦੇ ਹਾਂ।\n",
    "\n",
    "![ਇੱਕ ਉਦਾਹਰਣ ਰਿਕਰੰਟ ਨਿਊਰਲ ਨੈਟਵਰਕ ਜਨਰੇਸ਼ਨ ਦਿਖਾਉਂਦੀ ਤਸਵੀਰ।](../../../../../translated_images/pa/rnn.27f5c29c53d727b5.webp)\n",
    "\n",
    "ਜਦੋਂ ਟੋਕਨ ਦਾ ਇਨਪੁਟ ਕ੍ਰਮ $X_0,\\dots,X_n$ ਦਿੱਤਾ ਜਾਂਦਾ ਹੈ, RNN ਨਿਊਰਲ ਨੈਟਵਰਕ ਬਲਾਕਾਂ ਦੀ ਇੱਕ ਲੜੀ ਬਣਾਉਂਦਾ ਹੈ ਅਤੇ ਇਸ ਲੜੀ ਨੂੰ ਬੈਕਪ੍ਰੋਪਾਗੇਸ਼ਨ ਦੀ ਵਰਤੋਂ ਕਰਕੇ ਐਂਡ-ਟੂ-ਐਂਡ ਟ੍ਰੇਨ ਕਰਦਾ ਹੈ। ਹਰ ਨੈਟਵਰਕ ਬਲਾਕ ਇੱਕ ਜੋੜੇ $(X_i,S_i)$ ਨੂੰ ਇਨਪੁਟ ਵਜੋਂ ਲੈਂਦਾ ਹੈ ਅਤੇ ਨਤੀਜੇ ਵਜੋਂ $S_{i+1}$ ਤਿਆਰ ਕਰਦਾ ਹੈ। ਅੰਤਿਮ ਸਟੇਟ $S_n$ ਜਾਂ ਆਉਟਪੁਟ $Y_n$ ਨੂੰ ਨਤੀਜਾ ਤਿਆਰ ਕਰਨ ਲਈ ਇੱਕ ਲੀਨੀਅਰ ਕਲਾਸੀਫਾਇਰ ਵਿੱਚ ਪਾਸ ਕੀਤਾ ਜਾਂਦਾ ਹੈ। ਸਾਰੇ ਨੈਟਵਰਕ ਬਲਾਕ ਇੱਕੋ ਜਿਹੇ ਵਜ਼ਨ ਸਾਂਝੇ ਕਰਦੇ ਹਨ ਅਤੇ ਇੱਕੋ ਬੈਕਪ੍ਰੋਪਾਗੇਸ਼ਨ ਪਾਸ ਦੀ ਵਰਤੋਂ ਕਰਕੇ ਐਂਡ-ਟੂ-ਐਂਡ ਟ੍ਰੇਨ ਕੀਤੇ ਜਾਂਦੇ ਹਨ।\n",
    "\n",
    "> ਉੱਪਰ ਦਿੱਤੀ ਤਸਵੀਰ ਵਿੱਚ ਰਿਕਰੰਟ ਨਿਊਰਲ ਨੈਟਵਰਕ ਨੂੰ ਅਨਰੋਲਡ ਰੂਪ ਵਿੱਚ (ਖੱਬੇ ਪਾਸੇ) ਅਤੇ ਜ਼ਿਆਦਾ ਸੰਕੁਚਿਤ ਰਿਕਰੰਟ ਪ੍ਰਸਤੁਤੀ ਵਿੱਚ (ਸੱਜੇ ਪਾਸੇ) ਦਿਖਾਇਆ ਗਿਆ ਹੈ। ਇਹ ਸਮਝਣਾ ਮਹੱਤਵਪੂਰਨ ਹੈ ਕਿ ਸਾਰੇ RNN ਸੈੱਲ ਇੱਕੋ **ਸਾਂਝੇ ਵਜ਼ਨ** ਰੱਖਦੇ ਹਨ।\n",
    "\n",
    "ਕਿਉਂਕਿ ਸਟੇਟ ਵੇਕਟਰ $S_0,\\dots,S_n$ ਨੈਟਵਰਕ ਵਿੱਚ ਪਾਸ ਕੀਤੇ ਜਾਂਦੇ ਹਨ, RNN ਸ਼ਬਦਾਂ ਦੇ ਕ੍ਰਮਿਕ ਨਾਤਿਆਂ ਨੂੰ ਸਿੱਖਣ ਦੇ ਯੋਗ ਹੁੰਦਾ ਹੈ। ਉਦਾਹਰਣ ਲਈ, ਜਦੋਂ ਸ਼ਬਦ *ਨਹੀਂ* ਕ੍ਰਮ ਵਿੱਚ ਕਿਤੇ ਵੀ ਆਉਂਦਾ ਹੈ, ਇਹ ਸਟੇਟ ਵੇਕਟਰ ਦੇ ਕੁਝ ਤੱਤਾਂ ਨੂੰ ਨਕਾਰਾਤਮਕ ਕਰਨ ਲਈ ਸਿੱਖ ਸਕਦਾ ਹੈ।\n",
    "\n",
    "ਅੰਦਰੋਂ, ਹਰ RNN ਸੈੱਲ ਵਿੱਚ ਦੋ ਵਜ਼ਨ ਮੈਟ੍ਰਿਕਸ $W_H$ ਅਤੇ $W_I$, ਅਤੇ ਬਾਇਸ $b$ ਹੁੰਦੇ ਹਨ। ਹਰ RNN ਕਦਮ 'ਤੇ, ਦਿੱਤੇ ਗਏ ਇਨਪੁਟ $X_i$ ਅਤੇ ਇਨਪੁਟ ਸਟੇਟ $S_i$, ਆਉਟਪੁਟ ਸਟੇਟ ਨੂੰ ਇਸ ਤਰ੍ਹਾਂ ਗਣਨਾ ਕੀਤੀ ਜਾਂਦੀ ਹੈ: $S_{i+1} = f(W_H\\times S_i + W_I\\times X_i+b)$, ਜਿੱਥੇ $f$ ਇੱਕ ਐਕਟੀਵੇਸ਼ਨ ਫੰਕਸ਼ਨ ਹੈ (ਅਕਸਰ $\\tanh$)।\n",
    "\n",
    "> ਪਾਠ ਜਨਰੇਸ਼ਨ (ਜਿਸ ਬਾਰੇ ਅਸੀਂ ਅਗਲੇ ਯੂਨਿਟ ਵਿੱਚ ਗੱਲ ਕਰਾਂਗੇ) ਜਾਂ ਮਸ਼ੀਨ ਅਨੁਵਾਦ ਵਰਗੀਆਂ ਸਮੱਸਿਆਵਾਂ ਲਈ, ਅਸੀਂ ਹਰ RNN ਕਦਮ 'ਤੇ ਕੁਝ ਆਉਟਪੁਟ ਮੁੱਲ ਵੀ ਪ੍ਰਾਪਤ ਕਰਨਾ ਚਾਹੁੰਦੇ ਹਾਂ। ਇਸ ਮਾਮਲੇ ਵਿੱਚ, ਇੱਕ ਹੋਰ ਮੈਟ੍ਰਿਕਸ $W_O$ ਵੀ ਹੁੰਦੀ ਹੈ, ਅਤੇ ਆਉਟਪੁਟ ਨੂੰ ਇਸ ਤਰ੍ਹਾਂ ਗਣਨਾ ਕੀਤੀ ਜਾਂਦੀ ਹੈ: $Y_i=f(W_O\\times S_i+b_O)$।\n",
    "\n",
    "ਆਓ ਵੇਖੀਏ ਕਿ ਰਿਕਰੰਟ ਨਿਊਰਲ ਨੈਟਵਰਕਸ ਕਿਵੇਂ ਸਾਡੇ ਖ਼ਬਰਾਂ ਦੇ ਡਾਟਾਸੈਟ ਨੂੰ ਕਲਾਸੀਫਾਈ ਕਰਨ ਵਿੱਚ ਸਹਾਇਤਾ ਕਰ ਸਕਦੇ ਹਨ।\n",
    "\n",
    "> ਸੈਂਡਬਾਕਸ ਵਾਤਾਵਰਣ ਲਈ, ਸਾਨੂੰ ਇਹ ਯਕੀਨੀ ਬਣਾਉਣ ਲਈ ਹੇਠਾਂ ਦਿੱਤਾ ਸੈੱਲ ਚਲਾਉਣਾ ਪਵੇਗਾ ਕਿ ਲੋੜੀਂਦੀ ਲਾਇਬ੍ਰੇਰੀ ਇੰਸਟਾਲ ਹੈ ਅਤੇ ਡਾਟਾ ਪਹਿਲਾਂ ਤੋਂ ਪ੍ਰਾਪਤ ਕੀਤਾ ਗਿਆ ਹੈ। ਜੇ ਤੁਸੀਂ ਲੋਕਲ ਮਸ਼ੀਨ 'ਤੇ ਚਲਾ ਰਹੇ ਹੋ, ਤਾਂ ਤੁਸੀਂ ਹੇਠਾਂ ਦਿੱਤੇ ਸੈੱਲ ਨੂੰ ਛੱਡ ਸਕਦੇ ਹੋ।\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install --quiet tensorflow_datasets==4.4.0\n",
    "!cd ~ && wget -q -O - https://mslearntensorflowlp.blob.core.windows.net/data/tfds-ag-news.tgz | tar xz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "\n",
    "# We are going to be training pretty large models. In order not to face errors, we need\n",
    "# to set tensorflow option to grow GPU memory allocation when required\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "if len(physical_devices)>0:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "ds_train, ds_test = tfds.load('ag_news_subset').values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "ਜਦੋਂ ਵੱਡੇ ਮਾਡਲਾਂ ਨੂੰ ਟ੍ਰੇਨ ਕੀਤਾ ਜਾਂਦਾ ਹੈ, ਤਾਂ GPU ਮੈਮੋਰੀ ਦਾ ਵੰਡਣ ਸਮੱਸਿਆ ਬਣ ਸਕਦਾ ਹੈ। ਸਾਨੂੰ ਵੱਖ-ਵੱਖ ਮਿਨੀਬੈਚ ਆਕਾਰਾਂ ਨਾਲ ਪ੍ਰਯੋਗ ਕਰਨ ਦੀ ਲੋੜ ਵੀ ਪੈ ਸਕਦੀ ਹੈ, ਤਾਂ ਜੋ ਡਾਟਾ ਸਾਡੇ GPU ਮੈਮੋਰੀ ਵਿੱਚ ਫਿੱਟ ਹੋ ਜਾਵੇ, ਫਿਰ ਵੀ ਟ੍ਰੇਨਿੰਗ ਕਾਫੀ ਤੇਜ਼ ਹੋਵੇ। ਜੇ ਤੁਸੀਂ ਇਹ ਕੋਡ ਆਪਣੇ GPU ਮਸ਼ੀਨ 'ਤੇ ਚਲਾ ਰਹੇ ਹੋ, ਤਾਂ ਤੁਸੀਂ ਟ੍ਰੇਨਿੰਗ ਦੀ ਗਤੀ ਵਧਾਉਣ ਲਈ ਮਿਨੀਬੈਚ ਆਕਾਰ ਨੂੰ ਸਮਾਯੋਜਿਤ ਕਰਨ ਦਾ ਪ੍ਰਯੋਗ ਕਰ ਸਕਦੇ ਹੋ।\n",
    "\n",
    "> **Note**: ਕੁਝ ਖਾਸ ਵਰਜਨਾਂ ਦੇ NVidia ਡਰਾਈਵਰਾਂ ਨੂੰ ਮਾਡਲ ਟ੍ਰੇਨਿੰਗ ਤੋਂ ਬਾਅਦ ਮੈਮੋਰੀ ਰਿਲੀਜ਼ ਨਾ ਕਰਨ ਲਈ ਜਾਣਿਆ ਜਾਂਦਾ ਹੈ। ਅਸੀਂ ਇਸ ਨੋਟਬੁੱਕ ਵਿੱਚ ਕਈ ਉਦਾਹਰਣ ਚਲਾ ਰਹੇ ਹਾਂ, ਅਤੇ ਇਹ ਕੁਝ ਸੈਟਅੱਪਾਂ ਵਿੱਚ ਮੈਮੋਰੀ ਖਤਮ ਹੋਣ ਦਾ ਕਾਰਨ ਬਣ ਸਕਦਾ ਹੈ, ਖਾਸ ਕਰਕੇ ਜੇ ਤੁਸੀਂ ਇਸੇ ਨੋਟਬੁੱਕ ਦੇ ਹਿੱਸੇ ਵਜੋਂ ਆਪਣੇ ਪ੍ਰਯੋਗ ਕਰ ਰਹੇ ਹੋ। ਜੇ ਤੁਸੀਂ ਮਾਡਲ ਟ੍ਰੇਨਿੰਗ ਸ਼ੁਰੂ ਕਰਦੇ ਸਮੇਂ ਕੁਝ ਅਜੀਬ ਗਲਤੀਆਂ ਦਾ ਸਾਹਮਣਾ ਕਰਦੇ ਹੋ, ਤਾਂ ਤੁਸੀਂ ਨੋਟਬੁੱਕ ਕਰਨਲ ਨੂੰ ਰੀਸਟਾਰਟ ਕਰਨ ਦੀ ਸੋਚ ਸਕਦੇ ਹੋ।\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "embed_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ਸਧਾਰਨ RNN ਕਲਾਸੀਫਾਇਰ\n",
    "\n",
    "ਸਧਾਰਨ RNN ਦੇ ਮਾਮਲੇ ਵਿੱਚ, ਹਰ ਰਿਕਰਨਟ ਯੂਨਿਟ ਇੱਕ ਸਧਾਰਨ ਰੇਖੀ ਨੈੱਟਵਰਕ ਹੁੰਦਾ ਹੈ, ਜੋ ਇੱਕ ਇਨਪੁਟ ਵੇਕਟਰ ਅਤੇ ਸਟੇਟ ਵੇਕਟਰ ਲੈਂਦਾ ਹੈ ਅਤੇ ਇੱਕ ਨਵਾਂ ਸਟੇਟ ਵੇਕਟਰ ਤਿਆਰ ਕਰਦਾ ਹੈ। Keras ਵਿੱਚ, ਇਸਨੂੰ `SimpleRNN` ਲੇਅਰ ਨਾਲ ਦਰਸਾਇਆ ਜਾ ਸਕਦਾ ਹੈ।\n",
    "\n",
    "ਜਦੋਂ ਕਿ ਅਸੀਂ ਇੱਕ-ਹਾਟ ਕੋਡ ਕੀਤੇ ਟੋਕਨ ਨੂੰ ਸਿੱਧੇ RNN ਲੇਅਰ ਵਿੱਚ ਪਾਸ ਕਰ ਸਕਦੇ ਹਾਂ, ਇਹ ਚੰਗਾ ਵਿਚਾਰ ਨਹੀਂ ਹੈ ਕਿਉਂਕਿ ਇਹਨਾਂ ਦੀ ਉੱਚ ਡਾਈਮੇਂਸ਼ਨਲਿਟੀ ਹੁੰਦੀ ਹੈ। ਇਸ ਲਈ, ਅਸੀਂ ਸ਼ਬਦ ਵੇਕਟਰਾਂ ਦੀ ਡਾਈਮੇਂਸ਼ਨਲਿਟੀ ਘਟਾਉਣ ਲਈ ਇੱਕ ਐਮਬੈਡਿੰਗ ਲੇਅਰ ਦੀ ਵਰਤੋਂ ਕਰਾਂਗੇ, ਜਿਸ ਤੋਂ ਬਾਅਦ ਇੱਕ RNN ਲੇਅਰ ਅਤੇ ਅੰਤ ਵਿੱਚ ਇੱਕ `Dense` ਕਲਾਸੀਫਾਇਰ।\n",
    "\n",
    "> **Note**: ਜਿਨ੍ਹਾਂ ਮਾਮਲਿਆਂ ਵਿੱਚ ਡਾਈਮੇਂਸ਼ਨਲਿਟੀ ਇੰਨੀ ਉੱਚ ਨਹੀਂ ਹੁੰਦੀ, ਉਦਾਹਰਣ ਲਈ ਜਦੋਂ ਅੱਖਰ-ਪੱਧਰ ਦੀ ਟੋਕਨਾਈਜ਼ੇਸ਼ਨ ਵਰਤੀ ਜਾਂਦੀ ਹੈ, ਤਾਂ ਇੱਕ-ਹਾਟ ਕੋਡ ਕੀਤੇ ਟੋਕਨ ਨੂੰ ਸਿੱਧੇ RNN ਸੈੱਲ ਵਿੱਚ ਪਾਸ ਕਰਨਾ ਸਮਝਦਾਰ ਹੋ ਸਕਦਾ ਹੈ।\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "text_vectorization (TextVect (None, None)              0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, None, 64)          1280000   \n",
      "_________________________________________________________________\n",
      "simple_rnn (SimpleRNN)       (None, 16)                1296      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 4)                 68        \n",
      "=================================================================\n",
      "Total params: 1,281,364\n",
      "Trainable params: 1,281,364\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 20000\n",
    "\n",
    "vectorizer = keras.layers.experimental.preprocessing.TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    input_shape=(1,))\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    vectorizer,\n",
    "    keras.layers.Embedding(vocab_size, embed_size),\n",
    "    keras.layers.SimpleRNN(16),\n",
    "    keras.layers.Dense(4,activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **ਨੋਟ:** ਅਸੀਂ ਸਾਦਗੀ ਲਈ ਇੱਥੇ ਇੱਕ ਅਣ-ਟ੍ਰੇਨ ਕੀਤੀ ਗਈ ਐਮਬੈਡਿੰਗ ਲੇਅਰ ਦੀ ਵਰਤੋਂ ਕਰਦੇ ਹਾਂ, ਪਰ ਵਧੀਆ ਨਤੀਜੇ ਪ੍ਰਾਪਤ ਕਰਨ ਲਈ ਅਸੀਂ ਪਿਛਲੇ ਯੂਨਿਟ ਵਿੱਚ ਵਰਣਨ ਕੀਤੇ ਗਏ Word2Vec ਦੀ ਵਰਤੋਂ ਕਰਕੇ ਇੱਕ ਪ੍ਰੀ-ਟ੍ਰੇਨ ਕੀਤੀ ਗਈ ਐਮਬੈਡਿੰਗ ਲੇਅਰ ਦੀ ਵਰਤੋਂ ਕਰ ਸਕਦੇ ਹਾਂ। ਇਹ ਤੁਹਾਡੇ ਲਈ ਇੱਕ ਚੰਗਾ ਅਭਿਆਸ ਹੋਵੇਗਾ ਕਿ ਤੁਸੀਂ ਇਸ ਕੋਡ ਨੂੰ ਪ੍ਰੀ-ਟ੍ਰੇਨ ਕੀਤੇ ਐਮਬੈਡਿੰਗ ਨਾਲ ਕੰਮ ਕਰਨ ਲਈ ਅਨੁਕੂਲਿਤ ਕਰੋ।\n",
    "\n",
    "ਹੁਣ ਆਓ ਅਸੀਂ ਆਪਣੀ RNN ਨੂੰ ਟ੍ਰੇਨ ਕਰੀਏ। ਆਮ ਤੌਰ 'ਤੇ RNN ਨੂੰ ਟ੍ਰੇਨ ਕਰਨਾ ਕਾਫ਼ੀ ਮੁਸ਼ਕਲ ਹੁੰਦਾ ਹੈ, ਕਿਉਂਕਿ ਜਦੋਂ RNN ਸੈਲਜ਼ ਨੂੰ ਸੀਕਵੈਂਸ ਦੀ ਲੰਬਾਈ ਦੇ ਨਾਲ ਅਨਰੋਲ ਕੀਤਾ ਜਾਂਦਾ ਹੈ, ਤਾਂ ਬੈਕਪ੍ਰੋਪਾਗੇਸ਼ਨ ਵਿੱਚ ਸ਼ਾਮਲ ਲੇਅਰਾਂ ਦੀ ਗਿਣਤੀ ਕਾਫ਼ੀ ਵੱਧ ਹੁੰਦੀ ਹੈ। ਇਸ ਲਈ ਸਾਨੂੰ ਇੱਕ ਛੋਟਾ ਲਰਨਿੰਗ ਰੇਟ ਚੁਣਨਾ ਪੈਂਦਾ ਹੈ ਅਤੇ ਵਧੀਆ ਨਤੀਜੇ ਪ੍ਰਾਪਤ ਕਰਨ ਲਈ ਨੈੱਟਵਰਕ ਨੂੰ ਇੱਕ ਵੱਡੇ ਡਾਟਾਸੈੱਟ 'ਤੇ ਟ੍ਰੇਨ ਕਰਨਾ ਪੈਂਦਾ ਹੈ। ਇਹ ਕਾਫ਼ੀ ਸਮਾਂ ਲੈ ਸਕਦਾ ਹੈ, ਇਸ ਲਈ GPU ਦੀ ਵਰਤੋਂ ਕਰਨਾ ਤਰਜੀਹੀ ਹੈ।\n",
    "\n",
    "ਚੀਜ਼ਾਂ ਨੂੰ ਤੇਜ਼ ਕਰਨ ਲਈ, ਅਸੀਂ ਸਿਰਫ਼ ਖ਼ਬਰਾਂ ਦੇ ਸਿਰਲੇਖਾਂ 'ਤੇ RNN ਮਾਡਲ ਨੂੰ ਟ੍ਰੇਨ ਕਰਾਂਗੇ, ਵੇਰਵਾ ਨੂੰ ਛੱਡ ਕੇ। ਤੁਸੀਂ ਵੇਰਵਾ ਨਾਲ ਟ੍ਰੇਨ ਕਰਨ ਦੀ ਕੋਸ਼ਿਸ਼ ਕਰ ਸਕਦੇ ਹੋ ਅਤੇ ਦੇਖ ਸਕਦੇ ਹੋ ਕਿ ਕੀ ਤੁਸੀਂ ਮਾਡਲ ਨੂੰ ਟ੍ਰੇਨ ਕਰ ਸਕਦੇ ਹੋ।\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training vectorizer\n"
     ]
    }
   ],
   "source": [
    "def extract_title(x):\n",
    "    return x['title']\n",
    "\n",
    "def tupelize_title(x):\n",
    "    return (extract_title(x),x['label'])\n",
    "\n",
    "print('Training vectorizer')\n",
    "vectorizer.adapt(ds_train.take(2000).map(extract_title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 82s 11ms/step - loss: 0.6629 - acc: 0.7623 - val_loss: 0.5559 - val_acc: 0.7995\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f3e0030d350>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'], optimizer='adam')\n",
    "model.fit(ds_train.map(tupelize_title).batch(batch_size),validation_data=ds_test.map(tupelize_title).batch(batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "**ਨੋਟ** ਕਿ ਇੱਥੇ ਸਹੀਪਨ ਦੀ ਸੰਭਾਵਨਾ ਘੱਟ ਹੋ ਸਕਦੀ ਹੈ, ਕਿਉਂਕਿ ਅਸੀਂ ਸਿਰਫ਼ ਖ਼ਬਰਾਂ ਦੇ ਸਿਰਲੇਖਾਂ 'ਤੇ ਹੀ ਪ੍ਰਸ਼ਿਕਸ਼ਣ ਕਰ ਰਹੇ ਹਾਂ।\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ਵੈਰੀਏਬਲ ਸੀਕਵੈਂਸ ਦੁਬਾਰਾ ਵੇਖਣਾ\n",
    "\n",
    "ਯਾਦ ਰੱਖੋ ਕਿ `TextVectorization` ਲੇਅਰ ਮਿਨੀਬੈਚ ਵਿੱਚ ਵੱਖ-ਵੱਖ ਲੰਬਾਈਆਂ ਵਾਲੀਆਂ ਸੀਕਵੈਂਸ ਨੂੰ ਆਟੋਮੈਟਿਕ ਤੌਰ 'ਤੇ ਪੈਡ ਟੋਕਨ ਨਾਲ ਪੈਡ ਕਰ ਦੇਵੇਗੀ। ਇਹ ਪਤਾ ਲੱਗਿਆ ਹੈ ਕਿ ਇਹ ਟੋਕਨ ਵੀ ਟ੍ਰੇਨਿੰਗ ਵਿੱਚ ਹਿੱਸਾ ਲੈਂਦੇ ਹਨ, ਅਤੇ ਇਹ ਮਾਡਲ ਦੀ ਕਨਵਰਜੈਂਸ ਨੂੰ ਮੁਸ਼ਕਲ ਬਣਾ ਸਕਦੇ ਹਨ।\n",
    "\n",
    "ਪੈਡਿੰਗ ਦੀ ਮਾਤਰਾ ਨੂੰ ਘਟਾਉਣ ਲਈ ਅਸੀਂ ਕਈ ਤਰੀਕੇ ਅਪਣਾ ਸਕਦੇ ਹਾਂ। ਇਨ੍ਹਾਂ ਵਿੱਚੋਂ ਇੱਕ ਤਰੀਕਾ ਹੈ ਡਾਟਾਸੈਟ ਨੂੰ ਸੀਕਵੈਂਸ ਦੀ ਲੰਬਾਈ ਦੇ ਅਨੁਸਾਰ ਦੁਬਾਰਾ ਕ੍ਰਮਬੱਧ ਕਰਨਾ ਅਤੇ ਸਾਰੀਆਂ ਸੀਕਵੈਂਸ ਨੂੰ ਉਨ੍ਹਾਂ ਦੇ ਆਕਾਰ ਦੇ ਅਨੁਸਾਰ ਗਰੁੱਪ ਕਰਨਾ। ਇਹ ਕੰਮ `tf.data.experimental.bucket_by_sequence_length` ਫੰਕਸ਼ਨ ਦੀ ਵਰਤੋਂ ਕਰਕੇ ਕੀਤਾ ਜਾ ਸਕਦਾ ਹੈ (ਵੇਖੋ [ਡਾਕੂਮੈਂਟੇਸ਼ਨ](https://www.tensorflow.org/api_docs/python/tf/data/experimental/bucket_by_sequence_length))।\n",
    "\n",
    "ਇੱਕ ਹੋਰ ਤਰੀਕਾ ਹੈ **ਮਾਸਕਿੰਗ** ਦੀ ਵਰਤੋਂ। Keras ਵਿੱਚ, ਕੁਝ ਲੇਅਰ ਅਤਿਰਿਕਤ ਇਨਪੁਟ ਨੂੰ ਸਹਾਰਾ ਦਿੰਦੇ ਹਨ ਜੋ ਦੱਸਦਾ ਹੈ ਕਿ ਕਿਹੜੇ ਟੋਕਨ ਨੂੰ ਟ੍ਰੇਨਿੰਗ ਦੌਰਾਨ ਧਿਆਨ ਵਿੱਚ ਰੱਖਣਾ ਚਾਹੀਦਾ ਹੈ। ਆਪਣੇ ਮਾਡਲ ਵਿੱਚ ਮਾਸਕਿੰਗ ਨੂੰ ਸ਼ਾਮਲ ਕਰਨ ਲਈ, ਅਸੀਂ ਜਾਂ ਤਾਂ ਇੱਕ ਵੱਖਰਾ `Masking` ਲੇਅਰ ਸ਼ਾਮਲ ਕਰ ਸਕਦੇ ਹਾਂ ([ਡਾਕਸ](https://keras.io/api/layers/core_layers/masking/)), ਜਾਂ ਅਸੀਂ ਆਪਣੇ `Embedding` ਲੇਅਰ ਦੇ `mask_zero=True` ਪੈਰਾਮੀਟਰ ਨੂੰ ਸੈਟ ਕਰ ਸਕਦੇ ਹਾਂ।\n",
    "\n",
    "> **Note**: ਇਸ ਟ੍ਰੇਨਿੰਗ ਨੂੰ ਪੂਰੇ ਡਾਟਾਸੈਟ 'ਤੇ ਇੱਕ ਐਪੋਕ ਪੂਰਾ ਕਰਨ ਵਿੱਚ ਲਗਭਗ 5 ਮਿੰਟ ਲੱਗਣਗੇ। ਜੇਕਰ ਤੁਹਾਡਾ ਧੀਰਜ ਖਤਮ ਹੋ ਜਾਵੇ ਤਾਂ ਟ੍ਰੇਨਿੰਗ ਨੂੰ ਕਿਸੇ ਵੀ ਸਮੇਂ ਰੋਕ ਸਕਦੇ ਹੋ। ਤੁਸੀਂ ਇਹ ਵੀ ਕਰ ਸਕਦੇ ਹੋ ਕਿ ਟ੍ਰੇਨਿੰਗ ਲਈ ਵਰਤੇ ਜਾਣ ਵਾਲੇ ਡਾਟੇ ਦੀ ਮਾਤਰਾ ਨੂੰ ਸੀਮਿਤ ਕਰ ਲਵੋ, `ds_train` ਅਤੇ `ds_test` ਡਾਟਾਸੈਟਾਂ ਦੇ ਬਾਅਦ `.take(...)` ਕਲੌਜ਼ ਸ਼ਾਮਲ ਕਰਕੇ।\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 371s 49ms/step - loss: 0.5401 - acc: 0.8079 - val_loss: 0.3780 - val_acc: 0.8822\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f3dec118850>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_text(x):\n",
    "    return x['title']+' '+x['description']\n",
    "\n",
    "def tupelize(x):\n",
    "    return (extract_text(x),x['label'])\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    vectorizer,\n",
    "    keras.layers.Embedding(vocab_size,embed_size,mask_zero=True),\n",
    "    keras.layers.SimpleRNN(16),\n",
    "    keras.layers.Dense(4,activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'], optimizer='adam')\n",
    "model.fit(ds_train.map(tupelize).batch(batch_size),validation_data=ds_test.map(tupelize).batch(batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ਹੁਣ ਜਦੋਂ ਅਸੀਂ ਮਾਸਕਿੰਗ ਵਰਤ ਰਹੇ ਹਾਂ, ਅਸੀਂ ਸਾਰੇ ਟਾਈਟਲ ਅਤੇ ਵੇਰਵੇ ਦੇ ਡੇਟਾਸੈੱਟ 'ਤੇ ਮਾਡਲ ਨੂੰ ਟ੍ਰੇਨ ਕਰ ਸਕਦੇ ਹਾਂ।\n",
    "\n",
    "> **Note**: ਕੀ ਤੁਸੀਂ ਧਿਆਨ ਦਿੱਤਾ ਹੈ ਕਿ ਅਸੀਂ ਖ਼ਬਰਾਂ ਦੇ ਟਾਈਟਲ 'ਤੇ ਟ੍ਰੇਨ ਕੀਤੇ ਗਏ ਵੈਕਟੋਰਾਈਜ਼ਰ ਨੂੰ ਵਰਤ ਰਹੇ ਹਾਂ, ਨਾ ਕਿ ਲੇਖ ਦੇ ਪੂਰੇ ਸਰੀਰ 'ਤੇ? ਸੰਭਾਵਨਾ ਹੈ ਕਿ ਇਸ ਕਾਰਨ ਕੁਝ ਟੋਕਨਜ਼ ਨੂੰ ਅਣਡਿੱਠਾ ਛੱਡਿਆ ਜਾ ਸਕਦਾ ਹੈ, ਇਸ ਲਈ ਵੈਕਟੋਰਾਈਜ਼ਰ ਨੂੰ ਦੁਬਾਰਾ ਟ੍ਰੇਨ ਕਰਨਾ ਬਿਹਤਰ ਹੋਵੇਗਾ। ਹਾਲਾਂਕਿ, ਇਸਦਾ ਪ੍ਰਭਾਵ ਬਹੁਤ ਘੱਟ ਹੋ ਸਕਦਾ ਹੈ, ਇਸ ਲਈ ਸਾਦਗੀ ਲਈ ਅਸੀਂ ਪਿਛਲੇ ਪ੍ਰੀ-ਟ੍ਰੇਨ ਕੀਤੇ ਵੈਕਟੋਰਾਈਜ਼ਰ ਨਾਲ ਹੀ ਚੱਲਾਂਗੇ।\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM: ਲਾਂਗ ਸ਼ਾਰਟ-ਟਰਮ ਮੈਮੋਰੀ\n",
    "\n",
    "RNNs ਦਾ ਇੱਕ ਮੁੱਖ ਸਮੱਸਿਆ **ਵੈਨਿਸ਼ਿੰਗ ਗ੍ਰੇਡੀਅੰਟਸ** ਹੈ। RNNs ਕਾਫੀ ਲੰਬੇ ਹੋ ਸਕਦੇ ਹਨ, ਅਤੇ ਬੈਕਪ੍ਰੋਪਾਗੇਸ਼ਨ ਦੌਰਾਨ ਨੈਟਵਰਕ ਦੀ ਪਹਿਲੀ ਲੇਅਰ ਤੱਕ ਗ੍ਰੇਡੀਅੰਟਸ ਨੂੰ ਪਹੁੰਚਾਉਣ ਵਿੱਚ ਮੁਸ਼ਕਲ ਹੋ ਸਕਦੀ ਹੈ। ਜਦੋਂ ਇਹ ਹੁੰਦਾ ਹੈ, ਨੈਟਵਰਕ ਦੂਰਲੇ ਟੋਕਨਜ਼ ਦੇ ਵਿਚਕਾਰ ਸੰਬੰਧ ਸਿੱਖ ਨਹੀਂ ਸਕਦਾ। ਇਸ ਸਮੱਸਿਆ ਤੋਂ ਬਚਣ ਦਾ ਇੱਕ ਤਰੀਕਾ **ਗੇਟਸ** ਦੀ ਵਰਤੋਂ ਕਰਕੇ **ਸਪਸ਼ਟ ਸਟੇਟ ਮੈਨੇਜਮੈਂਟ** ਲਿਆਉਣਾ ਹੈ। ਦੋ ਸਭ ਤੋਂ ਆਮ ਆਰਕੀਟੈਕਚਰ ਜੋ ਗੇਟਸ ਲਿਆਉਂਦੇ ਹਨ ਉਹ ਹਨ **ਲਾਂਗ ਸ਼ਾਰਟ-ਟਰਮ ਮੈਮੋਰੀ** (LSTM) ਅਤੇ **ਗੇਟਡ ਰੀਲੇਅ ਯੂਨਿਟ** (GRU)। ਅਸੀਂ ਇੱਥੇ LSTMs ਨੂੰ ਕਵਰ ਕਰਾਂਗੇ।\n",
    "\n",
    "![ਲਾਂਗ ਸ਼ਾਰਟ-ਟਰਮ ਮੈਮੋਰੀ ਸੈਲ ਦਾ ਉਦਾਹਰਨ ਦਿਖਾਉਣ ਵਾਲੀ ਚਿੱਤਰ](../../../../../lessons/5-NLP/16-RNN/images/long-short-term-memory-cell.svg)\n",
    "\n",
    "ਇੱਕ LSTM ਨੈਟਵਰਕ RNN ਦੇ ਸਮਾਨ ਢੰਗ ਨਾਲ ਆਯੋਜਿਤ ਹੁੰਦਾ ਹੈ, ਪਰ ਦੋ ਸਟੇਟਸ ਹਨ ਜੋ ਲੇਅਰ ਤੋਂ ਲੇਅਰ ਤੱਕ ਪਾਸ ਕੀਤੇ ਜਾਂਦੇ ਹਨ: ਅਸਲ ਸਟੇਟ $c$, ਅਤੇ ਹਿਡਨ ਵੈਕਟਰ $h$। ਹਰ ਯੂਨਿਟ 'ਤੇ, ਹਿਡਨ ਵੈਕਟਰ $h_{t-1}$ ਨੂੰ ਇਨਪੁਟ $x_t$ ਨਾਲ ਜੋੜਿਆ ਜਾਂਦਾ ਹੈ, ਅਤੇ ਇਹ ਦੋਵੇਂ ਮਿਲ ਕੇ **ਗੇਟਸ** ਰਾਹੀਂ ਸਟੇਟ $c_t$ ਅਤੇ ਆਉਟਪੁਟ $h_{t}$ 'ਤੇ ਕੀ ਹੁੰਦਾ ਹੈ, ਇਸ ਨੂੰ ਨਿਰਧਾਰਤ ਕਰਦੇ ਹਨ। ਹਰ ਗੇਟ ਵਿੱਚ ਸਿਗਮਾਇਡ ਐਕਟੀਵੇਸ਼ਨ ਹੁੰਦੀ ਹੈ (ਆਉਟਪੁਟ $[0,1]$ ਦੀ ਰੇਂਜ ਵਿੱਚ), ਜਿਸ ਨੂੰ ਸਟੇਟ ਵੈਕਟਰ ਨਾਲ ਗੁਣਾ ਕਰਨ 'ਤੇ ਬਿਟਵਾਈਜ਼ ਮਾਸਕ ਵਜੋਂ ਸੋਚਿਆ ਜਾ ਸਕਦਾ ਹੈ। LSTMs ਵਿੱਚ ਹੇਠਾਂ ਦਿੱਤੇ ਗੇਟਸ ਹੁੰਦੇ ਹਨ (ਉਪਰਲੇ ਚਿੱਤਰ ਵਿੱਚ ਖੱਬੇ ਤੋਂ ਸੱਜੇ ਤੱਕ):\n",
    "* **ਫੋਰਗੇਟ ਗੇਟ** ਜੋ ਇਹ ਨਿਰਧਾਰਤ ਕਰਦਾ ਹੈ ਕਿ ਸਟੇਟ ਵੈਕਟਰ $c_{t-1}$ ਦੇ ਕਿਹੜੇ ਹਿੱਸੇ ਨੂੰ ਭੁੱਲਣਾ ਹੈ ਅਤੇ ਕਿਹੜੇ ਨੂੰ ਪਾਸ ਕਰਨਾ ਹੈ।\n",
    "* **ਇਨਪੁਟ ਗੇਟ** ਜੋ ਇਹ ਨਿਰਧਾਰਤ ਕਰਦਾ ਹੈ ਕਿ ਇਨਪੁਟ ਵੈਕਟਰ ਅਤੇ ਪਿਛਲੇ ਹਿਡਨ ਵੈਕਟਰ ਤੋਂ ਕਿੰਨੀ ਜਾਣਕਾਰੀ ਸਟੇਟ ਵੈਕਟਰ ਵਿੱਚ ਸ਼ਾਮਲ ਕੀਤੀ ਜਾਣੀ ਚਾਹੀਦੀ ਹੈ।\n",
    "* **ਆਉਟਪੁਟ ਗੇਟ** ਜੋ ਨਵੇਂ ਸਟੇਟ ਵੈਕਟਰ ਨੂੰ ਲੈਂਦਾ ਹੈ ਅਤੇ ਇਹ ਨਿਰਧਾਰਤ ਕਰਦਾ ਹੈ ਕਿ ਇਸ ਦੇ ਕਿਹੜੇ ਹਿੱਸੇ ਨੂੰ ਨਵੇਂ ਹਿਡਨ ਵੈਕਟਰ $h_t$ ਪੈਦਾ ਕਰਨ ਲਈ ਵਰਤਿਆ ਜਾਵੇਗਾ।\n",
    "\n",
    "ਸਟੇਟ $c$ ਦੇ ਹਿੱਸਿਆਂ ਨੂੰ ਝੰਡਿਆਂ ਵਜੋਂ ਸੋਚਿਆ ਜਾ ਸਕਦਾ ਹੈ ਜੋ ਚਾਲੂ ਅਤੇ ਬੰਦ ਕੀਤੇ ਜਾ ਸਕਦੇ ਹਨ। ਉਦਾਹਰਨ ਲਈ, ਜਦੋਂ ਅਸੀਂ ਕ੍ਰਮ ਵਿੱਚ ਨਾਮ *Alice* ਨੂੰ ਮਿਲਦੇ ਹਾਂ, ਤਾਂ ਅਸੀਂ ਅਨੁਮਾਨ ਲਗਾਉਂਦੇ ਹਾਂ ਕਿ ਇਹ ਇੱਕ ਔਰਤ ਨੂੰ ਦਰਸਾਉਂਦਾ ਹੈ, ਅਤੇ ਸਟੇਟ ਵਿੱਚ ਝੰਡਾ ਚੁੱਕਦੇ ਹਾਂ ਜੋ ਕਹਿੰਦਾ ਹੈ ਕਿ ਸਾਡੇ ਕੋਲ ਵਾਕ ਵਿੱਚ ਇੱਕ ਮਹਿਲਾ ਸੰਗਿਆ ਹੈ। ਜਦੋਂ ਅਸੀਂ ਅੱਗੇ ਸ਼ਬਦ *and Tom* ਨੂੰ ਮਿਲਦੇ ਹਾਂ, ਤਾਂ ਅਸੀਂ ਝੰਡਾ ਚੁੱਕਦੇ ਹਾਂ ਜੋ ਕਹਿੰਦਾ ਹੈ ਕਿ ਸਾਡੇ ਕੋਲ ਬਹੁਵਚਨ ਸੰਗਿਆ ਹੈ। ਇਸ ਤਰ੍ਹਾਂ ਸਟੇਟ ਨੂੰ ਮੈਨੇਜ ਕਰਕੇ ਅਸੀਂ ਵਾਕ ਦੇ ਵਿਆਕਰਣਕ ਗੁਣਾਂ ਦਾ ਟ੍ਰੈਕ ਰੱਖ ਸਕਦੇ ਹਾਂ।\n",
    "\n",
    "> **Note**: LSTMs ਦੀਆਂ ਅੰਦਰੂਨੀ ਵਿਵਸਥਾਵਾਂ ਨੂੰ ਸਮਝਣ ਲਈ ਇਹ ਇੱਕ ਸ਼ਾਨਦਾਰ ਸਰੋਤ ਹੈ: [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) ਕ੍ਰਿਸਟੋਫਰ ਓਲਾਹ ਦੁਆਰਾ।\n",
    "\n",
    "ਜਦੋਂ ਕਿ ਇੱਕ LSTM ਸੈਲ ਦੀ ਅੰਦਰੂਨੀ ਬਣਤਰ ਜਟਿਲ ਲੱਗ ਸਕਦੀ ਹੈ, Keras ਇਸ ਨੂੰ `LSTM` ਲੇਅਰ ਦੇ ਅੰਦਰ ਲੁਕਾਉਂਦਾ ਹੈ, ਇਸ ਲਈ ਉਪਰਲੇ ਉਦਾਹਰਨ ਵਿੱਚ ਸਾਨੂੰ ਸਿਰਫ ਰੀਕਰਨਟ ਲੇਅਰ ਨੂੰ ਬਦਲਣਾ ਹੈ:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15000/15000 [==============================] - 188s 13ms/step - loss: 0.5692 - acc: 0.7916 - val_loss: 0.3441 - val_acc: 0.8870\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f3d6af5c350>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    vectorizer,\n",
    "    keras.layers.Embedding(vocab_size, embed_size),\n",
    "    keras.layers.LSTM(8),\n",
    "    keras.layers.Dense(4,activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'], optimizer='adam')\n",
    "model.fit(ds_train.map(tupelize).batch(8),validation_data=ds_test.map(tupelize).batch(8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ਦੋ-ਦਿਸ਼ਾਵਾਂ ਅਤੇ ਬਹੁ-ਪਤਰੀ RNNs\n",
    "\n",
    "ਅਜੇ ਤੱਕ ਦੇ ਸਾਡੇ ਉਦਾਹਰਣਾਂ ਵਿੱਚ, ਰਿਕਰੰਟ ਨੈਟਵਰਕ ਸ਼੍ਰੇਣੀ ਦੀ ਸ਼ੁਰੂਆਤ ਤੋਂ ਅੰਤ ਤੱਕ ਕੰਮ ਕਰਦੇ ਹਨ। ਇਹ ਸਾਡੇ ਲਈ ਕੁਦਰਤੀ ਮਹਿਸੂਸ ਹੁੰਦਾ ਹੈ ਕਿਉਂਕਿ ਇਹ ਉਸੇ ਦਿਸ਼ਾ ਵਿੱਚ ਹੈ ਜਿਸ ਵਿੱਚ ਅਸੀਂ ਪੜ੍ਹਦੇ ਹਾਂ ਜਾਂ ਬੋਲਣ ਨੂੰ ਸੁਣਦੇ ਹਾਂ। ਹਾਲਾਂਕਿ, ਉਹ ਸਥਿਤੀਆਂ ਜਿਨ੍ਹਾਂ ਵਿੱਚ ਇਨਪੁਟ ਸ਼੍ਰੇਣੀ ਦੀ ਰੈਂਡਮ ਐਕਸੈਸ ਦੀ ਲੋੜ ਹੁੰਦੀ ਹੈ, ਉਨ੍ਹਾਂ ਲਈ ਦੋਵੇਂ ਦਿਸ਼ਾਵਾਂ ਵਿੱਚ ਰਿਕਰੰਟ ਗਣਨਾ ਚਲਾਉਣਾ ਵਧੀਆ ਹੁੰਦਾ ਹੈ। RNNs ਜੋ ਦੋਵੇਂ ਦਿਸ਼ਾਵਾਂ ਵਿੱਚ ਗਣਨਾ ਦੀ ਆਗਿਆ ਦਿੰਦੇ ਹਨ, **ਦੋ-ਦਿਸ਼ਾਵਾਂ** RNNs ਕਹਾਏ ਜਾਂਦੇ ਹਨ, ਅਤੇ ਇਹਨਾਂ ਨੂੰ ਇੱਕ ਵਿਸ਼ੇਸ਼ `Bidirectional` ਲੇਅਰ ਨਾਲ ਰਿਕਰੰਟ ਲੇਅਰ ਨੂੰ ਲਪੇਟ ਕੇ ਬਣਾਇਆ ਜਾ ਸਕਦਾ ਹੈ।\n",
    "\n",
    "> **Note**: `Bidirectional` ਲੇਅਰ ਆਪਣੇ ਅੰਦਰ ਲੇਅਰ ਦੀਆਂ ਦੋ ਕਾਪੀਆਂ ਬਣਾਉਂਦਾ ਹੈ ਅਤੇ ਉਨ੍ਹਾਂ ਵਿੱਚੋਂ ਇੱਕ ਦੀ `go_backwards` ਪ੍ਰਾਪਰਟੀ ਨੂੰ `True` ਸੈਟ ਕਰਦਾ ਹੈ, ਜਿਸ ਨਾਲ ਇਹ ਸ਼੍ਰੇਣੀ ਦੇ ਉਲਟ ਦਿਸ਼ਾ ਵਿੱਚ ਜਾਂਦਾ ਹੈ।\n",
    "\n",
    "ਰਿਕਰੰਟ ਨੈਟਵਰਕ, ਚਾਹੇ ਇੱਕ-ਦਿਸ਼ਾਵਾਂ ਹੋਣ ਜਾਂ ਦੋ-ਦਿਸ਼ਾਵਾਂ, ਸ਼੍ਰੇਣੀ ਦੇ ਅੰਦਰ ਪੈਟਰਨ ਨੂੰ ਕੈਪਚਰ ਕਰਦੇ ਹਨ ਅਤੇ ਉਨ੍ਹਾਂ ਨੂੰ ਸਟੇਟ ਵੈਕਟਰਾਂ ਵਿੱਚ ਸਟੋਰ ਕਰਦੇ ਹਨ ਜਾਂ ਉਨ੍ਹਾਂ ਨੂੰ ਆਉਟਪੁਟ ਵਜੋਂ ਵਾਪਸ ਕਰਦੇ ਹਨ। ਜਿਵੇਂ ਕਿ ਕਨਵੋਲੂਸ਼ਨਲ ਨੈਟਵਰਕ ਵਿੱਚ ਹੁੰਦਾ ਹੈ, ਅਸੀਂ ਪਹਿਲੇ ਲੇਅਰ ਦੇ ਬਾਅਦ ਇੱਕ ਹੋਰ ਰਿਕਰੰਟ ਲੇਅਰ ਬਣਾਉਣ ਦੇ ਯੋਗ ਹੋ ਸਕਦੇ ਹਾਂ, ਜੋ ਉੱਚ ਪੱਧਰ ਦੇ ਪੈਟਰਨ ਨੂੰ ਕੈਪਚਰ ਕਰਦਾ ਹੈ, ਜੋ ਪਹਿਲੇ ਲੇਅਰ ਦੁਆਰਾ ਕੈਪਚਰ ਕੀਤੇ ਨੀਵੇਂ ਪੱਧਰ ਦੇ ਪੈਟਰਨ ਤੋਂ ਬਣੇ ਹੁੰਦੇ ਹਨ। ਇਸ ਨਾਲ ਸਾਨੂੰ **ਬਹੁ-ਪਤਰੀ RNN** ਦਾ ਧਾਰਨਾ ਮਿਲਦੀ ਹੈ, ਜਿਸ ਵਿੱਚ ਦੋ ਜਾਂ ਵੱਧ ਰਿਕਰੰਟ ਨੈਟਵਰਕ ਹੁੰਦੇ ਹਨ, ਜਿੱਥੇ ਪਿਛਲੇ ਲੇਅਰ ਦਾ ਆਉਟਪੁਟ ਅਗਲੇ ਲੇਅਰ ਨੂੰ ਇਨਪੁਟ ਵਜੋਂ ਦਿੱਤਾ ਜਾਂਦਾ ਹੈ।\n",
    "\n",
    "![ਬਹੁ-ਪਤਰੀ ਲੰਬੇ-ਛੋਟੇ-ਸਮੇਂ-ਯਾਦاشت RNN ਦਿਖਾਉਣ ਵਾਲੀ ਚਿੱਤਰ](../../../../../translated_images/pa/multi-layer-lstm.dd975e29bb2a59fe.webp)\n",
    "\n",
    "*ਫਰਨਾਂਡੋ ਲੋਪੇਜ਼ ਦੁਆਰਾ [ਇਸ ਸ਼ਾਨਦਾਰ ਪੋਸਟ](https://towardsdatascience.com/from-a-lstm-cell-to-a-multilayer-lstm-network-with-pytorch-2899eb5696f3) ਤੋਂ ਚਿੱਤਰ।*\n",
    "\n",
    "Keras ਇਹਨਾਂ ਨੈਟਵਰਕਾਂ ਨੂੰ ਬਣਾਉਣ ਨੂੰ ਇੱਕ ਆਸਾਨ ਕੰਮ ਬਣਾਉਂਦਾ ਹੈ, ਕਿਉਂਕਿ ਤੁਹਾਨੂੰ ਸਿਰਫ ਮਾਡਲ ਵਿੱਚ ਹੋਰ ਰਿਕਰੰਟ ਲੇਅਰਾਂ ਸ਼ਾਮਲ ਕਰਨ ਦੀ ਲੋੜ ਹੁੰਦੀ ਹੈ। ਆਖਰੀ ਲੇਅਰ ਤੋਂ ਇਲਾਵਾ ਸਾਰੇ ਲੇਅਰਾਂ ਲਈ, ਸਾਨੂੰ `return_sequences=True` ਪੈਰਾਮੀਟਰ ਸਪਸ਼ਟ ਕਰਨ ਦੀ ਲੋੜ ਹੁੰਦੀ ਹੈ, ਕਿਉਂਕਿ ਸਾਨੂੰ ਲੇਅਰ ਤੋਂ ਸਾਰੇ ਮੱਧਵਰਤੀ ਸਟੇਟ ਵਾਪਸ ਚਾਹੀਦੇ ਹਨ, ਨਾ ਕਿ ਸਿਰਫ ਰਿਕਰੰਟ ਗਣਨਾ ਦੀ ਅੰਤਮ ਸਟੇਟ।\n",
    "\n",
    "ਆਓ ਸਾਡੇ ਵਰਗੀਕਰਨ ਸਮੱਸਿਆ ਲਈ ਇੱਕ ਦੋ-ਪਤਰੀ ਦੋ-ਦਿਸ਼ਾਵਾਂ LSTM ਬਣਾਈਏ।\n",
    "\n",
    "> **Note** ਇਹ ਕੋਡ ਮੁੜ ਕਾਫੀ ਸਮਾਂ ਲੈਂਦਾ ਹੈ, ਪਰ ਇਹ ਸਾਨੂੰ ਹੁਣ ਤੱਕ ਦੀ ਸਭ ਤੋਂ ਉੱਚ ਸਹੀਤਾ ਦਿੰਦਾ ਹੈ। ਇਸ ਲਈ ਸ਼ਾਇਦ ਇਹ ਉਡੀਕ ਕਰਨ ਅਤੇ ਨਤੀਜਾ ਦੇਖਣ ਵਾਲਾ ਹੈ।\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5044/7500 [===================>..........] - ETA: 2:33 - loss: 0.3709 - acc: 0.8706\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r5045/7500 [===================>..........] - ETA: 2:33 - loss: 0.3709 - acc: 0.8706"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    vectorizer,\n",
    "    keras.layers.Embedding(vocab_size, 128, mask_zero=True),\n",
    "    keras.layers.Bidirectional(keras.layers.LSTM(64,return_sequences=True)),\n",
    "    keras.layers.Bidirectional(keras.layers.LSTM(64)),    \n",
    "    keras.layers.Dense(4,activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'], optimizer='adam')\n",
    "model.fit(ds_train.map(tupelize).batch(batch_size),\n",
    "          validation_data=ds_test.map(tupelize).batch(batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ਹੋਰ ਕੰਮਾਂ ਲਈ RNNs\n",
    "\n",
    "ਅਜੇ ਤੱਕ, ਅਸੀਂ ਲਿਖਤ ਦੇ ਕ੍ਰਮਾਂ ਨੂੰ ਵਰਗਬੱਧ ਕਰਨ ਲਈ RNNs ਦੇ ਇਸਤੇਮਾਲ 'ਤੇ ਧਿਆਨ ਦਿੱਤਾ ਹੈ। ਪਰ ਇਹ ਹੋਰ ਕਈ ਕੰਮਾਂ ਨੂੰ ਸੰਭਾਲ ਸਕਦੇ ਹਨ, ਜਿਵੇਂ ਕਿ ਲਿਖਤ ਸਿਰਜਣਾ ਅਤੇ ਭਾਸ਼ਾ ਅਨੁਵਾਦ — ਅਸੀਂ ਅਗਲੇ ਯੂਨਿਟ ਵਿੱਚ ਉਹ ਕੰਮ ਵੇਖਾਂਗੇ।\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**ਅਸਵੀਕਰਤੀ**:  \nਇਹ ਦਸਤਾਵੇਜ਼ AI ਅਨੁਵਾਦ ਸੇਵਾ [Co-op Translator](https://github.com/Azure/co-op-translator) ਦੀ ਵਰਤੋਂ ਕਰਕੇ ਅਨੁਵਾਦ ਕੀਤਾ ਗਿਆ ਹੈ। ਜਦੋਂ ਕਿ ਅਸੀਂ ਸਹੀ ਹੋਣ ਦੀ ਕੋਸ਼ਿਸ਼ ਕਰਦੇ ਹਾਂ, ਕਿਰਪਾ ਕਰਕੇ ਧਿਆਨ ਦਿਓ ਕਿ ਸਵੈਚਾਲਿਤ ਅਨੁਵਾਦਾਂ ਵਿੱਚ ਗਲਤੀਆਂ ਜਾਂ ਅਸੁਚੱਜੇਪਣ ਹੋ ਸਕਦੇ ਹਨ। ਮੂਲ ਦਸਤਾਵੇਜ਼, ਜੋ ਇਸਦੀ ਮੂਲ ਭਾਸ਼ਾ ਵਿੱਚ ਹੈ, ਨੂੰ ਅਧਿਕਾਰਤ ਸਰੋਤ ਮੰਨਿਆ ਜਾਣਾ ਚਾਹੀਦਾ ਹੈ। ਮਹੱਤਵਪੂਰਨ ਜਾਣਕਾਰੀ ਲਈ, ਪੇਸ਼ੇਵਰ ਮਨੁੱਖੀ ਅਨੁਵਾਦ ਦੀ ਸਿਫਾਰਸ਼ ਕੀਤੀ ਜਾਂਦੀ ਹੈ। ਇਸ ਅਨੁਵਾਦ ਦੀ ਵਰਤੋਂ ਤੋਂ ਪੈਦਾ ਹੋਣ ਵਾਲੇ ਕਿਸੇ ਵੀ ਗਲਤਫਹਿਮੀ ਜਾਂ ਗਲਤ ਵਿਆਖਿਆ ਲਈ ਅਸੀਂ ਜ਼ਿੰਮੇਵਾਰ ਨਹੀਂ ਹਾਂ।\n"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "conda-env-py37_tensorflow-py"
  },
  "kernelspec": {
   "display_name": "py37_tensorflow",
   "language": "python",
   "name": "conda-env-py37_tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "coopTranslator": {
   "original_hash": "81351e61f619b432ff51010a4f993194",
   "translation_date": "2025-08-28T09:36:51+00:00",
   "source_file": "lessons/5-NLP/16-RNN/RNNTF.ipynb",
   "language_code": "pa"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}