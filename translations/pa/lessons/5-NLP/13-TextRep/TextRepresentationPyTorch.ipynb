{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ਟੈਕਸਟ ਵਰਗੀਕਰਨ ਟਾਸਕ\n",
    "\n",
    "ਜਿਵੇਂ ਕਿ ਅਸੀਂ ਜ਼ਿਕਰ ਕੀਤਾ ਹੈ, ਅਸੀਂ **AG_NEWS** ਡੇਟਾਸੈੱਟ ਦੇ ਆਧਾਰ 'ਤੇ ਸਧਾਰਣ ਟੈਕਸਟ ਵਰਗੀਕਰਨ ਟਾਸਕ 'ਤੇ ਧਿਆਨ ਦੇਵਾਂਗੇ, ਜਿਸ ਵਿੱਚ ਖ਼ਬਰਾਂ ਦੇ ਸਿਰਲੇਖਾਂ ਨੂੰ ਚਾਰ ਸ਼੍ਰੇਣੀਆਂ ਵਿੱਚ ਵਰਗੀਕ੍ਰਿਤ ਕਰਨਾ ਸ਼ਾਮਲ ਹੈ: ਵਿਸ਼ਵ, ਖੇਡਾਂ, ਕਾਰੋਬਾਰ ਅਤੇ ਵਿਗਿਆਨ/ਤਕਨਾਲੋਜੀ।\n",
    "\n",
    "## ਡੇਟਾਸੈੱਟ\n",
    "\n",
    "ਇਹ ਡੇਟਾਸੈੱਟ [`torchtext`](https://github.com/pytorch/text) ਮੌਡਿਊਲ ਵਿੱਚ ਸ਼ਾਮਲ ਹੈ, ਇਸ ਲਈ ਅਸੀਂ ਇਸ ਨੂੰ ਆਸਾਨੀ ਨਾਲ ਐਕਸੈਸ ਕਰ ਸਕਦੇ ਹਾਂ।\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "import os\n",
    "import collections\n",
    "os.makedirs('./data',exist_ok=True)\n",
    "train_dataset, test_dataset = torchtext.datasets.AG_NEWS(root='./data')\n",
    "classes = ['World', 'Sports', 'Business', 'Sci/Tech']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ਇੱਥੇ, `train_dataset` ਅਤੇ `test_dataset` ਵਿੱਚ ਉਹ ਕਲੈਕਸ਼ਨ ਸ਼ਾਮਲ ਹਨ ਜੋ ਲੇਬਲ (ਕਲਾਸ ਦੀ ਗਿਣਤੀ) ਅਤੇ ਟੈਕਸਟ ਦੇ ਜੋੜੇ ਵਾਪਸ ਕਰਦੇ ਹਨ, ਉਦਾਹਰਣ ਵਜੋਂ:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,\n",
       " \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(train_dataset)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ਅਸੀਂ ਆਪਣੇ ਡੇਟਾਸੈੱਟ ਵਿੱਚੋਂ ਪਹਿਲੀਆਂ 10 ਨਵੀਆਂ ਸੁਰਖੀਆਂ ਪ੍ਰਿੰਟ ਕਰਦੇ ਹਾਂ:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Sci/Tech** -> Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\band of ultra-cynics, are seeing green again.\n",
      "**Sci/Tech** -> Carlyle Looks Toward Commercial Aerospace (Reuters) Reuters - Private investment firm Carlyle Group,\\which has a reputation for making well-timed and occasionally\\controversial plays in the defense industry, has quietly placed\\its bets on another part of the market.\n",
      "**Sci/Tech** -> Oil and Economy Cloud Stocks' Outlook (Reuters) Reuters - Soaring crude prices plus worries\\about the economy and the outlook for earnings are expected to\\hang over the stock market next week during the depth of the\\summer doldrums.\n",
      "**Sci/Tech** -> Iraq Halts Oil Exports from Main Southern Pipeline (Reuters) Reuters - Authorities have halted oil export\\flows from the main pipeline in southern Iraq after\\intelligence showed a rebel militia could strike\\infrastructure, an oil official said on Saturday.\n",
      "**Sci/Tech** -> Oil prices soar to all-time record, posing new menace to US economy (AFP) AFP - Tearaway world oil prices, toppling records and straining wallets, present a new economic menace barely three months before the US presidential elections.\n"
     ]
    }
   ],
   "source": [
    "for i,x in zip(range(5),train_dataset):\n",
    "    print(f\"**{classes[x[0]]}** -> {x[1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ਡਾਟਾਸੈਟਸ ਇਟਰੇਟਰ ਹੁੰਦੇ ਹਨ, ਜੇਕਰ ਅਸੀਂ ਡਾਟਾ ਕਈ ਵਾਰ ਵਰਤਣਾ ਚਾਹੁੰਦੇ ਹਾਂ ਤਾਂ ਸਾਨੂੰ ਇਸਨੂੰ ਸੂਚੀ ਵਿੱਚ ਤਬਦੀਲ ਕਰਨਾ ਪਵੇਗਾ:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = torchtext.datasets.AG_NEWS(root='./data')\n",
    "train_dataset = list(train_dataset)\n",
    "test_dataset = list(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ਟੋਕਨਾਈਜ਼ੇਸ਼ਨ\n",
    "\n",
    "ਹੁਣ ਸਾਨੂੰ ਟੈਕਸਟ ਨੂੰ **ਨੰਬਰਾਂ** ਵਿੱਚ ਬਦਲਣ ਦੀ ਲੋੜ ਹੈ ਜੋ ਟੈਂਸਰ ਵਜੋਂ ਦਰਸਾਏ ਜਾ ਸਕਦੇ ਹਨ। ਜੇਕਰ ਅਸੀਂ ਸ਼ਬਦ-ਪੱਧਰ ਦੀ ਪ੍ਰਤੀਨਿਧਤਾ ਚਾਹੁੰਦੇ ਹਾਂ, ਤਾਂ ਸਾਨੂੰ ਦੋ ਕੰਮ ਕਰਨੇ ਪੈਂਦੇ ਹਨ:\n",
    "* **ਟੋਕਨਾਈਜ਼ਰ** ਦੀ ਵਰਤੋਂ ਕਰਕੇ ਟੈਕਸਟ ਨੂੰ **ਟੋਕਨ** ਵਿੱਚ ਵੰਡਣਾ\n",
    "* ਉਹਨਾਂ ਟੋਕਨ ਦਾ **ਸ਼ਬਦਕੋਸ਼** ਬਣਾਉਣਾ।\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['he', 'said', 'hello']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = torchtext.data.utils.get_tokenizer('basic_english')\n",
    "tokenizer('He said: hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = collections.Counter()\n",
    "for (label, line) in train_dataset:\n",
    "    counter.update(tokenizer(line))\n",
    "vocab = torchtext.vocab.vocab(counter, min_freq=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ਵਰਤਮਾਨ ਸ਼ਬਦਾਵਲੀ ਦੀ ਵਰਤੋਂ ਕਰਕੇ, ਅਸੀਂ ਆਪਣੀ ਟੋਕਨਾਈਜ਼ ਕੀਤੀ ਸਤਰ ਨੂੰ ਅੰਕਾਂ ਦੇ ਸੈਟ ਵਿੱਚ ਆਸਾਨੀ ਨਾਲ ਕੋਡ ਕਰ ਸਕਦੇ ਹਾਂ:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size if 95810\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[599, 3279, 97, 1220, 329, 225, 7368]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(vocab)\n",
    "print(f\"Vocab size if {vocab_size}\")\n",
    "\n",
    "stoi = vocab.get_stoi() # dict to convert tokens to indices\n",
    "\n",
    "def encode(x):\n",
    "    return [stoi[s] for s in tokenizer(x)]\n",
    "\n",
    "encode('I love to play with my words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ਸ਼ਬਦਾਂ ਦੀ ਥੈਲੀ ਟੈਕਸਟ ਪ੍ਰਤੀਨਿਧੀ\n",
    "\n",
    "ਕਿਉਂਕਿ ਸ਼ਬਦ ਅਰਥ ਨੂੰ ਦਰਸਾਉਂਦੇ ਹਨ, ਕਈ ਵਾਰ ਅਸੀਂ ਸਿਰਫ਼ ਵੱਖ-ਵੱਖ ਸ਼ਬਦਾਂ ਨੂੰ ਦੇਖ ਕੇ ਟੈਕਸਟ ਦਾ ਅਰਥ ਸਮਝ ਸਕਦੇ ਹਾਂ, ਭਾਵੇਂ ਉਹ ਵਾਕ ਵਿੱਚ ਕਿਸੇ ਵੀ ਕ੍ਰਮ ਵਿੱਚ ਹੋਣ। ਉਦਾਹਰਣ ਲਈ, ਜਦੋਂ ਅਸੀਂ ਖ਼ਬਰਾਂ ਦੀ ਵਰਗੀਕਰਨ ਕਰਦੇ ਹਾਂ, ਤਾਂ *weather*, *snow* ਵਰਗੇ ਸ਼ਬਦ *weather forecast* ਵੱਲ ਇਸ਼ਾਰਾ ਕਰਦੇ ਹਨ, ਜਦੋਂਕਿ *stocks*, *dollar* ਵਰਗੇ ਸ਼ਬਦ *financial news* ਵੱਲ ਗਿਣੇ ਜਾਂਦੇ ਹਨ।\n",
    "\n",
    "**ਸ਼ਬਦਾਂ ਦੀ ਥੈਲੀ** (BoW) ਵੇਕਟਰ ਪ੍ਰਤੀਨਿਧੀ ਸਭ ਤੋਂ ਆਮ ਤੌਰ 'ਤੇ ਵਰਤੀ ਜਾਣ ਵਾਲੀ ਰਵਾਇਤੀ ਵੇਕਟਰ ਪ੍ਰਤੀਨਿਧੀ ਹੈ। ਹਰ ਸ਼ਬਦ ਨੂੰ ਇੱਕ ਵੇਕਟਰ ਇੰਡੈਕਸ ਨਾਲ ਜੋੜਿਆ ਜਾਂਦਾ ਹੈ, ਅਤੇ ਵੇਕਟਰ ਤੱਤ ਵਿੱਚ ਦਿੱਤੇ ਗਏ ਦਸਤਾਵੇਜ਼ ਵਿੱਚ ਸ਼ਬਦ ਦੀ ਘਟਨਾ ਦੀ ਗਿਣਤੀ ਹੁੰਦੀ ਹੈ।\n",
    "\n",
    "![ਇਮੇਜ ਦਿਖਾ ਰਿਹਾ ਹੈ ਕਿ ਸ਼ਬਦਾਂ ਦੀ ਥੈਲੀ ਵੇਕਟਰ ਪ੍ਰਤੀਨਿਧੀ ਮੈਮਰੀ ਵਿੱਚ ਕਿਵੇਂ ਦਰਸਾਈ ਜਾਂਦੀ ਹੈ।](../../../../../translated_images/pa/bag-of-words-example.606fc1738f1d7ba9.webp) \n",
    "\n",
    "> **Note**: ਤੁਸੀਂ BoW ਨੂੰ ਟੈਕਸਟ ਵਿੱਚ ਵੱਖ-ਵੱਖ ਸ਼ਬਦਾਂ ਲਈ ਇੱਕ-ਹਾਟ-ਐਨਕੋਡ ਕੀਤੇ ਵੇਕਟਰਾਂ ਦੇ ਜੋੜ ਵਜੋਂ ਵੀ ਸੋਚ ਸਕਦੇ ਹੋ।\n",
    "\n",
    "ਹੇਠਾਂ Scikit Learn ਪਾਇਥਨ ਲਾਇਬ੍ਰੇਰੀ ਦੀ ਵਰਤੋਂ ਕਰਕੇ ਸ਼ਬਦਾਂ ਦੀ ਥੈਲੀ ਪ੍ਰਤੀਨਿਧੀ ਬਣਾਉਣ ਦਾ ਇੱਕ ਉਦਾਹਰਣ ਦਿੱਤਾ ਗਿਆ ਹੈ:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 2, 0, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "corpus = [\n",
    "        'I like hot dogs.',\n",
    "        'The dog ran fast.',\n",
    "        'Its hot outside.',\n",
    "    ]\n",
    "vectorizer.fit_transform(corpus)\n",
    "vectorizer.transform(['My dog likes hot dogs on a hot day.']).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AG_NEWS ਡੇਟਾਸੈੱਟ ਦੇ ਵੇਕਟਰ ਪ੍ਰਤੀਨਿਧੀ ਤੋਂ ਬੈਗ-ਆਫ-ਵਰਡਸ ਵੇਕਟਰ ਦੀ ਗਣਨਾ ਕਰਨ ਲਈ, ਅਸੀਂ ਹੇਠਾਂ ਦਿੱਤੀ ਫੰਕਸ਼ਨ ਦੀ ਵਰਤੋਂ ਕਰ ਸਕਦੇ ਹਾਂ:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 1., 2.,  ..., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(vocab)\n",
    "\n",
    "def to_bow(text,bow_vocab_size=vocab_size):\n",
    "    res = torch.zeros(bow_vocab_size,dtype=torch.float32)\n",
    "    for i in encode(text):\n",
    "        if i<bow_vocab_size:\n",
    "            res[i] += 1\n",
    "    return res\n",
    "\n",
    "print(to_bow(train_dataset[0][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **ਨੋਟ:** ਇੱਥੇ ਅਸੀਂ ਗਲੋਬਲ `vocab_size` ਵੈਰੀਏਬਲ ਦੀ ਵਰਤੋਂ ਕਰ ਰਹੇ ਹਾਂ ਤਾਂ ਜੋ ਸ਼ਬਦਾਵਲੀ ਦੇ ਡਿਫਾਲਟ ਆਕਾਰ ਨੂੰ ਦਰਸਾਇਆ ਜਾ ਸਕੇ। ਕਿਉਂਕਿ ਅਕਸਰ ਸ਼ਬਦਾਵਲੀ ਦਾ ਆਕਾਰ ਕਾਫੀ ਵੱਡਾ ਹੁੰਦਾ ਹੈ, ਅਸੀਂ ਸ਼ਬਦਾਵਲੀ ਦੇ ਆਕਾਰ ਨੂੰ ਸਭ ਤੋਂ ਆਮ ਸ਼ਬਦਾਂ ਤੱਕ ਸੀਮਿਤ ਕਰ ਸਕਦੇ ਹਾਂ। `vocab_size` ਦੀ ਮੁੱਲ ਘਟਾਉਣ ਦੀ ਕੋਸ਼ਿਸ਼ ਕਰੋ ਅਤੇ ਹੇਠਾਂ ਦਿੱਤਾ ਕੋਡ ਚਲਾਓ, ਅਤੇ ਦੇਖੋ ਕਿ ਇਸਦਾ ਸਹੀਤਾ 'ਤੇ ਕਿਵੇਂ ਅਸਰ ਪੈਂਦਾ ਹੈ। ਤੁਹਾਨੂੰ ਕੁਝ ਸਹੀਤਾ ਵਿੱਚ ਕਮੀ ਦੀ ਉਮੀਦ ਕਰਨੀ ਚਾਹੀਦੀ ਹੈ, ਪਰ ਇਹ ਬਹੁਤ ਵੱਡੀ ਨਹੀਂ ਹੋਵੇਗੀ, ਬਦਲੇ ਵਿੱਚ ਉੱਚੇ ਪ੍ਰਦਰਸ਼ਨ ਲਈ।\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BoW ਕਲਾਸੀਫਾਇਰ ਦੀ ਟ੍ਰੇਨਿੰਗ\n",
    "\n",
    "ਹੁਣ ਜਦੋਂ ਅਸੀਂ ਸਿੱਖ ਲਿਆ ਹੈ ਕਿ ਆਪਣੇ ਟੈਕਸਟ ਦਾ Bag-of-Words ਪ੍ਰਤੀਨਿਧਿਤਾ ਕਿਵੇਂ ਬਣਾਉਣਾ ਹੈ, ਆਓ ਇਸ 'ਤੇ ਇੱਕ ਕਲਾਸੀਫਾਇਰ ਟ੍ਰੇਨ ਕਰੀਏ। ਸਭ ਤੋਂ ਪਹਿਲਾਂ, ਸਾਨੂੰ ਆਪਣਾ ਡੇਟਾਸੈਟ ਟ੍ਰੇਨਿੰਗ ਲਈ ਇਸ ਤਰੀਕੇ ਨਾਲ ਬਦਲਣਾ ਪਵੇਗਾ ਕਿ ਸਾਰੇ ਪੋਜ਼ੀਸ਼ਨਲ ਵੈਕਟਰ ਪ੍ਰਤੀਨਿਧਿਤਾਵਾਂ ਨੂੰ Bag-of-Words ਪ੍ਰਤੀਨਿਧਿਤਾ ਵਿੱਚ ਬਦਲਿਆ ਜਾਵੇ। ਇਹ `bowify` ਫੰਕਸ਼ਨ ਨੂੰ `collate_fn` ਪੈਰਾਮੀਟਰ ਵਜੋਂ ਸਟੈਂਡਰਡ torch `DataLoader` ਵਿੱਚ ਪਾਸ ਕਰਕੇ ਹਾਸਲ ਕੀਤਾ ਜਾ ਸਕਦਾ ਹੈ:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import numpy as np \n",
    "\n",
    "# this collate function gets list of batch_size tuples, and needs to \n",
    "# return a pair of label-feature tensors for the whole minibatch\n",
    "def bowify(b):\n",
    "    return (\n",
    "            torch.LongTensor([t[0]-1 for t in b]),\n",
    "            torch.stack([to_bow(t[1]) for t in b])\n",
    "    )\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, collate_fn=bowify, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, collate_fn=bowify, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ਹੁਣ ਆਓ ਇੱਕ ਸਧਾਰਨ ਕਲਾਸੀਫਾਇਰ ਨਿਊਰਲ ਨੈਟਵਰਕ ਨੂੰ ਪਰਿਭਾਸ਼ਿਤ ਕਰੀਏ ਜੋ ਇੱਕ ਲੀਨੀਅਰ ਲੇਅਰ ਸ਼ਾਮਲ ਕਰਦਾ ਹੈ। ਇਨਪੁਟ ਵੇਕਟਰ ਦਾ ਆਕਾਰ `vocab_size` ਦੇ ਬਰਾਬਰ ਹੈ, ਅਤੇ ਆਉਟਪੁਟ ਆਕਾਰ ਕਲਾਸਾਂ ਦੀ ਗਿਣਤੀ (4) ਦੇ ਅਨੁਸਾਰ ਹੈ। ਕਿਉਂਕਿ ਅਸੀਂ ਵਰਗੀਕਰਨ ਦਾ ਕੰਮ ਹੱਲ ਕਰ ਰਹੇ ਹਾਂ, ਅੰਤਮ ਐਕਟੀਵੇਸ਼ਨ ਫੰਕਸ਼ਨ `LogSoftmax()` ਹੈ।\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = torch.nn.Sequential(torch.nn.Linear(vocab_size,4),torch.nn.LogSoftmax(dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ਹੁਣ ਅਸੀਂ ਮਿਆਰੀ PyTorch ਟ੍ਰੇਨਿੰਗ ਲੂਪ ਨੂੰ ਪਰਿਭਾਸ਼ਿਤ ਕਰਾਂਗੇ। ਕਿਉਂਕਿ ਸਾਡਾ ਡੇਟਾਸੈਟ ਕਾਫ਼ੀ ਵੱਡਾ ਹੈ, ਸਾਡੇ ਸਿੱਖਣ ਦੇ ਉਦੇਸ਼ ਲਈ ਅਸੀਂ ਸਿਰਫ਼ ਇੱਕ epoch ਲਈ ਟ੍ਰੇਨਿੰਗ ਕਰਾਂਗੇ, ਅਤੇ ਕਈ ਵਾਰ ਇੱਕ epoch ਤੋਂ ਵੀ ਘੱਟ (epoch_size ਪੈਰਾਮੀਟਰ ਨੂੰ ਨਿਰਧਾਰਤ ਕਰਨਾ ਸਾਨੂੰ ਟ੍ਰੇਨਿੰਗ ਨੂੰ ਸੀਮਿਤ ਕਰਨ ਦੀ ਆਗਿਆ ਦਿੰਦਾ ਹੈ)। ਅਸੀਂ ਟ੍ਰੇਨਿੰਗ ਦੌਰਾਨ ਜਮ੍ਹਾਂ ਕੀਤੀ ਟ੍ਰੇਨਿੰਗ ਸਹੀਤਾ ਦੀ ਰਿਪੋਰਟ ਵੀ ਕਰਾਂਗੇ; ਰਿਪੋਰਟਿੰਗ ਦੀ ਆਵ੍ਰਿਤੀ ਨੂੰ report_freq ਪੈਰਾਮੀਟਰ ਦੀ ਵਰਤੋਂ ਕਰਕੇ ਨਿਰਧਾਰਤ ਕੀਤਾ ਜਾਂਦਾ ਹੈ।\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(net,dataloader,lr=0.01,optimizer=None,loss_fn = torch.nn.NLLLoss(),epoch_size=None, report_freq=200):\n",
    "    optimizer = optimizer or torch.optim.Adam(net.parameters(),lr=lr)\n",
    "    net.train()\n",
    "    total_loss,acc,count,i = 0,0,0,0\n",
    "    for labels,features in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        out = net(features)\n",
    "        loss = loss_fn(out,labels) #cross_entropy(out,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss+=loss\n",
    "        _,predicted = torch.max(out,1)\n",
    "        acc+=(predicted==labels).sum()\n",
    "        count+=len(labels)\n",
    "        i+=1\n",
    "        if i%report_freq==0:\n",
    "            print(f\"{count}: acc={acc.item()/count}\")\n",
    "        if epoch_size and count>epoch_size:\n",
    "            break\n",
    "    return total_loss.item()/count, acc.item()/count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200: acc=0.8028125\n",
      "6400: acc=0.8371875\n",
      "9600: acc=0.8534375\n",
      "12800: acc=0.85765625\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.026090790722161722, 0.8620069296375267)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_epoch(net,train_loader,epoch_size=15000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ਬਾਈਗ੍ਰਾਮ, ਟ੍ਰਾਈਗ੍ਰਾਮ ਅਤੇ ਐਨ-ਗ੍ਰਾਮ\n",
    "\n",
    "ਬੈਗ ਆਫ ਵਰਡਸ ਪਹੁੰਚ ਦੀ ਇੱਕ ਸੀਮਿਤਤਾ ਇਹ ਹੈ ਕਿ ਕੁਝ ਸ਼ਬਦ ਬਹੁ-ਸ਼ਬਦ ਅਭਿਵਿਅਕਤੀਆਂ ਦਾ ਹਿੱਸਾ ਹੁੰਦੇ ਹਨ। ਉਦਾਹਰਨ ਵਜੋਂ, 'ਹਾਟ ਡੌਗ' ਸ਼ਬਦ ਦਾ ਅਰਥ ਬਿਲਕੁਲ ਵੱਖਰਾ ਹੁੰਦਾ ਹੈ ਜਿਵੇਂ 'ਹਾਟ' ਅਤੇ 'ਡੌਗ' ਸ਼ਬਦਾਂ ਦਾ ਹੋਰ ਸੰਦਰਭਾਂ ਵਿੱਚ ਹੁੰਦਾ ਹੈ। ਜੇਕਰ ਅਸੀਂ 'ਹਾਟ' ਅਤੇ 'ਡੌਗ' ਸ਼ਬਦਾਂ ਨੂੰ ਹਮੇਸ਼ਾ ਇੱਕੋ ਜਿਹੇ ਵੈਕਟਰਾਂ ਨਾਲ ਦਰਸਾਉਂਦੇ ਹਾਂ, ਤਾਂ ਇਹ ਸਾਡੇ ਮਾਡਲ ਨੂੰ ਗਲਤਫਹਿਮੀ ਵਿੱਚ ਪਾ ਸਕਦਾ ਹੈ।\n",
    "\n",
    "ਇਸ ਸਮੱਸਿਆ ਨੂੰ ਹੱਲ ਕਰਨ ਲਈ, **ਐਨ-ਗ੍ਰਾਮ ਪ੍ਰਤੀਨਿਧਤਾਵਾਂ** ਦਸਤਾਵੇਜ਼ ਵਰਗੀਕਰਨ ਦੇ ਤਰੀਕਿਆਂ ਵਿੱਚ ਅਕਸਰ ਵਰਤੀਆਂ ਜਾਂਦੀਆਂ ਹਨ, ਜਿੱਥੇ ਹਰ ਸ਼ਬਦ, ਦੋ-ਸ਼ਬਦ ਜਾਂ ਤਿੰਨ-ਸ਼ਬਦ ਦੀ ਆਵ੍ਰਿਤੀ ਵਰਗੀਕਰਨ ਮਾਡਲਾਂ ਨੂੰ ਸਿਖਾਉਣ ਲਈ ਇੱਕ ਉਪਯੋਗ ਫੀਚਰ ਹੁੰਦੀ ਹੈ। ਉਦਾਹਰਨ ਵਜੋਂ, ਬਾਈਗ੍ਰਾਮ ਪ੍ਰਤੀਨਿਧਤਾ ਵਿੱਚ, ਅਸੀਂ ਮੂਲ ਸ਼ਬਦਾਂ ਦੇ ਇਲਾਵਾ ਸਾਰੇ ਸ਼ਬਦ ਜੋੜਾਂ ਨੂੰ ਸ਼ਬਦਾਵਲੀ ਵਿੱਚ ਸ਼ਾਮਲ ਕਰਾਂਗੇ।\n",
    "\n",
    "ਹੇਠਾਂ ਦਿੱਤਾ ਗਿਆ ਹੈ ਕਿ ਕਿਵੇਂ ਸਿਕਿਟ ਲਰਨ ਦੀ ਵਰਤੋਂ ਕਰਕੇ ਬਾਈਗ੍ਰਾਮ ਬੈਗ ਆਫ ਵਰਡ ਪ੍ਰਤੀਨਿਧਤਾ ਬਣਾਈ ਜਾ ਸਕਦੀ ਹੈ:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:\n",
      " {'i': 7, 'like': 11, 'hot': 4, 'dogs': 2, 'i like': 8, 'like hot': 12, 'hot dogs': 5, 'the': 16, 'dog': 0, 'ran': 14, 'fast': 3, 'the dog': 17, 'dog ran': 1, 'ran fast': 15, 'its': 9, 'outside': 13, 'its hot': 10, 'hot outside': 6}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 1, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_vectorizer = CountVectorizer(ngram_range=(1, 2), token_pattern=r'\\b\\w+\\b', min_df=1)\n",
    "corpus = [\n",
    "        'I like hot dogs.',\n",
    "        'The dog ran fast.',\n",
    "        'Its hot outside.',\n",
    "    ]\n",
    "bigram_vectorizer.fit_transform(corpus)\n",
    "print(\"Vocabulary:\\n\",bigram_vectorizer.vocabulary_)\n",
    "bigram_vectorizer.transform(['My dog likes hot dogs on a hot day.']).toarray()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N-gram ਪਹੁੰਚ ਦਾ ਮੁੱਖ ਨੁਕਸਾਨ ਇਹ ਹੈ ਕਿ ਸ਼ਬਦਾਵਲੀ ਦਾ ਆਕਾਰ ਬਹੁਤ ਤੇਜ਼ੀ ਨਾਲ ਵਧਣ ਲੱਗਦਾ ਹੈ। ਅਮਲ ਵਿੱਚ, ਸਾਨੂੰ N-gram ਪ੍ਰਤੀਨਿਧਤਾ ਨੂੰ ਕੁਝ ਮਾਪ ਘਟਾਉਣ ਦੀਆਂ ਤਕਨੀਕਾਂ ਨਾਲ ਜੋੜਨ ਦੀ ਲੋੜ ਹੁੰਦੀ ਹੈ, ਜਿਵੇਂ ਕਿ *embeddings*, ਜਿਸ ਬਾਰੇ ਅਸੀਂ ਅਗਲੇ ਯੂਨਿਟ ਵਿੱਚ ਚਰਚਾ ਕਰਾਂਗੇ।\n",
    "\n",
    "**AG News** ਡੇਟਾਸੈੱਟ ਵਿੱਚ N-gram ਪ੍ਰਤੀਨਿਧਤਾ ਦੀ ਵਰਤੋਂ ਕਰਨ ਲਈ, ਸਾਨੂੰ ਵਿਸ਼ੇਸ਼ ngram ਸ਼ਬਦਾਵਲੀ ਬਣਾਉਣ ਦੀ ਲੋੜ ਹੈ:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram vocabulary length =  1308842\n"
     ]
    }
   ],
   "source": [
    "counter = collections.Counter()\n",
    "for (label, line) in train_dataset:\n",
    "    l = tokenizer(line)\n",
    "    counter.update(torchtext.data.utils.ngrams_iterator(l,ngrams=2))\n",
    "    \n",
    "bi_vocab = torchtext.vocab.vocab(counter, min_freq=1)\n",
    "\n",
    "print(\"Bigram vocabulary length = \",len(bi_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ਅਸੀਂ ਉਪਰੋਕਤ ਕੋਡ ਨੂੰ ਵਰਤ ਕੇ ਕਲਾਸੀਫਾਇਰ ਨੂੰ ਟ੍ਰੇਨ ਕਰ ਸਕਦੇ ਹਾਂ, ਪਰ ਇਹ ਬਹੁਤ ਹੀ ਮੈਮਰੀ-ਅਸਰਦਾਰ ਹੋਵੇਗਾ। ਅਗਲੇ ਯੂਨਿਟ ਵਿੱਚ, ਅਸੀਂ ਐਮਬੈਡਿੰਗਜ਼ ਦੀ ਵਰਤੋਂ ਕਰਕੇ ਬਿਗ੍ਰਾਮ ਕਲਾਸੀਫਾਇਰ ਨੂੰ ਟ੍ਰੇਨ ਕਰਾਂਗੇ।\n",
    "\n",
    "> **Note:** ਤੁਸੀਂ ਸਿਰਫ ਉਹ ngrams ਛੱਡ ਸਕਦੇ ਹੋ ਜੋ ਟੈਕਸਟ ਵਿੱਚ ਨਿਰਧਾਰਿਤ ਗਿਣਤੀ ਤੋਂ ਵੱਧ ਵਾਰ ਆਉਂਦੇ ਹਨ। ਇਹ ਯਕੀਨੀ ਬਣਾਏਗਾ ਕਿ ਅਨਿਯਮਿਤ ਬਿਗ੍ਰਾਮ ਛੱਡੇ ਜਾਣਗੇ, ਅਤੇ ਡਾਇਮੈਂਸ਼ਨਲਿਟੀ ਨੂੰ ਕਾਫੀ ਘਟਾਇਆ ਜਾਵੇਗਾ। ਇਸ ਲਈ, `min_freq` ਪੈਰਾਮੀਟਰ ਨੂੰ ਉੱਚ ਮੁੱਲ 'ਤੇ ਸੈਟ ਕਰੋ, ਅਤੇ ਸ਼ਬਦਾਵਲੀ ਦੀ ਲੰਬਾਈ ਵਿੱਚ ਬਦਲਾਅ ਨੂੰ ਦੇਖੋ।\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ਟਰਮ ਫ੍ਰੀਕਵੈਂਸੀ ਇਨਵਰਸ ਡੌਕੂਮੈਂਟ ਫ੍ਰੀਕਵੈਂਸੀ TF-IDF\n",
    "\n",
    "BoW ਪ੍ਰਸਤੁਤੀ ਵਿੱਚ, ਸ਼ਬਦ ਦੀ ਗਿਣਤੀ ਨੂੰ ਇੱਕੋ ਜਿਹਾ ਵਜ਼ਨ ਦਿੱਤਾ ਜਾਂਦਾ ਹੈ, ਚਾਹੇ ਸ਼ਬਦ ਕਿਹੜਾ ਵੀ ਹੋਵੇ। ਹਾਲਾਂਕਿ, ਇਹ ਸਪਸ਼ਟ ਹੈ ਕਿ ਆਮ ਸ਼ਬਦ, ਜਿਵੇਂ *a*, *in*, ਆਦਿ ਵਰਗੇ ਸ਼ਬਦ ਵਰਤੋਂ ਵਿੱਚ ਆਉਣ ਵਾਲੇ ਵਿਸ਼ੇਸ਼ ਸ਼ਬਦਾਂ ਦੇ ਮੁਕਾਬਲੇ ਘੱਟ ਮਹੱਤਵਪੂਰਨ ਹੁੰਦੇ ਹਨ। ਅਸਲ ਵਿੱਚ, ਜ਼ਿਆਦਾਤਰ NLP ਕੰਮਾਂ ਵਿੱਚ ਕੁਝ ਸ਼ਬਦ ਹੋਰਾਂ ਨਾਲੋਂ ਜ਼ਿਆਦਾ ਮਹੱਤਵਪੂਰਨ ਹੁੰਦੇ ਹਨ।\n",
    "\n",
    "**TF-IDF** ਦਾ ਅਰਥ ਹੈ **ਟਰਮ ਫ੍ਰੀਕਵੈਂਸੀ–ਇਨਵਰਸ ਡੌਕੂਮੈਂਟ ਫ੍ਰੀਕਵੈਂਸੀ**। ਇਹ ਬੈਗ ਆਫ ਵਰਡਸ ਦੀ ਇੱਕ ਤਰਕੀਬ ਹੈ, ਜਿਸ ਵਿੱਚ ਦਸਤਾਵੇਜ਼ ਵਿੱਚ ਸ਼ਬਦ ਦੀ ਮੌਜੂਦਗੀ ਨੂੰ ਦਰਸਾਉਣ ਵਾਲੇ ਬਾਈਨਰੀ 0/1 ਮੁੱਲ ਦੀ ਬਜਾਏ, ਇੱਕ ਫਲੋਟਿੰਗ-ਪੌਇੰਟ ਮੁੱਲ ਵਰਤਿਆ ਜਾਂਦਾ ਹੈ, ਜੋ ਕਿ ਕੋਰਪਸ ਵਿੱਚ ਸ਼ਬਦ ਦੀ ਵਰਤੋਂ ਦੀ ਆਵ੍ਰਿਤੀ ਨਾਲ ਸੰਬੰਧਿਤ ਹੁੰਦਾ ਹੈ।\n",
    "\n",
    "ਅਧਿਕਾਰਕ ਤੌਰ 'ਤੇ, ਦਸਤਾਵੇਜ਼ $j$ ਵਿੱਚ ਸ਼ਬਦ $i$ ਦਾ ਵਜ਼ਨ $w_{ij}$ ਇਸ ਤਰ੍ਹਾਂ ਪਰਿਭਾਸ਼ਿਤ ਕੀਤਾ ਜਾਂਦਾ ਹੈ:\n",
    "$$\n",
    "w_{ij} = tf_{ij}\\times\\log({N\\over df_i})\n",
    "$$\n",
    "ਜਿੱਥੇ\n",
    "* $tf_{ij}$ $j$ ਵਿੱਚ $i$ ਦੀ ਗਿਣਤੀ ਹੈ, ਅਰਥਾਤ BoW ਮੁੱਲ ਜੋ ਅਸੀਂ ਪਹਿਲਾਂ ਵੇਖਿਆ ਹੈ\n",
    "* $N$ ਸੰਗ੍ਰਹਿ ਵਿੱਚ ਦਸਤਾਵੇਜ਼ਾਂ ਦੀ ਗਿਣਤੀ ਹੈ\n",
    "* $df_i$ ਪੂਰੇ ਸੰਗ੍ਰਹਿ ਵਿੱਚ ਸ਼ਬਦ $i$ ਵਾਲੇ ਦਸਤਾਵੇਜ਼ਾਂ ਦੀ ਗਿਣਤੀ ਹੈ\n",
    "\n",
    "TF-IDF ਮੁੱਲ $w_{ij}$ ਦਸਤਾਵੇਜ਼ ਵਿੱਚ ਸ਼ਬਦ ਦੀ ਗਿਣਤੀ ਦੇ ਅਨੁਪਾਤ ਵਿੱਚ ਵਧਦਾ ਹੈ ਅਤੇ ਕੋਰਪਸ ਵਿੱਚ ਉਹਨਾਂ ਦਸਤਾਵੇਜ਼ਾਂ ਦੀ ਗਿਣਤੀ ਦੁਆਰਾ ਸੰਬੰਧਿਤ ਹੁੰਦਾ ਹੈ ਜਿਨ੍ਹਾਂ ਵਿੱਚ ਉਹ ਸ਼ਬਦ ਸ਼ਾਮਲ ਹੁੰਦਾ ਹੈ। ਇਹ ਇਸ ਗੱਲ ਨੂੰ ਠੀਕ ਕਰਨ ਵਿੱਚ ਮਦਦ ਕਰਦਾ ਹੈ ਕਿ ਕੁਝ ਸ਼ਬਦ ਹੋਰਾਂ ਨਾਲੋਂ ਵੱਧ ਵਾਰ ਆਉਂਦੇ ਹਨ। ਉਦਾਹਰਨ ਲਈ, ਜੇਕਰ ਸ਼ਬਦ *ਹਰ* ਦਸਤਾਵੇਜ਼ ਵਿੱਚ ਆਉਂਦਾ ਹੈ, $df_i=N$, ਅਤੇ $w_{ij}=0$, ਅਤੇ ਉਹ ਸ਼ਬਦ ਪੂਰੀ ਤਰ੍ਹਾਂ ਨਜ਼ਰਅੰਦਾਜ਼ ਕਰ ਦਿੱਤੇ ਜਾਂਦੇ ਹਨ।\n",
    "\n",
    "ਤੁਹਾਨੂੰ Scikit Learn ਦੀ ਵਰਤੋਂ ਕਰਕੇ ਟੈਕਸਟ ਦੀ TF-IDF ਵੈਕਟਰਾਈਜ਼ੇਸ਼ਨ ਆਸਾਨੀ ਨਾਲ ਬਣਾਉਣੀ ਹੋਵੇਗੀ:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.43381609, 0.        , 0.43381609, 0.        , 0.65985664,\n",
       "        0.43381609, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,2))\n",
    "vectorizer.fit_transform(corpus)\n",
    "vectorizer.transform(['My dog likes hot dogs on a hot day.']).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ਨਤੀਜਾ\n",
    "\n",
    "ਹਾਲਾਂਕਿ TF-IDF ਪ੍ਰਸਤੁਤੀਆਂ ਵੱਖ-ਵੱਖ ਸ਼ਬਦਾਂ ਨੂੰ ਆਵ੍ਰਿਤੀ ਦੇ ਅਧਾਰ 'ਤੇ ਵਜ਼ਨ ਪ੍ਰਦਾਨ ਕਰਦੀਆਂ ਹਨ, ਪਰ ਇਹ ਅਰਥ ਜਾਂ ਕ੍ਰਮ ਨੂੰ ਦਰਸਾਉਣ ਵਿੱਚ ਅਸਮਰਥ ਹਨ। ਪ੍ਰਸਿੱਧ ਭਾਸ਼ਾ ਵਿਗਿਆਨੀ J. R. Firth ਨੇ 1935 ਵਿੱਚ ਕਿਹਾ ਸੀ, \"ਸ਼ਬਦ ਦਾ ਪੂਰਾ ਅਰਥ ਹਮੇਸ਼ਾ ਸੰਦਰਭਕ ਹੁੰਦਾ ਹੈ, ਅਤੇ ਸੰਦਰਭ ਤੋਂ ਬਿਨਾਂ ਅਰਥ ਦਾ ਕੋਈ ਅਧਿਐਨ ਗੰਭੀਰਤਾ ਨਾਲ ਨਹੀਂ ਲਿਆ ਜਾ ਸਕਦਾ।\"। ਅਸੀਂ ਕੋਰਸ ਵਿੱਚ ਅੱਗੇ ਸਿੱਖਾਂਗੇ ਕਿ ਭਾਸ਼ਾ ਮਾਡਲਿੰਗ ਦੀ ਵਰਤੋਂ ਕਰਕੇ ਟੈਕਸਟ ਤੋਂ ਸੰਦਰਭਕ ਜਾਣਕਾਰੀ ਕਿਵੇਂ ਕੈਪਚਰ ਕੀਤੀ ਜਾ ਸਕਦੀ ਹੈ।\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**ਅਸਵੀਕਤੀ**:  \nਇਹ ਦਸਤਾਵੇਜ਼ AI ਅਨੁਵਾਦ ਸੇਵਾ [Co-op Translator](https://github.com/Azure/co-op-translator) ਦੀ ਵਰਤੋਂ ਕਰਕੇ ਅਨੁਵਾਦ ਕੀਤਾ ਗਿਆ ਹੈ। ਜਦੋਂ ਕਿ ਅਸੀਂ ਸਹੀ ਹੋਣ ਦੀ ਕੋਸ਼ਿਸ਼ ਕਰਦੇ ਹਾਂ, ਕਿਰਪਾ ਕਰਕੇ ਧਿਆਨ ਦਿਓ ਕਿ ਸਵੈਚਾਲਿਤ ਅਨੁਵਾਦਾਂ ਵਿੱਚ ਗਲਤੀਆਂ ਜਾਂ ਅਸੁੱਤੀਆਂ ਹੋ ਸਕਦੀਆਂ ਹਨ। ਮੂਲ ਦਸਤਾਵੇਜ਼ ਨੂੰ ਇਸਦੀ ਮੂਲ ਭਾਸ਼ਾ ਵਿੱਚ ਅਧਿਕਾਰਤ ਸਰੋਤ ਮੰਨਿਆ ਜਾਣਾ ਚਾਹੀਦਾ ਹੈ। ਮਹੱਤਵਪੂਰਨ ਜਾਣਕਾਰੀ ਲਈ, ਪੇਸ਼ੇਵਰ ਮਨੁੱਖੀ ਅਨੁਵਾਦ ਦੀ ਸਿਫਾਰਸ਼ ਕੀਤੀ ਜਾਂਦੀ ਹੈ। ਇਸ ਅਨੁਵਾਦ ਦੀ ਵਰਤੋਂ ਤੋਂ ਪੈਦਾ ਹੋਣ ਵਾਲੇ ਕਿਸੇ ਵੀ ਗਲਤ ਫਹਿਮੀ ਜਾਂ ਗਲਤ ਵਿਆਖਿਆ ਲਈ ਅਸੀਂ ਜ਼ਿੰਮੇਵਾਰ ਨਹੀਂ ਹਾਂ।\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "16af2a8bbb083ea23e5e41c7f5787656b2ce26968575d8763f2c4b17f9cd711f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('py38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "coopTranslator": {
   "original_hash": "7b9040985e748e4e2d4c689892456ad7",
   "translation_date": "2025-08-28T09:52:51+00:00",
   "source_file": "lessons/5-NLP/13-TextRep/TextRepresentationPyTorch.ipynb",
   "language_code": "pa"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}