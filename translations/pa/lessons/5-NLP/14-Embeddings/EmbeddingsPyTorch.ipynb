{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ਐਮਬੈਡਿੰਗਸ\n",
    "\n",
    "ਪਿਛਲੇ ਉਦਾਹਰਨ ਵਿੱਚ, ਅਸੀਂ `vocab_size` ਦੀ ਲੰਬਾਈ ਵਾਲੇ ਉੱਚ-ਮਾਪਦੰਡ bag-of-words ਵੇਕਟਰਾਂ 'ਤੇ ਕੰਮ ਕੀਤਾ ਸੀ, ਅਤੇ ਅਸੀਂ ਖੁਦ ਹੀ ਘੱਟ-ਮਾਪਦੰਡ positional representation ਵੇਕਟਰਾਂ ਨੂੰ sparse one-hot representation ਵਿੱਚ ਬਦਲ ਰਹੇ ਸੀ। ਇਹ one-hot representation ਮੈਮਰੀ-ਅਸਰਦਾਰ ਨਹੀਂ ਹੈ, ਇਸ ਤੋਂ ਇਲਾਵਾ, ਹਰ ਸ਼ਬਦ ਨੂੰ ਇੱਕ ਦੂਜੇ ਤੋਂ ਅਲੱਗ ਮੰਨਿਆ ਜਾਂਦਾ ਹੈ, ਜ਼ਿਆਦਾ one-hot encoded ਵੇਕਟਰ ਸ਼ਬਦਾਂ ਦੇ ਵਿਚਕਾਰ ਕੋਈ semantic similarity ਨਹੀਂ ਦਿਖਾਉਂਦੇ।\n",
    "\n",
    "ਇਸ ਯੂਨਿਟ ਵਿੱਚ, ਅਸੀਂ **News AG** ਡਾਟਾਸੈਟ ਦੀ ਖੋਜ ਜਾਰੀ ਰੱਖਾਂਗੇ। ਸ਼ੁਰੂ ਕਰਨ ਲਈ, ਆਓ ਡਾਟਾ ਲੋਡ ਕਰੀਏ ਅਤੇ ਪਿਛਲੇ ਨੋਟਬੁੱਕ ਤੋਂ ਕੁਝ ਪਰਿਭਾਸ਼ਾਵਾਂ ਪ੍ਰਾਪਤ ਕਰੀਏ।\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\WORK\\ai-for-beginners\\5-NLP\\14-Embeddings\\data\\train.csv: 29.5MB [00:01, 18.8MB/s]                            \n",
      "d:\\WORK\\ai-for-beginners\\5-NLP\\14-Embeddings\\data\\test.csv: 1.86MB [00:00, 11.2MB/s]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building vocab...\n",
      "Vocab size =  95812\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "import numpy as np\n",
    "from torchnlp import *\n",
    "train_dataset, test_dataset, classes, vocab = load_dataset()\n",
    "vocab_size = len(vocab)\n",
    "print(\"Vocab size = \",vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ਐਮਬੈਡਿੰਗ ਕੀ ਹੈ?\n",
    "\n",
    "**ਐਮਬੈਡਿੰਗ** ਦਾ ਮਤਲਬ ਹੈ ਸ਼ਬਦਾਂ ਨੂੰ ਘੱਟ-ਡਾਈਮੇਂਸ਼ਨਲ ਡੈਂਸ ਵੈਕਟਰਾਂ ਨਾਲ ਦਰਸਾਉਣਾ, ਜੋ ਕਿਸੇ ਤਰ੍ਹਾਂ ਸ਼ਬਦ ਦੇ ਅਰਥ ਨੂੰ ਪ੍ਰਗਟ ਕਰਦੇ ਹਨ। ਅਸੀਂ ਬਾਅਦ ਵਿੱਚ ਚਰਚਾ ਕਰਾਂਗੇ ਕਿ ਮਤਲਬਪੂਰਨ ਸ਼ਬਦ ਐਮਬੈਡਿੰਗਜ਼ ਕਿਵੇਂ ਬਣਾਈਆਂ ਜਾਂਦੀਆਂ ਹਨ, ਪਰ ਫਿਲਹਾਲ ਚਲੋ ਐਮਬੈਡਿੰਗਜ਼ ਨੂੰ ਇੱਕ ਤਰੀਕੇ ਵਜੋਂ ਸੋਚੀਏ ਜੋ ਸ਼ਬਦ ਵੈਕਟਰ ਦੀ ਡਾਈਮੇਂਸ਼ਨਲਟੀ ਨੂੰ ਘਟਾਉਂਦਾ ਹੈ।\n",
    "\n",
    "ਇਸ ਤਰ੍ਹਾਂ, ਐਮਬੈਡਿੰਗ ਲੇਅਰ ਇੱਕ ਸ਼ਬਦ ਨੂੰ ਇਨਪੁਟ ਵਜੋਂ ਲਵੇਗਾ ਅਤੇ ਨਿਰਧਾਰਤ `embedding_size` ਦਾ ਇੱਕ ਆਉਟਪੁਟ ਵੈਕਟਰ ਤਿਆਰ ਕਰੇਗਾ। ਇੱਕ ਅਰਥ ਵਿੱਚ, ਇਹ `Linear` ਲੇਅਰ ਦੇ ਕਾਫ਼ੀ ਮਿਲਦਾ-ਜੁਲਦਾ ਹੈ, ਪਰ ਇਹ ਇੱਕ-ਹਾਟ ਐਨਕੋਡ ਕੀਤੇ ਵੈਕਟਰ ਦੀ ਥਾਂ ਸ਼ਬਦ ਨੰਬਰ ਨੂੰ ਇਨਪੁਟ ਵਜੋਂ ਲੈ ਸਕੇਗਾ।\n",
    "\n",
    "ਜਦੋਂ ਅਸੀਂ ਆਪਣੇ ਨੈੱਟਵਰਕ ਵਿੱਚ ਪਹਿਲੇ ਲੇਅਰ ਵਜੋਂ ਐਮਬੈਡਿੰਗ ਲੇਅਰ ਦੀ ਵਰਤੋਂ ਕਰਦੇ ਹਾਂ, ਤਾਂ ਅਸੀਂ ਬੈਗ-ਆਫ-ਵਰਡਜ਼ ਤੋਂ **ਐਮਬੈਡਿੰਗ ਬੈਗ** ਮਾਡਲ ਵਿੱਚ ਬਦਲ ਸਕਦੇ ਹਾਂ, ਜਿੱਥੇ ਅਸੀਂ ਪਹਿਲਾਂ ਆਪਣੇ ਪਾਠ ਵਿੱਚ ਹਰ ਸ਼ਬਦ ਨੂੰ ਉਸਦੇ ਸੰਬੰਧਿਤ ਐਮਬੈਡਿੰਗ ਵਿੱਚ ਬਦਲਦੇ ਹਾਂ, ਅਤੇ ਫਿਰ ਸਾਰੇ ਐਮਬੈਡਿੰਗਜ਼ 'ਤੇ ਕੁਝ ਸਮੂਹ ਫੰਕਸ਼ਨ ਗਣਨਾ ਕਰਦੇ ਹਾਂ, ਜਿਵੇਂ ਕਿ `sum`, `average` ਜਾਂ `max`।\n",
    "\n",
    "![ਪੰਜ ਕ੍ਰਮ ਸ਼ਬਦਾਂ ਲਈ ਐਮਬੈਡਿੰਗ ਕਲਾਸੀਫਾਇਰ ਦਿਖਾਉਂਦੀ ਇੱਕ ਚਿੱਤਰ।](../../../../../translated_images/pa/embedding-classifier-example.b77f021a7ee67eee.webp)\n",
    "\n",
    "ਸਾਡਾ ਕਲਾਸੀਫਾਇਰ ਨਿਊਰਲ ਨੈੱਟਵਰਕ ਐਮਬੈਡਿੰਗ ਲੇਅਰ ਨਾਲ ਸ਼ੁਰੂ ਹੋਵੇਗਾ, ਫਿਰ ਐਗਰੀਗੇਸ਼ਨ ਲੇਅਰ, ਅਤੇ ਇਸਦੇ ਉੱਪਰ ਲੀਨਿਅਰ ਕਲਾਸੀਫਾਇਰ:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbedClassifier(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super().__init__()\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, embed_dim)\n",
    "        self.fc = torch.nn.Linear(embed_dim, num_class)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = torch.mean(x,dim=1)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ਵੱਖ-ਵੱਖ ਲੰਬਾਈ ਵਾਲੀਆਂ ਲੜੀਆਂ ਨਾਲ ਨਿਪਟਣਾ\n",
    "\n",
    "ਇਸ ਆਰਕੀਟੈਕਚਰ ਦੇ ਨਤੀਜੇ ਵਜੋਂ, ਸਾਡੇ ਨੈੱਟਵਰਕ ਲਈ ਮਿਨੀਬੈਚਜ਼ ਨੂੰ ਇੱਕ ਖਾਸ ਤਰੀਕੇ ਨਾਲ ਬਣਾਉਣਾ ਪਵੇਗਾ। ਪਿਛਲੇ ਯੂਨਿਟ ਵਿੱਚ, ਜਦੋਂ ਬੈਗ-ਆਫ-ਵਰਡਜ਼ ਵਰਤ ਰਹੇ ਸਨ, ਤਾਂ ਮਿਨੀਬੈਚ ਵਿੱਚ ਸਾਰੇ BoW ਟੈਂਸਰਾਂ ਦਾ ਆਕਾਰ `vocab_size` ਦੇ ਬਰਾਬਰ ਹੁੰਦਾ ਸੀ, ਭਾਵੇਂ ਸਾਡੇ ਟੈਕਸਟ ਲੜੀ ਦੀ ਅਸਲ ਲੰਬਾਈ ਜੋ ਮਰਜ਼ੀ ਹੋਵੇ। ਜਦੋਂ ਅਸੀਂ ਵਰਡ ਐਮਬੈਡਿੰਗਜ਼ ਵੱਲ ਜਾਵਾਂਗੇ, ਤਾਂ ਹਰ ਟੈਕਸਟ ਨਮੂਨੇ ਵਿੱਚ ਸ਼ਬਦਾਂ ਦੀ ਗਿਣਤੀ ਵੱਖ-ਵੱਖ ਹੋਵੇਗੀ, ਅਤੇ ਜਦੋਂ ਅਸੀਂ ਉਹਨਾਂ ਨਮੂਨਿਆਂ ਨੂੰ ਮਿਨੀਬੈਚਜ਼ ਵਿੱਚ ਜੋੜਾਂਗੇ, ਤਾਂ ਸਾਨੂੰ ਕੁਝ ਪੈਡਿੰਗ ਲਾਗੂ ਕਰਨੀ ਪਵੇਗੀ।\n",
    "\n",
    "ਇਹ ਕੰਮ `collate_fn` ਫੰਕਸ਼ਨ ਨੂੰ ਡੇਟਾਸੋਰਸ ਵਿੱਚ ਪ੍ਰਦਾਨ ਕਰਕੇ ਕੀਤਾ ਜਾ ਸਕਦਾ ਹੈ:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padify(b):\n",
    "    # b is the list of tuples of length batch_size\n",
    "    #   - first element of a tuple = label, \n",
    "    #   - second = feature (text sequence)\n",
    "    # build vectorized sequence\n",
    "    v = [encode(x[1]) for x in b]\n",
    "    # first, compute max length of a sequence in this minibatch\n",
    "    l = max(map(len,v))\n",
    "    return ( # tuple of two tensors - labels and features\n",
    "        torch.LongTensor([t[0]-1 for t in b]),\n",
    "        torch.stack([torch.nn.functional.pad(torch.tensor(t),(0,l-len(t)),mode='constant',value=0) for t in v])\n",
    "    )\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, collate_fn=padify, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ਐਮਬੈਡਿੰਗ ਕਲਾਸੀਫਾਇਰ ਦੀ ਟ੍ਰੇਨਿੰਗ\n",
    "\n",
    "ਹੁਣ ਜਦੋਂ ਕਿ ਅਸੀਂ ਸਹੀ ਡਾਟਾਲੋਡਰ ਨੂੰ ਪਰਿਭਾਸ਼ਿਤ ਕਰ ਲਿਆ ਹੈ, ਅਸੀਂ ਪਿਛਲੇ ਯੂਨਿਟ ਵਿੱਚ ਪਰਿਭਾਸ਼ਿਤ ਕੀਤੇ ਗਏ ਟ੍ਰੇਨਿੰਗ ਫੰਕਸ਼ਨ ਦੀ ਵਰਤੋਂ ਕਰਕੇ ਮਾਡਲ ਨੂੰ ਟ੍ਰੇਨ ਕਰ ਸਕਦੇ ਹਾਂ:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200: acc=0.6415625\n",
      "6400: acc=0.6865625\n",
      "9600: acc=0.7103125\n",
      "12800: acc=0.726953125\n",
      "16000: acc=0.739375\n",
      "19200: acc=0.75046875\n",
      "22400: acc=0.7572321428571429\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.889799795315499, 0.7623160588611644)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = EmbedClassifier(vocab_size,32,len(classes)).to(device)\n",
    "train_epoch(net,train_loader, lr=1, epoch_size=25000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **ਨੋਟ**: ਅਸੀਂ ਇੱਥੇ ਸਿਰਫ 25,000 ਰਿਕਾਰਡਾਂ ਲਈ ਟ੍ਰੇਨਿੰਗ ਕਰ ਰਹੇ ਹਾਂ (ਇੱਕ ਪੂਰੇ epoch ਤੋਂ ਘੱਟ) ਸਮੇਂ ਦੀ ਬਚਤ ਲਈ, ਪਰ ਤੁਸੀਂ ਟ੍ਰੇਨਿੰਗ ਜਾਰੀ ਰੱਖ ਸਕਦੇ ਹੋ, ਕਈ epoch ਲਈ ਟ੍ਰੇਨ ਕਰਨ ਲਈ ਇੱਕ ਫੰਕਸ਼ਨ ਲਿਖ ਸਕਦੇ ਹੋ, ਅਤੇ ਉੱਚ ਸਹੀਤਾ ਹਾਸਲ ਕਰਨ ਲਈ ਲਰਨਿੰਗ ਰੇਟ ਪੈਰਾਮੀਟਰ ਨਾਲ ਪ੍ਰਯੋਗ ਕਰ ਸਕਦੇ ਹੋ। ਤੁਸੀਂ ਲਗਭਗ 90% ਸਹੀਤਾ ਤੱਕ ਪਹੁੰਚ ਸਕਦੇ ਹੋ।\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ਐਮਬੈਡਿੰਗਬੈਗ ਲੇਅਰ ਅਤੇ ਵੱਖ-ਵੱਖ ਲੰਬਾਈ ਵਾਲੇ ਕ੍ਰਮ ਦੀ ਪ੍ਰਤੀਨਿਧਤਾ\n",
    "\n",
    "ਪਿਛਲੀ ਆਰਕੀਟੈਕਚਰ ਵਿੱਚ, ਸਾਨੂੰ ਸਾਰੇ ਕ੍ਰਮਾਂ ਨੂੰ ਇੱਕੋ ਲੰਬਾਈ ਵਿੱਚ ਪੈਡ ਕਰਨਾ ਪੈਂਦਾ ਸੀ ਤਾਂ ਜੋ ਉਹਨਾਂ ਨੂੰ ਇੱਕ ਮਿਨੀਬੈਚ ਵਿੱਚ ਫਿੱਟ ਕੀਤਾ ਜਾ ਸਕੇ। ਇਹ ਵੱਖ-ਵੱਖ ਲੰਬਾਈ ਵਾਲੇ ਕ੍ਰਮਾਂ ਨੂੰ ਪ੍ਰਤੀਨਿਧਤ ਕਰਨ ਦਾ ਸਭ ਤੋਂ ਕੁਸ਼ਲ ਤਰੀਕਾ ਨਹੀਂ ਹੈ - ਇੱਕ ਹੋਰ ਤਰੀਕਾ **ਆਫਸੈਟ** ਵੇਕਟਰ ਦੀ ਵਰਤੋਂ ਕਰਨਾ ਹੋਵੇਗਾ, ਜੋ ਇੱਕ ਵੱਡੇ ਵੇਕਟਰ ਵਿੱਚ ਸਟੋਰ ਕੀਤੇ ਸਾਰੇ ਕ੍ਰਮਾਂ ਦੇ ਆਫਸੈਟ ਨੂੰ ਰੱਖੇਗਾ।\n",
    "\n",
    "![ਆਫਸੈਟ ਕ੍ਰਮ ਦੀ ਪ੍ਰਤੀਨਿਧਤਾ ਦਿਖਾਉਂਦੀ ਤਸਵੀਰ](../../../../../translated_images/pa/offset-sequence-representation.eb73fcefb29b46ee.webp)\n",
    "\n",
    "> **Note**: ਉੱਪਰ ਦਿੱਤੀ ਤਸਵੀਰ ਵਿੱਚ, ਅਸੀਂ ਅੱਖਰਾਂ ਦੇ ਕ੍ਰਮ ਨੂੰ ਦਿਖਾਇਆ ਹੈ, ਪਰ ਸਾਡੇ ਉਦਾਹਰਨ ਵਿੱਚ ਅਸੀਂ ਸ਼ਬਦਾਂ ਦੇ ਕ੍ਰਮਾਂ ਨਾਲ ਕੰਮ ਕਰ ਰਹੇ ਹਾਂ। ਹਾਲਾਂਕਿ, ਆਫਸੈਟ ਵੇਕਟਰ ਨਾਲ ਕ੍ਰਮਾਂ ਦੀ ਪ੍ਰਤੀਨਿਧਤਾ ਕਰਨ ਦਾ ਆਮ ਸਿਧਾਂਤ ਇੱਕੋ ਜਿਹਾ ਰਹਿੰਦਾ ਹੈ।\n",
    "\n",
    "ਆਫਸੈਟ ਪ੍ਰਤੀਨਿਧਤਾ ਨਾਲ ਕੰਮ ਕਰਨ ਲਈ, ਅਸੀਂ [`EmbeddingBag`](https://pytorch.org/docs/stable/generated/torch.nn.EmbeddingBag.html) ਲੇਅਰ ਦੀ ਵਰਤੋਂ ਕਰਦੇ ਹਾਂ। ਇਹ `Embedding` ਦੇ ਸਮਾਨ ਹੈ, ਪਰ ਇਹ ਸਮੱਗਰੀ ਵੇਕਟਰ ਅਤੇ ਆਫਸੈਟ ਵੇਕਟਰ ਨੂੰ ਇਨਪੁਟ ਵਜੋਂ ਲੈਂਦਾ ਹੈ, ਅਤੇ ਇਸ ਵਿੱਚ ਇੱਕ ਐਵਰੇਜਿੰਗ ਲੇਅਰ ਵੀ ਸ਼ਾਮਲ ਹੁੰਦੀ ਹੈ, ਜੋ ਕਿ `mean`, `sum` ਜਾਂ `max` ਹੋ ਸਕਦੀ ਹੈ।\n",
    "\n",
    "ਹੇਠਾਂ ਇੱਕ ਸੋਧਿਆ ਹੋਇਆ ਨੈਟਵਰਕ ਦਿੱਤਾ ਗਿਆ ਹੈ ਜੋ `EmbeddingBag` ਦੀ ਵਰਤੋਂ ਕਰਦਾ ਹੈ:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbedClassifier(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super().__init__()\n",
    "        self.embedding = torch.nn.EmbeddingBag(vocab_size, embed_dim)\n",
    "        self.fc = torch.nn.Linear(embed_dim, num_class)\n",
    "\n",
    "    def forward(self, text, off):\n",
    "        x = self.embedding(text, off)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ਡਾਟਾਸੈੱਟ ਨੂੰ ਟ੍ਰੇਨਿੰਗ ਲਈ ਤਿਆਰ ਕਰਨ ਲਈ, ਸਾਨੂੰ ਇੱਕ ਰੂਪਾਂਤਰਨ ਫੰਕਸ਼ਨ ਪ੍ਰਦਾਨ ਕਰਨ ਦੀ ਲੋੜ ਹੈ ਜੋ ਆਫਸੈਟ ਵੇਕਟਰ ਤਿਆਰ ਕਰੇਗਾ:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def offsetify(b):\n",
    "    # first, compute data tensor from all sequences\n",
    "    x = [torch.tensor(encode(t[1])) for t in b]\n",
    "    # now, compute the offsets by accumulating the tensor of sequence lengths\n",
    "    o = [0] + [len(t) for t in x]\n",
    "    o = torch.tensor(o[:-1]).cumsum(dim=0)\n",
    "    return ( \n",
    "        torch.LongTensor([t[0]-1 for t in b]), # labels\n",
    "        torch.cat(x), # text \n",
    "        o\n",
    "    )\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, collate_fn=offsetify, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ਨੋਟ ਕਰੋ, ਕਿ ਪਿਛਲੇ ਸਾਰੇ ਉਦਾਹਰਣਾਂ ਤੋਂ ਵੱਖਰਾ, ਹੁਣ ਸਾਡਾ ਨੈਟਵਰਕ ਦੋ ਪੈਰਾਮੀਟਰਾਂ ਨੂੰ ਸਵੀਕਾਰ ਕਰਦਾ ਹੈ: ਡਾਟਾ ਵੈਕਟਰ ਅਤੇ ਆਫਸੈਟ ਵੈਕਟਰ, ਜੋ ਵੱਖ-ਵੱਖ ਆਕਾਰ ਦੇ ਹਨ। ਇਸੇ ਤਰ੍ਹਾਂ, ਸਾਡਾ ਡਾਟਾ ਲੋਡਰ ਵੀ ਸਾਨੂੰ 2 ਦੀ ਬਜਾਏ 3 ਮੁੱਲ ਪ੍ਰਦਾਨ ਕਰਦਾ ਹੈ: ਦੋਵੇਂ ਟੈਕਸਟ ਅਤੇ ਆਫਸੈਟ ਵੈਕਟਰ ਫੀਚਰਾਂ ਵਜੋਂ ਪ੍ਰਦਾਨ ਕੀਤੇ ਜਾਂਦੇ ਹਨ। ਇਸ ਲਈ, ਸਾਨੂੰ ਆਪਣੀ ਟ੍ਰੇਨਿੰਗ ਫੰਕਸ਼ਨ ਨੂੰ ਥੋੜ੍ਹਾ ਸਮਰਥਨ ਕਰਨ ਦੀ ਲੋੜ ਹੈ ਤਾਂ ਕਿ ਇਸਦਾ ਧਿਆਨ ਰੱਖਿਆ ਜਾ ਸਕੇ:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200: acc=0.6153125\n",
      "6400: acc=0.6615625\n",
      "9600: acc=0.6932291666666667\n",
      "12800: acc=0.715078125\n",
      "16000: acc=0.7270625\n",
      "19200: acc=0.7382291666666667\n",
      "22400: acc=0.7486160714285715\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(22.771553103007037, 0.7551983365323096)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = EmbedClassifier(vocab_size,32,len(classes)).to(device)\n",
    "\n",
    "def train_epoch_emb(net,dataloader,lr=0.01,optimizer=None,loss_fn = torch.nn.CrossEntropyLoss(),epoch_size=None, report_freq=200):\n",
    "    optimizer = optimizer or torch.optim.Adam(net.parameters(),lr=lr)\n",
    "    loss_fn = loss_fn.to(device)\n",
    "    net.train()\n",
    "    total_loss,acc,count,i = 0,0,0,0\n",
    "    for labels,text,off in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        labels,text,off = labels.to(device), text.to(device), off.to(device)\n",
    "        out = net(text, off)\n",
    "        loss = loss_fn(out,labels) #cross_entropy(out,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss+=loss\n",
    "        _,predicted = torch.max(out,1)\n",
    "        acc+=(predicted==labels).sum()\n",
    "        count+=len(labels)\n",
    "        i+=1\n",
    "        if i%report_freq==0:\n",
    "            print(f\"{count}: acc={acc.item()/count}\")\n",
    "        if epoch_size and count>epoch_size:\n",
    "            break\n",
    "    return total_loss.item()/count, acc.item()/count\n",
    "\n",
    "\n",
    "train_epoch_emb(net,train_loader, lr=4, epoch_size=25000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ਸੈਮੈਂਟਿਕ ਐਮਬੈਡਿੰਗਸ: ਵਰਡ2ਵੈਕ\n",
    "\n",
    "ਸਾਡੇ ਪਿਛਲੇ ਉਦਾਹਰਨ ਵਿੱਚ, ਮਾਡਲ ਐਮਬੈਡਿੰਗ ਲੇਅਰ ਨੇ ਸ਼ਬਦਾਂ ਨੂੰ ਵੈਕਟਰ ਪ੍ਰਤੀਨਿਧੀ ਵਿੱਚ ਮੈਪ ਕਰਨਾ ਸਿੱਖਿਆ, ਪਰ ਇਸ ਪ੍ਰਤੀਨਿਧੀ ਵਿੱਚ ਜ਼ਿਆਦਾ ਸੈਮੈਂਟਿਕ ਅਰਥ ਨਹੀਂ ਸੀ। ਇਹ ਵਧੀਆ ਹੋਵੇਗਾ ਕਿ ਅਜਿਹੇ ਵੈਕਟਰ ਪ੍ਰਤੀਨਿਧੀ ਸਿੱਖੇ ਜਾਣ, ਜਿੱਥੇ ਸਮਾਨ ਸ਼ਬਦ ਜਾਂ ਪਰਿਆਇ ਵਾਕ ਸ਼ਬਦ ਕੁਝ ਵੈਕਟਰ ਦੂਰੀ (ਜਿਵੇਂ ਕਿ ਯੂਕਲਿਡੀਅਨ ਦੂਰੀ) ਦੇ ਹਿਸਾਬ ਨਾਲ ਇੱਕ ਦੂਜੇ ਦੇ ਨੇੜੇ ਹੋਣ।\n",
    "\n",
    "ਇਸ ਨੂੰ ਹਾਸਲ ਕਰਨ ਲਈ, ਸਾਨੂੰ ਆਪਣੇ ਐਮਬੈਡਿੰਗ ਮਾਡਲ ਨੂੰ ਇੱਕ ਵਿਸ਼ੇਸ਼ ਢੰਗ ਨਾਲ ਵੱਡੇ ਟੈਕਸਟ ਸੰਗ੍ਰਹਿ 'ਤੇ ਪ੍ਰੀ-ਟ੍ਰੇਨ ਕਰਨ ਦੀ ਲੋੜ ਹੈ। ਸੈਮੈਂਟਿਕ ਐਮਬੈਡਿੰਗਸ ਨੂੰ ਟ੍ਰੇਨ ਕਰਨ ਦੇ ਪਹਿਲੇ ਤਰੀਕਿਆਂ ਵਿੱਚੋਂ ਇੱਕ ਨੂੰ [ਵਰਡ2ਵੈਕ](https://en.wikipedia.org/wiki/Word2vec) ਕਿਹਾ ਜਾਂਦਾ ਹੈ। ਇਹ ਦੋ ਮੁੱਖ ਆਰਕੀਟੈਕਚਰਾਂ 'ਤੇ ਆਧਾਰਿਤ ਹੈ ਜੋ ਸ਼ਬਦਾਂ ਦੀ ਵੰਡਵਾਂ ਪ੍ਰਤੀਨਿਧੀ ਪੈਦਾ ਕਰਨ ਲਈ ਵਰਤੀ ਜਾਂਦੀ ਹੈ:\n",
    "\n",
    "- **ਕੰਟਿਨਿਊਅਸ ਬੈਗ-ਆਫ-ਵਰਡਸ** (CBoW) — ਇਸ ਆਰਕੀਟੈਕਚਰ ਵਿੱਚ, ਅਸੀਂ ਮਾਡਲ ਨੂੰ ਆਸਪਾਸ ਦੇ ਸੰਦਰਭ ਤੋਂ ਇੱਕ ਸ਼ਬਦ ਦੀ ਭਵਿੱਖਵਾਣੀ ਕਰਨ ਲਈ ਟ੍ਰੇਨ ਕਰਦੇ ਹਾਂ। ਦਿੱਤੇ ਗਏ ngram $(W_{-2},W_{-1},W_0,W_1,W_2)$, ਮਾਡਲ ਦਾ ਉਦੇਸ਼ $(W_{-2},W_{-1},W_1,W_2)$ ਤੋਂ $W_0$ ਦੀ ਭਵਿੱਖਵਾਣੀ ਕਰਨਾ ਹੈ।\n",
    "- **ਕੰਟਿਨਿਊਅਸ ਸਕਿਪ-ਗ੍ਰਾਮ** CBoW ਦੇ ਉਲਟ ਹੈ। ਮਾਡਲ ਆਸਪਾਸ ਦੇ ਸੰਦਰਭ ਸ਼ਬਦਾਂ ਦੀ ਵਿੰਡੋ ਨੂੰ ਵਰਤਦਾ ਹੈ ਤਾਕਿ ਮੌਜੂਦਾ ਸ਼ਬਦ ਦੀ ਭਵਿੱਖਵਾਣੀ ਕੀਤੀ ਜਾ ਸਕੇ।\n",
    "\n",
    "CBoW ਤੇਜ਼ ਹੈ, ਜਦਕਿ ਸਕਿਪ-ਗ੍ਰਾਮ ਹੌਲੀ ਹੈ, ਪਰ ਇਹ ਅਲਭ ਸ਼ਬਦਾਂ ਦੀ ਵਧੀਆ ਪ੍ਰਤੀਨਿਧੀ ਕਰਦਾ ਹੈ।\n",
    "\n",
    "![ਦੋਵੇਂ CBoW ਅਤੇ ਸਕਿਪ-ਗ੍ਰਾਮ ਐਲਗੋਰਿਥਮਾਂ ਨੂੰ ਸ਼ਬਦਾਂ ਨੂੰ ਵੈਕਟਰ ਵਿੱਚ ਬਦਲਣ ਲਈ ਦਿਖਾਉਣ ਵਾਲੀ ਚਿੱਤਰ।](../../../../../translated_images/pa/example-algorithms-for-converting-words-to-vectors.fbe9207a726922f6.webp)\n",
    "\n",
    "ਗੂਗਲ ਨਿਊਜ਼ ਡੇਟਾਸੈਟ 'ਤੇ ਪ੍ਰੀ-ਟ੍ਰੇਨ ਕੀਤੇ ਵਰਡ2ਵੈਕ ਐਮਬੈਡਿੰਗ ਨਾਲ ਪ੍ਰਯੋਗ ਕਰਨ ਲਈ, ਅਸੀਂ **gensim** ਲਾਇਬ੍ਰੇਰੀ ਦੀ ਵਰਤੋਂ ਕਰ ਸਕਦੇ ਹਾਂ। ਹੇਠਾਂ ਅਸੀਂ 'neural' ਦੇ ਸਭ ਤੋਂ ਸਮਾਨ ਸ਼ਬਦਾਂ ਨੂੰ ਲੱਭਦੇ ਹਾਂ।\n",
    "\n",
    "> **Note:** ਜਦੋਂ ਤੁਸੀਂ ਪਹਿਲੀ ਵਾਰ ਸ਼ਬਦ ਵੈਕਟਰ ਬਣਾਉਂਦੇ ਹੋ, ਤਾਂ ਇਹਨਾਂ ਨੂੰ ਡਾਊਨਲੋਡ ਕਰਨ ਵਿੱਚ ਕੁਝ ਸਮਾਂ ਲੱਗ ਸਕਦਾ ਹੈ!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "w2v = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neuronal -> 0.7804799675941467\n",
      "neurons -> 0.7326500415802002\n",
      "neural_circuits -> 0.7252851724624634\n",
      "neuron -> 0.7174385190010071\n",
      "cortical -> 0.6941086649894714\n",
      "brain_circuitry -> 0.6923246383666992\n",
      "synaptic -> 0.6699118614196777\n",
      "neural_circuitry -> 0.6638563275337219\n",
      "neurochemical -> 0.6555314064025879\n",
      "neuronal_activity -> 0.6531826257705688\n"
     ]
    }
   ],
   "source": [
    "for w,p in w2v.most_similar('neural'):\n",
    "    print(f\"{w} -> {p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ਅਸੀਂ ਸ਼ਬਦ ਤੋਂ ਵੀਕਟਰ ਐਮਬੈਡਿੰਗਜ਼ ਦੀ ਗਣਨਾ ਕਰ ਸਕਦੇ ਹਾਂ, ਜੋ ਕਿ ਕਲਾਸੀਫਿਕੇਸ਼ਨ ਮਾਡਲ ਦੀ ਟ੍ਰੇਨਿੰਗ ਵਿੱਚ ਵਰਤੀ ਜਾਣੀ ਹੈ (ਸਪਸ਼ਟਤਾ ਲਈ ਅਸੀਂ ਸਿਰਫ਼ ਵੀਕਟਰ ਦੇ ਪਹਿਲੇ 20 ਘਟਕੇ ਦਿਖਾਉਂਦੇ ਹਾਂ):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01226807,  0.06225586,  0.10693359,  0.05810547,  0.23828125,\n",
       "        0.03686523,  0.05151367, -0.20703125,  0.01989746,  0.10058594,\n",
       "       -0.03759766, -0.1015625 , -0.15820312, -0.08105469, -0.0390625 ,\n",
       "       -0.05053711,  0.16015625,  0.2578125 ,  0.10058594, -0.25976562],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.word_vec('play')[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ਸਮਾਂਤਿਕਲ ਐਮਬੈਡਿੰਗਜ਼ ਬਾਰੇ ਵਧੀਆ ਗੱਲ ਇਹ ਹੈ ਕਿ ਤੁਸੀਂ ਸਿਮਾਂਤਿਕਸ ਨੂੰ ਬਦਲਣ ਲਈ ਵੈਕਟਰ ਕੋਡਿੰਗ ਨੂੰ ਮੋੜ ਸਕਦੇ ਹੋ। ਉਦਾਹਰਨ ਵਜੋਂ, ਅਸੀਂ ਇੱਕ ਸ਼ਬਦ ਲੱਭਣ ਲਈ ਪੁੱਛ ਸਕਦੇ ਹਾਂ, ਜਿਸਦਾ ਵੈਕਟਰ ਪ੍ਰਤੀਨਿਧਿਤਾ ਸ਼ਬਦ *king* ਅਤੇ *woman* ਦੇ ਜਿੰਨਾ ਨਜ਼ਦੀਕ ਹੋ ਸਕੇ ਹੋਵੇ, ਅਤੇ ਸ਼ਬਦ *man* ਤੋਂ ਜਿੰਨਾ ਦੂਰ ਹੋ ਸਕੇ:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('queen', 0.7118192911148071)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar(positive=['king','woman'],negative=['man'])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ਦੋਵੇਂ CBoW ਅਤੇ Skip-Grams \"ਪ੍ਰਿਡਿਕਟਿਵ\" ਐਮਬੈਡਿੰਗ ਹਨ, ਕਿਉਂਕਿ ਇਹ ਸਿਰਫ਼ ਸਥਾਨਕ ਸੰਦਰਭਾਂ ਨੂੰ ਹੀ ਧਿਆਨ ਵਿੱਚ ਰੱਖਦੇ ਹਨ। Word2Vec ਗਲੋਬਲ ਸੰਦਰਭ ਦਾ ਫਾਇਦਾ ਨਹੀਂ ਲੈਂਦਾ।\n",
    "\n",
    "**FastText**, Word2Vec 'ਤੇ ਆਧਾਰਿਤ ਹੈ, ਜੋ ਹਰ ਸ਼ਬਦ ਅਤੇ ਉਸ ਵਿੱਚ ਪਾਏ ਜਾਣ ਵਾਲੇ ਅੱਖਰ n-grams ਲਈ ਵੈਕਟਰ ਪ੍ਰਸਤੁਤੀਆਂ ਸਿੱਖਦਾ ਹੈ। ਪ੍ਰਸਤੁਤੀਆਂ ਦੇ ਮੁੱਲ ਹਰ ਟ੍ਰੇਨਿੰਗ ਕਦਮ 'ਤੇ ਇੱਕ ਵੈਕਟਰ ਵਿੱਚ ਔਸਤ ਕੀਤੇ ਜਾਂਦੇ ਹਨ। ਹਾਲਾਂਕਿ ਇਹ ਪ੍ਰੀ-ਟ੍ਰੇਨਿੰਗ ਵਿੱਚ ਕਾਫ਼ੀ ਵਾਧੂ ਗਣਨਾ ਜੋੜਦਾ ਹੈ, ਪਰ ਇਹ ਸ਼ਬਦ ਐਮਬੈਡਿੰਗ ਨੂੰ ਉਪ-ਸ਼ਬਦ ਜਾਣਕਾਰੀ ਨੂੰ ਕੋਡ ਕਰਨ ਯੋਗ ਬਣਾਉਂਦਾ ਹੈ।\n",
    "\n",
    "ਇਕ ਹੋਰ ਵਿਧੀ, **GloVe**, ਸਹ-ਘਟਨਾ ਮੈਟ੍ਰਿਕਸ ਦੇ ਵਿਚਾਰ ਨੂੰ ਲਾਭਦਾਇਕ ਬਣਾਉਂਦੀ ਹੈ ਅਤੇ ਸਹ-ਘਟਨਾ ਮੈਟ੍ਰਿਕਸ ਨੂੰ ਹੋਰ ਪ੍ਰਗਟਾਵਕ ਅਤੇ ਗੈਰ-ਰੇਖਿਕ ਸ਼ਬਦ ਵੈਕਟਰਾਂ ਵਿੱਚ ਵਿਘਟਿਤ ਕਰਨ ਲਈ ਨਿਊਰਲ ਤਰੀਕੇ ਵਰਤਦੀ ਹੈ।\n",
    "\n",
    "ਤੁਹਾਨੂੰ FastText ਅਤੇ GloVe ਐਮਬੈਡਿੰਗ ਨੂੰ ਬਦਲ ਕੇ ਉਦਾਹਰਣ ਨਾਲ ਖੇਡਣ ਦਾ ਮੌਕਾ ਮਿਲਦਾ ਹੈ, ਕਿਉਂਕਿ gensim ਕਈ ਵੱਖ-ਵੱਖ ਸ਼ਬਦ ਐਮਬੈਡਿੰਗ ਮਾਡਲਾਂ ਦਾ ਸਮਰਥਨ ਕਰਦਾ ਹੈ।\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ਪਾਈਟਾਰਚ ਵਿੱਚ ਪਹਿਲਾਂ ਤੋਂ ਤਿਆਰ ਕੀਤੇ ਗਏ ਐਮਬੈਡਿੰਗਜ਼ ਦੀ ਵਰਤੋਂ\n",
    "\n",
    "ਅਸੀਂ ਉਪਰੋਕਤ ਉਦਾਹਰਨ ਨੂੰ ਇਸ ਤਰ੍ਹਾਂ ਬਦਲ ਸਕਦੇ ਹਾਂ ਕਿ ਆਪਣੇ ਐਮਬੈਡਿੰਗ ਲੇਅਰ ਵਿੱਚ ਮੈਟ੍ਰਿਕਸ ਨੂੰ ਸੈਮਾਂਟਿਕ ਐਮਬੈਡਿੰਗਜ਼, ਜਿਵੇਂ ਕਿ Word2Vec ਨਾਲ ਪਹਿਲਾਂ ਤੋਂ ਭਰ ਸਕੀਏ। ਸਾਨੂੰ ਇਹ ਧਿਆਨ ਵਿੱਚ ਰੱਖਣਾ ਪਵੇਗਾ ਕਿ ਪਹਿਲਾਂ ਤੋਂ ਤਿਆਰ ਕੀਤੇ ਗਏ ਐਮਬੈਡਿੰਗ ਅਤੇ ਸਾਡੇ ਟੈਕਸਟ ਕੋਰਪਸ ਦੇ ਵੋਕੈਬੁਲਰੀਜ਼ ਅਕਸਰ ਮਿਲਦੇ ਨਹੀਂ ਹਨ, ਇਸ ਲਈ ਅਸੀਂ ਗੁੰਮ ਹੋਏ ਸ਼ਬਦਾਂ ਲਈ ਵਜ਼ਨ ਨੂੰ ਰੈਂਡਮ ਵੈਲਿਊਜ਼ ਨਾਲ ਸ਼ੁਰੂ ਕਰਾਂਗੇ:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding size: 300\n",
      "Populating matrix, this will take some time...Done, found 41080 words, 54732 words missing\n"
     ]
    }
   ],
   "source": [
    "embed_size = len(w2v.get_vector('hello'))\n",
    "print(f'Embedding size: {embed_size}')\n",
    "\n",
    "net = EmbedClassifier(vocab_size,embed_size,len(classes))\n",
    "\n",
    "print('Populating matrix, this will take some time...',end='')\n",
    "found, not_found = 0,0\n",
    "for i,w in enumerate(vocab.get_itos()):\n",
    "    try:\n",
    "        net.embedding.weight[i].data = torch.tensor(w2v.get_vector(w))\n",
    "        found+=1\n",
    "    except:\n",
    "        net.embedding.weight[i].data = torch.normal(0.0,1.0,(embed_size,))\n",
    "        not_found+=1\n",
    "\n",
    "print(f\"Done, found {found} words, {not_found} words missing\")\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ਹੁਣ ਆਓ ਅਸੀਂ ਆਪਣੇ ਮਾਡਲ ਨੂੰ ਟ੍ਰੇਨ ਕਰੀਏ। ਧਿਆਨ ਦਿਓ ਕਿ ਮਾਡਲ ਨੂੰ ਟ੍ਰੇਨ ਕਰਨ ਵਿੱਚ ਲੱਗਣ ਵਾਲਾ ਸਮਾਂ ਪਿਛਲੇ ਉਦਾਹਰਨ ਦੇ ਮੁਕਾਬਲੇ ਕਾਫ਼ੀ ਵੱਧ ਹੈ, ਵੱਡੇ ਐਮਬੈਡਿੰਗ ਲੇਅਰ ਸਾਈਜ਼ ਦੇ ਕਾਰਨ, ਅਤੇ ਇਸ ਤਰ੍ਹਾਂ ਕਾਫ਼ੀ ਵੱਧ ਪੈਰਾਮੀਟਰਾਂ ਦੀ ਗਿਣਤੀ। ਇਸ ਦੇ ਨਾਲ ਹੀ, ਇਸ ਕਾਰਨ, ਜੇਕਰ ਅਸੀਂ ਓਵਰਫਿਟਿੰਗ ਤੋਂ ਬਚਣਾ ਚਾਹੁੰਦੇ ਹਾਂ ਤਾਂ ਸਾਨੂੰ ਆਪਣੇ ਮਾਡਲ ਨੂੰ ਹੋਰ ਉਦਾਹਰਨਾਂ 'ਤੇ ਟ੍ਰੇਨ ਕਰਨ ਦੀ ਲੋੜ ਹੋ ਸਕਦੀ ਹੈ।\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200: acc=0.6359375\n",
      "6400: acc=0.68109375\n",
      "9600: acc=0.7067708333333333\n",
      "12800: acc=0.723671875\n",
      "16000: acc=0.73625\n",
      "19200: acc=0.7463541666666667\n",
      "22400: acc=0.7560714285714286\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(214.1013875559821, 0.7626759436980166)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_epoch_emb(net,train_loader, lr=4, epoch_size=25000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ਸਾਡੇ ਮਾਮਲੇ ਵਿੱਚ, ਸਾਨੂੰ ਸ਼ੁੱਧਤਾ ਵਿੱਚ ਵੱਡਾ ਵਾਧਾ ਨਹੀਂ ਦਿਖਾਈ ਦਿੰਦਾ, ਜੋ ਸੰਭਵਤ: ਬਹੁਤ ਵੱਖਰੇ ਸ਼ਬਦਾਵਲੀ ਕਾਰਨ ਹੈ।  \n",
    "ਵੱਖਰੀ ਸ਼ਬਦਾਵਲੀ ਦੀ ਸਮੱਸਿਆ ਨੂੰ ਹੱਲ ਕਰਨ ਲਈ, ਅਸੀਂ ਹੇਠਾਂ ਦਿੱਤੇ ਹੱਲਾਂ ਵਿੱਚੋਂ ਇੱਕ ਦੀ ਵਰਤੋਂ ਕਰ ਸਕਦੇ ਹਾਂ:  \n",
    "* ਆਪਣੇ ਸ਼ਬਦਾਵਲੀ 'ਤੇ word2vec ਮਾਡਲ ਨੂੰ ਮੁੜ-ਟ੍ਰੇਨ ਕਰੋ  \n",
    "* ਪ੍ਰੀ-ਟ੍ਰੇਨ ਕੀਤੇ word2vec ਮਾਡਲ ਦੀ ਸ਼ਬਦਾਵਲੀ ਨਾਲ ਸਾਡਾ ਡਾਟਾਸੈਟ ਲੋਡ ਕਰੋ। ਡਾਟਾਸੈਟ ਨੂੰ ਲੋਡ ਕਰਨ ਲਈ ਵਰਤੀ ਗਈ ਸ਼ਬਦਾਵਲੀ ਨੂੰ ਲੋਡਿੰਗ ਦੌਰਾਨ ਨਿਰਧਾਰਤ ਕੀਤਾ ਜਾ ਸਕਦਾ ਹੈ।  \n",
    "\n",
    "ਦੂਜਾ ਤਰੀਕਾ ਆਸਾਨ ਲੱਗਦਾ ਹੈ, ਖਾਸ ਕਰਕੇ ਕਿਉਂਕਿ PyTorch `torchtext` ਫਰੇਮਵਰਕ ਵਿੱਚ embeddings ਲਈ ਬਿਲਟ-ਇਨ ਸਹਾਇਤਾ ਹੈ।  \n",
    "ਉਦਾਹਰਣ ਵਜੋਂ, ਅਸੀਂ ਹੇਠਾਂ ਦਿੱਤੇ ਤਰੀਕੇ ਨਾਲ GloVe-ਅਧਾਰਿਤ ਸ਼ਬਦਾਵਲੀ ਨੂੰ ਇੰਸਟੈਂਸ਼ੀਏਟ ਕਰ ਸਕਦੇ ਹਾਂ:  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 399999/400000 [00:15<00:00, 25411.14it/s]\n"
     ]
    }
   ],
   "source": [
    "vocab = torchtext.vocab.GloVe(name='6B', dim=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ਲੋਡ ਕੀਤੇ ਸ਼ਬਦ-ਭੰਡਾਰ ਵਿੱਚ ਹੇਠ ਲਿਖੀਆਂ ਮੁੱਢਲੀਆਂ ਕਾਰਵਾਈਆਂ ਸ਼ਾਮਲ ਹਨ:\n",
    "* `vocab.stoi` ਡਿਕਸ਼ਨਰੀ ਸਾਨੂੰ ਸ਼ਬਦ ਨੂੰ ਇਸਦੇ ਡਿਕਸ਼ਨਰੀ ਇੰਡੈਕਸ ਵਿੱਚ ਬਦਲਣ ਦੀ ਆਗਿਆ ਦਿੰਦੀ ਹੈ\n",
    "* `vocab.itos` ਇਸਦਾ ਉਲਟ ਕਰਦਾ ਹੈ - ਨੰਬਰ ਨੂੰ ਸ਼ਬਦ ਵਿੱਚ ਬਦਲਦਾ ਹੈ\n",
    "* `vocab.vectors` ਐਮਬੈਡਿੰਗ ਵੇਕਟਰਾਂ ਦੀ ਐਰੇ ਹੈ, ਇਸ ਲਈ ਕਿਸੇ ਸ਼ਬਦ `s` ਦੀ ਐਮਬੈਡਿੰਗ ਪ੍ਰਾਪਤ ਕਰਨ ਲਈ ਸਾਨੂੰ `vocab.vectors[vocab.stoi[s]]` ਵਰਤਣਾ ਪਵੇਗਾ\n",
    "\n",
    "ਇੱਥੇ ਐਮਬੈਡਿੰਗਜ਼ ਨਾਲ ਹੇਰਫੇਰ ਕਰਨ ਦਾ ਉਦਾਹਰਨ ਦਿੱਤਾ ਗਿਆ ਹੈ ਤਾਂ ਜੋ ਸਮੀਕਰਨ **kind-man+woman = queen** ਨੂੰ ਦਰਸਾਇਆ ਜਾ ਸਕੇ (ਮੈਨੂੰ ਇਸਨੂੰ ਕੰਮ ਕਰਨ ਲਈ ਗੁਣਾਂਕ ਵਿੱਚ ਥੋੜ੍ਹਾ ਬਦਲਾਅ ਕਰਨਾ ਪਿਆ):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'queen'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the vector corresponding to kind-man+woman\n",
    "qvec = vocab.vectors[vocab.stoi['king']]-vocab.vectors[vocab.stoi['man']]+1.3*vocab.vectors[vocab.stoi['woman']]\n",
    "# find the index of the closest embedding vector \n",
    "d = torch.sum((vocab.vectors-qvec)**2,dim=1)\n",
    "min_idx = torch.argmin(d)\n",
    "# find the corresponding word\n",
    "vocab.itos[min_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ਕਲਾਸੀਫਾਇਰ ਨੂੰ ਉਹਨਾਂ ਐਮਬੈਡਿੰਗਜ਼ ਦੀ ਵਰਤੋਂ ਕਰਕੇ ਟ੍ਰੇਨ ਕਰਨ ਲਈ, ਸਾਨੂੰ ਪਹਿਲਾਂ ਆਪਣਾ ਡੇਟਾਸੈੱਟ GloVe ਸ਼ਬਦਾਵਲੀ ਦੀ ਵਰਤੋਂ ਕਰਕੇ ਐਨਕੋਡ ਕਰਨ ਦੀ ਲੋੜ ਹੈ:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def offsetify(b):\n",
    "    # first, compute data tensor from all sequences\n",
    "    x = [torch.tensor(encode(t[1],voc=vocab)) for t in b] # pass the instance of vocab to encode function!\n",
    "    # now, compute the offsets by accumulating the tensor of sequence lengths\n",
    "    o = [0] + [len(t) for t in x]\n",
    "    o = torch.tensor(o[:-1]).cumsum(dim=0)\n",
    "    return ( \n",
    "        torch.LongTensor([t[0]-1 for t in b]), # labels\n",
    "        torch.cat(x), # text \n",
    "        o\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ਜਿਵੇਂ ਕਿ ਅਸੀਂ ਉੱਪਰ ਦੇਖਿਆ ਹੈ, ਸਾਰੇ ਵੇਕਟਰ ਐਮਬੈਡਿੰਗਜ਼ `vocab.vectors` ਮੈਟ੍ਰਿਕਸ ਵਿੱਚ ਸਟੋਰ ਕੀਤੇ ਜਾਂਦੇ ਹਨ। ਇਹਨਾਂ ਵਜ਼ਨਾਂ ਨੂੰ ਸਧਾਰਨ ਕਾਪੀ ਕਰਨ ਦੁਆਰਾ ਐਮਬੈਡਿੰਗ ਲੇਅਰ ਦੇ ਵਜ਼ਨਾਂ ਵਿੱਚ ਲੋਡ ਕਰਨਾ ਬਹੁਤ ਆਸਾਨ ਬਣਾ ਦਿੰਦਾ ਹੈ:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = EmbedClassifier(len(vocab),len(vocab.vectors[0]),len(classes))\n",
    "net.embedding.weight.data = vocab.vectors\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200: acc=0.6271875\n",
      "6400: acc=0.68078125\n",
      "9600: acc=0.7030208333333333\n",
      "12800: acc=0.71984375\n",
      "16000: acc=0.7346875\n",
      "19200: acc=0.7455729166666667\n",
      "22400: acc=0.7529464285714286\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(35.53972978646833, 0.7575175943698017)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, collate_fn=offsetify, shuffle=True)\n",
    "train_epoch_emb(net,train_loader, lr=4, epoch_size=25000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ਹੇਠਾਂ ਦਿੱਤੇ ਕਾਰਨਾਂ ਵਿੱਚੋਂ ਇੱਕ ਇਹ ਹੈ ਕਿ ਸਾਨੂੰ ਸ਼ੁੱਧਤਾ ਵਿੱਚ ਮਹੱਤਵਪੂਰਨ ਵਾਧਾ ਨਹੀਂ ਦਿਖਾਈ ਦੇ ਰਿਹਾ ਹੈ ਕਿਉਂਕਿ ਸਾਡੇ ਡੇਟਾਸੈਟ ਦੇ ਕੁਝ ਸ਼ਬਦ ਪਹਿਲਾਂ ਤੋਂ ਪ੍ਰਸ਼ਿਕਸ਼ਿਤ GloVe ਸ਼ਬਦਾਵਲੀ ਵਿੱਚ ਮੌਜੂਦ ਨਹੀਂ ਹਨ, ਅਤੇ ਇਸ ਲਈ ਉਹ ਅਸਲ ਵਿੱਚ ਅਣਗੌਰ ਕੀਤੇ ਜਾਂਦੇ ਹਨ। ਇਸ ਤੱਥ ਨੂੰ ਦੂਰ ਕਰਨ ਲਈ, ਅਸੀਂ ਆਪਣੇ ਡੇਟਾਸੈਟ 'ਤੇ ਆਪਣੇ ਐਮਬੈਡਿੰਗਜ਼ ਨੂੰ ਪ੍ਰਸ਼ਿਕਸ਼ਿਤ ਕਰ ਸਕਦੇ ਹਾਂ।\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ਸੰਦਰਭਿਕ ਐਮਬੈਡਿੰਗਸ\n",
    "\n",
    "ਪ੍ਰੰਪਰਾਗਤ ਪ੍ਰੀ-ਟ੍ਰੇਨ ਕੀਤੀਆਂ ਐਮਬੈਡਿੰਗ ਪ੍ਰਸਤੁਤੀਆਂ ਜਿਵੇਂ ਕਿ Word2Vec ਦੀ ਇੱਕ ਮੁੱਖ ਸੀਮਿਤਤਾ ਸ਼ਬਦ ਅਰਥ ਸਪਸ਼ਟੀਕਰਨ ਦੀ ਸਮੱਸਿਆ ਹੈ। ਜਦੋਂ ਕਿ ਪ੍ਰੀ-ਟ੍ਰੇਨ ਕੀਤੀਆਂ ਐਮਬੈਡਿੰਗਸ ਸ਼ਬਦਾਂ ਦੇ ਸੰਦਰਭ ਵਿੱਚ ਕੁਝ ਅਰਥ ਨੂੰ ਕੈਪਚਰ ਕਰ ਸਕਦੀਆਂ ਹਨ, ਇੱਕ ਸ਼ਬਦ ਦੇ ਹਰ ਸੰਭਾਵਿਤ ਅਰਥ ਨੂੰ ਇੱਕੋ ਐਮਬੈਡਿੰਗ ਵਿੱਚ ਕੋਡ ਕੀਤਾ ਜਾਂਦਾ ਹੈ। ਇਹ ਹੇਠਾਂ ਦਿੱਤੇ ਮਾਡਲਾਂ ਵਿੱਚ ਸਮੱਸਿਆ ਪੈਦਾ ਕਰ ਸਕਦਾ ਹੈ, ਕਿਉਂਕਿ ਬਹੁਤ ਸਾਰੇ ਸ਼ਬਦ, ਜਿਵੇਂ 'play', ਦਾ ਅਰਥ ਸੰਦਰਭ ਦੇ ਅਨੁਸਾਰ ਵੱਖ-ਵੱਖ ਹੋ ਸਕਦਾ ਹੈ।\n",
    "\n",
    "ਉਦਾਹਰਣ ਲਈ, 'play' ਸ਼ਬਦ ਦਾ ਅਰਥ ਹੇਠਾਂ ਦਿੱਤੇ ਦੋ ਵਾਕਾਂ ਵਿੱਚ ਬਹੁਤ ਵੱਖਰਾ ਹੈ:\n",
    "- ਮੈਂ ਥੀਏਟਰ ਵਿੱਚ ਇੱਕ **play** ਦੇਖਣ ਗਿਆ।\n",
    "- ਜੌਨ ਆਪਣੇ ਦੋਸਤਾਂ ਨਾਲ **play** ਕਰਨਾ ਚਾਹੁੰਦਾ ਹੈ।\n",
    "\n",
    "ਉਪਰੋਕਤ ਪ੍ਰੀ-ਟ੍ਰੇਨ ਕੀਤੀਆਂ ਐਮਬੈਡਿੰਗਸ 'play' ਸ਼ਬਦ ਦੇ ਦੋਨੋ ਅਰਥਾਂ ਨੂੰ ਇੱਕੋ ਐਮਬੈਡਿੰਗ ਵਿੱਚ ਦਰਸਾਉਂਦੀਆਂ ਹਨ। ਇਸ ਸੀਮਿਤਤਾ ਨੂੰ ਦੂਰ ਕਰਨ ਲਈ, ਸਾਨੂੰ **ਭਾਸ਼ਾ ਮਾਡਲ** ਦੇ ਆਧਾਰ 'ਤੇ ਐਮਬੈਡਿੰਗ ਬਣਾਉਣ ਦੀ ਲੋੜ ਹੈ, ਜੋ ਕਿ ਵੱਡੇ ਟੈਕਸਟ ਕੋਰਪਸ 'ਤੇ ਟ੍ਰੇਨ ਕੀਤਾ ਜਾਂਦਾ ਹੈ ਅਤੇ *ਜਾਣਦਾ ਹੈ* ਕਿ ਸ਼ਬਦਾਂ ਨੂੰ ਵੱਖ-ਵੱਖ ਸੰਦਰਭਾਂ ਵਿੱਚ ਕਿਵੇਂ ਜੋੜਿਆ ਜਾ ਸਕਦਾ ਹੈ। ਸੰਦਰਭਿਕ ਐਮਬੈਡਿੰਗਸ ਬਾਰੇ ਚਰਚਾ ਇਸ ਟਿਊਟੋਰਿਅਲ ਦੇ ਦਾਇਰੇ ਤੋਂ ਬਾਹਰ ਹੈ, ਪਰ ਅਸੀਂ ਇਸ ਬਾਰੇ ਅਗਲੇ ਯੂਨਿਟ ਵਿੱਚ ਭਾਸ਼ਾ ਮਾਡਲਾਂ ਦੀ ਗੱਲ ਕਰਦੇ ਸਮੇਂ ਵਾਪਸ ਆਵਾਂਗੇ।\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**ਅਸਵੀਕਤੀ**:  \nਇਹ ਦਸਤਾਵੇਜ਼ ਨੂੰ AI ਅਨੁਵਾਦ ਸੇਵਾ [Co-op Translator](https://github.com/Azure/co-op-translator) ਦੀ ਵਰਤੋਂ ਕਰਕੇ ਅਨੁਵਾਦ ਕੀਤਾ ਗਿਆ ਹੈ। ਜਦੋਂ ਕਿ ਅਸੀਂ ਸਹੀਤਾ ਲਈ ਯਤਨਸ਼ੀਲ ਹਾਂ, ਕਿਰਪਾ ਕਰਕੇ ਧਿਆਨ ਦਿਓ ਕਿ ਸਵੈਚਾਲਿਤ ਅਨੁਵਾਦਾਂ ਵਿੱਚ ਗਲਤੀਆਂ ਜਾਂ ਅਸੁਚਤਤਾਵਾਂ ਹੋ ਸਕਦੀਆਂ ਹਨ। ਇਸ ਦੀ ਮੂਲ ਭਾਸ਼ਾ ਵਿੱਚ ਮੌਜੂਦ ਮੂਲ ਦਸਤਾਵੇਜ਼ ਨੂੰ ਅਧਿਕਾਰਤ ਸਰੋਤ ਮੰਨਿਆ ਜਾਣਾ ਚਾਹੀਦਾ ਹੈ। ਮਹੱਤਵਪੂਰਨ ਜਾਣਕਾਰੀ ਲਈ, ਪੇਸ਼ੇਵਰ ਮਨੁੱਖੀ ਅਨੁਵਾਦ ਦੀ ਸਿਫਾਰਸ਼ ਕੀਤੀ ਜਾਂਦੀ ਹੈ। ਇਸ ਅਨੁਵਾਦ ਦੀ ਵਰਤੋਂ ਤੋਂ ਪੈਦਾ ਹੋਣ ਵਾਲੇ ਕਿਸੇ ਵੀ ਗਲਤਫਹਿਮੀ ਜਾਂ ਗਲਤ ਵਿਆਖਿਆ ਲਈ ਅਸੀਂ ਜ਼ਿੰਮੇਵਾਰ ਨਹੀਂ ਹਾਂ।\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0cb620c6d4b9f7a635928804c26cf22403d89d98d79684e4529119355ee6d5a5"
  },
  "kernelspec": {
   "display_name": "py37_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "coopTranslator": {
   "original_hash": "f50b026abce5cf36783a560ea72cb9b1",
   "translation_date": "2025-08-28T09:49:23+00:00",
   "source_file": "lessons/5-NLP/14-Embeddings/EmbeddingsPyTorch.ipynb",
   "language_code": "pa"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}