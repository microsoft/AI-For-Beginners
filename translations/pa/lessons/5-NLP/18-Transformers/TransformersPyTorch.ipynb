{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ਧਿਆਨ ਮਕੈਨਿਜ਼ਮ ਅਤੇ ਟ੍ਰਾਂਸਫਾਰਮਰ\n",
    "\n",
    "ਰੀਕਰਨਟ ਨੈਟਵਰਕਸ ਦਾ ਇੱਕ ਵੱਡਾ ਨੁਕਸਾਨ ਇਹ ਹੈ ਕਿ ਕ੍ਰਮ ਵਿੱਚ ਸਾਰੇ ਸ਼ਬਦ ਨਤੀਜੇ 'ਤੇ ਇੱਕੋ ਜਿਹਾ ਪ੍ਰਭਾਵ ਪਾਉਂਦੇ ਹਨ। ਇਸ ਕਾਰਨ ਸੈਕਵੈਂਸ-ਟੂ-ਸੈਕਵੈਂਸ ਕੰਮਾਂ ਲਈ ਸਧਾਰਨ LSTM ਐਨਕੋਡਰ-ਡਿਕੋਡਰ ਮਾਡਲਾਂ ਨਾਲ ਸਬ-ਆਪਟੀਮਲ ਪ੍ਰਦਰਸ਼ਨ ਹੁੰਦਾ ਹੈ, ਜਿਵੇਂ ਕਿ Named Entity Recognition ਅਤੇ Machine Translation। ਹਕੀਕਤ ਵਿੱਚ, ਇਨਪੁਟ ਕ੍ਰਮ ਵਿੱਚ ਕੁਝ ਵਿਸ਼ੇਸ਼ ਸ਼ਬਦ ਅਕਸਰ ਕ੍ਰਮਿਕ ਨਤੀਜਿਆਂ 'ਤੇ ਹੋਰਾਂ ਨਾਲੋਂ ਵੱਧ ਪ੍ਰਭਾਵ ਪਾਉਂਦੇ ਹਨ।\n",
    "\n",
    "ਸੈਕਵੈਂਸ-ਟੂ-ਸੈਕਵੈਂਸ ਮਾਡਲ ਨੂੰ ਧਿਆਨ ਵਿੱਚ ਰੱਖੋ, ਜਿਵੇਂ ਕਿ ਮਸ਼ੀਨ ਅਨੁਵਾਦ। ਇਹ ਦੋ ਰੀਕਰਨਟ ਨੈਟਵਰਕਸ ਦੁਆਰਾ ਲਾਗੂ ਕੀਤਾ ਜਾਂਦਾ ਹੈ, ਜਿੱਥੇ ਇੱਕ ਨੈਟਵਰਕ (**ਐਨਕੋਡਰ**) ਇਨਪੁਟ ਕ੍ਰਮ ਨੂੰ ਹਿਡਨ ਸਟੇਟ ਵਿੱਚ ਸੰਗ੍ਰਹਿਤ ਕਰੇਗਾ, ਅਤੇ ਦੂਜਾ (**ਡਿਕੋਡਰ**) ਇਸ ਹਿਡਨ ਸਟੇਟ ਨੂੰ ਅਨੁਵਾਦਿਤ ਨਤੀਜੇ ਵਿੱਚ ਬਦਲ ਦੇਵੇਗਾ। ਇਸ ਪਹੁੰਚ ਨਾਲ ਸਮੱਸਿਆ ਇਹ ਹੈ ਕਿ ਨੈਟਵਰਕ ਦੀ ਅੰਤਮ ਸਟੇਟ ਨੂੰ ਵਾਕ ਦੇ ਸ਼ੁਰੂਆਤ ਨੂੰ ਯਾਦ ਰੱਖਣ ਵਿੱਚ ਮੁਸ਼ਕਲ ਹੋਵੇਗੀ, ਜਿਸ ਕਾਰਨ ਲੰਬੇ ਵਾਕਾਂ 'ਤੇ ਮਾਡਲ ਦੀ ਗੁਣਵੱਤਾ ਘਟ ਜਾਂਦੀ ਹੈ।\n",
    "\n",
    "**ਧਿਆਨ ਮਕੈਨਿਜ਼ਮ** RNN ਦੇ ਹਰ ਆਉਟਪੁੱਟ ਅਨੁਮਾਨ 'ਤੇ ਹਰ ਇਨਪੁਟ ਵੇਕਟਰ ਦੇ ਸੰਦਰਭਕ ਪ੍ਰਭਾਵ ਨੂੰ ਵਜਨ ਦੇਣ ਦਾ ਇੱਕ ਢੰਗ ਪ੍ਰਦਾਨ ਕਰਦੇ ਹਨ। ਇਹ ਇਸ ਤਰੀਕੇ ਨਾਲ ਲਾਗੂ ਕੀਤਾ ਜਾਂਦਾ ਹੈ ਕਿ ਇਨਪੁਟ RNN ਦੇ ਮੱਧਵਰਤੀ ਸਟੇਟਸ ਅਤੇ ਆਉਟਪੁੱਟ RNN ਦੇ ਵਿਚਕਾਰ ਸ਼ਾਰਟਕਟ ਬਣਾਏ ਜਾਂਦੇ ਹਨ। ਇਸ ਤਰੀਕੇ ਨਾਲ, ਜਦੋਂ ਆਉਟਪੁੱਟ ਚਿੰਨ੍ਹ $y_t$ ਬਣਾਇਆ ਜਾ ਰਿਹਾ ਹੈ, ਤਾਂ ਅਸੀਂ ਸਾਰੇ ਇਨਪੁਟ ਹਿਡਨ ਸਟੇਟਸ $h_i$ ਨੂੰ ਵੱਖ-ਵੱਖ ਵਜਨ ਗੁਣਾਂਕ $\\alpha_{t,i}$ ਦੇ ਨਾਲ ਧਿਆਨ ਵਿੱਚ ਲਵਾਂਗੇ।\n",
    "\n",
    "![ਇੱਕ ਐਨਕੋਡਰ/ਡਿਕੋਡਰ ਮਾਡਲ ਨੂੰ additive attention layer ਨਾਲ ਦਿਖਾਉਂਦੀ ਚਿੱਤਰ](../../../../../translated_images/pa/encoder-decoder-attention.7a726296894fb567.webp)\n",
    "*[Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) ਵਿੱਚ additive attention ਮਕੈਨਿਜ਼ਮ ਵਾਲਾ ਐਨਕੋਡਰ-ਡਿਕੋਡਰ ਮਾਡਲ, [ਇਸ ਬਲੌਗ ਪੋਸਟ](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html) ਤੋਂ ਲਿਆ ਗਿਆ।]*\n",
    "\n",
    "Attention ਮੈਟ੍ਰਿਕਸ $\\{\\alpha_{i,j}\\}$ ਇਹ ਦਰਸਾਉਂਦੀ ਹੈ ਕਿ ਕਿਸ ਹੱਦ ਤੱਕ ਕੁਝ ਇਨਪੁਟ ਸ਼ਬਦ ਆਉਟਪੁੱਟ ਕ੍ਰਮ ਵਿੱਚ ਦਿੱਤੇ ਗਏ ਸ਼ਬਦ ਦੇ ਜਨਰੇਸ਼ਨ ਵਿੱਚ ਭੂਮਿਕਾ ਨਿਭਾਉਂਦੇ ਹਨ। ਹੇਠਾਂ ਇਸ ਮੈਟ੍ਰਿਕਸ ਦਾ ਇੱਕ ਉਦਾਹਰਨ ਦਿੱਤਾ ਗਿਆ ਹੈ:\n",
    "\n",
    "![Bahdanau - arviz.org ਤੋਂ ਲਿਆ ਗਿਆ RNNsearch-50 ਦੁਆਰਾ ਪਾਈ ਗਈ ਇੱਕ ਨਮੂਨਾ ਅਲਾਈਨਮੈਂਟ ਦਿਖਾਉਂਦੀ ਚਿੱਤਰ](../../../../../translated_images/pa/bahdanau-fig3.09ba2d37f202a6af.webp)\n",
    "\n",
    "*[Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) (Fig.3) ਤੋਂ ਲਿਆ ਗਿਆ ਚਿੱਤਰ]*\n",
    "\n",
    "ਧਿਆਨ ਮਕੈਨਿਜ਼ਮ ਪ੍ਰਾਕ੍ਰਿਤਿਕ ਭਾਸ਼ਾ ਪ੍ਰਕਿਰਿਆ ਵਿੱਚ ਮੌਜੂਦਾ ਜਾਂ ਲਗਭਗ ਮੌਜੂਦਾ state-of-the-art ਲਈ ਜ਼ਿੰਮੇਵਾਰ ਹਨ। ਹਾਲਾਂਕਿ, ਧਿਆਨ ਸ਼ਾਮਲ ਕਰਨ ਨਾਲ ਮਾਡਲ ਪੈਰਾਮੀਟਰਾਂ ਦੀ ਗਿਣਤੀ ਵਿੱਚ ਬਹੁਤ ਵਾਧਾ ਹੁੰਦਾ ਹੈ, ਜਿਸ ਕਾਰਨ RNNs ਨਾਲ ਸਕੇਲਿੰਗ ਸਮੱਸਿਆਵਾਂ ਆਉਂਦੀਆਂ ਹਨ। RNNs ਨੂੰ ਸਕੇਲ ਕਰਨ ਦੀ ਇੱਕ ਮੁੱਖ ਪਾਬੰਦੀ ਇਹ ਹੈ ਕਿ ਮਾਡਲਾਂ ਦੀ ਰੀਕਰਨਟ ਪ੍ਰਕਿਰਤੀ ਟ੍ਰੇਨਿੰਗ ਨੂੰ ਬੈਚ ਅਤੇ ਪੈਰਲਲਾਈਜ਼ ਕਰਨਾ ਮੁਸ਼ਕਲ ਬਣਾ ਦਿੰਦੀ ਹੈ। RNN ਵਿੱਚ ਕ੍ਰਮ ਦੇ ਹਰ ਤੱਤ ਨੂੰ ਕ੍ਰਮਵਾਰ ਤੌਰ 'ਤੇ ਪ੍ਰਕਿਰਿਆਵਾਨ ਕੀਤਾ ਜਾਣਾ ਚਾਹੀਦਾ ਹੈ, ਜਿਸਦਾ ਮਤਲਬ ਹੈ ਕਿ ਇਸਨੂੰ ਆਸਾਨੀ ਨਾਲ ਪੈਰਲਲਾਈਜ਼ ਨਹੀਂ ਕੀਤਾ ਜਾ ਸਕਦਾ।\n",
    "\n",
    "ਧਿਆਨ ਮਕੈਨਿਜ਼ਮ ਦੀ ਅਪਨਾਉਣ ਅਤੇ ਇਸ ਪਾਬੰਦੀ ਨੇ ਅੱਜ ਦੇ ਮੌਜੂਦਾ state-of-the-art ਟ੍ਰਾਂਸਫਾਰਮਰ ਮਾਡਲਾਂ ਦੀ ਰਚਨਾ ਲਈ ਰਾਹ ਹਮਵਾਰ ਕੀਤਾ, ਜਿਨ੍ਹਾਂ ਨੂੰ ਅਸੀਂ BERT ਤੋਂ OpenGPT3 ਤੱਕ ਵਰਤਦੇ ਹਾਂ।\n",
    "\n",
    "## ਟ੍ਰਾਂਸਫਾਰਮਰ ਮਾਡਲ\n",
    "\n",
    "ਪਿਛਲੇ ਅਨੁਮਾਨ ਦੇ ਸੰਦਰਭ ਨੂੰ ਅਗਲੇ ਮੁਲਾਂਕਣ ਕਦਮ ਵਿੱਚ ਅੱਗੇ ਭੇਜਣ ਦੀ ਬਜਾਏ, **ਟ੍ਰਾਂਸਫਾਰਮਰ ਮਾਡਲ** **ਪੋਜ਼ੀਸ਼ਨਲ ਐਨਕੋਡਿੰਗ** ਅਤੇ ਧਿਆਨ ਦੀ ਵਰਤੋਂ ਕਰਦੇ ਹਨ, ਤਾਂ ਜੋ ਦਿੱਤੇ ਗਏ ਇਨਪੁਟ ਦਾ ਸੰਦਰਭ ਇੱਕ ਪ੍ਰਦਾਨ ਕੀਤੇ ਗਏ ਟੈਕਸਟ ਵਿੰਡੋ ਵਿੱਚ ਕੈਪਚਰ ਕੀਤਾ ਜਾ ਸਕੇ। ਹੇਠਾਂ ਦਿੱਤੀ ਚਿੱਤਰ ਦਿਖਾਉਂਦੀ ਹੈ ਕਿ ਕਿਵੇਂ ਪੋਜ਼ੀਸ਼ਨਲ ਐਨਕੋਡਿੰਗ ਅਤੇ ਧਿਆਨ ਇੱਕ ਦਿੱਤੇ ਗਏ ਵਿੰਡੋ ਵਿੱਚ ਸੰਦਰਭ ਕੈਪਚਰ ਕਰ ਸਕਦੇ ਹਨ।\n",
    "\n",
    "![Animated GIF ਦਿਖਾਉਂਦੀ ਹੈ ਕਿ ਟ੍ਰਾਂਸਫਾਰਮਰ ਮਾਡਲਾਂ ਵਿੱਚ ਮੁਲਾਂਕਣ ਕਿਵੇਂ ਕੀਤੇ ਜਾਂਦੇ ਹਨ।](../../../../../lessons/5-NLP/18-Transformers/images/transformer-animated-explanation.gif)\n",
    "\n",
    "ਜਦੋਂ ਕਿ ਹਰ ਇਨਪੁਟ ਸਥਿਤੀ ਨੂੰ ਹਰ ਆਉਟਪੁੱਟ ਸਥਿਤੀ ਨਾਲ ਸਵਤੰਤਰ ਤੌਰ 'ਤੇ ਮੈਪ ਕੀਤਾ ਜਾਂਦਾ ਹੈ, ਟ੍ਰਾਂਸਫਾਰਮਰ RNNs ਨਾਲੋਂ ਵਧੀਆ ਪੈਰਲਲਾਈਜ਼ ਕਰ ਸਕਦੇ ਹਨ, ਜਿਸ ਨਾਲ ਬਹੁਤ ਵੱਡੇ ਅਤੇ ਹੋਰ ਪ੍ਰਗਟ ਭਾਸ਼ਾ ਮਾਡਲ ਬਣਾਉਣ ਦੀ ਸਹੂਲਤ ਮਿਲਦੀ ਹੈ। ਹਰ attention head ਨੂੰ ਸ਼ਬਦਾਂ ਦੇ ਵਿਚਕਾਰ ਵੱਖ-ਵੱਖ ਸੰਬੰਧ ਸਿੱਖਣ ਲਈ ਵਰਤਿਆ ਜਾ ਸਕਦਾ ਹੈ, ਜੋ ਡਾਊਨਸਟ੍ਰੀਮ ਪ੍ਰਾਕ੍ਰਿਤਿਕ ਭਾਸ਼ਾ ਪ੍ਰਕਿਰਿਆ ਕੰਮਾਂ ਨੂੰ ਸੁਧਾਰਦਾ ਹੈ।\n",
    "\n",
    "**BERT** (Bidirectional Encoder Representations from Transformers) ਇੱਕ ਬਹੁਤ ਵੱਡਾ ਮਲਟੀ ਲੇਅਰ ਟ੍ਰਾਂਸਫਾਰਮਰ ਨੈਟਵਰਕ ਹੈ, ਜਿਸ ਵਿੱਚ *BERT-base* ਲਈ 12 ਲੇਅਰ ਹਨ, ਅਤੇ *BERT-large* ਲਈ 24। ਮਾਡਲ ਨੂੰ ਪਹਿਲਾਂ ਵੱਡੇ ਟੈਕਸਟ ਡਾਟਾ (WikiPedia + ਕਿਤਾਬਾਂ) 'ਤੇ ਅਨਸੁਪਰਵਾਈਜ਼ਡ ਟ੍ਰੇਨਿੰਗ (ਵਾਕ ਵਿੱਚ ਮਾਸਕ ਕੀਤੇ ਸ਼ਬਦਾਂ ਦੀ ਪੇਸ਼ਕਸ਼) ਦੀ ਵਰਤੋਂ ਕਰਕੇ ਪ੍ਰੀ-ਟ੍ਰੇਨ ਕੀਤਾ ਜਾਂਦਾ ਹੈ। ਪ੍ਰੀ-ਟ੍ਰੇਨਿੰਗ ਦੌਰਾਨ ਮਾਡਲ ਭਾਸ਼ਾ ਦੀ ਮਹੱਤਵਪੂਰਨ ਸਮਝ ਹਾਸਲ ਕਰਦਾ ਹੈ, ਜਿਸਨੂੰ ਫਿਰ ਹੋਰ ਡਾਟਾਸੈਟਸ ਨਾਲ ਫਾਈਨ ਟਿਊਨਿੰਗ ਦੁਆਰਾ ਲਾਭਕਾਰੀ ਬਣਾਇਆ ਜਾ ਸਕਦਾ ਹੈ। ਇਸ ਪ੍ਰਕਿਰਿਆ ਨੂੰ **ਟ੍ਰਾਂਸਫਰ ਲਰਨਿੰਗ** ਕਿਹਾ ਜਾਂਦਾ ਹੈ।\n",
    "\n",
    "![http://jalammar.github.io/illustrated-bert/ ਤੋਂ ਚਿੱਤਰ](../../../../../translated_images/pa/jalammarBERT-language-modeling-masked-lm.34f113ea5fec4362.webp)\n",
    "\n",
    "ਟ੍ਰਾਂਸਫਾਰਮਰ ਆਰਕੀਟੈਕਚਰਾਂ ਦੇ ਕਈ ਰੂਪ ਹਨ, ਜਿਵੇਂ ਕਿ BERT, DistilBERT, BigBird, OpenGPT3 ਅਤੇ ਹੋਰ, ਜਿਨ੍ਹਾਂ ਨੂੰ ਫਾਈਨ ਟਿਊਨ ਕੀਤਾ ਜਾ ਸਕਦਾ ਹੈ। [HuggingFace ਪੈਕੇਜ](https://github.com/huggingface/) PyTorch ਨਾਲ ਇਨ੍ਹਾਂ ਆਰਕੀਟੈਕਚਰਾਂ ਨੂੰ ਟ੍ਰੇਨ ਕਰਨ ਲਈ ਰਿਪੋਜ਼ਟਰੀ ਪ੍ਰਦਾਨ ਕਰਦਾ ਹੈ।\n",
    "\n",
    "## ਟੈਕਸਟ ਕਲਾਸੀਫਿਕੇਸ਼ਨ ਲਈ BERT ਦੀ ਵਰਤੋਂ\n",
    "\n",
    "ਆਓ ਵੇਖੀਏ ਕਿ ਅਸੀਂ ਪ੍ਰੀ-ਟ੍ਰੇਨ ਕੀਤੇ BERT ਮਾਡਲ ਦੀ ਵਰਤੋਂ ਕਰਕੇ ਆਪਣੇ ਰਵਾਇਤੀ ਕੰਮ: ਸੈਕਵੈਂਸ ਕਲਾਸੀਫਿਕੇਸ਼ਨ ਨੂੰ ਕਿਵੇਂ ਹੱਲ ਕਰ ਸਕਦੇ ਹਾਂ। ਅਸੀਂ ਆਪਣੇ ਮੂਲ AG News ਡਾਟਾਸੈਟ ਨੂੰ ਕਲਾਸੀਫਾਈ ਕਰਾਂਗੇ।\n",
    "\n",
    "ਸਭ ਤੋਂ ਪਹਿਲਾਂ, ਆਓ HuggingFace ਲਾਇਬ੍ਰੇਰੀ ਅਤੇ ਆਪਣਾ ਡਾਟਾਸੈਟ ਲੋਡ ਕਰੀਏ:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Building vocab...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "from torchnlp import *\n",
    "import transformers\n",
    "train_dataset, test_dataset, classes, vocab = load_dataset()\n",
    "vocab_len = len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ਕਿਉਂਕਿ ਅਸੀਂ ਪਹਿਲਾਂ ਤੋਂ ਸਿਖਲਾਈ ਪ੍ਰਾਪਤ BERT ਮਾਡਲ ਦੀ ਵਰਤੋਂ ਕਰਨ ਜਾ ਰਹੇ ਹਾਂ, ਸਾਨੂੰ ਇੱਕ ਵਿਸ਼ੇਸ਼ ਟੋਕਨਾਈਜ਼ਰ ਦੀ ਲੋੜ ਹੋਵੇਗੀ। ਸਭ ਤੋਂ ਪਹਿਲਾਂ, ਅਸੀਂ ਪਹਿਲਾਂ ਤੋਂ ਸਿਖਲਾਈ ਪ੍ਰਾਪਤ BERT ਮਾਡਲ ਨਾਲ ਸੰਬੰਧਿਤ ਟੋਕਨਾਈਜ਼ਰ ਨੂੰ ਲੋਡ ਕਰਾਂਗੇ।\n",
    "\n",
    "HuggingFace ਲਾਇਬ੍ਰੇਰੀ ਵਿੱਚ ਪਹਿਲਾਂ ਤੋਂ ਸਿਖਲਾਈ ਪ੍ਰਾਪਤ ਮਾਡਲਾਂ ਦਾ ਇੱਕ ਰਿਪੋਜ਼ਿਟਰੀ ਹੈ, ਜਿਸਨੂੰ ਤੁਸੀਂ ਸਿਰਫ਼ ਉਨ੍ਹਾਂ ਦੇ ਨਾਮਾਂ ਨੂੰ `from_pretrained` ਫੰਕਸ਼ਨ ਦੇ ਤਰਕਾਂ ਵਜੋਂ ਦੱਸ ਕੇ ਵਰਤ ਸਕਦੇ ਹੋ। ਮਾਡਲ ਲਈ ਸਾਰੇ ਜ਼ਰੂਰੀ ਬਾਈਨਰੀ ਫਾਈਲਾਂ ਆਪਣੇ ਆਪ ਡਾਊਨਲੋਡ ਹੋ ਜਾਣਗੀਆਂ।\n",
    "\n",
    "ਹਾਲਾਂਕਿ, ਕੁਝ ਸਮਿਆਂ 'ਤੇ ਤੁਹਾਨੂੰ ਆਪਣੇ ਮਾਡਲਾਂ ਨੂੰ ਲੋਡ ਕਰਨ ਦੀ ਲੋੜ ਹੋ ਸਕਦੀ ਹੈ, ਇਸ ਸਥਿਤੀ ਵਿੱਚ ਤੁਸੀਂ ਉਹ ਡਾਇਰੈਕਟਰੀ ਦੱਸ ਸਕਦੇ ਹੋ ਜਿਸ ਵਿੱਚ ਸਾਰੇ ਸੰਬੰਧਿਤ ਫਾਈਲਾਂ ਸ਼ਾਮਲ ਹਨ, ਜਿਵੇਂ ਕਿ ਟੋਕਨਾਈਜ਼ਰ ਲਈ ਪੈਰਾਮੀਟਰ, ਮਾਡਲ ਪੈਰਾਮੀਟਰਾਂ ਵਾਲੀ `config.json` ਫਾਈਲ, ਬਾਈਨਰੀ ਵਜ਼ਨ ਆਦਿ।\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load the model from Internet repository using model name. \n",
    "# Use this if you are running from your own copy of the notebooks\n",
    "bert_model = 'bert-base-uncased' \n",
    "\n",
    "# To load the model from the directory on disk. Use this for Microsoft Learn module, because we have\n",
    "# prepared all required files for you.\n",
    "bert_model = './bert'\n",
    "\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(bert_model)\n",
    "\n",
    "MAX_SEQ_LEN = 128\n",
    "PAD_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "UNK_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tokenizer` ਆਬਜੈਕਟ ਵਿੱਚ `encode` ਫੰਕਸ਼ਨ ਸ਼ਾਮਲ ਹੈ ਜੋ ਸਿੱਧੇ ਹੀ ਟੈਕਸਟ ਨੂੰ ਐਨਕੋਡ ਕਰਨ ਲਈ ਵਰਤਿਆ ਜਾ ਸਕਦਾ ਹੈ:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 1052, 22123, 2953, 2818, 2003, 1037, 2307, 7705, 2005, 17953, 2361, 102]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('PyTorch is a great framework for NLP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ਫਿਰ, ਆਓ ਅਸੀਂ ਇਟਰੇਟਰ ਬਣਾਈਏ ਜਿਨ੍ਹਾਂ ਨੂੰ ਅਸੀਂ ਟ੍ਰੇਨਿੰਗ ਦੌਰਾਨ ਡਾਟਾ ਤੱਕ ਪਹੁੰਚ ਕਰਨ ਲਈ ਵਰਤਾਂਗੇ। ਕਿਉਂਕਿ BERT ਆਪਣਾ ਖੁਦ ਦਾ ਐਨਕੋਡਿੰਗ ਫੰਕਸ਼ਨ ਵਰਤਦਾ ਹੈ, ਸਾਨੂੰ ਇੱਕ ਪੈਡਿੰਗ ਫੰਕਸ਼ਨ ਨੂੰ ਪਰਿਭਾਸ਼ਿਤ ਕਰਨ ਦੀ ਲੋੜ ਹੋਵੇਗੀ ਜੋ ਕਿ ਪਹਿਲਾਂ ਪਰਿਭਾਸ਼ਿਤ `padify` ਵਰਗਾ ਹੋਵੇ:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_bert(b):\n",
    "    # b is the list of tuples of length batch_size\n",
    "    #   - first element of a tuple = label, \n",
    "    #   - second = feature (text sequence)\n",
    "    # build vectorized sequence\n",
    "    v = [tokenizer.encode(x[1]) for x in b]\n",
    "    # compute max length of a sequence in this minibatch\n",
    "    l = max(map(len,v))\n",
    "    return ( # tuple of two tensors - labels and features\n",
    "        torch.LongTensor([t[0] for t in b]),\n",
    "        torch.stack([torch.nn.functional.pad(torch.tensor(t),(0,l-len(t)),mode='constant',value=0) for t in v])\n",
    "    )\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=8, collate_fn=pad_bert, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=8, collate_fn=pad_bert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ਸਾਡੇ ਮਾਮਲੇ ਵਿੱਚ, ਅਸੀਂ `bert-base-uncased` ਨਾਂ ਦੇ ਪ੍ਰੀ-ਟ੍ਰੇਨਡ BERT ਮਾਡਲ ਦੀ ਵਰਤੋਂ ਕਰ ਰਹੇ ਹਾਂ। ਆਓ `BertForSequenceClassfication` ਪੈਕੇਜ ਦੀ ਵਰਤੋਂ ਕਰਕੇ ਮਾਡਲ ਨੂੰ ਲੋਡ ਕਰੀਏ। ਇਹ ਯਕੀਨੀ ਬਣਾਉਂਦਾ ਹੈ ਕਿ ਸਾਡੇ ਮਾਡਲ ਵਿੱਚ ਪਹਿਲਾਂ ਹੀ ਵਰਗੀਕਰਨ ਲਈ ਲੋੜੀਂਦਾ ਆਰਕੀਟੈਕਚਰ ਮੌਜੂਦ ਹੈ, ਜਿਸ ਵਿੱਚ ਅੰਤਿਮ ਵਰਗੀਕਰਤਾ ਸ਼ਾਮਲ ਹੈ। ਤੁਹਾਨੂੰ ਇੱਕ ਚੇਤਾਵਨੀ ਸੁਨੇਹਾ ਵੇਖਣ ਨੂੰ ਮਿਲੇਗਾ ਜੋ ਦੱਸਦਾ ਹੈ ਕਿ ਅੰਤਿਮ ਵਰਗੀਕਰਤਾ ਦੇ ਵਜ਼ਨ ਸ਼ੁਰੂਆਤ ਨਹੀਂ ਕੀਤੇ ਗਏ ਹਨ, ਅਤੇ ਮਾਡਲ ਨੂੰ ਪ੍ਰੀ-ਟ੍ਰੇਨਿੰਗ ਦੀ ਲੋੜ ਹੋਵੇਗੀ - ਇਹ ਬਿਲਕੁਲ ਠੀਕ ਹੈ, ਕਿਉਂਕਿ ਅਸੀਂ ਸਹੀ ਇਹੀ ਕਰਨ ਜਾ ਰਹੇ ਹਾਂ!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./bert were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./bert and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = transformers.BertForSequenceClassification.from_pretrained(bert_model,num_labels=4).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ਹੁਣ ਅਸੀਂ ਟ੍ਰੇਨਿੰਗ ਸ਼ੁਰੂ ਕਰਨ ਲਈ ਤਿਆਰ ਹਾਂ! ਕਿਉਂਕਿ BERT ਪਹਿਲਾਂ ਹੀ ਪ੍ਰੀ-ਟ੍ਰੇਨ ਕੀਤਾ ਹੋਇਆ ਹੈ, ਅਸੀਂ ਸ਼ੁਰੂ ਵਿੱਚ ਇੱਕ ਛੋਟਾ ਲਰਨਿੰਗ ਰੇਟ ਰੱਖਣਾ ਚਾਹੁੰਦੇ ਹਾਂ ਤਾਂ ਜੋ ਸ਼ੁਰੂਆਤੀ ਵਜ਼ਨਾਂ ਨੂੰ ਖਰਾਬ ਨਾ ਕੀਤਾ ਜਾਵੇ।\n",
    "\n",
    "ਸਾਰਾ ਮੁਸ਼ਕਲ ਕੰਮ `BertForSequenceClassification` ਮਾਡਲ ਕਰਦਾ ਹੈ। ਜਦੋਂ ਅਸੀਂ ਮਾਡਲ ਨੂੰ ਟ੍ਰੇਨਿੰਗ ਡਾਟਾ 'ਤੇ ਕਾਲ ਕਰਦੇ ਹਾਂ, ਇਹ ਇਨਪੁਟ ਮਿਨੀਬੈਚ ਲਈ ਦੋਨੋਂ ਲਾਸ ਅਤੇ ਨੈਟਵਰਕ ਆਉਟਪੁਟ ਵਾਪਸ ਕਰਦਾ ਹੈ। ਅਸੀਂ ਪੈਰਾਮੀਟਰ ਅਪਟੀਮਾਈਜ਼ੇਸ਼ਨ ਲਈ ਲਾਸ ਦਾ ਇਸਤੇਮਾਲ ਕਰਦੇ ਹਾਂ (`loss.backward()` ਬੈਕਵਰਡ ਪਾਸ ਕਰਦਾ ਹੈ), ਅਤੇ ਟ੍ਰੇਨਿੰਗ ਐਕਯੂਰੇਸੀ ਦੀ ਗਣਨਾ ਕਰਨ ਲਈ `out` ਦਾ ਇਸਤੇਮਾਲ ਕਰਦੇ ਹਾਂ, ਜਿੱਥੇ ਪ੍ਰਾਪਤ ਲੇਬਲ `labs` (ਜੋ `argmax` ਦੀ ਵਰਤੋਂ ਕਰਕੇ ਗਣਨਾ ਕੀਤੇ ਜਾਂਦੇ ਹਨ) ਦੀ ਤੁਲਨਾ ਉਮੀਦ ਕੀਤੇ ਗਏ `labels` ਨਾਲ ਕੀਤੀ ਜਾਂਦੀ ਹੈ।\n",
    "\n",
    "ਪ੍ਰਕਿਰਿਆ ਨੂੰ ਨਿਯੰਤਰਿਤ ਕਰਨ ਲਈ, ਅਸੀਂ ਕਈ ਇਟਰੇਸ਼ਨਾਂ 'ਤੇ ਲਾਸ ਅਤੇ ਐਕਯੂਰੇਸੀ ਨੂੰ ਇਕੱਠਾ ਕਰਦੇ ਹਾਂ ਅਤੇ ਹਰ `report_freq` ਟ੍ਰੇਨਿੰਗ ਸਾਈਕਲਾਂ ਦੇ ਬਾਅਦ ਉਨ੍ਹਾਂ ਨੂੰ ਪ੍ਰਿੰਟ ਕਰਦੇ ਹਾਂ।\n",
    "\n",
    "ਇਹ ਟ੍ਰੇਨਿੰਗ ਸੰਭਵਤ: ਕਾਫ਼ੀ ਲੰਬਾ ਸਮਾਂ ਲਵੇਗੀ, ਇਸ ਲਈ ਅਸੀਂ ਇਟਰੇਸ਼ਨ ਦੀ ਗਿਣਤੀ ਨੂੰ ਸੀਮਿਤ ਕਰਦੇ ਹਾਂ।\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 1.1254194641113282, Accuracy = 0.585\n",
      "Loss = 0.6194715118408203, Accuracy = 0.83\n",
      "Loss = 0.46665248870849607, Accuracy = 0.8475\n",
      "Loss = 0.4309701919555664, Accuracy = 0.8575\n",
      "Loss = 0.35427074432373046, Accuracy = 0.8825\n",
      "Loss = 0.3306886291503906, Accuracy = 0.8975\n",
      "Loss = 0.30340143203735354, Accuracy = 0.8975\n",
      "Loss = 0.26139299392700194, Accuracy = 0.915\n",
      "Loss = 0.26708646774291994, Accuracy = 0.9225\n",
      "Loss = 0.3667240524291992, Accuracy = 0.8675\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
    "\n",
    "report_freq = 50\n",
    "iterations = 500 # make this larger to train for longer time!\n",
    "\n",
    "model.train()\n",
    "\n",
    "i,c = 0,0\n",
    "acc_loss = 0\n",
    "acc_acc = 0\n",
    "\n",
    "for labels,texts in train_loader:\n",
    "    labels = labels.to(device)-1 # get labels in the range 0-3         \n",
    "    texts = texts.to(device)\n",
    "    loss, out = model(texts, labels=labels)[:2]\n",
    "    labs = out.argmax(dim=1)\n",
    "    acc = torch.mean((labs==labels).type(torch.float32))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    acc_loss += loss\n",
    "    acc_acc += acc\n",
    "    i+=1\n",
    "    c+=1\n",
    "    if i%report_freq==0:\n",
    "        print(f\"Loss = {acc_loss.item()/c}, Accuracy = {acc_acc.item()/c}\")\n",
    "        c = 0\n",
    "        acc_loss = 0\n",
    "        acc_acc = 0\n",
    "    iterations-=1\n",
    "    if not iterations:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ਤੁਸੀਂ ਦੇਖ ਸਕਦੇ ਹੋ (ਖਾਸ ਕਰਕੇ ਜੇ ਤੁਸੀਂ ਇਟਰੇਸ਼ਨ ਦੀ ਗਿਣਤੀ ਵਧਾਉਂਦੇ ਹੋ ਅਤੇ ਕਾਫ਼ੀ ਸਮਾਂ ਤੱਕ ਇੰਤਜ਼ਾਰ ਕਰਦੇ ਹੋ) ਕਿ BERT ਕਲਾਸੀਫਿਕੇਸ਼ਨ ਸਾਨੂੰ ਕਾਫ਼ੀ ਵਧੀਆ ਸਹੀ ਨਤੀਜੇ ਦਿੰਦਾ ਹੈ! ਇਹ ਇਸ ਲਈ ਹੈ ਕਿਉਂਕਿ BERT ਪਹਿਲਾਂ ਹੀ ਭਾਸ਼ਾ ਦੀ ਬਣਤਰ ਨੂੰ ਕਾਫ਼ੀ ਚੰਗੀ ਤਰ੍ਹਾਂ ਸਮਝਦਾ ਹੈ, ਅਤੇ ਸਾਨੂੰ ਸਿਰਫ਼ ਅੰਤਿਮ ਕਲਾਸੀਫਾਇਰ ਨੂੰ ਫਾਈਨ-ਟਿਊਨ ਕਰਨ ਦੀ ਲੋੜ ਹੁੰਦੀ ਹੈ। ਹਾਲਾਂਕਿ, ਕਿਉਂਕਿ BERT ਇੱਕ ਵੱਡਾ ਮਾਡਲ ਹੈ, ਪੂਰਾ ਟ੍ਰੇਨਿੰਗ ਪ੍ਰਕਿਰਿਆ ਕਾਫ਼ੀ ਸਮਾਂ ਲੈਂਦੀ ਹੈ ਅਤੇ ਇਸ ਲਈ ਗੰਭੀਰ ਗਣਨਾਤਮਕ ਸ਼ਕਤੀ ਦੀ ਲੋੜ ਹੁੰਦੀ ਹੈ! (GPU, ਅਤੇ ਵਧੀਆ ਹੋਵੇ ਤਾਂ ਇੱਕ ਤੋਂ ਵੱਧ GPU)।\n",
    "\n",
    "> **Note:** ਸਾਡੇ ਉਦਾਹਰਨ ਵਿੱਚ, ਅਸੀਂ ਸਭ ਤੋਂ ਛੋਟੇ ਪ੍ਰੀ-ਟ੍ਰੇਨਡ BERT ਮਾਡਲਾਂ ਵਿੱਚੋਂ ਇੱਕ ਦੀ ਵਰਤੋਂ ਕਰ ਰਹੇ ਹਾਂ। ਵੱਡੇ ਮਾਡਲ ਹਨ ਜੋ ਸੰਭਾਵਤ ਤੌਰ 'ਤੇ ਹੋਰ ਵਧੀਆ ਨਤੀਜੇ ਦੇ ਸਕਦੇ ਹਨ।\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ਮਾਡਲ ਦੇ ਪ੍ਰਦਰਸ਼ਨ ਦਾ ਮੁਲਾਂਕਣ ਕਰਨਾ\n",
    "\n",
    "ਹੁਣ ਅਸੀਂ ਟੈਸਟ ਡੇਟਾਸੈੱਟ 'ਤੇ ਆਪਣੇ ਮਾਡਲ ਦੇ ਪ੍ਰਦਰਸ਼ਨ ਦਾ ਮੁਲਾਂਕਣ ਕਰ ਸਕਦੇ ਹਾਂ। ਮੁਲਾਂਕਣ ਲੂਪ ਤਕਰੀਬਨ ਟ੍ਰੇਨਿੰਗ ਲੂਪ ਵਰਗਾ ਹੀ ਹੁੰਦਾ ਹੈ, ਪਰ ਸਾਨੂੰ ਇਹ ਨਹੀਂ ਭੁੱਲਣਾ ਚਾਹੀਦਾ ਕਿ `model.eval()` ਕਾਲ ਕਰਕੇ ਮਾਡਲ ਨੂੰ ਮੁਲਾਂਕਣ ਮੋਡ ਵਿੱਚ ਸਵਿੱਚ ਕਰਨਾ ਲਾਜ਼ਮੀ ਹੈ।\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy: 0.9047029702970297\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "iterations = 100\n",
    "acc = 0\n",
    "i = 0\n",
    "for labels,texts in test_loader:\n",
    "    labels = labels.to(device)-1      \n",
    "    texts = texts.to(device)\n",
    "    _, out = model(texts, labels=labels)[:2]\n",
    "    labs = out.argmax(dim=1)\n",
    "    acc += torch.mean((labs==labels).type(torch.float32))\n",
    "    i+=1\n",
    "    if i>iterations: break\n",
    "        \n",
    "print(f\"Final accuracy: {acc.item()/i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ਮੁੱਖ ਗੱਲ\n",
    "\n",
    "ਇਸ ਯੂਨਿਟ ਵਿੱਚ, ਅਸੀਂ ਵੇਖਿਆ ਕਿ **transformers** ਲਾਇਬ੍ਰੇਰੀ ਤੋਂ ਪਹਿਲਾਂ ਤੋਂ ਪ੍ਰਸ਼ਿਕਸ਼ਿਤ ਭਾਸ਼ਾ ਮਾਡਲ ਲੈਣਾ ਅਤੇ ਇਸਨੂੰ ਸਾਡੇ ਟੈਕਸਟ ਵਰਗੀਕਰਨ ਦੇ ਕੰਮ ਲਈ ਅਨੁਕੂਲਿਤ ਕਰਨਾ ਕਿੰਨਾ ਆਸਾਨ ਹੈ। ਇਸੇ ਤਰ੍ਹਾਂ, BERT ਮਾਡਲ ਨੂੰ ਇਨਟਿਟੀ ਐਕਸਟ੍ਰੈਕਸ਼ਨ, ਪ੍ਰਸ਼ਨ ਉੱਤਰ ਦੇਣ, ਅਤੇ ਹੋਰ NLP ਕੰਮਾਂ ਲਈ ਵਰਤਿਆ ਜਾ ਸਕਦਾ ਹੈ।\n",
    "\n",
    "ਟ੍ਰਾਂਸਫਾਰਮਰ ਮਾਡਲ NLP ਵਿੱਚ ਮੌਜੂਦਾ ਸਥਿਤੀ ਦੇ ਸਭ ਤੋਂ ਅੱਗੇ ਹਨ, ਅਤੇ ਜ਼ਿਆਦਾਤਰ ਮਾਮਲਿਆਂ ਵਿੱਚ ਇਹ ਪਹਿਲਾ ਹੱਲ ਹੋਣਾ ਚਾਹੀਦਾ ਹੈ ਜਿਸ ਨਾਲ ਤੁਸੀਂ ਕਸਟਮ NLP ਹੱਲਾਂ ਨੂੰ ਲਾਗੂ ਕਰਦੇ ਸਮੇਂ ਪ੍ਰਯੋਗ ਕਰਨਾ ਸ਼ੁਰੂ ਕਰਦੇ ਹੋ। ਹਾਲਾਂਕਿ, ਜੇ ਤੁਸੀਂ ਉੱਚ-ਪੱਧਰੀ ਨਿਊਰਲ ਮਾਡਲ ਬਣਾਉਣਾ ਚਾਹੁੰਦੇ ਹੋ, ਤਾਂ ਇਸ ਮੌਡਿਊਲ ਵਿੱਚ ਚਰਚਾ ਕੀਤੇ ਗਏ ਰਿਕਰੰਟ ਨਿਊਰਲ ਨੈੱਟਵਰਕ ਦੇ ਮੁੱਢਲੇ ਸਿਧਾਂਤਾਂ ਨੂੰ ਸਮਝਣਾ ਬਹੁਤ ਜ਼ਰੂਰੀ ਹੈ।\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**ਅਸਵੀਕਰਤੀ**:  \nਇਹ ਦਸਤਾਵੇਜ਼ AI ਅਨੁਵਾਦ ਸੇਵਾ [Co-op Translator](https://github.com/Azure/co-op-translator) ਦੀ ਵਰਤੋਂ ਕਰਕੇ ਅਨੁਵਾਦ ਕੀਤਾ ਗਿਆ ਹੈ। ਜਦੋਂ ਕਿ ਅਸੀਂ ਸਹੀ ਹੋਣ ਦਾ ਯਤਨ ਕਰਦੇ ਹਾਂ, ਕਿਰਪਾ ਕਰਕੇ ਧਿਆਨ ਦਿਓ ਕਿ ਸਵੈਚਾਲਿਤ ਅਨੁਵਾਦਾਂ ਵਿੱਚ ਗਲਤੀਆਂ ਜਾਂ ਅਸੁੱਤੀਆਂ ਹੋ ਸਕਦੀਆਂ ਹਨ। ਇਸ ਦੀ ਮੂਲ ਭਾਸ਼ਾ ਵਿੱਚ ਮੌਜੂਦ ਮੂਲ ਦਸਤਾਵੇਜ਼ ਨੂੰ ਪ੍ਰਮਾਣਿਕ ਸਰੋਤ ਮੰਨਿਆ ਜਾਣਾ ਚਾਹੀਦਾ ਹੈ। ਮਹੱਤਵਪੂਰਨ ਜਾਣਕਾਰੀ ਲਈ, ਪੇਸ਼ੇਵਰ ਮਨੁੱਖੀ ਅਨੁਵਾਦ ਦੀ ਸਿਫਾਰਸ਼ ਕੀਤੀ ਜਾਂਦੀ ਹੈ। ਇਸ ਅਨੁਵਾਦ ਦੇ ਪ੍ਰਯੋਗ ਤੋਂ ਪੈਦਾ ਹੋਣ ਵਾਲੇ ਕਿਸੇ ਵੀ ਗਲਤਫਹਮੀ ਜਾਂ ਗਲਤ ਵਿਆਖਿਆ ਲਈ ਅਸੀਂ ਜ਼ਿੰਮੇਵਾਰ ਨਹੀਂ ਹਾਂ।  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37_pytorch",
   "language": "python",
   "name": "conda-env-py37_pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "coopTranslator": {
   "original_hash": "753865967678a92dbce7d7efbd36d980",
   "translation_date": "2025-08-28T09:24:56+00:00",
   "source_file": "lessons/5-NLP/18-Transformers/TransformersPyTorch.ipynb",
   "language_code": "pa"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}