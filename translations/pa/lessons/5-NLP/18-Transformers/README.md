# ‡®ß‡®ø‡®Ü‡®® ‡®Æ‡®ï‡©à‡®®‡®ø‡®ú‡®º‡®Æ ‡®Ö‡®§‡©á ‡®ü‡©ç‡®∞‡®æ‡®Ç‡®∏‡®´‡®æ‡®∞‡®Æ‡®∞

## [‡®™‡©ç‡®∞‡©Ä-‡®≤‡©à‡®ï‡®ö‡®∞ ‡®ï‡®µ‡®ø‡®ú‡®º](https://ff-quizzes.netlify.app/en/ai/quiz/35)

NLP ‡®ñ‡©á‡®§‡®∞ ‡®µ‡®ø‡©±‡®ö ‡®∏‡®≠ ‡®§‡©ã‡®Ç ‡®Æ‡®π‡©±‡®§‡®µ‡®™‡©Ç‡®∞‡®® ‡®∏‡®Æ‡©±‡®∏‡®ø‡®Ü‡®µ‡®æ‡®Ç ‡®µ‡®ø‡©±‡®ö‡©ã‡®Ç ‡®á‡©±‡®ï ‡®π‡©à **‡®Æ‡®∏‡®º‡©Ä‡®® ‡®Ö‡®®‡©Å‡®µ‡®æ‡®¶**, ‡®ú‡©ã Google Translate ‡®µ‡®∞‡®ó‡©á ‡®ü‡©Ç‡®≤‡®æ‡®Ç ‡®¶‡©á ‡®Ö‡®ß‡®æ‡®∞ ‡®µ‡®ø‡©±‡®ö ‡®á‡©±‡®ï ‡®ú‡®∞‡©Ç‡®∞‡©Ä ‡®ï‡©∞‡®Æ ‡®π‡©à‡•§ ‡®á‡®∏ ‡®≠‡®æ‡®ó ‡®µ‡®ø‡©±‡®ö, ‡®Ö‡®∏‡©Ä‡®Ç ‡®Æ‡®∏‡®º‡©Ä‡®® ‡®Ö‡®®‡©Å‡®µ‡®æ‡®¶ '‡®§‡©á ‡®ß‡®ø‡®Ü‡®® ‡®¶‡©á‡®µ‡®æ‡®Ç‡®ó‡©á, ‡®ú‡®æ‡®Ç, ‡®µ‡®ß‡©á‡®∞‡©á ‡®Ü‡®Æ ‡®§‡©å‡®∞ '‡®§‡©á, ‡®ï‡®ø‡®∏‡©á ‡®µ‡©Ä *‡®∏‡©Ä‡®ï‡®µ‡©à‡®Ç‡®∏-‡®ü‡©Ç-‡®∏‡©Ä‡®ï‡®µ‡©à‡®Ç‡®∏* ‡®ï‡©∞‡®Æ (‡®ú‡®ø‡®∏‡®®‡©Ç‡©∞ **‡®µ‡®æ‡®ï ‡®∏‡©∞‡®ö‡®æ‡®∞** ‡®µ‡©Ä ‡®ï‡®ø‡®π‡®æ ‡®ú‡®æ‡®Ç‡®¶‡®æ ‡®π‡©à) '‡®§‡©á‡•§

RNNs ‡®®‡®æ‡®≤, ‡®∏‡©Ä‡®ï‡®µ‡©à‡®Ç‡®∏-‡®ü‡©Ç-‡®∏‡©Ä‡®ï‡®µ‡©à‡®Ç‡®∏ ‡®¶‡©ã ‡®∞‡®ø‡®ï‡®∞‡©∞‡®ü ‡®®‡©à‡®ü‡®µ‡®∞‡®ï‡®æ‡®Ç ‡®¶‡©Å‡®Ü‡®∞‡®æ ‡®≤‡®æ‡®ó‡©Ç ‡®ï‡©Ä‡®§‡®æ ‡®ú‡®æ‡®Ç‡®¶‡®æ ‡®π‡©à, ‡®ú‡®ø‡©±‡®•‡©á ‡®á‡©±‡®ï ‡®®‡©à‡®ü‡®µ‡®∞‡®ï, **‡®ê‡®®‡®ï‡©ã‡®°‡®∞**, ‡®á‡©±‡®ï ‡®á‡®®‡®™‡©Å‡®ü ‡®∏‡©Ä‡®ï‡®µ‡©à‡®Ç‡®∏ ‡®®‡©Ç‡©∞ ‡®á‡©±‡®ï ‡®π‡®ø‡®°‡®® ‡®∏‡®ü‡©á‡®ü ‡®µ‡®ø‡©±‡®ö ‡®∏‡©∞‡®ó‡©ç‡®∞‡®π‡®ø‡®§ ‡®ï‡®∞‡®¶‡®æ ‡®π‡©à, ‡®ú‡®¶‡®ï‡®ø ‡®¶‡©Ç‡®ú‡®æ ‡®®‡©à‡®ü‡®µ‡®∞‡®ï, **‡®°‡®ø‡®ï‡©ã‡®°‡®∞**, ‡®á‡®∏ ‡®π‡®ø‡®°‡®® ‡®∏‡®ü‡©á‡®ü ‡®®‡©Ç‡©∞ ‡®á‡©±‡®ï ‡®Ö‡®®‡©Å‡®µ‡®æ‡®¶‡©Ä ‡®®‡®§‡©Ä‡®ú‡©á ‡®µ‡®ø‡©±‡®ö ‡®ñ‡©ã‡®≤‡©ç‡®π‡®¶‡®æ ‡®π‡©à‡•§ ‡®á‡®∏ ‡®™‡®π‡©Å‡©∞‡®ö ‡®®‡®æ‡®≤ ‡®ï‡©Å‡®ù ‡®∏‡®Æ‡©±‡®∏‡®ø‡®Ü‡®µ‡®æ‡®Ç ‡®π‡®®:

* ‡®ê‡®®‡®ï‡©ã‡®°‡®∞ ‡®®‡©à‡®ü‡®µ‡®∞‡®ï ‡®¶‡©Ä ‡®Ü‡®ñ‡®∞‡©Ä ‡®∏‡®ü‡©á‡®ü ‡®®‡©Ç‡©∞ ‡®µ‡®æ‡®ï ‡®¶‡©á ‡®∏‡®º‡©Å‡®∞‡©Ç ‡®®‡©Ç‡©∞ ‡®Ø‡®æ‡®¶ ‡®∞‡©±‡®ñ‡®£ ‡®µ‡®ø‡©±‡®ö ‡®Æ‡©Å‡®∏‡®º‡®ï‡®≤ ‡®π‡©Å‡©∞‡®¶‡©Ä ‡®π‡©à, ‡®ú‡®ø‡®∏ ‡®ï‡®æ‡®∞‡®® ‡®≤‡©∞‡®¨‡©á ‡®µ‡®æ‡®ï‡®æ‡®Ç ‡®≤‡®à ‡®Æ‡®æ‡®°‡®≤ ‡®¶‡©Ä ‡®ó‡©Å‡®£‡®µ‡©±‡®§‡®æ ‡®ò‡®ü ‡®ú‡®æ‡®Ç‡®¶‡©Ä ‡®π‡©à‡•§
* ‡®á‡©±‡®ï ‡®∏‡©Ä‡®ï‡®µ‡©à‡®Ç‡®∏ ‡®µ‡®ø‡©±‡®ö ‡®∏‡®æ‡®∞‡©á ‡®∏‡®º‡®¨‡®¶‡®æ‡®Ç ‡®¶‡®æ ‡®®‡®§‡©Ä‡®ú‡©á '‡®§‡©á ‡®á‡©±‡®ï‡©ã ‡®ú‡®ø‡®π‡®æ ‡®™‡©ç‡®∞‡®≠‡®æ‡®µ ‡®π‡©Å‡©∞‡®¶‡®æ ‡®π‡©à‡•§ ‡®π‡®ï‡©Ä‡®ï‡®§ ‡®µ‡®ø‡©±‡®ö, ‡®π‡®æ‡®≤‡®æ‡®Ç‡®ï‡®ø, ‡®á‡®®‡®™‡©Å‡®ü ‡®∏‡©Ä‡®ï‡®µ‡©à‡®Ç‡®∏ ‡®µ‡®ø‡©±‡®ö ‡®ï‡©Å‡®ù ‡®µ‡®ø‡®∂‡©á‡®∏‡®º ‡®∏‡®º‡®¨‡®¶‡®æ‡®Ç ‡®¶‡®æ ‡®ï‡®à ‡®µ‡®æ‡®∞ ‡®ï‡©ç‡®∞‡®Æ‡®µ‡®æ‡®∞ ‡®®‡®§‡©Ä‡®ú‡®ø‡®Ü‡®Ç '‡®§‡©á ‡®π‡©ã‡®∞‡®æ‡®Ç ‡®®‡®æ‡®≤‡©ã‡®Ç ‡®µ‡©±‡®ß ‡®™‡©ç‡®∞‡®≠‡®æ‡®µ ‡®π‡©Å‡©∞‡®¶‡®æ ‡®π‡©à‡•§

**‡®ß‡®ø‡®Ü‡®® ‡®Æ‡®ï‡©à‡®®‡®ø‡®ú‡®º‡®Æ** RNN ‡®¶‡©á ‡®π‡®∞ ‡®Ü‡®â‡®ü‡®™‡©Å‡©±‡®ü ‡®Ö‡®®‡©Å‡®Æ‡®æ‡®® '‡®§‡©á ‡®π‡®∞ ‡®á‡®®‡®™‡©Å‡®ü ‡®µ‡©á‡®ï‡®ü‡®∞ ‡®¶‡©á ‡®∏‡©∞‡®¶‡®∞‡®≠‡®ï ‡®™‡©ç‡®∞‡®≠‡®æ‡®µ ‡®®‡©Ç‡©∞ ‡®µ‡®ú‡®® ‡®¶‡©á‡®£ ‡®¶‡®æ ‡®á‡©±‡®ï ‡®¢‡©∞‡®ó ‡®™‡©ç‡®∞‡®¶‡®æ‡®® ‡®ï‡®∞‡®¶‡©á ‡®π‡®®‡•§ ‡®á‡®π ‡®á‡®∏ ‡®§‡®∞‡©Ä‡®ï‡©á ‡®®‡®æ‡®≤ ‡®≤‡®æ‡®ó‡©Ç ‡®ï‡©Ä‡®§‡®æ ‡®ú‡®æ‡®Ç‡®¶‡®æ ‡®π‡©à ‡®ï‡®ø ‡®á‡®®‡®™‡©Å‡®ü RNN ‡®¶‡©á ‡®Æ‡©±‡®ß‡®µ‡®∞‡®§‡©Ä ‡®∏‡®ü‡©á‡®ü‡®æ‡®Ç ‡®Ö‡®§‡©á ‡®Ü‡®â‡®ü‡®™‡©Å‡©±‡®ü RNN ‡®¶‡©á ‡®µ‡®ø‡®ö‡®ï‡®æ‡®∞ ‡®∏‡®º‡®æ‡®∞‡®ü‡®ï‡®ü ‡®¨‡®£‡®æ‡®è ‡®ú‡®æ‡®Ç‡®¶‡©á ‡®π‡®®‡•§ ‡®á‡®∏ ‡®§‡®∞‡©Ä‡®ï‡©á ‡®®‡®æ‡®≤, ‡®ú‡®¶‡©ã‡®Ç ‡®Ü‡®â‡®ü‡®™‡©Å‡©±‡®ü ‡®ö‡®ø‡©∞‡®®‡©ç‡®π y<sub>t</sub> ‡®¨‡®£‡®æ‡®á‡®Ü ‡®ú‡®æ‡®Ç‡®¶‡®æ ‡®π‡©à, ‡®Ö‡®∏‡©Ä‡®Ç ‡®∏‡®æ‡®∞‡©á ‡®á‡®®‡®™‡©Å‡®ü ‡®π‡®ø‡®°‡®® ‡®∏‡®ü‡©á‡®ü h<sub>i</sub> ‡®®‡©Ç‡©∞ ‡®µ‡©±‡®ñ-‡®µ‡©±‡®ñ ‡®µ‡®ú‡®® ‡®ó‡©Å‡®£‡®æ‡®Ç‡®ï &alpha;<sub>t,i</sub> ‡®¶‡©á ‡®®‡®æ‡®≤ ‡®ß‡®ø‡®Ü‡®® ‡®µ‡®ø‡©±‡®ö ‡®≤‡®µ‡®æ‡®Ç‡®ó‡©á‡•§

![‡®á‡©±‡®ï ‡®ê‡®®‡®ï‡©ã‡®°‡®∞/‡®°‡®ø‡®ï‡©ã‡®°‡®∞ ‡®Æ‡®æ‡®°‡®≤ ‡®®‡©Ç‡©∞ additive attention layer ‡®®‡®æ‡®≤ ‡®¶‡®ø‡®ñ‡®æ‡®â‡®Ç‡®¶‡©Ä ‡®ö‡®ø‡©±‡®§‡®∞](../../../../../translated_images/pa/encoder-decoder-attention.7a726296894fb567.webp)

> [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) ‡®µ‡®ø‡©±‡®ö additive attention ‡®Æ‡®ï‡©à‡®®‡®ø‡®ú‡®º‡®Æ ‡®®‡®æ‡®≤ ‡®ê‡®®‡®ï‡©ã‡®°‡®∞-‡®°‡®ø‡®ï‡©ã‡®°‡®∞ ‡®Æ‡®æ‡®°‡®≤, [‡®á‡®∏ ‡®¨‡®≤‡©å‡®ó ‡®™‡©ã‡®∏‡®ü](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html) ‡®§‡©ã‡®Ç ‡®≤‡®ø‡®Ü ‡®ó‡®ø‡®Ü‡•§

Attention ‡®Æ‡©à‡®ü‡©ç‡®∞‡®ø‡®ï‡®∏ {&alpha;<sub>i,j</sub>} ‡®á‡®π ‡®¶‡®∞‡®∏‡®æ‡®â‡®Ç‡®¶‡©Ä ‡®π‡©à ‡®ï‡®ø ‡®ï‡©Å‡®ù ‡®á‡®®‡®™‡©Å‡®ü ‡®∏‡®º‡®¨‡®¶‡®æ‡®Ç ‡®¶‡®æ ‡®á‡©±‡®ï ‡®¶‡®ø‡©±‡®§‡©á ‡®ó‡®è ‡®Ü‡®â‡®ü‡®™‡©Å‡©±‡®ü ‡®∏‡©Ä‡®ï‡®µ‡©à‡®Ç‡®∏ ‡®µ‡®ø‡©±‡®ö ‡®∏‡®º‡®¨‡®¶ ‡®¨‡®£‡®æ‡®â‡®£ ‡®µ‡®ø‡©±‡®ö ‡®ï‡®ø‡©∞‡®®‡®æ ‡®Ø‡©ã‡®ó‡®¶‡®æ‡®® ‡®π‡©à‡•§ ‡®π‡©á‡®†‡®æ‡®Ç ‡®á‡®∏ ‡®Æ‡©à‡®ü‡©ç‡®∞‡®ø‡®ï‡®∏ ‡®¶‡®æ ‡®á‡©±‡®ï ‡®â‡®¶‡®æ‡®π‡®∞‡®® ‡®¶‡®ø‡©±‡®§‡®æ ‡®ó‡®ø‡®Ü ‡®π‡©à:

![Bahdanau - arviz.org ‡®§‡©ã‡®Ç ‡®≤‡®ø‡®Ü ‡®ó‡®ø‡®Ü RNNsearch-50 ‡®¶‡©Å‡®Ü‡®∞‡®æ ‡®™‡®æ‡®à ‡®ó‡®à ‡®á‡©±‡®ï ‡®®‡®Æ‡©Ç‡®®‡®æ ‡®Ö‡®≤‡®æ‡®à‡®®‡®Æ‡©à‡®Ç‡®ü ‡®¶‡®ø‡®ñ‡®æ‡®â‡®Ç‡®¶‡©Ä ‡®ö‡®ø‡©±‡®§‡®∞](../../../../../translated_images/pa/bahdanau-fig3.09ba2d37f202a6af.webp)

> [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) (Fig.3) ‡®§‡©ã‡®Ç ‡®ö‡®ø‡©±‡®§‡®∞

Attention ‡®Æ‡®ï‡©à‡®®‡®ø‡®ú‡®º‡®Æ NLP ‡®µ‡®ø‡©±‡®ö ‡®Æ‡©å‡®ú‡©Ç‡®¶‡®æ ‡®ú‡®æ‡®Ç ‡®≤‡®ó‡®≠‡®ó ‡®Æ‡©å‡®ú‡©Ç‡®¶‡®æ state-of-the-art ‡®≤‡®à ‡®ú‡®º‡®ø‡©∞‡®Æ‡©á‡®µ‡®æ‡®∞ ‡®π‡®®‡•§ Attention ‡®∏‡®º‡®æ‡®Æ‡®≤ ‡®ï‡®∞‡®® ‡®®‡®æ‡®≤ ‡®Æ‡®æ‡®°‡®≤ ‡®™‡©à‡®∞‡®æ‡®Æ‡©Ä‡®ü‡®∞‡®æ‡®Ç ‡®¶‡©Ä ‡®ó‡®ø‡®£‡®§‡©Ä ‡®µ‡®ø‡©±‡®ö ‡®ï‡®æ‡®´‡®º‡©Ä ‡®µ‡®æ‡®ß‡®æ ‡®π‡©Å‡©∞‡®¶‡®æ ‡®π‡©à, ‡®ú‡®ø‡®∏ ‡®ï‡®æ‡®∞‡®® RNNs ‡®®‡®æ‡®≤ ‡®∏‡®ï‡©á‡®≤‡®ø‡©∞‡®ó ‡®∏‡®Æ‡©±‡®∏‡®ø‡®Ü‡®µ‡®æ‡®Ç ‡®Ü‡®à‡®Ü‡®Ç‡•§ RNNs ‡®®‡©Ç‡©∞ ‡®∏‡®ï‡©á‡®≤ ‡®ï‡®∞‡®® ‡®¶‡©Ä ‡®á‡©±‡®ï ‡®Æ‡©Å‡©±‡®ñ ‡®™‡®æ‡®¨‡©∞‡®¶‡©Ä ‡®á‡®π ‡®π‡©à ‡®ï‡®ø ‡®Æ‡®æ‡®°‡®≤‡®æ‡®Ç ‡®¶‡©Ä ‡®∞‡®ø‡®ï‡®∞‡©∞‡®ü ‡®™‡©ç‡®∞‡®ï‡®ø‡®∞‡®ø‡®Ü ‡®ü‡©ç‡®∞‡©á‡®®‡®ø‡©∞‡®ó ‡®®‡©Ç‡©∞ ‡®¨‡©à‡®ö ‡®Ö‡®§‡©á ‡®™‡©à‡®∞‡®æ‡®≤‡®≤‡®æ‡®à‡®ú‡®º ‡®ï‡®∞‡®®‡®æ ‡®Æ‡©Å‡®∏‡®º‡®ï‡®≤ ‡®¨‡®£‡®æ ‡®¶‡®ø‡©∞‡®¶‡©Ä ‡®π‡©à‡•§ RNN ‡®µ‡®ø‡©±‡®ö ‡®á‡©±‡®ï ‡®∏‡©Ä‡®ï‡®µ‡©à‡®Ç‡®∏ ‡®¶‡©á ‡®π‡®∞ ‡®§‡©±‡®§ ‡®®‡©Ç‡©∞ ‡®ï‡©ç‡®∞‡®Æ‡®µ‡®æ‡®∞ ‡®™‡©ç‡®∞‡®ï‡®ø‡®∞‡®ø‡®Ü‡®µ‡®ß‡©Ä ‡®ï‡®∞‡®® ‡®¶‡©Ä ‡®≤‡©ã‡©ú ‡®π‡©Å‡©∞‡®¶‡©Ä ‡®π‡©à, ‡®ú‡®ø‡®∏‡®¶‡®æ ‡®Æ‡®§‡®≤‡®¨ ‡®π‡©à ‡®ï‡®ø ‡®á‡®∏‡®®‡©Ç‡©∞ ‡®Ü‡®∏‡®æ‡®®‡©Ä ‡®®‡®æ‡®≤ ‡®™‡©à‡®∞‡®æ‡®≤‡®≤‡®æ‡®à‡®ú‡®º ‡®®‡®π‡©Ä‡®Ç ‡®ï‡©Ä‡®§‡®æ ‡®ú‡®æ ‡®∏‡®ï‡®¶‡®æ‡•§

![Attention ‡®®‡®æ‡®≤ ‡®ê‡®®‡®ï‡©ã‡®°‡®∞ ‡®°‡®ø‡®ï‡©ã‡®°‡®∞](../../../../../lessons/5-NLP/18-Transformers/images/EncDecAttention.gif)

> [Google's Blog](https://research.googleblog.com/2016/09/a-neural-network-for-machine.html) ‡®§‡©ã‡®Ç ‡®ö‡®ø‡©±‡®§‡®∞

Attention ‡®Æ‡®ï‡©à‡®®‡®ø‡®ú‡®º‡®Æ ‡®¶‡©Ä ‡®Ö‡®™‡®£‡®æ‡®â‡®£‡©Ä ‡®Ö‡®§‡©á ‡®á‡®∏ ‡®™‡®æ‡®¨‡©∞‡®¶‡©Ä ‡®®‡©á ‡®Ö‡©±‡®ú ‡®¶‡©á ‡®Æ‡©å‡®ú‡©Ç‡®¶‡®æ state-of-the-art ‡®ü‡©ç‡®∞‡®æ‡®Ç‡®∏‡®´‡®æ‡®∞‡®Æ‡®∞ ‡®Æ‡®æ‡®°‡®≤‡®æ‡®Ç ‡®¶‡©Ä ‡®∞‡®ö‡®®‡®æ ‡®≤‡®à ‡®∞‡®æ‡®π ‡®π‡®Æ‡®µ‡®æ‡®∞ ‡®ï‡©Ä‡®§‡®æ, ‡®ú‡®ø‡®µ‡©á‡®Ç ‡®ï‡®ø BERT ‡®§‡©ã‡®Ç Open-GPT3‡•§

## ‡®ü‡©ç‡®∞‡®æ‡®Ç‡®∏‡®´‡®æ‡®∞‡®Æ‡®∞ ‡®Æ‡®æ‡®°‡®≤

‡®ü‡©ç‡®∞‡®æ‡®Ç‡®∏‡®´‡®æ‡®∞‡®Æ‡®∞‡®æ‡®Ç ‡®¶‡©á ‡®™‡®ø‡©±‡®õ‡©á ‡®á‡©±‡®ï ‡®Æ‡©Å‡©±‡®ñ ‡®µ‡®ø‡®ö‡®æ‡®∞ ‡®á‡®π ‡®π‡©à ‡®ï‡®ø RNNs ‡®¶‡©Ä ‡®ï‡©ç‡®∞‡®Æ‡®µ‡®æ‡®∞ ‡®™‡©ç‡®∞‡®ï‡®ø‡®∞‡®ø‡®Ü‡®µ‡®ß‡©Ä ‡®®‡©Ç‡©∞ ‡®ü‡®æ‡®≤‡®ø‡®Ü ‡®ú‡®æ‡®µ‡©á ‡®Ö‡®§‡©á ‡®á‡©±‡®ï ‡®Æ‡®æ‡®°‡®≤ ‡®¨‡®£‡®æ‡®á‡®Ü ‡®ú‡®æ‡®µ‡©á ‡®ú‡©ã ‡®ü‡©ç‡®∞‡©á‡®®‡®ø‡©∞‡®ó ‡®¶‡©å‡®∞‡®æ‡®® ‡®™‡©à‡®∞‡®æ‡®≤‡®≤‡®æ‡®à‡®ú‡®º ‡®ï‡©Ä‡®§‡®æ ‡®ú‡®æ ‡®∏‡®ï‡©á‡•§ ‡®á‡®π ‡®¶‡©ã ‡®µ‡®ø‡®ö‡®æ‡®∞‡®æ‡®Ç ‡®®‡©Ç‡©∞ ‡®≤‡®æ‡®ó‡©Ç ‡®ï‡®∞‡®ï‡©á ‡®π‡®æ‡®∏‡®≤ ‡®ï‡©Ä‡®§‡®æ ‡®ú‡®æ‡®Ç‡®¶‡®æ ‡®π‡©à:

* positional encoding
* RNNs (‡®ú‡®æ‡®Ç CNNs) ‡®¶‡©Ä ‡®¨‡®ú‡®æ‡®è ‡®™‡©à‡®ü‡®∞‡®® ‡®®‡©Ç‡©∞ ‡®ï‡©à‡®™‡®ö‡®∞ ‡®ï‡®∞‡®® ‡®≤‡®à self-attention ‡®Æ‡®ï‡©à‡®®‡®ø‡®ú‡®º‡®Æ ‡®¶‡©Ä ‡®µ‡®∞‡®§‡©ã‡®Ç (‡®á‡®∏ ‡®≤‡®à ‡®ü‡©ç‡®∞‡®æ‡®Ç‡®∏‡®´‡®æ‡®∞‡®Æ‡®∞‡®æ‡®Ç ‡®®‡©Ç‡©∞ ‡®™‡©á‡®∏‡®º ‡®ï‡®∞‡®® ‡®µ‡®æ‡®≤‡©á ‡®™‡©á‡®™‡®∞ ‡®¶‡®æ ‡®®‡®æ‡®Æ *[Attention is all you need](https://arxiv.org/abs/1706.03762)* ‡®π‡©à)

### Positional Encoding/Embedding

Positional encoding ‡®¶‡®æ ‡®µ‡®ø‡®ö‡®æ‡®∞ ‡®π‡©á‡®†‡®æ‡®Ç ‡®¶‡®ø‡©±‡®§‡®æ ‡®ó‡®ø‡®Ü ‡®π‡©à‡•§  
1. RNNs ‡®¶‡©Ä ‡®µ‡®∞‡®§‡©ã‡®Ç ‡®ï‡®∞‡®¶‡©á ‡®∏‡®Æ‡©á‡®Ç, ‡®ü‡©ã‡®ï‡®® ‡®¶‡®æ ‡®∏‡®¨‡©∞‡®ß‡®ø‡®§ ‡®∏‡®•‡®æ‡®® ‡®ï‡®¶‡®Æ‡®æ‡®Ç ‡®¶‡©Ä ‡®ó‡®ø‡®£‡®§‡©Ä ‡®¶‡©Å‡®Ü‡®∞‡®æ ‡®¶‡®∞‡®∏‡®æ‡®á‡®Ü ‡®ú‡®æ‡®Ç‡®¶‡®æ ‡®π‡©à, ‡®Ö‡®§‡©á ‡®á‡®∏ ‡®≤‡®à ‡®á‡®∏‡®®‡©Ç‡©∞ ‡®∏‡®™‡®∏‡®º‡®ü ‡®§‡©å‡®∞ '‡®§‡©á ‡®¶‡®∞‡®∏‡®æ‡®â‡®£ ‡®¶‡©Ä ‡®≤‡©ã‡©ú ‡®®‡®π‡©Ä‡®Ç ‡®π‡©Å‡©∞‡®¶‡©Ä‡•§  
2. ‡®π‡®æ‡®≤‡®æ‡®Ç‡®ï‡®ø, ‡®ú‡®¶‡©ã‡®Ç ‡®Ö‡®∏‡©Ä‡®Ç attention ‡®¶‡©Ä ‡®µ‡®∞‡®§‡©ã‡®Ç ‡®ï‡®∞‡®¶‡©á ‡®π‡®æ‡®Ç, ‡®§‡®æ‡®Ç ‡®∏‡®æ‡®®‡©Ç‡©∞ ‡®á‡©±‡®ï ‡®∏‡©Ä‡®ï‡®µ‡©à‡®Ç‡®∏ ‡®µ‡®ø‡©±‡®ö ‡®ü‡©ã‡®ï‡®® ‡®¶‡©á ‡®∏‡®¨‡©∞‡®ß‡®ø‡®§ ‡®∏‡®•‡®æ‡®®‡®æ‡®Ç ‡®®‡©Ç‡©∞ ‡®ú‡®æ‡®£‡®® ‡®¶‡©Ä ‡®≤‡©ã‡©ú ‡®π‡©Å‡©∞‡®¶‡©Ä ‡®π‡©à‡•§  
3. Positional encoding ‡®™‡©ç‡®∞‡®æ‡®™‡®§ ‡®ï‡®∞‡®® ‡®≤‡®à, ‡®Ö‡®∏‡©Ä‡®Ç ‡®Ü‡®™‡®£‡©á ‡®ü‡©ã‡®ï‡®® ‡®¶‡©á ‡®∏‡©Ä‡®ï‡®µ‡©à‡®Ç‡®∏ ‡®®‡©Ç‡©∞ ‡®∏‡©Ä‡®ï‡®µ‡©à‡®Ç‡®∏ ‡®µ‡®ø‡©±‡®ö ‡®ü‡©ã‡®ï‡®® ‡®∏‡®•‡®æ‡®®‡®æ‡®Ç ‡®¶‡©á ‡®∏‡©Ä‡®ï‡®µ‡©à‡®Ç‡®∏ (‡®ú‡®æ‡®Ç, 0,1, ...) ‡®®‡®æ‡®≤ ‡®µ‡®ß‡®æ‡®â‡®Ç‡®¶‡©á ‡®π‡®æ‡®Ç‡•§  
4. ‡®´‡®ø‡®∞ ‡®Ö‡®∏‡©Ä‡®Ç ‡®ü‡©ã‡®ï‡®® ‡®∏‡®•‡®æ‡®® ‡®®‡©Ç‡©∞ ‡®ü‡©ã‡®ï‡®® embedding ‡®µ‡©á‡®ï‡®ü‡®∞ ‡®®‡®æ‡®≤ ‡®Æ‡®ø‡®≤‡®æ‡®â‡®Ç‡®¶‡©á ‡®π‡®æ‡®Ç‡•§ ‡®∏‡®•‡®æ‡®® (integer) ‡®®‡©Ç‡©∞ ‡®á‡©±‡®ï ‡®µ‡©á‡®ï‡®ü‡®∞ ‡®µ‡®ø‡©±‡®ö ‡®¨‡®¶‡®≤‡®£ ‡®≤‡®à, ‡®Ö‡®∏‡©Ä‡®Ç ‡®µ‡©±‡®ñ-‡®µ‡©±‡®ñ ‡®™‡®π‡©Å‡©∞‡®ö‡®æ‡®Ç ‡®¶‡©Ä ‡®µ‡®∞‡®§‡©ã‡®Ç ‡®ï‡®∞ ‡®∏‡®ï‡®¶‡©á ‡®π‡®æ‡®Ç:

* Trainable embedding, ‡®ü‡©ã‡®ï‡®® embedding ‡®¶‡©á ‡®∏‡®Æ‡®æ‡®®‡•§ ‡®á‡®π ‡®™‡®π‡©Å‡©∞‡®ö ‡®Ö‡®∏‡©Ä‡®Ç ‡®á‡©±‡®•‡©á ‡®µ‡®ø‡®ö‡®æ‡®∞ ‡®ï‡®∞‡®¶‡©á ‡®π‡®æ‡®Ç‡•§ ‡®Ö‡®∏‡©Ä‡®Ç ‡®ü‡©ã‡®ï‡®® ‡®Ö‡®§‡©á ‡®â‡®®‡©ç‡®π‡®æ‡®Ç ‡®¶‡©á ‡®∏‡®•‡®æ‡®®‡®æ‡®Ç ‡®¶‡©ã‡®µ‡®æ‡®Ç '‡®§‡©á embedding layers ‡®≤‡®æ‡®ó‡©Ç ‡®ï‡®∞‡®¶‡©á ‡®π‡®æ‡®Ç, ‡®ú‡®ø‡®∏ ‡®®‡®æ‡®≤ ‡®á‡©±‡®ï‡©ã ‡®Æ‡®æ‡®™ ‡®¶‡©á embedding ‡®µ‡©á‡®ï‡®ü‡®∞ ‡®™‡©ç‡®∞‡®æ‡®™‡®§ ‡®π‡©Å‡©∞‡®¶‡©á ‡®π‡®®, ‡®ú‡®ø‡®®‡©ç‡®π‡®æ‡®Ç ‡®®‡©Ç‡©∞ ‡®Ö‡®∏‡©Ä‡®Ç ‡®´‡®ø‡®∞ ‡®á‡®ï‡©±‡®†‡©á ‡®ú‡©ã‡©ú‡®¶‡©á ‡®π‡®æ‡®Ç‡•§  
* Fixed position encoding function, ‡®ú‡®ø‡®µ‡©á‡®Ç ‡®ï‡®ø ‡®Æ‡©Ç‡®≤ ‡®™‡©á‡®™‡®∞ ‡®µ‡®ø‡©±‡®ö ‡®™‡©ç‡®∞‡®∏‡®§‡®æ‡®µ‡®ø‡®§ ‡®ï‡©Ä‡®§‡®æ ‡®ó‡®ø‡®Ü ‡®π‡©à‡•§  

<img src="../../../../../translated_images/pa/pos-embedding.e41ce9b6cf6078af.webp" width="50%"/>

> ‡®≤‡©á‡®ñ‡®ï ‡®¶‡©Å‡®Ü‡®∞‡®æ ‡®ö‡®ø‡©±‡®§‡®∞

‡®ú‡©ã ‡®®‡®§‡©Ä‡®ú‡®æ ‡®Ö‡®∏‡©Ä‡®Ç positional embedding ‡®®‡®æ‡®≤ ‡®™‡©ç‡®∞‡®æ‡®™‡®§ ‡®ï‡®∞‡®¶‡©á ‡®π‡®æ‡®Ç, ‡®â‡®π ‡®Æ‡©Ç‡®≤ ‡®ü‡©ã‡®ï‡®® ‡®Ö‡®§‡©á ‡®á‡®∏‡®¶‡®æ ‡®á‡©±‡®ï ‡®∏‡©Ä‡®ï‡®µ‡©à‡®Ç‡®∏ ‡®µ‡®ø‡©±‡®ö ‡®∏‡®•‡®æ‡®® ‡®¶‡©ã‡®µ‡®æ‡®Ç ‡®®‡©Ç‡©∞ embed ‡®ï‡®∞‡®¶‡®æ ‡®π‡©à‡•§

### Multi-Head Self-Attention

‡®Ö‡®ó‡®≤‡©á, ‡®∏‡®æ‡®®‡©Ç‡©∞ ‡®Ü‡®™‡®£‡©á ‡®∏‡©Ä‡®ï‡®µ‡©à‡®Ç‡®∏ ‡®µ‡®ø‡©±‡®ö ‡®ï‡©Å‡®ù ‡®™‡©à‡®ü‡®∞‡®® ‡®ï‡©à‡®™‡®ö‡®∞ ‡®ï‡®∞‡®® ‡®¶‡©Ä ‡®≤‡©ã‡©ú ‡®π‡©à‡•§ ‡®á‡®π ‡®ï‡®∞‡®® ‡®≤‡®à, ‡®ü‡©ç‡®∞‡®æ‡®Ç‡®∏‡®´‡®æ‡®∞‡®Æ‡®∞ **self-attention** ‡®Æ‡®ï‡©à‡®®‡®ø‡®ú‡®º‡®Æ ‡®¶‡©Ä ‡®µ‡®∞‡®§‡©ã‡®Ç ‡®ï‡®∞‡®¶‡©á ‡®π‡®®, ‡®ú‡©ã ‡®Æ‡©Ç‡®≤ ‡®§‡©å‡®∞ '‡®§‡©á attention ‡®π‡©à ‡®ú‡©ã ‡®á‡®®‡®™‡©Å‡®ü ‡®Ö‡®§‡©á ‡®Ü‡®â‡®ü‡®™‡©Å‡©±‡®ü ‡®¶‡©á ‡®§‡©å‡®∞ '‡®§‡©á ‡®á‡©±‡®ï‡©ã ‡®∏‡©Ä‡®ï‡®µ‡©à‡®Ç‡®∏ '‡®§‡©á ‡®≤‡®æ‡®ó‡©Ç ‡®π‡©Å‡©∞‡®¶‡©Ä ‡®π‡©à‡•§ Self-attention ‡®≤‡®æ‡®ó‡©Ç ‡®ï‡®∞‡®® ‡®®‡®æ‡®≤ ‡®∏‡®æ‡®®‡©Ç‡©∞ **context** ‡®®‡©Ç‡©∞ ‡®µ‡®æ‡®ï ‡®µ‡®ø‡©±‡®ö ‡®ß‡®ø‡®Ü‡®® ‡®µ‡®ø‡©±‡®ö ‡®≤‡©à‡®£ ‡®¶‡©Ä ‡®Ü‡®ó‡®ø‡®Ü ‡®Æ‡®ø‡®≤‡®¶‡©Ä ‡®π‡©à, ‡®Ö‡®§‡©á ‡®µ‡©á‡®ñ‡®£ ‡®¶‡©Ä ‡®Ü‡®ó‡®ø‡®Ü ‡®Æ‡®ø‡®≤‡®¶‡©Ä ‡®π‡©à ‡®ï‡®ø ‡®ï‡®ø‡®π‡©ú‡©á ‡®∏‡®º‡®¨‡®¶ ‡®Ü‡®™‡®∏ ‡®µ‡®ø‡©±‡®ö ‡®ú‡©Å‡©ú‡©á ‡®π‡©ã‡®è ‡®π‡®®‡•§ ‡®â‡®¶‡®æ‡®π‡®∞‡®® ‡®≤‡®à, ‡®á‡®π ‡®∏‡®æ‡®®‡©Ç‡©∞ ‡®á‡®π ‡®µ‡©á‡®ñ‡®£ ‡®¶‡©Ä ‡®Ü‡®ó‡®ø‡®Ü ‡®¶‡®ø‡©∞‡®¶‡®æ ‡®π‡©à ‡®ï‡®ø ‡®ï‡®ø‡®π‡©ú‡©á ‡®∏‡®º‡®¨‡®¶ coreferences ‡®¶‡©Å‡®Ü‡®∞‡®æ ‡®¶‡®∞‡®∏‡®æ‡®è ‡®ú‡®æ‡®Ç‡®¶‡©á ‡®π‡®®, ‡®ú‡®ø‡®µ‡©á‡®Ç ‡®ï‡®ø *it*, ‡®Ö‡®§‡©á context ‡®®‡©Ç‡©∞ ‡®µ‡©Ä ‡®ß‡®ø‡®Ü‡®® ‡®µ‡®ø‡©±‡®ö ‡®≤‡©à‡®£ ‡®¶‡©Ä ‡®Ü‡®ó‡®ø‡®Ü ‡®¶‡®ø‡©∞‡®¶‡®æ ‡®π‡©à:

![](../../../../../translated_images/pa/CoreferenceResolution.861924d6d384a7d6.webp)

> [Google Blog](https://research.googleblog.com/2017/08/transformer-novel-neural-network.html) ‡®§‡©ã‡®Ç ‡®ö‡®ø‡©±‡®§‡®∞

‡®ü‡©ç‡®∞‡®æ‡®Ç‡®∏‡®´‡®æ‡®∞‡®Æ‡®∞‡®æ‡®Ç ‡®µ‡®ø‡©±‡®ö, ‡®Ö‡®∏‡©Ä‡®Ç **Multi-Head Attention** ‡®¶‡©Ä ‡®µ‡®∞‡®§‡©ã‡®Ç ‡®ï‡®∞‡®¶‡©á ‡®π‡®æ‡®Ç ‡®§‡®æ‡®Ç ‡®ú‡©ã ‡®®‡©à‡®ü‡®µ‡®∞‡®ï ‡®®‡©Ç‡©∞ ‡®µ‡©±‡®ñ-‡®µ‡©±‡®ñ ‡®ï‡®ø‡®∏‡®Æ ‡®¶‡©á dependencies ‡®ï‡©à‡®™‡®ö‡®∞ ‡®ï‡®∞‡®® ‡®¶‡©Ä ‡®∏‡®º‡®ï‡®§‡©Ä ‡®¶‡®ø‡©±‡®§‡©Ä ‡®ú‡®æ ‡®∏‡®ï‡©á, ‡®ú‡®ø‡®µ‡©á‡®Ç ‡®ï‡®ø long-term vs. short-term ‡®∏‡®º‡®¨‡®¶ ‡®∏‡©∞‡®¨‡©∞‡®ß, co-reference vs. ‡®ï‡©Å‡®ù ‡®π‡©ã‡®∞, ‡®Ü‡®¶‡®ø‡•§

[TensorFlow Notebook](TransformersTF.ipynb) ‡®µ‡®ø‡©±‡®ö ‡®ü‡©ç‡®∞‡®æ‡®Ç‡®∏‡®´‡®æ‡®∞‡®Æ‡®∞ ‡®≤‡©á‡®Ö‡®∞‡®æ‡®Ç ‡®¶‡©Ä ‡®≤‡®æ‡®ó‡©Ç ‡®ï‡®∞‡®® ‡®¶‡©Ä ‡®µ‡®ß‡©á‡®∞‡©á ‡®ú‡®æ‡®£‡®ï‡®æ‡®∞‡©Ä ‡®π‡©à‡•§

### Encoder-Decoder Attention

‡®ü‡©ç‡®∞‡®æ‡®Ç‡®∏‡®´‡®æ‡®∞‡®Æ‡®∞‡®æ‡®Ç ‡®µ‡®ø‡©±‡®ö, attention ‡®¶‡©ã ‡®∏‡®•‡®æ‡®®‡®æ‡®Ç '‡®§‡©á ‡®µ‡®∞‡®§‡©Ä ‡®ú‡®æ‡®Ç‡®¶‡©Ä ‡®π‡©à:

* ‡®á‡®®‡®™‡©Å‡®ü ‡®ü‡©à‡®ï‡®∏‡®ü ‡®µ‡®ø‡©±‡®ö ‡®™‡©à‡®ü‡®∞‡®® ‡®ï‡©à‡®™‡®ö‡®∞ ‡®ï‡®∞‡®® ‡®≤‡®à self-attention ‡®¶‡©Ä ‡®µ‡®∞‡®§‡©ã‡®Ç  
* ‡®∏‡©Ä‡®ï‡®µ‡©à‡®Ç‡®∏ ‡®Ö‡®®‡©Å‡®µ‡®æ‡®¶ ‡®ï‡®∞‡®® ‡®≤‡®à - ‡®á‡®π attention layer ‡®π‡©à ‡®ú‡©ã ‡®ê‡®®‡®ï‡©ã‡®°‡®∞ ‡®Ö‡®§‡©á ‡®°‡®ø‡®ï‡©ã‡®°‡®∞ ‡®¶‡©á ‡®µ‡®ø‡®ö‡®ï‡®æ‡®∞ ‡®π‡©à‡•§  

Encoder-decoder attention RNNs ‡®µ‡®ø‡©±‡®ö ‡®µ‡®∞‡®§‡©á attention ‡®Æ‡®ï‡©à‡®®‡®ø‡®ú‡®º‡®Æ ‡®¶‡©á ‡®¨‡®π‡©Å‡®§ ‡®π‡©Ä ‡®∏‡®Æ‡®æ‡®® ‡®π‡©à, ‡®ú‡®ø‡®µ‡©á‡®Ç ‡®ï‡®ø ‡®á‡®∏ ‡®≠‡®æ‡®ó ‡®¶‡©á ‡®∏‡®º‡©Å‡®∞‡©Ç ‡®µ‡®ø‡©±‡®ö ‡®µ‡®∞‡®£‡®® ‡®ï‡©Ä‡®§‡®æ ‡®ó‡®ø‡®Ü ‡®π‡©à‡•§ ‡®á‡®π animated diagram ‡®ü‡©ç‡®∞‡®æ‡®Ç‡®∏‡®´‡®æ‡®∞‡®Æ‡®∞ ‡®Æ‡®æ‡®°‡®≤‡®æ‡®Ç ‡®µ‡®ø‡©±‡®ö encoder-decoder attention ‡®¶‡©Ä ‡®≠‡©Ç‡®Æ‡®ø‡®ï‡®æ ‡®®‡©Ç‡©∞ ‡®∏‡®Æ‡®ù‡®æ‡®â‡®Ç‡®¶‡®æ ‡®π‡©à‡•§

![Animated GIF ‡®¶‡®ø‡®ñ‡®æ‡®â‡®Ç‡®¶‡©Ä ‡®π‡©à ‡®ï‡®ø ‡®ü‡©ç‡®∞‡®æ‡®Ç‡®∏‡®´‡®æ‡®∞‡®Æ‡®∞ ‡®Æ‡®æ‡®°‡®≤‡®æ‡®Ç ‡®µ‡®ø‡©±‡®ö ‡®Æ‡©Å‡®≤‡®æ‡®Ç‡®ï‡®£ ‡®ï‡®ø‡®µ‡©á‡®Ç ‡®ï‡©Ä‡®§‡©á ‡®ú‡®æ‡®Ç‡®¶‡©á ‡®π‡®®‡•§](../../../../../lessons/5-NLP/18-Transformers/images/transformer-animated-explanation.gif)

‡®ï‡®ø‡®â‡®Ç‡®ï‡®ø ‡®π‡®∞ ‡®á‡®®‡®™‡©Å‡®ü ‡®∏‡®•‡®æ‡®® ‡®®‡©Ç‡©∞ ‡®π‡®∞ ‡®Ü‡®â‡®ü‡®™‡©Å‡©±‡®ü ‡®∏‡®•‡®æ‡®® ‡®®‡®æ‡®≤ ‡®Ö‡®ú‡®º‡®æ‡®¶‡©Ä ‡®®‡®æ‡®≤ ‡®Æ‡©à‡®™ ‡®ï‡©Ä‡®§‡®æ ‡®ú‡®æ‡®Ç‡®¶‡®æ ‡®π‡©à, ‡®ü‡©ç‡®∞‡®æ‡®Ç‡®∏‡®´‡®æ‡®∞‡®Æ‡®∞ RNNs ‡®®‡®æ‡®≤‡©ã‡®Ç ‡®µ‡®ß‡©Ä‡®Ü ‡®™‡©à‡®∞‡®æ‡®≤‡®≤‡®æ‡®à‡®ú‡®º ‡®ï‡®∞ ‡®∏‡®ï‡®¶‡©á ‡®π‡®®, ‡®ú‡®ø‡®∏ ‡®®‡®æ‡®≤ ‡®ï‡®æ‡®´‡®º‡©Ä ‡®µ‡©±‡®°‡©á ‡®Ö‡®§‡©á ‡®π‡©ã‡®∞ ‡®™‡©ç‡®∞‡®ó‡®ü‡®µ‡®æ‡®¶‡©Ä ‡®≠‡®æ‡®∏‡®º‡®æ ‡®Æ‡®æ‡®°‡®≤‡®æ‡®Ç ‡®®‡©Ç‡©∞ ‡®Ø‡©ã‡®ó ‡®¨‡®£‡®æ‡®á‡®Ü ‡®ú‡®æ‡®Ç‡®¶‡®æ ‡®π‡©à‡•§ ‡®π‡®∞ attention head ‡®∏‡®º‡®¨‡®¶‡®æ‡®Ç ‡®¶‡©á ‡®µ‡®ø‡®ö‡®ï‡®æ‡®∞ ‡®µ‡©±‡®ñ-‡®µ‡©±‡®ñ ‡®∏‡©∞‡®¨‡©∞‡®ß‡®æ‡®Ç ‡®®‡©Ç‡©∞ ‡®∏‡®ø‡©±‡®ñ‡®£ ‡®≤‡®à ‡®µ‡®∞‡®§‡©Ä ‡®ú‡®æ ‡®∏‡®ï‡®¶‡©Ä ‡®π‡©à, ‡®ú‡©ã NLP ‡®¶‡©á ‡®π‡©á‡®†‡®æ‡®Ç ‡®¶‡©á ‡®ï‡©∞‡®Æ‡®æ‡®Ç ‡®®‡©Ç‡©∞ ‡®∏‡©Å‡®ß‡®æ‡®∞‡®¶‡®æ ‡®π‡©à‡•§

## BERT

**BERT** (Bidirectional Encoder Representations from Transformers) ‡®á‡©±‡®ï ‡®¨‡®π‡©Å‡®§ ‡®µ‡©±‡®°‡®æ multi-layer ‡®ü‡©ç‡®∞‡®æ‡®Ç‡®∏‡®´‡®æ‡®∞‡®Æ‡®∞ ‡®®‡©à‡®ü‡®µ‡®∞‡®ï ‡®π‡©à ‡®ú‡®ø‡®∏ ‡®µ‡®ø‡©±‡®ö *BERT-base* ‡®≤‡®à 12 ‡®≤‡©á‡®Ö‡®∞ ‡®π‡®®, ‡®Ö‡®§‡©á *BERT-large* ‡®≤‡®à 24‡•§ ‡®Æ‡®æ‡®°‡®≤ ‡®®‡©Ç‡©∞ ‡®™‡®π‡®ø‡®≤‡®æ‡®Ç ‡®á‡©±‡®ï ‡®µ‡©±‡®°‡©á ‡®ü‡©à‡®ï‡®∏‡®ü ‡®°‡®æ‡®ü‡®æ ‡®ï‡©ã‡®∞‡®™‡®∏ (WikiPedia + ‡®ï‡®ø‡®§‡®æ‡®¨‡®æ‡®Ç) '‡®§‡©á unsupervised training (‡®µ‡®æ‡®ï ‡®µ‡®ø‡©±‡®ö masked ‡®∏‡®º‡®¨‡®¶‡®æ‡®Ç ‡®¶‡©Ä ‡®™‡©á‡®∏‡®º‡®ï‡®∏‡®º) ‡®¶‡©Ä ‡®µ‡®∞‡®§‡©ã‡®Ç ‡®ï‡®∞‡®ï‡©á pre-train ‡®ï‡©Ä‡®§‡®æ ‡®ú‡®æ‡®Ç‡®¶‡®æ ‡®π‡©à‡•§ Pre-training ‡®¶‡©å‡®∞‡®æ‡®® ‡®Æ‡®æ‡®°‡®≤ ‡®≠‡®æ‡®∏‡®º‡®æ ‡®∏‡®Æ‡®ù‡®£ ‡®¶‡©á ‡®Æ‡®π‡©±‡®§‡®µ‡®™‡©Ç‡®∞‡®® ‡®™‡©±‡®ß‡®∞‡®æ‡®Ç ‡®®‡©Ç‡©∞ ‡®Ö‡®™‡®£‡®æ‡®â‡®Ç‡®¶‡®æ ‡®π‡©à, ‡®ú‡®ø‡®∏‡®®‡©Ç‡©∞ ‡®´‡®ø‡®∞ ‡®π‡©ã‡®∞ ‡®°‡®æ‡®ü‡®æ‡®∏‡©à‡®ü‡®æ‡®Ç ‡®®‡®æ‡®≤ fine-tuning ‡®¶‡©Å‡®Ü‡®∞‡®æ leveraged ‡®ï‡©Ä‡®§‡®æ ‡®ú‡®æ ‡®∏‡®ï‡®¶‡®æ ‡®π‡©à‡•§ ‡®á‡®∏ ‡®™‡©ç‡®∞‡®ï‡®ø‡®∞‡®ø‡®Ü ‡®®‡©Ç‡©∞ **transfer learning** ‡®ï‡®ø‡®π‡®æ ‡®ú‡®æ‡®Ç‡®¶‡®æ ‡®π‡©à‡•§

![http://jalammar.github.io/illustrated-bert/ ‡®§‡©ã‡®Ç ‡®ö‡®ø‡©±‡®§‡®∞](../../../../../translated_images/pa/jalammarBERT-language-modeling-masked-lm.34f113ea5fec4362.webp)

> ‡®ö‡®ø‡©±‡®§‡®∞ [source](http://jalammar.github.io/illustrated-bert/)

## ‚úçÔ∏è ‡®Ö‡®≠‡®ø‡®Ü‡®∏: ‡®ü‡©ç‡®∞‡®æ‡®Ç‡®∏‡®´‡®æ‡®∞‡®Æ‡®∞

‡®π‡©á‡®†‡®æ‡®Ç ‡®¶‡®ø‡©±‡®§‡©á ‡®®‡©ã‡®ü‡®¨‡©Å‡©±‡®ï‡®æ‡®Ç ‡®µ‡®ø‡©±‡®ö ‡®Ü‡®™‡®£‡©Ä ‡®∏‡®ø‡©±‡®ñ‡®ø‡®Ü ‡®ú‡®æ‡®∞‡©Ä ‡®∞‡©±‡®ñ‡©ã:

* [Transformers in PyTorch](TransformersPyTorch.ipynb)
* [Transformers in TensorFlow](TransformersTF.ipynb)

## ‡®®‡®ø‡®∏‡®ï‡®∞‡®∏‡®º

‡®á‡®∏ ‡®™‡®æ‡®† ‡®µ‡®ø‡©±‡®ö ‡®§‡©Å‡®∏‡©Ä‡®Ç ‡®ü‡©ç‡®∞‡®æ‡®Ç‡®∏‡®´‡®æ‡®∞‡®Æ‡®∞ ‡®Ö‡®§‡©á Attention ‡®Æ‡®ï‡©à‡®®‡®ø‡®ú‡®º‡®Æ ‡®¨‡®æ‡®∞‡©á ‡®∏‡®ø‡©±‡®ñ‡®ø‡®Ü, ‡®ú‡©ã NLP ‡®ü‡©Ç‡®≤‡®¨‡®æ‡®ï‡®∏ ‡®µ‡®ø‡©±‡®ö ‡®ú‡®º‡®∞‡©Ç‡®∞‡©Ä ‡®∏‡©∞‡®¶ ‡®π‡®®‡•§ ‡®ü‡©ç‡®∞‡®æ‡®Ç‡®∏‡®´‡®æ‡®∞‡®Æ‡®∞ ‡®Ü‡®∞‡®ï‡©Ä‡®ü‡©à‡®ï‡®ö‡®∞‡®æ‡®Ç ‡®¶‡©á ‡®ï‡®à ‡®∞‡©Ç‡®™ ‡®π‡®® ‡®ú‡®ø‡®µ‡©á‡®Ç ‡®ï‡®ø BERT, DistilBERT, BigBird, OpenGPT3 ‡®Ö‡®§‡©á ‡®π‡©ã‡®∞ ‡®ú‡©ã fine-tuned ‡®ï‡©Ä‡®§‡©á ‡®ú‡®æ ‡®∏‡®ï‡®¶‡©á ‡®π‡®®‡•§ [HuggingFace ‡®™‡©à‡®ï‡©á‡®ú](https://github.com/huggingface/) PyTorch ‡®Ö‡®§‡©á TensorFlow ‡®¶‡©ã‡®µ‡®æ‡®Ç ‡®®‡®æ‡®≤ ‡®ï‡®à ‡®Ü‡®∞‡®ï‡©Ä‡®ü‡©à‡®ï‡®ö‡®∞‡®æ‡®Ç ‡®®‡©Ç‡©∞ ‡®ü‡©ç‡®∞‡©á‡®® ‡®ï‡®∞‡®® ‡®≤‡®à repository ‡®™‡©ç‡®∞‡®¶‡®æ‡®® ‡®ï‡®∞‡®¶‡®æ ‡®π‡©à‡•§

## üöÄ ‡®ö‡©Å‡®£‡©å‡®§‡©Ä

## [‡®™‡©ã‡®∏‡®ü-‡®≤‡©à‡®ï‡®ö‡®∞ ‡®ï‡®µ‡®ø‡®ú‡®º](https://ff-quizzes.netlify.app/en/ai/quiz/36)

## ‡®∏‡®Æ‡©Ä‡®ñ‡®ø‡®Ü ‡®Ö‡®§‡©á ‡®∏‡®µ‡©à-‡®Ö‡®ß‡®ø‡®ê‡®®

* [Blog post](https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/), explaining the classical [Attention is all you need](https://arxiv.org/abs/1706.03762) paper on transformers.
* [A series of blog posts](https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452) on transformers, explaining the architecture in detail.

## [‡®Ö‡®∏‡®æ‡®à‡®®‡®Æ‡©à‡®Ç‡®ü](assignment.md)

---

