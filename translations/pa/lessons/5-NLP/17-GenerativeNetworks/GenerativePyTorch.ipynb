{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ਜਨਰੇਟਿਵ ਨੈਟਵਰਕਸ\n",
    "\n",
    "ਰੀਕਰਨਟ ਨਿਊਰਲ ਨੈਟਵਰਕਸ (RNNs) ਅਤੇ ਉਨ੍ਹਾਂ ਦੇ ਗੇਟਡ ਸੈੱਲ ਵੈਰੀਐਂਟਸ ਜਿਵੇਂ ਕਿ ਲਾਂਗ ਸ਼ਾਰਟ ਟਰਮ ਮੈਮੋਰੀ ਸੈੱਲਸ (LSTMs) ਅਤੇ ਗੇਟਡ ਰੀਕਰਨਟ ਯੂਨਿਟਸ (GRUs) ਨੇ ਭਾਸ਼ਾ ਮਾਡਲਿੰਗ ਲਈ ਇੱਕ ਮਕੈਨਿਜ਼ਮ ਮੁਹੱਈਆ ਕਰਵਾਇਆ, ਯਾਨੀ ਇਹ ਸ਼ਬਦਾਂ ਦੀ ਕ੍ਰਮਵਾਰਤਾ ਸਿੱਖ ਸਕਦੇ ਹਨ ਅਤੇ ਕ੍ਰਮ ਵਿੱਚ ਅਗਲੇ ਸ਼ਬਦ ਦੀ ਭਵਿੱਖਵਾਣੀ ਕਰ ਸਕਦੇ ਹਨ। ਇਸ ਨਾਲ ਸਾਨੂੰ RNNs ਨੂੰ **ਜਨਰੇਟਿਵ ਕੰਮਾਂ** ਲਈ ਵਰਤਣ ਦੀ ਆਗਿਆ ਮਿਲਦੀ ਹੈ, ਜਿਵੇਂ ਕਿ ਆਮ ਪਾਠ ਜਨਰੇਸ਼ਨ, ਮਸ਼ੀਨ ਅਨੁਵਾਦ, ਅਤੇ ਇੱਥੋਂ ਤੱਕ ਕਿ ਚਿੱਤਰ ਕੈਪਸ਼ਨਿੰਗ।\n",
    "\n",
    "ਪਿਛਲੇ ਯੂਨਿਟ ਵਿੱਚ ਚਰਚਾ ਕੀਤੀ ਗਈ RNN ਆਰਕੀਟੈਕਚਰ ਵਿੱਚ, ਹਰ RNN ਯੂਨਿਟ ਨੇ ਅਗਲਾ ਹਿਡਨ ਸਟੇਟ ਇੱਕ ਆਉਟਪੁਟ ਵਜੋਂ ਤਿਆਰ ਕੀਤਾ। ਹਾਲਾਂਕਿ, ਅਸੀਂ ਹਰ ਰੀਕਰਨਟ ਯੂਨਿਟ ਵਿੱਚ ਇੱਕ ਹੋਰ ਆਉਟਪੁਟ ਵੀ ਸ਼ਾਮਲ ਕਰ ਸਕਦੇ ਹਾਂ, ਜੋ ਸਾਨੂੰ ਇੱਕ **ਕ੍ਰਮ** (ਜੋ ਮੂਲ ਕ੍ਰਮ ਦੇ ਬਰਾਬਰ ਲੰਬਾਈ ਵਾਲਾ ਹੈ) ਆਉਟਪੁਟ ਕਰਨ ਦੀ ਆਗਿਆ ਦਿੰਦਾ ਹੈ। ਇਸ ਤੋਂ ਇਲਾਵਾ, ਅਸੀਂ RNN ਯੂਨਿਟਸ ਦੀ ਵਰਤੋਂ ਕਰ ਸਕਦੇ ਹਾਂ ਜੋ ਹਰ ਕਦਮ 'ਤੇ ਇਨਪੁਟ ਸਵੀਕਾਰ ਨਹੀਂ ਕਰਦੇ, ਸਗੋਂ ਸਿਰਫ਼ ਕੁਝ ਸ਼ੁਰੂਆਤੀ ਸਟੇਟ ਵੈਕਟਰ ਲੈਂਦੇ ਹਨ ਅਤੇ ਫਿਰ ਆਉਟਪੁਟ ਦਾ ਇੱਕ ਕ੍ਰਮ ਤਿਆਰ ਕਰਦੇ ਹਨ।\n",
    "\n",
    "ਇਸ ਨੋਟਬੁੱਕ ਵਿੱਚ, ਅਸੀਂ ਸਧਾਰਨ ਜਨਰੇਟਿਵ ਮਾਡਲਾਂ 'ਤੇ ਧਿਆਨ ਕੇਂਦਰਿਤ ਕਰਾਂਗੇ ਜੋ ਸਾਨੂੰ ਪਾਠ ਤਿਆਰ ਕਰਨ ਵਿੱਚ ਮਦਦ ਕਰਦੇ ਹਨ। ਸਧਾਰਨਤਾ ਲਈ, ਆਓ ਇੱਕ **ਕਿਰਦਾਰ-ਪੱਧਰ ਦਾ ਨੈਟਵਰਕ** ਬਣਾਈਏ, ਜੋ ਪਾਠ ਨੂੰ ਇੱਕ-ਇੱਕ ਅੱਖਰ ਕਰਕੇ ਤਿਆਰ ਕਰਦਾ ਹੈ। ਟ੍ਰੇਨਿੰਗ ਦੌਰਾਨ, ਸਾਨੂੰ ਕੁਝ ਪਾਠ ਕੋਰਪਸ ਲੈਣਾ ਪਵੇਗਾ ਅਤੇ ਇਸਨੂੰ ਅੱਖਰ ਕ੍ਰਮਾਂ ਵਿੱਚ ਵੰਡਣਾ ਪਵੇਗਾ।\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Building vocab...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "import numpy as np\n",
    "from torchnlp import *\n",
    "train_dataset,test_dataset,classes,vocab = load_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ਅੱਖਰ ਸ਼ਬਦਾਵਲੀ ਬਣਾਉਣਾ\n",
    "\n",
    "ਅੱਖਰ-ਪੱਧਰੀ ਜਨਰੇਟਿਵ ਨੈੱਟਵਰਕ ਬਣਾਉਣ ਲਈ, ਸਾਨੂੰ ਪੂਰੇ ਪਾਠ ਨੂੰ ਸ਼ਬਦਾਂ ਦੀ ਬਜਾਏ ਵੱਖ-ਵੱਖ ਅੱਖਰਾਂ ਵਿੱਚ ਵੰਡਣਾ ਪਵੇਗਾ। ਇਹ ਕੰਮ ਇੱਕ ਵੱਖਰੇ ਟੋਕਨਾਈਜ਼ਰ ਨੂੰ ਪਰਿਭਾਸ਼ਿਤ ਕਰਕੇ ਕੀਤਾ ਜਾ ਸਕਦਾ ਹੈ:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size = 82\n",
      "Encoding of 'a' is 1\n",
      "Character with code 13 is c\n"
     ]
    }
   ],
   "source": [
    "def char_tokenizer(words):\n",
    "    return list(words) #[word for word in words]\n",
    "\n",
    "counter = collections.Counter()\n",
    "for (label, line) in train_dataset:\n",
    "    counter.update(char_tokenizer(line))\n",
    "vocab = torchtext.vocab.vocab(counter)\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "print(f\"Vocabulary size = {vocab_size}\")\n",
    "print(f\"Encoding of 'a' is {vocab.get_stoi()['a']}\")\n",
    "print(f\"Character with code 13 is {vocab.get_itos()[13]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ਆਓ ਦੇਖੀਏ ਕਿ ਅਸੀਂ ਆਪਣੇ ਡੇਟਾਸੈੱਟ ਤੋਂ ਟੈਕਸਟ ਨੂੰ ਕਿਵੇਂ ਐਨਕੋਡ ਕਰ ਸਕਦੇ ਹਾਂ:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  2,  3,  4,  5,  6,  3,  7,  8,  1,  9, 10,  3, 11,  2,  1,\n",
       "        12,  3,  7,  1, 13, 14,  3, 15, 16,  5, 17,  3,  5, 18,  8,  3,  7,  2,\n",
       "         1, 13, 14,  3, 19, 20,  8, 21,  5,  8,  9, 10, 22,  3, 20,  8, 21,  5,\n",
       "         8,  9, 10,  3, 23,  3,  4, 18, 17,  9,  5, 23, 10,  8,  2,  2,  8,  9,\n",
       "        10, 24,  3,  0,  1,  2,  2,  3,  4,  5,  9,  8,  8,  5, 25, 10,  3, 26,\n",
       "        12, 27, 16, 26,  2, 27, 16, 28, 29, 30,  1, 16, 26,  3, 17, 31,  3, 21,\n",
       "         2,  5,  9,  1, 23, 13, 32, 16, 27, 13, 10, 24,  3,  1,  9,  8,  3, 10,\n",
       "         8,  8, 27, 16, 28,  3, 28,  9,  8,  8, 16,  3,  1, 28,  1, 27, 16,  6])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def enc(x):\n",
    "    return torch.LongTensor(encode(x,voc=vocab,tokenizer=char_tokenizer))\n",
    "\n",
    "enc(train_dataset[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ਜਨਰੇਟਿਵ RNN ਨੂੰ ਟ੍ਰੇਨ ਕਰਨਾ\n",
    "\n",
    "ਅਸੀਂ RNN ਨੂੰ ਟੈਕਸਟ ਜਨਰੇਟ ਕਰਨ ਲਈ ਇਸ ਤਰੀਕੇ ਨਾਲ ਟ੍ਰੇਨ ਕਰਾਂਗੇ। ਹਰ ਕਦਮ 'ਤੇ, ਅਸੀਂ ਅੱਖਰਾਂ ਦੀ `nchars` ਲੰਬਾਈ ਦੀ ਲੜੀ ਲਵਾਂਗੇ ਅਤੇ ਨੈਟਵਰਕ ਨੂੰ ਹਰ ਇਨਪੁਟ ਅੱਖਰ ਲਈ ਅਗਲਾ ਆਉਟਪੁੱਟ ਅੱਖਰ ਜਨਰੇਟ ਕਰਨ ਲਈ ਕਹਾਂਗੇ:\n",
    "\n",
    "![ਇਮੇਜ 'HELLO' ਸ਼ਬਦ ਦੇ RNN ਜਨਰੇਸ਼ਨ ਦਾ ਉਦਾਹਰਨ ਦਿਖਾਉਂਦੀ ਹੈ।](../../../../../translated_images/pa/rnn-generate.56c54afb52f9781d.webp)\n",
    "\n",
    "ਅਸਲ ਸਥਿਤੀ ਦੇ ਅਨੁਸਾਰ, ਅਸੀਂ ਕੁਝ ਖਾਸ ਅੱਖਰ ਵੀ ਸ਼ਾਮਲ ਕਰਨਾ ਚਾਹੁੰਦੇ ਹੋ ਸਕਦੇ ਹਾਂ, ਜਿਵੇਂ ਕਿ *end-of-sequence* `<eos>`। ਸਾਡੇ ਕੇਸ ਵਿੱਚ, ਅਸੀਂ ਨੈਟਵਰਕ ਨੂੰ ਅਨੰਤ ਟੈਕਸਟ ਜਨਰੇਸ਼ਨ ਲਈ ਟ੍ਰੇਨ ਕਰਨਾ ਚਾਹੁੰਦੇ ਹਾਂ, ਇਸ ਲਈ ਅਸੀਂ ਹਰ ਲੜੀ ਦਾ ਆਕਾਰ `nchars` ਟੋਕਨ ਦੇ ਬਰਾਬਰ ਰੱਖਾਂਗੇ। ਇਸ ਤਰ੍ਹਾਂ, ਹਰ ਟ੍ਰੇਨਿੰਗ ਉਦਾਹਰਨ `nchars` ਇਨਪੁਟ ਅਤੇ `nchars` ਆਉਟਪੁੱਟ (ਜੋ ਇਨਪੁਟ ਲੜੀ ਨੂੰ ਇੱਕ ਚਿੰਨ੍ਹ ਖੱਬੇ ਵੱਲ ਸ਼ਿਫਟ ਕਰਕੇ ਬਣਦੀ ਹੈ) 'ਤੇ ਮੁਸ਼ਤਮਿਲ ਹੋਵੇਗੀ। ਮਿਨੀਬੈਚ ਕਈ ਇਸ ਤਰ੍ਹਾਂ ਦੀਆਂ ਲੜੀਆਂ 'ਤੇ ਮੁਸ਼ਤਮਿਲ ਹੋਵੇਗਾ।\n",
    "\n",
    "ਅਸੀਂ ਮਿਨੀਬੈਚਾਂ ਨੂੰ ਇਸ ਤਰੀਕੇ ਨਾਲ ਜਨਰੇਟ ਕਰਾਂਗੇ ਕਿ ਹਰ ਖ਼ਬਰ ਦੇ ਟੈਕਸਟ ਦੀ ਲੰਬਾਈ `l` ਹੋਵੇਗੀ, ਅਤੇ ਉਸ ਵਿੱਚੋਂ ਸਾਰੀਆਂ ਸੰਭਾਵਿਤ ਇਨਪੁਟ-ਆਉਟਪੁੱਟ ਕkombination ਬਣਾਈਆਂ ਜਾਣਗੀਆਂ (ਇਸ ਤਰ੍ਹਾਂ ਦੇ `l-nchars` ਕkombination ਹੋਣਗੇ)। ਇਹ ਇੱਕ ਮਿਨੀਬੈਚ ਬਣਾਉਣਗੇ, ਅਤੇ ਮਿਨੀਬੈਚਾਂ ਦਾ ਆਕਾਰ ਹਰ ਟ੍ਰੇਨਿੰਗ ਕਦਮ 'ਤੇ ਵੱਖ-ਵੱਖ ਹੋਵੇਗਾ।\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0,  1,  2,  ..., 28, 29, 30],\n",
       "         [ 1,  2,  2,  ..., 29, 30,  1],\n",
       "         [ 2,  2,  3,  ..., 30,  1, 16],\n",
       "         ...,\n",
       "         [20,  8, 21,  ...,  1, 28,  1],\n",
       "         [ 8, 21,  5,  ..., 28,  1, 27],\n",
       "         [21,  5,  8,  ...,  1, 27, 16]]),\n",
       " tensor([[ 1,  2,  2,  ..., 29, 30,  1],\n",
       "         [ 2,  2,  3,  ..., 30,  1, 16],\n",
       "         [ 2,  3,  4,  ...,  1, 16, 26],\n",
       "         ...,\n",
       "         [ 8, 21,  5,  ..., 28,  1, 27],\n",
       "         [21,  5,  8,  ...,  1, 27, 16],\n",
       "         [ 5,  8,  9,  ..., 27, 16,  6]]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nchars = 100\n",
    "\n",
    "def get_batch(s,nchars=nchars):\n",
    "    ins = torch.zeros(len(s)-nchars,nchars,dtype=torch.long,device=device)\n",
    "    outs = torch.zeros(len(s)-nchars,nchars,dtype=torch.long,device=device)\n",
    "    for i in range(len(s)-nchars):\n",
    "        ins[i] = enc(s[i:i+nchars])\n",
    "        outs[i] = enc(s[i+1:i+nchars+1])\n",
    "    return ins,outs\n",
    "\n",
    "get_batch(train_dataset[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ਹੁਣ ਆਓ ਜਨਰੇਟਰ ਨੈੱਟਵਰਕ ਨੂੰ ਪਰਿਭਾਸ਼ਿਤ ਕਰੀਏ। ਇਹ ਕਿਸੇ ਵੀ ਰਿਕਰੰਟ ਸੈੱਲ 'ਤੇ ਆਧਾਰਿਤ ਹੋ ਸਕਦਾ ਹੈ ਜਿਸ ਬਾਰੇ ਅਸੀਂ ਪਿਛਲੇ ਯੂਨਿਟ ਵਿੱਚ ਚਰਚਾ ਕੀਤੀ ਸੀ (ਸਧਾਰਨ, LSTM ਜਾਂ GRU)। ਸਾਡੇ ਉਦਾਹਰਨ ਵਿੱਚ ਅਸੀਂ LSTM ਦੀ ਵਰਤੋਂ ਕਰਾਂਗੇ।\n",
    "\n",
    "ਕਿਉਂਕਿ ਨੈੱਟਵਰਕ ਅੱਖਰਾਂ ਨੂੰ ਇਨਪੁਟ ਵਜੋਂ ਲੈਂਦਾ ਹੈ, ਅਤੇ ਸ਼ਬਦਾਵਲੀ ਦਾ ਆਕਾਰ ਕਾਫ਼ੀ ਛੋਟਾ ਹੈ, ਸਾਨੂੰ ਐਮਬੈਡਿੰਗ ਲੇਅਰ ਦੀ ਲੋੜ ਨਹੀਂ ਹੈ। ਇੱਕ-ਹੌਟ-ਇੰਕੋਡ ਕੀਤਾ ਇਨਪੁਟ ਸਿੱਧੇ LSTM ਸੈੱਲ ਵਿੱਚ ਜਾ ਸਕਦਾ ਹੈ। ਹਾਲਾਂਕਿ, ਕਿਉਂਕਿ ਅਸੀਂ ਅੱਖਰਾਂ ਦੇ ਨੰਬਰਾਂ ਨੂੰ ਇਨਪੁਟ ਵਜੋਂ ਪਾਸ ਕਰਦੇ ਹਾਂ, ਸਾਨੂੰ ਉਹਨਾਂ ਨੂੰ LSTM ਵਿੱਚ ਭੇਜਣ ਤੋਂ ਪਹਿਲਾਂ ਇੱਕ-ਹੌਟ-ਇੰਕੋਡ ਕਰਨਾ ਪਵੇਗਾ। ਇਹ `forward` ਪਾਸ ਦੌਰਾਨ `one_hot` ਫੰਕਸ਼ਨ ਨੂੰ ਕਾਲ ਕਰਕੇ ਕੀਤਾ ਜਾਂਦਾ ਹੈ। ਆਉਟਪੁਟ ਐਨਕੋਡਰ ਇੱਕ ਲੀਨੀਅਰ ਲੇਅਰ ਹੋਵੇਗਾ ਜੋ ਲੁਕਵੇਂ ਹਾਲਤ ਨੂੰ ਇੱਕ-ਹੌਟ-ਇੰਕੋਡ ਕੀਤੇ ਆਉਟਪੁਟ ਵਿੱਚ ਬਦਲ ਦੇਵੇਗਾ।\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMGenerator(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.rnn = torch.nn.LSTM(vocab_size,hidden_dim,batch_first=True)\n",
    "        self.fc = torch.nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, s=None):\n",
    "        x = torch.nn.functional.one_hot(x,vocab_size).to(torch.float32)\n",
    "        x,s = self.rnn(x,s)\n",
    "        return self.fc(x),s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ਤਰਬੀਤ ਦੇ ਦੌਰਾਨ, ਅਸੀਂ ਚਾਹੁੰਦੇ ਹਾਂ ਕਿ ਜਨਰੇਟ ਕੀਤੇ ਟੈਕਸਟ ਨੂੰ ਸੈਂਪਲ ਕਰ ਸਕੀਏ। ਇਸ ਲਈ, ਅਸੀਂ `generate` ਫੰਕਸ਼ਨ ਨੂੰ ਪਰਿਭਾਸ਼ਿਤ ਕਰਾਂਗੇ ਜੋ ਸ਼ੁਰੂਆਤੀ ਸਤਰ `start` ਤੋਂ ਸ਼ੁਰੂ ਕਰਦੇ ਹੋਏ, ਲੰਬਾਈ `size` ਦਾ ਆਉਟਪੁੱਟ ਸਤਰ ਤਿਆਰ ਕਰੇਗਾ।\n",
    "\n",
    "ਇਹ ਤਰੀਕਾ ਇਸ ਤਰ੍ਹਾਂ ਕੰਮ ਕਰਦਾ ਹੈ। ਸਭ ਤੋਂ ਪਹਿਲਾਂ, ਅਸੀਂ ਪੂਰੀ ਸ਼ੁਰੂਆਤੀ ਸਤਰ ਨੂੰ ਨੈਟਵਰਕ ਵਿੱਚ ਪਾਸ ਕਰਾਂਗੇ, ਅਤੇ ਆਉਟਪੁੱਟ ਸਥਿਤੀ `s` ਅਤੇ ਅਗਲਾ ਅਨੁਮਾਨਿਤ ਅੱਖਰ `out` ਲਵਾਂਗੇ। ਕਿਉਂਕਿ `out` ਇੱਕ-ਹਾਟ ਕੋਡ ਕੀਤਾ ਹੁੰਦਾ ਹੈ, ਅਸੀਂ `argmax` ਲੈਂਦੇ ਹਾਂ ਤਾਂ ਜੋ ਸ਼ਬਦਾਵਲੀ ਵਿੱਚ ਅੱਖਰ `nc` ਦਾ ਇੰਡੈਕਸ ਪ੍ਰਾਪਤ ਕਰ ਸਕੀਏ, ਅਤੇ `itos` ਦੀ ਵਰਤੋਂ ਕਰਕੇ ਅਸਲ ਅੱਖਰ ਦਾ ਪਤਾ ਲਗਾ ਕੇ ਇਸਨੂੰ ਅੱਖਰਾਂ ਦੀ ਨਤੀਜਾ ਸੂਚੀ `chars` ਵਿੱਚ ਸ਼ਾਮਲ ਕਰਦੇ ਹਾਂ। ਇੱਕ ਅੱਖਰ ਤਿਆਰ ਕਰਨ ਦੀ ਇਹ ਪ੍ਰਕਿਰਿਆ `size` ਵਾਰ ਦੁਹਰਾਈ ਜਾਂਦੀ ਹੈ ਤਾਂ ਜੋ ਲੋੜੀਂਦੇ ਅੱਖਰਾਂ ਦੀ ਗਿਣਤੀ ਤਿਆਰ ਕੀਤੀ ਜਾ ਸਕੇ।\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(net,size=100,start='today '):\n",
    "        chars = list(start)\n",
    "        out, s = net(enc(chars).view(1,-1).to(device))\n",
    "        for i in range(size):\n",
    "            nc = torch.argmax(out[0][-1])\n",
    "            chars.append(vocab.get_itos()[nc])\n",
    "            out, s = net(nc.view(1,-1),s)\n",
    "        return ''.join(chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ਆਓ ਹੁਣ ਟ੍ਰੇਨਿੰਗ ਕਰੀਏ! ਟ੍ਰੇਨਿੰਗ ਲੂਪ ਲਗਭਗ ਸਾਰੇ ਪਿਛਲੇ ਉਦਾਹਰਣਾਂ ਵਾਂਗ ਹੀ ਹੈ, ਪਰ ਸਹੀਤਾ (accuracy) ਦੀ ਬਜਾਏ ਅਸੀਂ ਹਰ 1000 epochs 'ਤੇ ਜਨਰੇਟ ਕੀਤਾ ਗਿਆ ਟੈਕਸਟ ਪ੍ਰਿੰਟ ਕਰਦੇ ਹਾਂ।\n",
    "\n",
    "ਖਾਸ ਧਿਆਨ ਇਸ ਗੱਲ 'ਤੇ ਦੇਣਾ ਹੈ ਕਿ ਅਸੀਂ ਲਾਸ (loss) ਕਿਵੇਂ ਕਲਕੁਲੇਟ ਕਰਦੇ ਹਾਂ। ਸਾਨੂੰ ਇੱਕ-ਹਾਟ-ਇਨਕੋਡ ਕੀਤੇ ਆਉਟਪੁਟ `out` ਅਤੇ ਉਮੀਦਵਾਰ ਟੈਕਸਟ `text_out` (ਜੋ ਕਿ ਕਿਰਦਾਰ ਇੰਡੈਕਸਾਂ ਦੀ ਲਿਸਟ ਹੈ) ਦੇ ਆਧਾਰ 'ਤੇ ਲਾਸ ਕਲਕੁਲੇਟ ਕਰਨੀ ਹੈ। ਖੁਸ਼ਕਿਸਮਤੀ ਨਾਲ, `cross_entropy` ਫੰਕਸ਼ਨ ਪਹਿਲੇ ਆਰਗੂਮੈਂਟ ਵਜੋਂ ਅਣਨਾਰਮਲ ਨੈਟਵਰਕ ਆਉਟਪੁਟ ਅਤੇ ਦੂਜੇ ਆਰਗੂਮੈਂਟ ਵਜੋਂ ਕਲਾਸ ਨੰਬਰ ਦੀ ਉਮੀਦ ਕਰਦਾ ਹੈ, ਜੋ ਕਿ ਸਾਡੇ ਕੋਲ ਹੈ। ਇਹ ਮਿਨੀਬੈਚ ਸਾਈਜ਼ 'ਤੇ ਆਟੋਮੈਟਿਕ ਐਵਰੇਜਿੰਗ ਵੀ ਕਰਦਾ ਹੈ।\n",
    "\n",
    "ਅਸੀਂ ਟ੍ਰੇਨਿੰਗ ਨੂੰ `samples_to_train` ਸੈਂਪਲਾਂ ਤੱਕ ਸੀਮਿਤ ਕਰਦੇ ਹਾਂ, ਤਾਂ ਜੋ ਬਹੁਤ ਜ਼ਿਆਦਾ ਸਮਾਂ ਨਾ ਲੱਗੇ। ਅਸੀਂ ਤੁਹਾਨੂੰ ਪ੍ਰੋਤਸਾਹਿਤ ਕਰਦੇ ਹਾਂ ਕਿ ਤੁਸੀਂ ਪ੍ਰਯੋਗ ਕਰੋ ਅਤੇ ਹੋਰ ਲੰਬੀ ਟ੍ਰੇਨਿੰਗ ਦੀ ਕੋਸ਼ਿਸ਼ ਕਰੋ, ਸੰਭਵ ਹੈ ਕਿ ਕਈ epochs ਲਈ (ਇਸ ਸਥਿਤੀ ਵਿੱਚ ਤੁਹਾਨੂੰ ਇਸ ਕੋਡ ਦੇ ਆਲੇ-ਦੁਆਲੇ ਇੱਕ ਹੋਰ ਲੂਪ ਬਣਾਉਣ ਦੀ ਲੋੜ ਹੋਵੇਗੀ)।\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current loss = 4.398899078369141\n",
      "today sr sr sr sr sr sr sr sr sr sr sr sr sr sr sr sr sr sr sr sr sr sr sr sr sr sr sr sr sr sr sr sr sr s\n",
      "Current loss = 2.161320447921753\n",
      "today and to the tor to to the tor to to the tor to to the tor to to the tor to to the tor to to the tor t\n",
      "Current loss = 1.6722588539123535\n",
      "today and the court to the could to the could to the could to the could to the could to the could to the c\n",
      "Current loss = 2.423795223236084\n",
      "today and a second to the conternation of the conternation of the conternation of the conternation of the \n",
      "Current loss = 1.702607274055481\n",
      "today and the company to the company to the company to the company to the company to the company to the co\n",
      "Current loss = 1.692358136177063\n",
      "today and the company to the company to the company to the company to the company to the company to the co\n",
      "Current loss = 1.9722288846969604\n",
      "today and the control the control the control the control the control the control the control the control \n",
      "Current loss = 1.8705692291259766\n",
      "today and the second to the second to the second to the second to the second to the second to the second t\n",
      "Current loss = 1.7626899480819702\n",
      "today and a security and a security and a security and a security and a security and a security and a secu\n",
      "Current loss = 1.5574463605880737\n",
      "today and the company and the company and the company and the company and the company and the company and \n",
      "Current loss = 1.5620026588439941\n",
      "today and the be that the be the be that the be the be that the be the be that the be the be that the be t\n"
     ]
    }
   ],
   "source": [
    "net = LSTMGenerator(vocab_size,64).to(device)\n",
    "\n",
    "samples_to_train = 10000\n",
    "optimizer = torch.optim.Adam(net.parameters(),0.01)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "net.train()\n",
    "for i,x in enumerate(train_dataset):\n",
    "    # x[0] is class label, x[1] is text\n",
    "    if len(x[1])-nchars<10:\n",
    "        continue\n",
    "    samples_to_train-=1\n",
    "    if not samples_to_train: break\n",
    "    text_in, text_out = get_batch(x[1])\n",
    "    optimizer.zero_grad()\n",
    "    out,s = net(text_in)\n",
    "    loss = torch.nn.functional.cross_entropy(out.view(-1,vocab_size),text_out.flatten()) #cross_entropy(out,labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if i%1000==0:\n",
    "        print(f\"Current loss = {loss.item()}\")\n",
    "        print(generate(net))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ਇਹ ਉਦਾਹਰਨ ਪਹਿਲਾਂ ਹੀ ਕਾਫ਼ੀ ਵਧੀਆ ਟੈਕਸਟ ਤਿਆਰ ਕਰਦਾ ਹੈ, ਪਰ ਇਸਨੂੰ ਕੁਝ ਤਰੀਕਿਆਂ ਨਾਲ ਹੋਰ ਬਿਹਤਰ ਬਣਾਇਆ ਜਾ ਸਕਦਾ ਹੈ:\n",
    "\n",
    "* **ਬਿਹਤਰ ਮਿਨੀਬੈਚ ਜਨਰੇਸ਼ਨ**। ਜਿਵੇਂ ਅਸੀਂ ਟ੍ਰੇਨਿੰਗ ਲਈ ਡਾਟਾ ਤਿਆਰ ਕੀਤਾ, ਉਹ ਇੱਕ ਸੈਂਪਲ ਤੋਂ ਇੱਕ ਮਿਨੀਬੈਚ ਜਨਰੇਟ ਕਰਨ ਦਾ ਤਰੀਕਾ ਸੀ। ਇਹ ਆਦਰਸ਼ ਨਹੀਂ ਹੈ, ਕਿਉਂਕਿ ਮਿਨੀਬੈਚਜ਼ ਦੇ ਸਾਰੇ ਆਕਾਰ ਵੱਖ-ਵੱਖ ਹੁੰਦੇ ਹਨ, ਅਤੇ ਕੁਝ ਮਿਨੀਬੈਚ ਜਨਰੇਟ ਵੀ ਨਹੀਂ ਹੋ ਸਕਦੇ, ਕਿਉਂਕਿ ਟੈਕਸਟ `nchars` ਤੋਂ ਛੋਟਾ ਹੁੰਦਾ ਹੈ। ਇਸ ਤੋਂ ਇਲਾਵਾ, ਛੋਟੇ ਮਿਨੀਬੈਚ GPU ਨੂੰ ਕਾਫ਼ੀ ਲੋਡ ਨਹੀਂ ਕਰਦੇ। ਇਹ ਹੋਰ ਸਮਝਦਾਰੀ ਵਾਲੀ ਗੱਲ ਹੋਵੇਗੀ ਕਿ ਸਾਰੇ ਸੈਂਪਲਾਂ ਤੋਂ ਇੱਕ ਵੱਡਾ ਟੈਕਸਟ ਟੁਕੜਾ ਲਿਆ ਜਾਵੇ, ਫਿਰ ਸਾਰੇ ਇਨਪੁਟ-ਆਉਟਪੁਟ ਜੋੜੇ ਜਨਰੇਟ ਕੀਤੇ ਜਾਣ, ਉਨ੍ਹਾਂ ਨੂੰ ਸ਼ਫਲ ਕੀਤਾ ਜਾਵੇ, ਅਤੇ ਸਮਾਨ ਆਕਾਰ ਦੇ ਮਿਨੀਬੈਚ ਜਨਰੇਟ ਕੀਤੇ ਜਾਣ।\n",
    "\n",
    "* **ਮਲਟੀਲੇਅਰ LSTM**। 2 ਜਾਂ 3 ਲੇਅਰਾਂ ਦੇ LSTM ਸੈਲਜ਼ ਦੀ ਕੋਸ਼ਿਸ਼ ਕਰਨਾ ਸਮਝਦਾਰੀ ਵਾਲੀ ਗੱਲ ਹੈ। ਜਿਵੇਂ ਅਸੀਂ ਪਿਛਲੇ ਯੂਨਿਟ ਵਿੱਚ ਜ਼ਿਕਰ ਕੀਤਾ ਸੀ, LSTM ਦੀ ਹਰ ਲੇਅਰ ਟੈਕਸਟ ਤੋਂ ਕੁਝ ਪੈਟਰਨ ਕੱਢਦੀ ਹੈ, ਅਤੇ ਕੈਰੈਕਟਰ-ਲੈਵਲ ਜਨਰੇਟਰ ਦੇ ਮਾਮਲੇ ਵਿੱਚ ਅਸੀਂ ਉਮੀਦ ਕਰ ਸਕਦੇ ਹਾਂ ਕਿ ਹੇਠਲੀ LSTM ਲੇਅਰ ਅੱਖਰਾਂ ਨੂੰ ਕੱਢਣ ਲਈ ਜ਼ਿੰਮੇਵਾਰ ਹੋਵੇਗੀ, ਅਤੇ ਉੱਚੀ ਲੇਅਰਾਂ ਸ਼ਬਦ ਅਤੇ ਸ਼ਬਦਾਂ ਦੇ ਸੰਯੋਜਨਾਂ ਲਈ। ਇਹ ਸਿਰਫ਼ LSTM ਕੰਸਟ੍ਰਕਟਰ ਨੂੰ ਨੰਬਰ-ਆਫ-ਲੇਅਰ ਪੈਰਾਮੀਟਰ ਪਾਸ ਕਰਕੇ ਲਾਗੂ ਕੀਤਾ ਜਾ ਸਕਦਾ ਹੈ।\n",
    "\n",
    "* ਤੁਸੀਂ **GRU ਯੂਨਿਟਸ** ਨਾਲ ਪ੍ਰਯੋਗ ਕਰਨਾ ਚਾਹੁੰਦੇ ਹੋ ਸਕਦੇ ਹੋ ਅਤੇ ਵੇਖ ਸਕਦੇ ਹੋ ਕਿ ਕਿਹੜੇ ਵਧੀਆ ਕੰਮ ਕਰਦੇ ਹਨ, ਅਤੇ **ਵੱਖ-ਵੱਖ ਹਿਡਨ ਲੇਅਰ ਆਕਾਰਾਂ** ਨਾਲ। ਬਹੁਤ ਵੱਡਾ ਹਿਡਨ ਲੇਅਰ ਓਵਰਫਿਟਿੰਗ ਦਾ ਕਾਰਨ ਬਣ ਸਕਦਾ ਹੈ (ਜਿਵੇਂ ਕਿ ਨੈਟਵਰਕ ਸਹੀ ਟੈਕਸਟ ਸਿੱਖ ਲਵੇਗਾ), ਅਤੇ ਛੋਟਾ ਆਕਾਰ ਚੰਗਾ ਨਤੀਜਾ ਨਹੀਂ ਦੇ ਸਕਦਾ।\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ਨਰਮ ਪਾਠ ਪੈਦਾ ਕਰਨਾ ਅਤੇ ਤਾਪਮਾਨ\n",
    "\n",
    "ਪਿਛਲੀ ਵਿਆਖਿਆ ਵਿੱਚ `generate`, ਅਸੀਂ ਹਮੇਸ਼ਾ ਉਹ ਅੱਖਰ ਲੈਂਦੇ ਸੀ ਜਿਸਦੀ ਸੰਭਾਵਨਾ ਸਭ ਤੋਂ ਉੱਚੀ ਹੁੰਦੀ ਸੀ, ਇਸਨੂੰ ਪੈਦਾ ਕੀਤੇ ਗਏ ਪਾਠ ਵਿੱਚ ਅਗਲਾ ਅੱਖਰ ਬਣਾਉਣ ਲਈ ਵਰਤਿਆ ਜਾਂਦਾ ਸੀ। ਇਸ ਦਾ ਨਤੀਜਾ ਇਹ ਸੀ ਕਿ ਪਾਠ ਅਕਸਰ ਇੱਕੋ ਜਿਹੇ ਅੱਖਰ ਕ੍ਰਮਾਂ ਵਿੱਚ \"ਚੱਕਰ\" ਲਗਾਉਂਦਾ ਸੀ, ਜਿਵੇਂ ਕਿ ਇਸ ਉਦਾਹਰਨ ਵਿੱਚ:\n",
    "```\n",
    "today of the second the company and a second the company ...\n",
    "```\n",
    "\n",
    "ਹਾਲਾਂਕਿ, ਜੇ ਅਸੀਂ ਅਗਲੇ ਅੱਖਰ ਲਈ ਸੰਭਾਵਨਾ ਵੰਡ ਨੂੰ ਵੇਖੀਏ, ਤਾਂ ਇਹ ਹੋ ਸਕਦਾ ਹੈ ਕਿ ਕੁਝ ਸਭ ਤੋਂ ਉੱਚੀਆਂ ਸੰਭਾਵਨਾਵਾਂ ਵਿੱਚ ਅੰਤਰ ਬਹੁਤ ਵੱਡਾ ਨਾ ਹੋਵੇ, ਉਦਾਹਰਨ ਲਈ, ਇੱਕ ਅੱਖਰ ਦੀ ਸੰਭਾਵਨਾ 0.2 ਹੋ ਸਕਦੀ ਹੈ, ਦੂਜੇ ਦੀ - 0.19, ਆਦਿ। ਉਦਾਹਰਨ ਲਈ, ਜਦੋਂ ਅੱਖਰ ਕ੍ਰਮ '*play*' ਵਿੱਚ ਅਗਲੇ ਅੱਖਰ ਦੀ ਭਾਲ ਕਰਦੇ ਹਾਂ, ਤਾਂ ਅਗਲਾ ਅੱਖਰ ਇੱਕੋ ਜਿਹੇ ਤੌਰ 'ਤੇ ਖਾਲੀ ਜਗ੍ਹਾ ਹੋ ਸਕਦਾ ਹੈ, ਜਾਂ **e** (ਜਿਵੇਂ ਸ਼ਬਦ *player* ਵਿੱਚ)।\n",
    "\n",
    "ਇਸ ਤੋਂ ਸਾਨੂੰ ਇਹ ਨਤੀਜਾ ਕੱਢਣ ਨੂੰ ਮਿਲਦਾ ਹੈ ਕਿ ਹਮੇਸ਼ਾ ਸਭ ਤੋਂ ਉੱਚੀ ਸੰਭਾਵਨਾ ਵਾਲੇ ਅੱਖਰ ਨੂੰ ਚੁਣਨਾ \"ਨਿਆਂਯੋਗ\" ਨਹੀਂ ਹੈ, ਕਿਉਂਕਿ ਦੂਜੀ ਸਭ ਤੋਂ ਉੱਚੀ ਸੰਭਾਵਨਾ ਵਾਲੇ ਅੱਖਰ ਨੂੰ ਚੁਣਨਾ ਵੀ ਅਰਥਪੂਰਨ ਪਾਠ ਤੱਕ ਲੈ ਜਾ ਸਕਦਾ ਹੈ। ਇਹ ਜ਼ਿਆਦਾ ਸਮਝਦਾਰੀ ਵਾਲੀ ਗੱਲ ਹੈ ਕਿ ਨੈੱਟਵਰਕ ਆਉਟਪੁੱਟ ਦੁਆਰਾ ਦਿੱਤੀ ਗਈ ਸੰਭਾਵਨਾ ਵੰਡ ਤੋਂ ਅੱਖਰਾਂ ਨੂੰ **ਨਮੂਨਾ ਲੈ ਕੇ** ਚੁਣਿਆ ਜਾਵੇ।\n",
    "\n",
    "ਇਹ ਨਮੂਨਾ ਲੈਣ ਦੀ ਪ੍ਰਕਿਰਿਆ `multinomial` ਫੰਕਸ਼ਨ ਦੀ ਵਰਤੋਂ ਕਰਕੇ ਕੀਤੀ ਜਾ ਸਕਦੀ ਹੈ, ਜੋ ਕਿ ਇਸਨੂੰ **ਮਲਟੀਨੋਮਿਅਲ ਵੰਡ** ਕਿਹਾ ਜਾਂਦਾ ਹੈ। ਇੱਕ ਫੰਕਸ਼ਨ ਜੋ ਇਸ ਤਰ੍ਹਾਂ ਦੇ **ਨਰਮ** ਪਾਠ ਪੈਦਾ ਕਰਨ ਨੂੰ ਲਾਗੂ ਕਰਦਾ ਹੈ, ਹੇਠਾਂ ਦਿੱਤਾ ਗਿਆ ਹੈ:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Temperature = 0.3\n",
      "Today and a company and complete an all the land the restrational the as a security and has provers the pay to and a report and the computer in the stand has filities and working the law the stations for a company and with the company and the final the first company and refight of the state and and workin\n",
      "\n",
      "--- Temperature = 0.8\n",
      "Today he oniis its first to Aus bomblaties the marmation a to manan  boogot that pirate assaid a relaid their that goverfin the the Cappets Ecrotional Assonia Cition targets it annight the w scyments Blamity #39;s TVeer Diercheg Reserals fran envyuil that of ster said access what succers of Dour-provelith\n",
      "\n",
      "--- Temperature = 1.0\n",
      "Today holy they a 11 will meda a toket subsuaties, engins for Chanos, they's has stainger past to opening orital his thempting new Nattona was al innerforder advan-than #36;s night year his religuled talitatian what the but with Wednesday to Justment will wemen of Mark CCC Camp as Timed Nae wome a leaders\n",
      "\n",
      "--- Temperature = 1.3\n",
      "Today gpone 2.5 fech atcusion poor cocles toparsdorM.cht Line Pamage put 43 his calt lowed to the book, that has authh-the silia rruch ailing to'ory andhes beutirsimi- Aefffive heading offil an auf eacklets is charged evis, Gunymy oy) Mony has it after-sloythyor loveId out filme, the Natabl -Najuntaxiggs \n",
      "\n",
      "--- Temperature = 1.8\n",
      "Today plary, P.slan chly\\401 mardregationly #39;t 8.1Mide) closes ,filtcon alfly playin roven!\\grea.-QFBEP: Iss onfarchQ/itilia CCf Zivesigntwasta orce.-Peul-aw.uicrin of fuglinfsut aftaningwo, MIEX awayew Aice Woiduar Corvagiugge oppo esig ThusBratourid canthly-RyI.co lagitems\\eexciaishes.conBabntusmor I\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def generate_soft(net,size=100,start='today ',temperature=1.0):\n",
    "        chars = list(start)\n",
    "        out, s = net(enc(chars).view(1,-1).to(device))\n",
    "        for i in range(size):\n",
    "            #nc = torch.argmax(out[0][-1])\n",
    "            out_dist = out[0][-1].div(temperature).exp()\n",
    "            nc = torch.multinomial(out_dist,1)[0]\n",
    "            chars.append(vocab.get_itos()[nc])\n",
    "            out, s = net(nc.view(1,-1),s)\n",
    "        return ''.join(chars)\n",
    "    \n",
    "for i in [0.3,0.8,1.0,1.3,1.8]:\n",
    "    print(f\"--- Temperature = {i}\\n{generate_soft(net,size=300,start='Today ',temperature=i)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ਅਸੀਂ ਇੱਕ ਹੋਰ ਪੈਰਾਮੀਟਰ ਪੇਸ਼ ਕੀਤਾ ਹੈ ਜਿਸਨੂੰ **ਤਾਪਮਾਨ** ਕਿਹਾ ਜਾਂਦਾ ਹੈ, ਜੋ ਇਹ ਦਰਸਾਉਣ ਲਈ ਵਰਤਿਆ ਜਾਂਦਾ ਹੈ ਕਿ ਸਾਨੂੰ ਸਭ ਤੋਂ ਉੱਚੀ ਸੰਭਾਵਨਾ ਨਾਲ ਕਿੰਨਾ ਕੜਾਈ ਨਾਲ ਚਿਪਕਣਾ ਚਾਹੀਦਾ ਹੈ। ਜੇ ਤਾਪਮਾਨ 1.0 ਹੈ, ਤਾਂ ਅਸੀਂ ਨਿਆਇਕ ਮਲਟੀਨੋਮਿਅਲ ਸੈਂਪਲਿੰਗ ਕਰਦੇ ਹਾਂ, ਅਤੇ ਜਦੋਂ ਤਾਪਮਾਨ ਅਨੰਤ ਵੱਲ ਜਾਂਦਾ ਹੈ - ਸਾਰੀਆਂ ਸੰਭਾਵਨਾਵਾਂ ਬਰਾਬਰ ਹੋ ਜਾਂਦੀਆਂ ਹਨ, ਅਤੇ ਅਸੀਂ ਅਗਲਾ ਅੱਖਰ ਬੇਤਰਤੀਬੀ ਨਾਲ ਚੁਣਦੇ ਹਾਂ। ਹੇਠਾਂ ਦਿੱਤੇ ਉਦਾਹਰਨ ਵਿੱਚ ਅਸੀਂ ਦੇਖ ਸਕਦੇ ਹਾਂ ਕਿ ਜਦੋਂ ਅਸੀਂ ਤਾਪਮਾਨ ਨੂੰ ਬਹੁਤ ਜ਼ਿਆਦਾ ਵਧਾ ਦਿੰਦੇ ਹਾਂ ਤਾਂ ਪਾਠ ਬੇਮਤਲਬ ਹੋ ਜਾਂਦਾ ਹੈ, ਅਤੇ ਜਦੋਂ ਇਹ 0 ਦੇ ਨੇੜੇ ਹੋ ਜਾਂਦਾ ਹੈ ਤਾਂ ਇਹ \"ਚੱਕਰਦਾਰ\" ਕਠੋਰ-ਉਤਪੰਨ ਪਾਠ ਵਰਗਾ ਲੱਗਦਾ ਹੈ।\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**ਅਸਵੀਕਰਤੀ**:  \nਇਹ ਦਸਤਾਵੇਜ਼ AI ਅਨੁਵਾਦ ਸੇਵਾ [Co-op Translator](https://github.com/Azure/co-op-translator) ਦੀ ਵਰਤੋਂ ਕਰਕੇ ਅਨੁਵਾਦ ਕੀਤਾ ਗਿਆ ਹੈ। ਜਦੋਂ ਕਿ ਅਸੀਂ ਸਹੀ ਹੋਣ ਦਾ ਯਤਨ ਕਰਦੇ ਹਾਂ, ਕਿਰਪਾ ਕਰਕੇ ਧਿਆਨ ਦਿਓ ਕਿ ਸਵੈਚਾਲਿਤ ਅਨੁਵਾਦਾਂ ਵਿੱਚ ਗਲਤੀਆਂ ਜਾਂ ਅਸੁੱਤੀਆਂ ਹੋ ਸਕਦੀਆਂ ਹਨ। ਇਸ ਦੀ ਮੂਲ ਭਾਸ਼ਾ ਵਿੱਚ ਮੌਜੂਦ ਮੂਲ ਦਸਤਾਵੇਜ਼ ਨੂੰ ਅਧਿਕਾਰਤ ਸਰੋਤ ਮੰਨਿਆ ਜਾਣਾ ਚਾਹੀਦਾ ਹੈ। ਮਹੱਤਵਪੂਰਨ ਜਾਣਕਾਰੀ ਲਈ, ਪੇਸ਼ੇਵਰ ਮਨੁੱਖੀ ਅਨੁਵਾਦ ਦੀ ਸਿਫਾਰਸ਼ ਕੀਤੀ ਜਾਂਦੀ ਹੈ। ਇਸ ਅਨੁਵਾਦ ਦੇ ਪ੍ਰਯੋਗ ਤੋਂ ਪੈਦਾ ਹੋਣ ਵਾਲੇ ਕਿਸੇ ਵੀ ਗਲਤਫਹਮੀਆਂ ਜਾਂ ਗਲਤ ਵਿਆਖਿਆਵਾਂ ਲਈ ਅਸੀਂ ਜ਼ਿੰਮੇਵਾਰ ਨਹੀਂ ਹਾਂ।  \n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "16af2a8bbb083ea23e5e41c7f5787656b2ce26968575d8763f2c4b17f9cd711f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('py38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "coopTranslator": {
   "original_hash": "7673cd150d96c74c6d6011460094efb4",
   "translation_date": "2025-08-28T09:17:30+00:00",
   "source_file": "lessons/5-NLP/17-GenerativeNetworks/GenerativePyTorch.ipynb",
   "language_code": "pa"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}