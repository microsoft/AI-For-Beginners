# 신경망 프레임워크

이미 배운 바와 같이, 신경망을 효율적으로 학습시키기 위해서는 두 가지를 해야 합니다:

* 텐서를 다루는 작업, 예를 들어 곱셈, 덧셈, 그리고 sigmoid나 softmax 같은 함수 계산
* 모든 표현식의 기울기를 계산하여 경사 하강 최적화를 수행

## [강의 전 퀴즈](https://ff-quizzes.netlify.app/en/ai/quiz/9)

`numpy` 라이브러리는 첫 번째 작업을 수행할 수 있지만, 기울기를 계산할 메커니즘이 필요합니다. 이전 섹션에서 개발한 [우리의 프레임워크](../04-OwnFramework/OwnFramework.ipynb)에서는 `backward` 메서드 안에서 모든 미분 함수를 수동으로 프로그래밍해야 했습니다. 이상적으로는 프레임워크가 우리가 정의할 수 있는 *어떤 표현식*의 기울기를 계산할 수 있는 기능을 제공해야 합니다.

또 다른 중요한 점은 GPU나 [TPU](https://en.wikipedia.org/wiki/Tensor_Processing_Unit) 같은 특수한 계산 장치에서 계산을 수행할 수 있어야 한다는 것입니다. 딥러닝 신경망 학습은 *매우 많은* 계산을 필요로 하며, 이러한 계산을 GPU에서 병렬화할 수 있는 능력이 매우 중요합니다.

> ✅ '병렬화'라는 용어는 계산을 여러 장치에 분배하는 것을 의미합니다.

현재 가장 인기 있는 신경망 프레임워크는 [TensorFlow](http://TensorFlow.org)와 [PyTorch](https://pytorch.org/)입니다. 두 프레임워크 모두 CPU와 GPU에서 텐서를 다룰 수 있는 저수준 API를 제공합니다. 저수준 API 위에는 [Keras](https://keras.io/)와 [PyTorch Lightning](https://pytorchlightning.ai/)이라는 고수준 API도 있습니다.

저수준 API | [TensorFlow](http://TensorFlow.org) | [PyTorch](https://pytorch.org/)
------------|-------------------------------------|--------------------------------
고수준 API  | [Keras](https://keras.io/) | [PyTorch Lightning](https://pytorchlightning.ai/)

**저수준 API**는 두 프레임워크 모두에서 **계산 그래프**를 구축할 수 있게 합니다. 이 그래프는 주어진 입력 매개변수로 출력(보통 손실 함수)을 계산하는 방법을 정의하며, GPU가 사용 가능하다면 GPU에서 계산을 수행할 수 있습니다. 이 계산 그래프를 미분하고 기울기를 계산하는 함수가 있으며, 이를 통해 모델 매개변수를 최적화할 수 있습니다.

**고수준 API**는 신경망을 **레이어의 순서**로 간주하며, 대부분의 신경망을 훨씬 쉽게 구성할 수 있게 합니다. 모델 학습은 보통 데이터를 준비한 후 `fit` 함수를 호출하여 수행됩니다.

고수준 API는 일반적인 신경망을 매우 빠르게 구성할 수 있게 하며, 많은 세부 사항에 대해 걱정할 필요가 없습니다. 반면, 저수준 API는 학습 과정에 대한 훨씬 더 많은 제어를 제공하며, 새로운 신경망 아키텍처를 다룰 때 연구에서 많이 사용됩니다.

또한 두 API를 함께 사용할 수 있다는 점도 중요합니다. 예를 들어, 저수준 API를 사용하여 자체 네트워크 레이어 아키텍처를 개발한 후, 이를 고수준 API로 구성된 더 큰 네트워크 안에서 사용할 수 있습니다. 또는 고수준 API를 사용하여 레이어의 순서로 네트워크를 정의한 후, 자체 저수준 학습 루프를 사용하여 최적화를 수행할 수도 있습니다. 두 API는 동일한 기본 개념을 사용하며, 서로 잘 작동하도록 설계되었습니다.

## 학습

이 과정에서는 PyTorch와 TensorFlow에 대한 대부분의 내용을 제공합니다. 선호하는 프레임워크를 선택하고 해당 노트북만 학습하면 됩니다. 어떤 프레임워크를 선택해야 할지 모르겠다면, **PyTorch vs. TensorFlow**에 대한 인터넷 논의를 읽어보세요. 두 프레임워크를 살펴보며 더 나은 이해를 얻을 수도 있습니다.

가능한 경우, 간단함을 위해 고수준 API를 사용할 것입니다. 하지만 신경망이 기본적으로 어떻게 작동하는지 이해하는 것이 중요하다고 믿기 때문에, 처음에는 저수준 API와 텐서를 다루는 작업부터 시작합니다. 그러나 빠르게 시작하고 이러한 세부 사항을 배우는 데 많은 시간을 들이고 싶지 않다면, 이를 건너뛰고 고수준 API 노트북으로 바로 넘어갈 수 있습니다.

## ✍️ 연습: 프레임워크

다음 노트북에서 학습을 이어가세요:

저수준 API | [TensorFlow+Keras 노트북](IntroKerasTF.ipynb) | [PyTorch](IntroPyTorch.ipynb)
------------|-------------------------------------|--------------------------------
고수준 API  | [Keras](IntroKeras.ipynb) | *PyTorch Lightning*

프레임워크를 숙달한 후, 과적합의 개념을 다시 살펴보겠습니다.

# 과적합

과적합은 머신러닝에서 매우 중요한 개념이며, 이를 올바르게 이해하는 것이 중요합니다!

다음 그래프에서 5개의 점(`x`로 표시된 점)을 근사하는 문제를 고려해봅시다:

![linear](../../../../../translated_images/ko/overfit1.f24b71c6f652e59e.webp) | ![overfit](../../../../../translated_images/ko/overfit2.131f5800ae10ca5e.webp)
-------------------------|--------------------------
**선형 모델, 2개의 매개변수** | **비선형 모델, 7개의 매개변수**
학습 오류 = 5.3 | 학습 오류 = 0
검증 오류 = 5.1 | 검증 오류 = 20

* 왼쪽에서는 적절한 직선 근사가 보입니다. 매개변수의 수가 적절하기 때문에 모델이 점 분포의 패턴을 올바르게 이해합니다.
* 오른쪽에서는 모델이 너무 강력합니다. 점이 5개밖에 없고 모델의 매개변수가 7개이기 때문에, 모든 점을 통과하도록 조정할 수 있어 학습 오류가 0이 됩니다. 하지만 이는 데이터의 올바른 패턴을 이해하지 못하게 하여 검증 오류가 매우 높아집니다.

모델의 복잡도(매개변수 수)와 학습 샘플 수 사이의 적절한 균형을 맞추는 것이 매우 중요합니다.

## 과적합이 발생하는 이유

  * 학습 데이터가 부족함
  * 모델이 너무 강력함
  * 입력 데이터에 너무 많은 노이즈가 있음

## 과적합을 감지하는 방법

위 그래프에서 볼 수 있듯이, 과적합은 매우 낮은 학습 오류와 높은 검증 오류로 감지할 수 있습니다. 일반적으로 학습 중에는 학습 오류와 검증 오류가 모두 감소하다가, 어느 시점에서 검증 오류가 감소를 멈추고 증가하기 시작할 수 있습니다. 이는 과적합의 신호이며, 이 시점에서 학습을 멈추거나 모델의 스냅샷을 저장해야 한다는 표시입니다.

![overfitting](../../../../../translated_images/ko/Overfitting.408ad91cd90b4371.webp)

## 과적합을 방지하는 방법

과적합이 발생한다고 판단되면 다음 중 하나를 수행할 수 있습니다:

 * 학습 데이터 양을 늘리기
 * 모델의 복잡도를 줄이기
 * [Dropout](../../4-ComputerVision/08-TransferLearning/TrainingTricks.md#Dropout)과 같은 [정규화 기법](../../4-ComputerVision/08-TransferLearning/TrainingTricks.md)을 사용하기 (이후에 다룰 예정)

## 과적합과 Bias-Variance Tradeoff

과적합은 통계학에서 [Bias-Variance Tradeoff](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff)라는 더 일반적인 문제의 사례입니다. 모델의 오류 원인을 고려하면 두 가지 유형의 오류를 볼 수 있습니다:

* **Bias 오류**는 알고리즘이 학습 데이터 간의 관계를 올바르게 포착하지 못해 발생합니다. 이는 모델이 충분히 강력하지 않을 때 발생할 수 있습니다 (**과소적합**).
* **Variance 오류**는 모델이 입력 데이터의 노이즈를 의미 있는 관계 대신 근사하려고 할 때 발생합니다 (**과적합**).

학습 중에는 Bias 오류가 감소하고(모델이 데이터를 근사하는 법을 배우면서), Variance 오류가 증가합니다. 과적합을 방지하기 위해 학습을 멈추는 것이 중요합니다. 이는 수동으로(과적합을 감지했을 때) 또는 자동으로(정규화를 도입하여) 이루어질 수 있습니다.

## 결론

이 강의에서는 TensorFlow와 PyTorch라는 두 가지 인기 있는 AI 프레임워크의 다양한 API 차이점을 배웠습니다. 또한 과적합이라는 매우 중요한 주제에 대해 배웠습니다.

## 🚀 도전 과제

첨부된 노트북의 하단에서 '작업'을 찾을 수 있습니다. 노트북을 학습하고 작업을 완료하세요.

## [강의 후 퀴즈](https://ff-quizzes.netlify.app/en/ai/quiz/10)

## 복습 및 자기 학습

다음 주제에 대해 연구해보세요:

- TensorFlow
- PyTorch
- 과적합

다음 질문을 스스로에게 던져보세요:

- TensorFlow와 PyTorch의 차이점은 무엇인가요?
- 과적합과 과소적합의 차이점은 무엇인가요?

## [과제](lab/README.md)

이 실습에서는 PyTorch 또는 TensorFlow를 사용하여 단일 및 다층 완전 연결 네트워크를 통해 두 가지 분류 문제를 해결해야 합니다.

* [지침](lab/README.md)
* [노트북](lab/LabFrameworks.ipynb)

---

