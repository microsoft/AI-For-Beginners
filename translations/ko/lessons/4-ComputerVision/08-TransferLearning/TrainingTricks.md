# 딥러닝 학습 팁

신경망이 깊어질수록 학습 과정이 점점 더 어려워집니다. 주요 문제 중 하나는 [소실 기울기](https://en.wikipedia.org/wiki/Vanishing_gradient_problem) 또는 [폭발 기울기](https://deepai.org/machine-learning-glossary-and-terms/exploding-gradient-problem#:~:text=Exploding%20gradients%20are%20a%20problem,updates%20are%20small%20and%20controlled.)입니다. [이 글](https://towardsdatascience.com/the-vanishing-exploding-gradient-problem-in-deep-neural-networks-191358470c11)은 이러한 문제에 대한 좋은 소개를 제공합니다.

깊은 네트워크의 학습을 더 효율적으로 만들기 위해 사용할 수 있는 몇 가지 기술이 있습니다.

## 값을 적절한 범위로 유지하기

수치 계산을 더 안정적으로 만들기 위해, 신경망 내 모든 값이 적절한 범위, 일반적으로 [-1..1] 또는 [0..1]에 있도록 해야 합니다. 이는 엄격한 요구 사항은 아니지만, 부동소수점 계산의 특성상 서로 다른 크기의 값을 정확히 조작하기 어렵습니다. 예를 들어, 10<sup>-10</sup>과 10<sup>10</sup>을 더하면, 작은 값이 큰 값과 동일한 차수로 "변환"되어 가수부가 손실되기 때문에 10<sup>10</sup>이 될 가능성이 높습니다.

대부분의 활성화 함수는 [-1..1] 범위에서 비선형성을 가지므로, 모든 입력 데이터를 [-1..1] 또는 [0..1] 범위로 스케일링하는 것이 합리적입니다.

## 초기 가중치 초기화

이상적으로는 네트워크 계층을 통과한 후에도 값이 동일한 범위에 있도록 하고 싶습니다. 따라서 가중치를 초기화할 때 값의 분포를 유지하도록 설정하는 것이 중요합니다.

정규 분포 **N(0,1)**는 좋은 선택이 아닙니다. 입력이 *n*개라면 출력의 표준 편차는 *n*이 되고, 값이 [0..1] 범위를 벗어날 가능성이 높기 때문입니다.

다음과 같은 초기화 방법이 자주 사용됩니다:

- 균등 분포 -- `uniform`
- **N(0,1/n)** -- `gaussian`
- **N(0,1/√n_in)**: 평균이 0이고 표준 편차가 1인 입력에 대해 동일한 평균/표준 편차를 유지
- **N(0,√2/(n_in+n_out))**: **Xavier 초기화** (`glorot`)라고 불리며, 순전파와 역전파 동안 신호를 범위 내에 유지하는 데 도움을 줌

## 배치 정규화

적절한 가중치 초기화에도 불구하고, 학습 중 가중치가 임의로 커지거나 작아질 수 있으며, 이로 인해 신호가 적절한 범위를 벗어날 수 있습니다. **정규화** 기술을 사용하여 신호를 다시 적절한 범위로 되돌릴 수 있습니다. 여러 정규화 기법(가중치 정규화, 계층 정규화 등)이 있지만, 가장 자주 사용되는 것은 배치 정규화입니다.

**배치 정규화**의 아이디어는 미니배치의 모든 값을 고려하여 평균을 빼고 표준 편차로 나누는 방식으로 정규화를 수행하는 것입니다. 이는 가중치를 적용한 후, 활성화 함수 전에 수행되는 네트워크 계층으로 구현됩니다. 결과적으로 더 높은 최종 정확도와 더 빠른 학습을 기대할 수 있습니다.

여기 배치 정규화에 대한 [원본 논문](https://arxiv.org/pdf/1502.03167.pdf), [위키백과 설명](https://en.wikipedia.org/wiki/Batch_normalization), 그리고 [좋은 입문 블로그 글](https://towardsdatascience.com/batch-normalization-in-3-levels-of-understanding-14c2da90a338) (그리고 [러시아어 버전](https://habrahabr.ru/post/309302/))이 있습니다.

## 드롭아웃

**드롭아웃**은 학습 중 무작위로 일정 비율의 뉴런을 제거하는 흥미로운 기술입니다. 이는 하나의 매개변수(제거할 뉴런의 비율, 일반적으로 10%-50%)를 가진 계층으로 구현되며, 학습 중 입력 벡터의 무작위 요소를 0으로 설정한 후 다음 계층으로 전달합니다.

이것이 이상하게 들릴 수 있지만, [`Dropout.ipynb`](../../../../../lessons/4-ComputerVision/08-TransferLearning/Dropout.ipynb) 노트북에서 MNIST 숫자 분류기를 학습할 때 드롭아웃의 효과를 확인할 수 있습니다. 이는 학습 속도를 높이고 더 적은 학습 에포크로 더 높은 정확도를 달성할 수 있게 합니다.

이 효과는 여러 가지로 설명될 수 있습니다:

- 모델에 무작위 충격 요인을 제공하여 최적화가 국소 최소값에서 벗어나도록 할 수 있습니다.
- *암묵적 모델 평균화*로 간주될 수 있습니다. 드롭아웃 동안 약간 다른 모델을 학습한다고 볼 수 있기 때문입니다.

> *어떤 사람들은 술에 취한 사람이 무언가를 배우려고 할 때, 술에 취하지 않은 사람보다 다음 날 더 잘 기억한다고 말합니다. 이는 일부 오작동하는 뉴런을 가진 뇌가 의미를 더 잘 파악하려고 적응하기 때문이라고 합니다. 우리가 이 말이 사실인지 아닌지는 테스트해본 적이 없습니다.*

## 과적합 방지

딥러닝에서 매우 중요한 측면 중 하나는 [과적합](../../3-NeuralNetworks/05-Frameworks/Overfitting.md)을 방지하는 것입니다. 매우 강력한 신경망 모델을 사용하는 것이 유혹적일 수 있지만, 항상 모델 매개변수의 수와 학습 샘플 수의 균형을 맞춰야 합니다.

> 이전에 소개한 [과적합](../../3-NeuralNetworks/05-Frameworks/Overfitting.md)의 개념을 반드시 이해하세요!

과적합을 방지하는 몇 가지 방법이 있습니다:

- 조기 종료(Early stopping): 검증 세트의 오류를 지속적으로 모니터링하고, 검증 오류가 증가하기 시작하면 학습을 중단
- 명시적 가중치 감소 / 정규화: 손실 함수에 가중치의 절대값이 큰 경우 추가 페널티를 부여하여 모델이 매우 불안정한 결과를 내는 것을 방지
- 모델 평균화: 여러 모델을 학습한 후 결과를 평균화. 이는 분산을 최소화하는 데 도움을 줌
- 드롭아웃 (암묵적 모델 평균화)

## 최적화 알고리즘 / 학습 알고리즘

학습에서 또 다른 중요한 측면은 좋은 학습 알고리즘을 선택하는 것입니다. 고전적인 **경사 하강법**은 합리적인 선택이지만, 때로는 너무 느리거나 다른 문제를 초래할 수 있습니다.

딥러닝에서는 **확률적 경사 하강법**(SGD)을 사용합니다. 이는 학습 세트에서 무작위로 선택된 미니배치에 대해 경사 하강법을 적용하는 방식입니다. 가중치는 다음 공식을 사용하여 조정됩니다:

w<sup>t+1</sup> = w<sup>t</sup> - η∇ℒ

### 모멘텀

**모멘텀 SGD**에서는 이전 단계의 기울기의 일부를 유지합니다. 이는 우리가 관성으로 어딘가로 이동하고 있을 때, 다른 방향에서 충격을 받으면 즉시 궤적이 바뀌지 않고 원래 움직임의 일부를 유지하는 것과 유사합니다. 여기서 *속도*를 나타내는 또 다른 벡터 v를 도입합니다:

- v<sup>t+1</sup> = γ v<sup>t</sup> - η∇ℒ
- w<sup>t+1</sup> = w<sup>t</sup> + v<sup>t+1</sup>

여기서 매개변수 γ는 관성을 얼마나 고려할지를 나타냅니다: γ=0은 고전적인 SGD에 해당하며, γ=1은 순수한 운동 방정식입니다.

### Adam, Adagrad 등

각 계층에서 신호를 어떤 행렬 W<sub>i</sub>로 곱하기 때문에, ||W<sub>i</sub>||에 따라 기울기가 0에 가까워지거나 무한히 커질 수 있습니다. 이것이 소실/폭발 기울기 문제의 본질입니다.

이 문제를 해결하는 방법 중 하나는 방정식에서 기울기의 방향만 사용하고 절대값은 무시하는 것입니다. 즉,

w<sup>t+1</sup> = w<sup>t</sup> - η(∇ℒ/||∇ℒ||), 여기서 ||∇ℒ|| = √∑(∇ℒ)<sup>2</sup>

이 알고리즘은 **Adagrad**라고 불립니다. 동일한 아이디어를 사용하는 다른 알고리즘으로는 **RMSProp**, **Adam**이 있습니다.

> **Adam**은 많은 응용 분야에서 매우 효율적인 알고리즘으로 간주되므로, 어떤 것을 사용할지 확신이 없다면 Adam을 사용하세요.

### 기울기 클리핑

기울기 클리핑은 위 아이디어의 확장입니다. ||∇ℒ|| ≤ θ일 때는 가중치 최적화에서 원래 기울기를 고려하고, ||∇ℒ|| > θ일 때는 기울기를 그 노름으로 나눕니다. 여기서 θ는 매개변수이며, 대부분의 경우 θ=1 또는 θ=10을 사용할 수 있습니다.

### 학습률 감소

학습 성공은 종종 학습률 매개변수 η에 달려 있습니다. 학습 초반에는 η 값이 클수록 학습 속도가 빨라지고, 이후에는 η 값을 작게 하여 네트워크를 미세 조정하는 것이 논리적입니다. 따라서 대부분의 경우 학습 과정에서 η를 점진적으로 줄이고자 합니다.

이는 학습 에포크마다 η에 어떤 숫자(예: 0.98)를 곱하거나, 더 복잡한 **학습률 스케줄**을 사용하는 방식으로 수행할 수 있습니다.

## 다양한 네트워크 아키텍처

문제에 적합한 네트워크 아키텍처를 선택하는 것은 까다로울 수 있습니다. 일반적으로는 특정 작업(또는 유사한 작업)에 대해 효과가 입증된 아키텍처를 선택합니다. 여기 [컴퓨터 비전용 신경망 아키텍처에 대한 좋은 개요](https://www.topbots.com/a-brief-history-of-neural-network-architectures/)가 있습니다.

> 우리가 가진 학습 샘플 수에 대해 충분히 강력한 아키텍처를 선택하는 것이 중요합니다. 너무 강력한 모델을 선택하면 [과적합](../../3-NeuralNetworks/05-Frameworks/Overfitting.md)이 발생할 수 있습니다.

또 다른 좋은 방법은 필요한 복잡성에 자동으로 조정되는 아키텍처를 사용하는 것입니다. 어느 정도까지는 **ResNet** 아키텍처와 **Inception**이 자동 조정됩니다. [컴퓨터 비전 아키텍처에 대한 추가 정보](../07-ConvNets/CNN_Architectures.md)

**면책 조항**:  
이 문서는 AI 번역 서비스 [Co-op Translator](https://github.com/Azure/co-op-translator)를 사용하여 번역되었습니다. 정확성을 위해 최선을 다하고 있지만, 자동 번역에는 오류나 부정확성이 포함될 수 있습니다. 원본 문서를 해당 언어로 작성된 상태에서 권위 있는 자료로 간주해야 합니다. 중요한 정보의 경우, 전문적인 인간 번역을 권장합니다. 이 번역 사용으로 인해 발생하는 오해나 잘못된 해석에 대해 당사는 책임을 지지 않습니다.