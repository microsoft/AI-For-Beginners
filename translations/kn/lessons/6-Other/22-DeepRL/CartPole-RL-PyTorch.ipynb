{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ಕಾರ್ಟ್‌ಪೋಲ್ ಸಮತೋಲನ ಮಾಡಲು RL ತರಬೇತಿ\n",
    "\n",
    "ಈ ನೋಟ್ಬುಕ್ [AI for Beginners Curriculum](http://aka.ms/ai-beginners) ಭಾಗವಾಗಿದೆ. ಇದಕ್ಕೆ ಪ್ರೇರಣೆ ನೀಡಿದ್ದು [ಅಧಿಕೃತ PyTorch ಟ್ಯುಟೋರಿಯಲ್](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html) ಮತ್ತು [ಈ Cartpole PyTorch ಅನುಷ್ಠಾನ](https://github.com/yc930401/Actor-Critic-pytorch).\n",
    "\n",
    "ಈ ಉದಾಹರಣೆಯಲ್ಲಿ, ನಾವು RL ಬಳಸಿ ಕಾರ್ಟ್ ಮೇಲೆ ಸಮತೋಲನ ಸಾಧಿಸಲು ಮಾದರಿಯನ್ನು ತರಬೇತಿಮಾಡುತ್ತೇವೆ, ಕಾರ್ಟ್ ಎಡಕ್ಕೆ ಮತ್ತು ಬಲಕ್ಕೆ ಹೋರಿಜಾಂಟಲ್ ಮಾಪಕದಲ್ಲಿ ಚಲಿಸಬಹುದು. ನಾವು [OpenAI Gym](https://www.gymlibrary.ml/) ಪರಿಸರವನ್ನು ಪೋಲ್ ಅನುಕರಿಸಲು ಬಳಸುತ್ತೇವೆ.\n",
    "\n",
    "> **Note**: ನೀವು ಈ ಪಾಠದ ಕೋಡ್ ಅನ್ನು ಸ್ಥಳೀಯವಾಗಿ (ಉದಾ. Visual Studio Code ನಿಂದ) ಚಾಲನೆ ಮಾಡಬಹುದು, ಆ ಸಂದರ್ಭದಲ್ಲಿ ಅನುಕರಣ ಹೊಸ ವಿಂಡೋದಲ್ಲಿ ತೆರೆಯುತ್ತದೆ. ಆನ್‌ಲೈನ್‌ನಲ್ಲಿ ಕೋಡ್ ಚಾಲನೆ ಮಾಡುವಾಗ, ಕೆಲವು ಬದಲಾವಣೆಗಳನ್ನು ಮಾಡಬೇಕಾಗಬಹುದು, ಇದನ್ನು [ಇಲ್ಲಿ](https://towardsdatascience.com/rendering-openai-gym-envs-on-binder-and-google-colab-536f99391cc7) ವಿವರಿಸಲಾಗಿದೆ.\n",
    "\n",
    "ನಾವು ಮೊದಲು Gym ಸ್ಥಾಪಿತವಾಗಿದೆ ಎಂದು ಖಚಿತಪಡಿಸಿಕೊಳ್ಳೋಣ:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ಈಗ ನಾವು CartPole ಪರಿಸರವನ್ನು ರಚಿಸಿ ಅದನ್ನು ಹೇಗೆ ಕಾರ್ಯಗತಗೊಳಿಸಬಹುದು ಎಂದು ನೋಡೋಣ. ಒಂದು ಪರಿಸರದಲ್ಲಿ ಕೆಳಗಿನ ಗುಣಲಕ್ಷಣಗಳಿರುತ್ತವೆ:\n",
    "\n",
    "* **ಕ್ರಿಯೆ ಸ್ಥಳ** ಎಂದರೆ ಸಿಮ್ಯುಲೇಶನ್‌ನ ಪ್ರತಿ ಹಂತದಲ್ಲಿ ನಾವು ನಡೆಸಬಹುದಾದ ಸಾಧ್ಯವಾದ ಕ್ರಿಯೆಗಳ ಸಮೂಹ\n",
    "* **ನಿರೀಕ್ಷಣಾ ಸ್ಥಳ** ಎಂದರೆ ನಾವು ಮಾಡಬಹುದಾದ ನಿರೀಕ್ಷಣೆಗಳ ಸ್ಥಳ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "print(f\"Action space: {env.action_space}\")\n",
    "print(f\"Observation space: {env.observation_space}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ನಾವು ಸಿಮ್ಯುಲೇಶನ್ ಹೇಗೆ ಕಾರ್ಯನಿರ್ವಹಿಸುತ್ತದೆ ಎಂದು ನೋಡೋಣ. ಕೆಳಗಿನ ಲೂಪ್ ಸಿಮ್ಯುಲೇಶನ್ ಅನ್ನು ಚಲಾಯಿಸುತ್ತದೆ, `env.step` ಮುಗింపు ಧ್ವಜ `done` ಅನ್ನು ಹಿಂತಿರುಗಿಸುವವರೆಗೆ. ನಾವು ಕ್ರಿಯೆಗಳನ್ನು ಯಾದೃಚ್ಛಿಕವಾಗಿ ಆಯ್ಕೆಮಾಡುತ್ತೇವೆ `env.action_space.sample()` ಬಳಸಿ, ಅಂದರೆ ಪ್ರಯೋಗ ಬಹುಶಃ ಬಹಳ ಬೇಗ ವಿಫಲವಾಗಬಹುದು (CartPole ಪರಿಸರವು CartPole ನ ವೇಗ, ಅದರ ಸ್ಥಾನ ಅಥವಾ ಕೋನವು ನಿರ್ದಿಷ್ಟ ಮಿತಿಗಳನ್ನು ಮೀರಿ ಹೋದಾಗ ಮುಗಿಯುತ್ತದೆ).\n",
    "\n",
    "> ಸಿಮ್ಯುಲೇಶನ್ ಹೊಸ ವಿಂಡೋದಲ್ಲಿ ತೆರೆಯುತ್ತದೆ. ನೀವು ಕೋಡ್ ಅನ್ನು ಹಲವಾರು ಬಾರಿ ಚಲಾಯಿಸಿ ಅದರ ವರ್ತನೆ ಹೇಗಿದೆ ಎಂದು ನೋಡಬಹುದು.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "\n",
    "done = False\n",
    "total_reward = 0\n",
    "while not done:\n",
    "   env.render()\n",
    "   obs, rew, done, info = env.step(env.action_space.sample())\n",
    "   total_reward += rew\n",
    "   print(f\"{obs} -> {rew}\")\n",
    "print(f\"Total reward: {total_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ನೀವು ಗಮನಿಸಬಹುದು, ಅವಲೋಕನಗಳಲ್ಲಿ 4 ಸಂಖ್ಯೆಗಳು ಇರುತ್ತವೆ. ಅವುಗಳೇ:\n",
    "- ಕಾರ್ಟ್‌ನ ಸ್ಥಾನ\n",
    "- ಕಾರ್ಟ್‌ನ ವೇಗ\n",
    "- ಕಂಬದ ಕೋನ\n",
    "- ಕಂಬದ ತಿರುಗುವ ದರ\n",
    "\n",
    "`rew` ಎಂದರೆ ನಾವು ಪ್ರತಿ ಹಂತದಲ್ಲಿ ಪಡೆಯುವ ಬಹುಮಾನ. CartPole ಪರಿಸರದಲ್ಲಿ ಪ್ರತಿ ಸಿಮ್ಯುಲೇಶನ್ ಹಂತಕ್ಕೆ ನೀವು 1 ಪಾಯಿಂಟ್ ಬಹುಮಾನ ಪಡೆಯುತ್ತೀರಿ, ಮತ್ತು ಗುರಿ ಒಟ್ಟು ಬಹುಮಾನವನ್ನು ಗರಿಷ್ಠಗೊಳಿಸುವುದು, ಅಂದರೆ CartPole ಬಿದ್ದಿಲ್ಲದೆ ಸಮತೋಲನ ಸಾಧಿಸುವ ಸಮಯವನ್ನು ಹೆಚ್ಚಿಸುವುದು.\n",
    "\n",
    "ಪುನರಾವರ್ತಿತ ಕಲಿಕೆಯಲ್ಲಿ, ನಮ್ಮ ಗುರಿ ಒಂದು **ನೀತಿ** $\\pi$ ಅನ್ನು ತರಬೇತುಗೊಳಿಸುವುದು, ಅದು ಪ್ರತಿ ಸ್ಥಿತಿಗೆ $s$ ನಾವು ಯಾವ ಕ್ರಿಯೆ $a$ ತೆಗೆದುಕೊಳ್ಳಬೇಕು ಎಂದು ಹೇಳುತ್ತದೆ, ಅಂದರೆ $a = \\pi(s)$.\n",
    "\n",
    "ನೀವು ಪ್ರಾಬಬಿಲಿಸ್ಟಿಕ್ ಪರಿಹಾರವನ್ನು ಬಯಸಿದರೆ, ನೀತಿ ಪ್ರತಿ ಕ್ರಿಯೆಗೆ ಸಾಧ್ಯತೆಗಳ ಸಮೂಹವನ್ನು ನೀಡುತ್ತದೆ ಎಂದು ಭಾವಿಸಬಹುದು, ಅಂದರೆ $\\pi(a|s)$ ಎಂದರೆ ಸ್ಥಿತಿ $s$ ನಲ್ಲಿ ನಾವು ಕ್ರಿಯೆ $a$ ತೆಗೆದುಕೊಳ್ಳಬೇಕಾದ ಸಾಧ್ಯತೆ.\n",
    "\n",
    "## ನೀತಿ ಗ್ರೇಡಿಯಂಟ್ ವಿಧಾನ\n",
    "\n",
    "ಸರಳವಾದ RL ಅಲ್ಗೋರಿದಮ್, **ನೀತಿ ಗ್ರೇಡಿಯಂಟ್** ಎಂದು ಕರೆಯಲ್ಪಡುವುದರಲ್ಲಿ, ನಾವು ಮುಂದಿನ ಕ್ರಿಯೆಯನ್ನು ಊಹಿಸಲು ನ್ಯೂರಲ್ ನೆಟ್‌ವರ್ಕ್ ಅನ್ನು ತರಬೇತುಗೊಳಿಸುತ್ತೇವೆ.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "num_inputs = 4\n",
    "num_actions = 2\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(num_inputs, 128, bias=False, dtype=torch.float32),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(128, num_actions, bias = False, dtype=torch.float32),\n",
    "    torch.nn.Softmax(dim=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ನಾವು ಹಲವಾರು ಪ್ರಯೋಗಗಳನ್ನು ನಡೆಸಿ ನೆಟ್‌ವರ್ಕ್ ಅನ್ನು ತರಬೇತುಗೊಳಿಸುವೆವು, ಮತ್ತು ಪ್ರತಿ ರನ್ ನಂತರ ನಮ್ಮ ನೆಟ್‌ವರ್ಕ್ ಅನ್ನು ನವೀಕರಿಸುವೆವು. ಪ್ರಯೋಗವನ್ನು ನಡೆಸಿ ಫಲಿತಾಂಶಗಳನ್ನು (ಹೆಸರಿಸಿದಂತೆ **ಟ್ರೇಸ್**) ಹಿಂತಿರುಗಿಸುವ ಫಂಕ್ಷನ್ ಅನ್ನು ವ್ಯಾಖ್ಯಾನಿಸೋಣ - ಎಲ್ಲಾ ಸ್ಥಿತಿಗಳು, ಕ್ರಿಯೆಗಳು (ಮತ್ತು ಅವುಗಳ ಶಿಫಾರಸು ಮಾಡಿದ ಸಾಧ್ಯತೆಗಳು), ಮತ್ತು ಬಹುಮಾನಗಳು:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(max_steps_per_episode = 10000,render=False):    \n",
    "    states, actions, probs, rewards = [],[],[],[]\n",
    "    state = env.reset()\n",
    "    for _ in range(max_steps_per_episode):\n",
    "        if render:\n",
    "            env.render()\n",
    "        action_probs = model(torch.from_numpy(np.expand_dims(state,0)))[0]\n",
    "        action = np.random.choice(num_actions, p=np.squeeze(action_probs.detach().numpy()))\n",
    "        nstate, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            break\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        probs.append(action_probs.detach().numpy())\n",
    "        rewards.append(reward)\n",
    "        state = nstate\n",
    "    return np.vstack(states), np.vstack(actions), np.vstack(probs), np.vstack(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ನೀವು ಒಂದು ಎಪಿಸೋಡ್ ಅನ್ನು ತರಬೇತಿಗೊಳ್ಳದ ನೆಟ್‌ವರ್ಕ್‌ನೊಂದಿಗೆ ನಡೆಸಬಹುದು ಮತ್ತು ಒಟ್ಟು ಬಹುಮಾನ (ಅಥವಾ ಎಪಿಸೋಡ್‌ನ ಉದ್ದ) ತುಂಬಾ ಕಡಿಮೆ ಎಂದು ಗಮನಿಸಬಹುದು:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s, a, p, r = run_episode()\n",
    "print(f\"Total reward: {np.sum(r)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ನೀತಿ ಗ್ರೇಡಿಯಂಟ್ ಆಲ್ಗೋರಿದಮ್‌ನ ಒಂದು ಕಷ್ಟಕರ ಅಂಶವೆಂದರೆ **ರಿಯಾಯಿತಿಯ наградыಗಳನ್ನು** ಬಳಸುವುದು. ಆ ಕಲ್ಪನೆ ಎಂದರೆ ನಾವು ಆಟದ ಪ್ರತಿ ಹಂತದಲ್ಲಿ ಒಟ್ಟು ರಿಯಾಯಿತಿಗಳ ವೆಕ್ಟರ್ ಅನ್ನು ಲೆಕ್ಕಹಾಕುತ್ತೇವೆ, ಮತ್ತು ಈ ಪ್ರಕ್ರಿಯೆಯಲ್ಲಿ ನಾವು ಆರಂಭಿಕ ರಿಯಾಯಿತಿಗಳನ್ನು ಕೆಲವು ಗುಣಾಂಕ $gamma$ ಬಳಸಿ ರಿಯಾಯಿತಿಗೊಳಿಸುತ್ತೇವೆ. ನಾವು ಫಲಿತಾಂಶ ವೆಕ್ಟರ್ ಅನ್ನು ಸಾಮಾನ್ಯೀಕರಿಸುತ್ತೇವೆ, ಏಕೆಂದರೆ ನಾವು ಅದನ್ನು ತರಬೇತಿಗೆ ಪ್ರಭಾವ ಬೀರುವ ತೂಕವಾಗಿ ಬಳಸುತ್ತೇವೆ:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 0.0001\n",
    "\n",
    "def discounted_rewards(rewards,gamma=0.99,normalize=True):\n",
    "    ret = []\n",
    "    s = 0\n",
    "    for r in rewards[::-1]:\n",
    "        s = r + gamma * s\n",
    "        ret.insert(0, s)\n",
    "    if normalize:\n",
    "        ret = (ret-np.mean(ret))/(np.std(ret)+eps)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ಈಗ ನಿಜವಾದ ತರಬೇತಿಯನ್ನು ಮಾಡೋಣ! ನಾವು 300 ಎಪಿಸೋಡ್‌ಗಳನ್ನು ನಡೆಸುತ್ತೇವೆ, ಮತ್ತು ಪ್ರತಿ ಎಪಿಸೋಡ್‌ನಲ್ಲಿ ನಾವು ಕೆಳಗಿನವುಗಳನ್ನು ಮಾಡುತ್ತೇವೆ:\n",
    "\n",
    "1. ಪ್ರಯೋಗವನ್ನು ನಡೆಸಿ ಮತ್ತು ಟ್ರೇಸ್ ಅನ್ನು ಸಂಗ್ರಹಿಸಿ  \n",
    "1. ತೆಗೆದುಕೊಂಡ ಕ್ರಿಯೆಗಳು ಮತ್ತು ಭವಿಷ್ಯವಾಣಿ ಪ್ರಾಬಬಿಲಿಟಿಗಳ ನಡುವಿನ ವ್ಯತ್ಯಾಸವನ್ನು (`gradients`) ಲೆಕ್ಕಹಾಕಿ. ವ್ಯತ್ಯಾಸ ಕಡಿಮೆ ಇದ್ದರೆ, ನಾವು ಸರಿಯಾದ ಕ್ರಿಯೆಯನ್ನು ತೆಗೆದುಕೊಂಡಿದ್ದೇವೆ ಎಂದು ಹೆಚ್ಚು ಖಚಿತವಾಗಿರುತ್ತೇವೆ.  \n",
    "1. ಡಿಸ್ಕೌಂಟೆಡ್ ರಿವಾರ್ಡ್‌ಗಳನ್ನು ಲೆಕ್ಕಿಸಿ ಮತ್ತು gradients ಅನ್ನು ಡಿಸ್ಕೌಂಟೆಡ್ ರಿವಾರ್ಡ್‌ಗಳೊಂದಿಗೆ ಗುಣಿಸಿ - ಇದರಿಂದ ಹೆಚ್ಚಿನ ರಿವಾರ್ಡ್‌ಗಳಿರುವ ಹಂತಗಳು ಕಡಿಮೆ ರಿವಾರ್ಡ್‌ಗಳಿಗಿಂತ ಅಂತಿಮ ಫಲಿತಾಂಶದ ಮೇಲೆ ಹೆಚ್ಚು ಪ್ರಭಾವ ಬೀರುತ್ತವೆ.  \n",
    "1. ನಮ್ಮ ನ್ಯೂರಲ್ ನೆಟ್‌ವರ್ಕ್‌ಗೆ ನಿರೀಕ್ಷಿತ ಗುರಿ ಕ್ರಿಯೆಗಳು ಭಾಗಶಃ ರನ್ ಸಮಯದಲ್ಲಿ ಭವಿಷ್ಯವಾಣಿ ಪ್ರಾಬಬಿಲಿಟಿಗಳಿಂದ ಮತ್ತು ಭಾಗಶಃ ಲೆಕ್ಕಹಾಕಿದ gradients ನಿಂದ ತೆಗೆದುಕೊಳ್ಳಲಾಗುತ್ತದೆ. gradients ಮತ್ತು ರಿವಾರ್ಡ್‌ಗಳನ್ನು ಎಷ್ಟು ಪ್ರಮಾಣದಲ್ಲಿ ಪರಿಗಣಿಸಬೇಕೆಂದು ನಿರ್ಧರಿಸಲು ನಾವು `alpha` ಪರಿಮಾಣವನ್ನು ಬಳಸುತ್ತೇವೆ - ಇದನ್ನು ರೀಇನ್ಫೋರ್ಸ್ಮೆಂಟ್ ಆಲ್ಗೋರಿದಮ್‌ನ *ಲರ್ನಿಂಗ್ ರೇಟ್* ಎಂದು ಕರೆಯುತ್ತಾರೆ.  \n",
    "1. ಕೊನೆಗೆ, ನಾವು ನಮ್ಮ ನೆಟ್‌ವರ್ಕ್ ಅನ್ನು ಸ್ಥಿತಿಗಳ ಮತ್ತು ನಿರೀಕ್ಷಿತ ಕ್ರಿಯೆಗಳ ಮೇಲೆ ತರಬೇತಿಗೊಳಿಸಿ, ಮತ್ತು ಈ ಪ್ರಕ್ರಿಯೆಯನ್ನು ಪುನರಾವರ್ತಿಸುತ್ತೇವೆ.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "def train_on_batch(x, y):\n",
    "    x = torch.from_numpy(x)\n",
    "    y = torch.from_numpy(y)\n",
    "    optimizer.zero_grad()\n",
    "    predictions = model(x)\n",
    "    loss = -torch.mean(torch.log(predictions) * y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1e-4\n",
    "\n",
    "history = []\n",
    "for epoch in range(300):\n",
    "    states, actions, probs, rewards = run_episode()\n",
    "    one_hot_actions = np.eye(2)[actions.T][0]\n",
    "    gradients = one_hot_actions-probs\n",
    "    dr = discounted_rewards(rewards)\n",
    "    gradients *= dr\n",
    "    target = alpha*np.vstack([gradients])+probs\n",
    "    train_on_batch(states,target)\n",
    "    history.append(np.sum(rewards))\n",
    "    if epoch%100==0:\n",
    "        print(f\"{epoch} -> {np.sum(rewards)}\")\n",
    "\n",
    "plt.plot(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ಈಗ ಫಲಿತಾಂಶವನ್ನು ನೋಡಲು ರೆಂಡರಿಂಗ್ ಜೊತೆಗೆ ಎಪಿಸೋಡ್ ಅನ್ನು ಚಾಲನೆ ಮಾಡೋಣ:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = run_episode(render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ನೀವು ಈಗ ಧ್ರುವವನ್ನು ಚೆನ್ನಾಗಿ ಸಮತೋಲನಗೊಳಿಸಬಹುದೆಂದು ಕಾಣಬಹುದು!\n",
    "\n",
    "## ನಟ-ವಿಮರ್ಶಕ ಮಾದರಿ\n",
    "\n",
    "ನಟ-ವಿಮರ್ಶಕ ಮಾದರಿ ನೀತಿ ಗ್ರೇಡಿಯಂಟ್‌ಗಳ ಮುಂದಿನ ಅಭಿವೃದ್ಧಿಯಾಗಿದೆ, ಇದರಲ್ಲಿ ನಾವು ನೀತಿ ಮತ್ತು ಅಂದಾಜು ಮಾಡಿದ ಬಹುಮಾನಗಳನ್ನು ಎರಡನ್ನೂ ಕಲಿಯಲು ನ್ಯೂರಲ್ ನೆಟ್‌ವರ್ಕ್ ನಿರ್ಮಿಸುತ್ತೇವೆ. ನೆಟ್‌ವರ್ಕ್‌ಗೆ ಎರಡು ಔಟ್‌ಪುಟ್‌ಗಳು ಇರುತ್ತವೆ (ಅಥವಾ ನೀವು ಇದನ್ನು ಎರಡು ಪ್ರತ್ಯೇಕ ನೆಟ್‌ವರ್ಕ್‌ಗಳಾಗಿ ನೋಡಬಹುದು):\n",
    "* **ನಟ** ನೀತಿ ಗ್ರೇಡಿಯಂಟ್ ಮಾದರಿಯಂತೆ ನಮಗೆ ರಾಜ್ಯದ ಪ್ರಾಬಬಿಲಿಟಿ ವಿತರಣೆಯನ್ನು ನೀಡುವ ಮೂಲಕ ತೆಗೆದುಕೊಳ್ಳಬೇಕಾದ ಕ್ರಿಯೆಯನ್ನು ಶಿಫಾರಸು ಮಾಡುತ್ತದೆ\n",
    "* **ವಿಮರ್ಶಕ** ಆ ಕ್ರಿಯೆಗಳಿಂದ ಬಹುಮಾನ ಏನು ಆಗಬಹುದು ಎಂದು ಅಂದಾಜು ಮಾಡುತ್ತದೆ. ಅದು ನೀಡಲಾದ ರಾಜ್ಯದಲ್ಲಿ ಭವಿಷ್ಯದಲ್ಲಿ ಒಟ್ಟು ಅಂದಾಜು ಮಾಡಿದ ಬಹುಮಾನಗಳನ್ನು ಹಿಂತಿರುಗಿಸುತ್ತದೆ.\n",
    "\n",
    "ಹೀಗೆ ಒಂದು ಮಾದರಿಯನ್ನು ವ್ಯಾಖ್ಯಾನಿಸೋಣ:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import count\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "lr = 0.0001\n",
    "\n",
    "class Actor(torch.nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Actor, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.linear1 = torch.nn.Linear(self.state_size, 128)\n",
    "        self.linear2 = torch.nn.Linear(128, 256)\n",
    "        self.linear3 = torch.nn.Linear(256, self.action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        output = F.relu(self.linear1(state))\n",
    "        output = F.relu(self.linear2(output))\n",
    "        output = self.linear3(output)\n",
    "        distribution = torch.distributions.Categorical(F.softmax(output, dim=-1))\n",
    "        return distribution\n",
    "\n",
    "\n",
    "class Critic(torch.nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Critic, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.linear1 = torch.nn.Linear(self.state_size, 128)\n",
    "        self.linear2 = torch.nn.Linear(128, 256)\n",
    "        self.linear3 = torch.nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        output = F.relu(self.linear1(state))\n",
    "        output = F.relu(self.linear2(output))\n",
    "        value = self.linear3(output)\n",
    "        return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ನಾವು ನಮ್ಮ `discounted_rewards` ಮತ್ತು `run_episode` ಫಂಕ್ಷನ್‌ಗಳನ್ನು ಸ್ವಲ್ಪ ಬದಲಾಯಿಸಬೇಕಾಗುತ್ತದೆ:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discounted_rewards(next_value, rewards, masks, gamma=0.99):\n",
    "    R = next_value\n",
    "    returns = []\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        R = rewards[step] + gamma * R * masks[step]\n",
    "        returns.insert(0, R)\n",
    "    return returns\n",
    "\n",
    "def run_episode(actor, critic, n_iters):\n",
    "    optimizerA = torch.optim.Adam(actor.parameters())\n",
    "    optimizerC = torch.optim.Adam(critic.parameters())\n",
    "    for iter in range(n_iters):\n",
    "        state = env.reset()\n",
    "        log_probs = []\n",
    "        values = []\n",
    "        rewards = []\n",
    "        masks = []\n",
    "        entropy = 0\n",
    "        env.reset()\n",
    "\n",
    "        for i in count():\n",
    "            env.render()\n",
    "            state = torch.FloatTensor(state).to(device)\n",
    "            dist, value = actor(state), critic(state)\n",
    "\n",
    "            action = dist.sample()\n",
    "            next_state, reward, done, _ = env.step(action.cpu().numpy())\n",
    "\n",
    "            log_prob = dist.log_prob(action).unsqueeze(0)\n",
    "            entropy += dist.entropy().mean()\n",
    "\n",
    "            log_probs.append(log_prob)\n",
    "            values.append(value)\n",
    "            rewards.append(torch.tensor([reward], dtype=torch.float, device=device))\n",
    "            masks.append(torch.tensor([1-done], dtype=torch.float, device=device))\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                print('Iteration: {}, Score: {}'.format(iter, i))\n",
    "                break\n",
    "\n",
    "\n",
    "        next_state = torch.FloatTensor(next_state).to(device)\n",
    "        next_value = critic(next_state)\n",
    "        returns = discounted_rewards(next_value, rewards, masks)\n",
    "\n",
    "        log_probs = torch.cat(log_probs)\n",
    "        returns = torch.cat(returns).detach()\n",
    "        values = torch.cat(values)\n",
    "\n",
    "        advantage = returns - values\n",
    "\n",
    "        actor_loss = -(log_probs * advantage.detach()).mean()\n",
    "        critic_loss = advantage.pow(2).mean()\n",
    "\n",
    "        optimizerA.zero_grad()\n",
    "        optimizerC.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        critic_loss.backward()\n",
    "        optimizerA.step()\n",
    "        optimizerC.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ಈಗ ನಾವು ಮುಖ್ಯ ತರಬೇತಿ ಲೂಪ್ ಅನ್ನು ನಡೆಸಲಿದ್ದೇವೆ. ಸರಿಯಾದ ನಷ್ಟ ಕಾರ್ಯಗಳನ್ನು ಲೆಕ್ಕಹಾಕಿ ಮತ್ತು ನೆಟ್‌ವರ್ಕ್ ಪರಿಮಾಣಗಳನ್ನು ನವೀಕರಿಸುವ ಮೂಲಕ ಕೈಯಿಂದ ನೆಟ್‌ವರ್ಕ್ ತರಬೇತಿ ಪ್ರಕ್ರಿಯೆಯನ್ನು ಬಳಸಲಿದ್ದೇವೆ:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "actor = Actor(state_size, action_size).to(device)\n",
    "critic = Critic(state_size, action_size).to(device)\n",
    "run_episode(actor, critic, n_iters=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ಸಾರಾಂಶ\n",
    "\n",
    "ಈ ಡೆಮೋದಲ್ಲಿ ನಾವು ಎರಡು RL ಅಲ್ಗೋರಿದಮ್‌ಗಳನ್ನು ನೋಡಿದ್ದೇವೆ: ಸರಳ ಪಾಲಿಸಿ ಗ್ರೇಡಿಯಂಟ್ ಮತ್ತು ಹೆಚ್ಚು ಸುಧಾರಿತ ಆಕ್ಟರ್-ಕ್ರಿಟಿಕ್. ಆ ಅಲ್ಗೋರಿದಮ್‌ಗಳು ಸ್ಥಿತಿ, ಕ್ರಿಯೆ ಮತ್ತು ಬಹುಮಾನ ಎಂಬ抽象概念ಗಳೊಂದಿಗೆ ಕಾರ್ಯನಿರ್ವಹಿಸುತ್ತವೆ ಎಂದು ನೀವು ನೋಡಬಹುದು - ಆದ್ದರಿಂದ ಅವುಗಳನ್ನು ಬಹು ವಿಭಿನ್ನ ಪರಿಸರಗಳಲ್ಲಿ ಅನ್ವಯಿಸಬಹುದು.\n",
    "\n",
    "ರೀಇನ್ಫೋರ್ಸ್ಮೆಂಟ್ ಲರ್ನಿಂಗ್ ನಮಗೆ ಸಮಸ್ಯೆಯನ್ನು ಪರಿಹರಿಸಲು ಅತ್ಯುತ್ತಮ ತಂತ್ರವನ್ನು ಕೇವಲ ಅಂತಿಮ ಬಹುಮಾನವನ್ನು ನೋಡಿ ಕಲಿಯಲು ಅವಕಾಶ ನೀಡುತ್ತದೆ. ನಮಗೆ ಲೇಬಲ್ ಮಾಡಲಾದ ಡೇಟಾಸೆಟ್‌ಗಳ ಅಗತ್ಯವಿಲ್ಲ ಎಂಬುದು ನಮ್ಮ ಮಾದರಿಗಳನ್ನು ಉತ್ತಮಗೊಳಿಸಲು ಅನೇಕ ಬಾರಿ ಸಿಮ್ಯುಲೇಶನ್‌ಗಳನ್ನು ಪುನರಾವರ್ತಿಸಲು ಅವಕಾಶ ನೀಡುತ್ತದೆ. ಆದಾಗ್ಯೂ, RL ನಲ್ಲಿ ಇನ್ನೂ ಅನೇಕ ಸವಾಲುಗಳಿವೆ, ಮತ್ತು ನೀವು AI ಯ ಈ ಆಸಕ್ತಿದಾಯಕ ಕ್ಷೇತ್ರದಲ್ಲಿ ಹೆಚ್ಚು ಗಮನಹರಿಸಲು ನಿರ್ಧರಿಸಿದರೆ ಅವುಗಳನ್ನು ಕಲಿಯಬಹುದು.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n<!-- CO-OP TRANSLATOR DISCLAIMER START -->\n**ಅಸ್ವೀಕಾರ**:  \nಈ ದಸ್ತಾವೇಜು [Co-op Translator](https://github.com/Azure/co-op-translator) ಎಂಬ AI ಅನುವಾದ ಸೇವೆಯನ್ನು ಬಳಸಿ ಅನುವಾದಿಸಲಾಗಿದೆ. ನಾವು ಶುದ್ಧತೆಯತ್ತ ಪ್ರಯತ್ನಿಸುತ್ತಿದ್ದರೂ, ಸ್ವಯಂಚಾಲಿತ ಅನುವಾದಗಳಲ್ಲಿ ತಪ್ಪುಗಳು ಅಥವಾ ಅಸತ್ಯತೆಗಳು ಇರಬಹುದು ಎಂದು ದಯವಿಟ್ಟು ಗಮನಿಸಿ. ಮೂಲ ಭಾಷೆಯಲ್ಲಿರುವ ಮೂಲ ದಸ್ತಾವೇಜನ್ನು ಅಧಿಕೃತ ಮೂಲವೆಂದು ಪರಿಗಣಿಸಬೇಕು. ಮಹತ್ವದ ಮಾಹಿತಿಗಾಗಿ, ವೃತ್ತಿಪರ ಮಾನವ ಅನುವಾದವನ್ನು ಶಿಫಾರಸು ಮಾಡಲಾಗುತ್ತದೆ. ಈ ಅನುವಾದ ಬಳಕೆಯಿಂದ ಉಂಟಾಗುವ ಯಾವುದೇ ತಪ್ಪು ಅರ್ಥಮಾಡಿಕೊಳ್ಳುವಿಕೆ ಅಥವಾ ತಪ್ಪು ವಿವರಣೆಗಳಿಗೆ ನಾವು ಹೊಣೆಗಾರರಾಗುವುದಿಲ್ಲ.\n<!-- CO-OP TRANSLATOR DISCLAIMER END -->\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  },
  "coopTranslator": {
   "original_hash": "04f8d9978cd11281d81dd037cbf6ce20",
   "translation_date": "2025-11-26T02:41:16+00:00",
   "source_file": "lessons/6-Other/22-DeepRL/CartPole-RL-PyTorch.ipynb",
   "language_code": "kn"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}