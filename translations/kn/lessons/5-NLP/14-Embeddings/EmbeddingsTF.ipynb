{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ಎम्बೆಡ್ಡಿಂಗ್ಸ್\n",
    "\n",
    "ನಮ್ಮ ಹಿಂದಿನ ಉದಾಹರಣೆಯಲ್ಲಿ, ನಾವು `vocab_size` ಉದ್ದದ ಉನ್ನತ-ಆಯಾಮದ ಬ್ಯಾಗ್-ಆಫ್-ವರ್ಡ್ಸ್ ವೆಕ್ಟರ್‌ಗಳ ಮೇಲೆ ಕಾರ್ಯನಿರ್ವಹಿಸಿದ್ದೇವೆ, ಮತ್ತು ನಾವು ಕಡಿಮೆ-ಆಯಾಮದ ಸ್ಥಾನಿಕ ಪ್ರತಿನಿಧಿ ವೆಕ್ಟರ್‌ಗಳನ್ನು ಸ್ಪಾರ್ಸ್ ಒನ್-ಹಾಟ್ ಪ್ರತಿನಿಧಿಗೆ ಸ್ಪಷ್ಟವಾಗಿ ಪರಿವರ್ತಿಸಿದ್ದೇವೆ. ಈ ಒನ್-ಹಾಟ್ ಪ್ರತಿನಿಧಿ ಸ್ಮೃತಿ-ಕಾರ್ಯಕ್ಷಮವಲ್ಲ. ಜೊತೆಗೆ, ಪ್ರತಿ ಪದವನ್ನು ಪರಸ್ಪರ ಸ್ವತಂತ್ರವಾಗಿ ಪರಿಗಣಿಸಲಾಗುತ್ತದೆ, ಆದ್ದರಿಂದ ಒನ್-ಹಾಟ್ ಎನ್‌ಕೋಡ್ ಮಾಡಿದ ವೆಕ್ಟರ್‌ಗಳು ಪದಗಳSemantic ಸಮಾನತೆಗಳನ್ನು ವ್ಯಕ್ತಪಡಿಸುವುದಿಲ್ಲ.\n",
    "\n",
    "ಈ ಘಟಕದಲ್ಲಿ, ನಾವು **News AG** ಡೇಟಾಸೆಟ್ ಅನ್ನು ಮುಂದುವರೆಸಿ ಅನ್ವೇಷಿಸುವೆವು. ಪ್ರಾರಂಭಿಸಲು, ಡೇಟಾವನ್ನು ಲೋಡ್ ಮಾಡಿ ಮತ್ತು ಹಿಂದಿನ ಘಟಕದಿಂದ ಕೆಲವು ವ್ಯಾಖ್ಯಾನಗಳನ್ನು ಪಡೆಯೋಣ.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "\n",
    "ds_train, ds_test = tfds.load('ag_news_subset').values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ಎम्बೆಡ್ಡಿಂಗ್ ಎಂದರೆ ಏನು?\n",
    "\n",
    "**ಎಂಬೆಡ್ಡಿಂಗ್** ಎಂಬ ಕಲ್ಪನೆ ಎಂದರೆ ಪದಗಳನ್ನು ಕಡಿಮೆ ಆಯಾಮದ ಸಾಂದ್ರ ವೆಕ್ಟರ್‌ಗಳ ಮೂಲಕ ಪ್ರತಿನಿಧಿಸುವುದು, ಅವು ಪದದ ಅರ್ಥವನ್ನು ಪ್ರತಿಬಿಂಬಿಸುತ್ತವೆ. ನಾವು ನಂತರ ಅರ್ಥಪೂರ್ಣ ಪದ ಎम्बೆಡ್ಡಿಂಗ್‌ಗಳನ್ನು ಹೇಗೆ ನಿರ್ಮಿಸುವುದನ್ನು ಚರ್ಚಿಸುವೆವು, ಆದರೆ ಈಗಾಗಲೇ ಎम्बೆಡ್ಡಿಂಗ್‌ಗಳನ್ನು ಪದ ವೆಕ್ಟರ್‌ನ ಆಯಾಮವನ್ನು ಕಡಿಮೆ ಮಾಡುವ ವಿಧಾನವೆಂದು ಪರಿಗಣಿಸೋಣ.\n",
    "\n",
    "ಹೀಗಾಗಿ, ಒಂದು ಎम्बೆಡ್ಡಿಂಗ್ ಲೇಯರ್ ಒಂದು ಪದವನ್ನು ಇನ್‌ಪುಟ್ ಆಗಿ ತೆಗೆದುಕೊಳ್ಳುತ್ತದೆ ಮತ್ತು ನಿರ್ದಿಷ್ಟ `embedding_size` ಹೊಂದಿರುವ ಔಟ್‌ಪುಟ್ ವೆಕ್ಟರ್ ಅನ್ನು ಉತ್ಪಾದಿಸುತ್ತದೆ. ಒಂದು ಅರ್ಥದಲ್ಲಿ, ಇದು `Dense` ಲೇಯರ್‌ಗೆ ಬಹಳ ಸಮಾನವಾಗಿದೆ, ಆದರೆ ಒಂದು-ಹಾಟ್ ಎನ್‌ಕೋಡ್ ಮಾಡಿದ ವೆಕ್ಟರ್ ಅನ್ನು ಇನ್‌ಪುಟ್ ಆಗಿ ತೆಗೆದುಕೊಳ್ಳುವ ಬದಲು, ಇದು ಪದ ಸಂಖ್ಯೆಯನ್ನು ತೆಗೆದುಕೊಳ್ಳಲು ಸಾಧ್ಯವಾಗುತ್ತದೆ.\n",
    "\n",
    "ನಮ್ಮ ನೆಟ್‌ವರ್ಕ್‌ನಲ್ಲಿ ಮೊದಲ ಲೇಯರ್ ಆಗಿ ಎम्बೆಡ್ಡಿಂಗ್ ಲೇಯರ್ ಅನ್ನು ಬಳಸುವುದರಿಂದ, ನಾವು ಬ್ಯಾಗ್-ಆಫ್-ವರ್ಡ್ಸ್‌ನಿಂದ **ಎಂಬೆಡ್ಡಿಂಗ್ ಬ್ಯಾಗ್** ಮಾದರಿಯ ಕಡೆಗೆ ಬದಲಾಗಬಹುದು, ಅಲ್ಲಿ ನಾವು ಮೊದಲಿಗೆ ನಮ್ಮ ಪಠ್ಯದ ಪ್ರತಿಯೊಂದು ಪದವನ್ನು ಅದರ ಸಂಬಂಧಿಸಿದ ಎम्बೆಡ್ಡಿಂಗ್‌ಗೆ ಪರಿವರ್ತಿಸುತ್ತೇವೆ, ನಂತರ ಆ ಎಲ್ಲಾ ಎम्बೆಡ್ಡಿಂಗ್‌ಗಳ ಮೇಲೆ `sum`, `average` ಅಥವಾ `max` ಮುಂತಾದ ಸಂಗ್ರಹಣಾ ಕಾರ್ಯವನ್ನು ಲೆಕ್ಕಹಾಕುತ್ತೇವೆ.\n",
    "\n",
    "![ಐದು ಕ್ರಮ ಪದಗಳಿಗಾಗಿ ಎम्बೆಡ್ಡಿಂಗ್ ವರ್ಗೀಕರಣವನ್ನು ತೋರಿಸುವ ಚಿತ್ರ.](../../../../../translated_images/kn/embedding-classifier-example.b77f021a7ee67eee.webp)\n",
    "\n",
    "ನಮ್ಮ ವರ್ಗೀಕರಣ ನ್ಯೂರಲ್ ನೆಟ್‌ವರ್ಕ್ ಕೆಳಗಿನ ಲೇಯರ್‌ಗಳಿಂದ ಕೂಡಿದೆ:\n",
    "\n",
    "* `TextVectorization` ಲೇಯರ್, ಇದು ಸ್ಟ್ರಿಂಗ್ ಅನ್ನು ಇನ್‌ಪುಟ್ ಆಗಿ ತೆಗೆದುಕೊಳ್ಳುತ್ತದೆ ಮತ್ತು ಟೋಕನ್ ಸಂಖ್ಯೆಗಳ ಟೆನ್ಸರ್ ಅನ್ನು ಉತ್ಪಾದಿಸುತ್ತದೆ. ನಾವು ಕೆಲವು ಯುಕ್ತಿಪೂರ್ಣ ಶಬ್ದಕೋಶ ಗಾತ್ರ `vocab_size` ಅನ್ನು ನಿರ್ಧರಿಸುವೆವು ಮತ್ತು ಕಡಿಮೆ ಬಳಕೆಯ ಪದಗಳನ್ನು ನಿರ್ಲಕ್ಷಿಸುವೆವು. ಇನ್‌ಪುಟ್ ಆಕಾರ 1 ಆಗಿರುತ್ತದೆ ಮತ್ತು ಔಟ್‌ಪುಟ್ ಆಕಾರ $n$ ಆಗಿರುತ್ತದೆ, ಏಕೆಂದರೆ ನಾವು $n$ ಟೋಕನ್‌ಗಳನ್ನು ಫಲಿತಾಂಶವಾಗಿ ಪಡೆಯುತ್ತೇವೆ, ಪ್ರತಿಯೊಂದು 0 ರಿಂದ `vocab_size` ವರೆಗೆ ಸಂಖ್ಯೆಗಳನ್ನೊಳಗೊಂಡಿರುತ್ತದೆ.\n",
    "* `Embedding` ಲೇಯರ್, ಇದು $n$ ಸಂಖ್ಯೆಗಳನ್ನ ತೆಗೆದುಕೊಳ್ಳುತ್ತದೆ ಮತ್ತು ಪ್ರತಿಯೊಂದು ಸಂಖ್ಯೆಯನ್ನು ನಿರ್ದಿಷ್ಟ ಉದ್ದದ (ನಮ್ಮ ಉದಾಹರಣೆಯಲ್ಲಿ 100) ಸಾಂದ್ರ ವೆಕ್ಟರ್‌ಗೆ ಕಡಿಮೆ ಮಾಡುತ್ತದೆ. ಹೀಗಾಗಿ, $n$ ಆಕಾರದ ಇನ್‌ಪುಟ್ ಟೆನ್ಸರ್ ಅನ್ನು $n\\times 100$ ಆಕಾರದ ಟೆನ್ಸರ್‌ಗೆ ಪರಿವರ್ತಿಸಲಾಗುತ್ತದೆ.\n",
    "* ಸಂಗ್ರಹಣಾ ಲೇಯರ್, ಇದು ಮೊದಲ ಅಕ್ಷದ ಮೇಲೆ ಈ ಟೆನ್ಸರ್‌ನ ಸರಾಸರಿಯನ್ನು ತೆಗೆದುಕೊಳ್ಳುತ್ತದೆ, ಅಂದರೆ ವಿಭಿನ್ನ ಪದಗಳಿಗೆ ಹೊಂದಿರುವ ಎಲ್ಲಾ $n$ ಇನ್‌ಪುಟ್ ಟೆನ್ಸರ್‌ಗಳ ಸರಾಸರಿಯನ್ನು ಲೆಕ್ಕಹಾಕುತ್ತದೆ. ಈ ಲೇಯರ್ ಅನ್ನು ಅನುಷ್ಠಾನಗೊಳಿಸಲು, ನಾವು `Lambda` ಲೇಯರ್ ಅನ್ನು ಬಳಸುತ್ತೇವೆ ಮತ್ತು ಅದಕ್ಕೆ ಸರಾಸರಿ ಲೆಕ್ಕಹಾಕುವ ಫಂಕ್ಷನ್ ಅನ್ನು ಪಾಸ್ ಮಾಡುತ್ತೇವೆ. ಔಟ್‌ಪುಟ್ 100 ಆಕಾರದಿರುತ್ತದೆ ಮತ್ತು ಇದು ಸಂಪೂರ್ಣ ಇನ್‌ಪುಟ್ ಕ್ರಮದ ಸಂಖ್ಯಾತ್ಮಕ ಪ್ರತಿನಿಧನೆ ಆಗಿರುತ್ತದೆ.\n",
    "* ಅಂತಿಮ `Dense` ರೇಖೀಯ ವರ್ಗೀಕರಣ.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization (TextVec  (None, None)             0         \n",
      " torization)                                                     \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, None, 100)         3000000   \n",
      "                                                                 \n",
      " lambda (Lambda)             (None, 100)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4)                 404       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,000,404\n",
      "Trainable params: 3,000,404\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 30000\n",
    "batch_size = 128\n",
    "\n",
    "vectorizer = keras.layers.experimental.preprocessing.TextVectorization(max_tokens=vocab_size,input_shape=(1,))\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    vectorizer,    \n",
    "    keras.layers.Embedding(vocab_size,100),\n",
    "    keras.layers.Lambda(lambda x: tf.reduce_mean(x,axis=1)),\n",
    "    keras.layers.Dense(4, activation='softmax')\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`summary` ಮುದ್ರಣದಲ್ಲಿ, **output shape** ಕಾಲಮ್‌ನಲ್ಲಿ, ಮೊದಲ ಟೆನ್ಸರ್ ಆಯಾಮ `None` ಮಿನಿಬ್ಯಾಚ್ ಗಾತ್ರಕ್ಕೆ ಹೊಂದಿಕೆಯಾಗಿದ್ದು, ಎರಡನೇ ಆಯಾಮ ಟೋಕನ್ ಸರಣಿಯ ಉದ್ದಕ್ಕೆ ಹೊಂದಿಕೆಯಾಗುತ್ತದೆ. ಮಿನಿಬ್ಯಾಚ್‌ನಲ್ಲಿರುವ ಎಲ್ಲಾ ಟೋಕನ್ ಸರಣಿಗಳ ಉದ್ದಗಳು ವಿಭಿನ್ನವಾಗಿರುತ್ತವೆ. ಇದನ್ನು ಹೇಗೆ ನಿರ್ವಹಿಸುವುದು ಎಂಬುದನ್ನು ಮುಂದಿನ ವಿಭಾಗದಲ್ಲಿ ಚರ್ಚಿಸುವೆವು.\n",
    "\n",
    "ಈಗ ನೆಟ್‌ವರ್ಕ್ ಅನ್ನು ತರಬೇತಿಮಾಡೋಣ:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training vectorizer\n",
      "938/938 [==============================] - 20s 20ms/step - loss: 0.7891 - acc: 0.8155 - val_loss: 0.4470 - val_acc: 0.8642\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22255515100>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_text(x):\n",
    "    return x['title']+' '+x['description']\n",
    "\n",
    "def tupelize(x):\n",
    "    return (extract_text(x),x['label'])\n",
    "\n",
    "print(\"Training vectorizer\")\n",
    "vectorizer.adapt(ds_train.take(500).map(extract_text))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'])\n",
    "model.fit(ds_train.map(tupelize).batch(batch_size),validation_data=ds_test.map(tupelize).batch(batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "> **ಗಮನಿಸಿ** ನಾವು ಡೇಟಾದ ಉಪಸಮೂಹದ ಆಧಾರದ ಮೇಲೆ ವೆಕ್ಟರೈಜರ್ ಅನ್ನು ನಿರ್ಮಿಸುತ್ತಿದ್ದೇವೆ. ಪ್ರಕ್ರಿಯೆಯನ್ನು ವೇಗಗೊಳಿಸಲು ಇದನ್ನು ಮಾಡಲಾಗುತ್ತದೆ, ಮತ್ತು ಇದರಿಂದ ನಮ್ಮ ಪಠ್ಯದಲ್ಲಿನ ಎಲ್ಲಾ ಟೋಕನ್‌ಗಳು ಶಬ್ದಕೋಶದಲ್ಲಿ ಇರದ ಪರಿಸ್ಥಿತಿ ಉಂಟಾಗಬಹುದು. ಈ ಸಂದರ್ಭದಲ್ಲಿ, ಆ ಟೋಕನ್‌ಗಳನ್ನು ನಿರ್ಲಕ್ಷಿಸಲಾಗುತ್ತದೆ, ಇದರಿಂದ ಸ್ವಲ್ಪ ಕಡಿಮೆ ನಿಖರತೆ ಉಂಟಾಗಬಹುದು. ಆದಾಗ್ಯೂ, ನಿಜ ಜೀವನದಲ್ಲಿ ಪಠ್ಯದ ಉಪಸಮೂಹವು ಸಾಮಾನ್ಯವಾಗಿ ಉತ್ತಮ ಶಬ್ದಕೋಶ ಅಂದಾಜು ನೀಡುತ್ತದೆ.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ಬದಲಾಗುವ ಕ್ರಮದ ಗಾತ್ರಗಳನ್ನು ನಿರ್ವಹಿಸುವುದು\n",
    "\n",
    "ಮಿನಿಬ್ಯಾಚ್‌ಗಳಲ್ಲಿ ತರಬೇತಿ ಹೇಗೆ ನಡೆಯುತ್ತದೆ ಎಂಬುದನ್ನು ತಿಳಿದುಕೊಳ್ಳೋಣ. ಮೇಲಿನ ಉದಾಹರಣೆಯಲ್ಲಿ, ಇನ್‌ಪುಟ್ ಟೆನ್ಸರ್‌ನ ಆಯಾಮ 1 ಆಗಿದ್ದು, ನಾವು 128-ದೀರ್ಘ ಮಿನಿಬ್ಯಾಚ್‌ಗಳನ್ನು ಬಳಸುತ್ತೇವೆ, ಆದ್ದರಿಂದ ಟೆನ್ಸರ್‌ನ ನಿಜವಾದ ಗಾತ್ರ $128 \\times 1$ ಆಗಿದೆ. ಆದರೆ, ಪ್ರತಿ ವಾಕ್ಯದ ಟೋಕನ್‌ಗಳ ಸಂಖ್ಯೆ ವಿಭಿನ್ನವಾಗಿರುತ್ತದೆ. ನಾವು `TextVectorization` ಲೇಯರ್ ಅನ್ನು ಒಬ್ಬೊಬ್ಬ ಇನ್‌ಪುಟ್‌ಗೆ ಅನ್ವಯಿಸಿದರೆ, ಟೋಕನ್‌ಗಳ ಸಂಖ್ಯೆ ವಿಭಿನ್ನವಾಗಿರುತ್ತದೆ, ಅದು ಪಠ್ಯವನ್ನು ಹೇಗೆ ಟೋಕನೈಸ್ ಮಾಡಲಾಗಿದೆ ಎಂಬುದರ ಮೇಲೆ ಅವಲಂಬಿತವಾಗಿರುತ್ತದೆ:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 1 45], shape=(2,), dtype=int64)\n",
      "tf.Tensor([ 112 1271    1    3 1747  158], shape=(6,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer('Hello, world!'))\n",
    "print(vectorizer('I am glad to meet you!'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ಆದರೆ, ನಾವು ವೆಕ್ಟರೈಜರ್ ಅನ್ನು ಹಲವಾರು ಕ್ರಮಗಳಿಗೆ ಅನ್ವಯಿಸಿದಾಗ, ಅದು ಆಯತಾಕಾರದ ಟೆನ್ಸರ್ ಅನ್ನು ಉತ್ಪಾದಿಸಬೇಕು, ಆದ್ದರಿಂದ ಅದು ಬಳಸದ ಅಂಶಗಳನ್ನು PAD ಟೋಕನ್ (ನಮ್ಮ ಪ್ರಕರಣದಲ್ಲಿ ಶೂನ್ಯ) ಮೂಲಕ ತುಂಬುತ್ತದೆ:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 6), dtype=int64, numpy=\n",
       "array([[   1,   45,    0,    0,    0,    0],\n",
       "       [ 112, 1271,    1,    3, 1747,  158]], dtype=int64)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer(['Hello, world!','I am glad to meet you!'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ಇಲ್ಲಿ ನಾವು ಎम्बೆಡ್ಡಿಂಗ್‌ಗಳನ್ನು ನೋಡಬಹುದು:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 1.53059261e-02,  6.80514947e-02,  3.14026810e-02, ...,\n",
       "         -8.92002955e-02,  1.52911525e-04, -5.65562584e-02],\n",
       "        [ 2.57456154e-01,  2.79364467e-01, -2.03605562e-01, ...,\n",
       "         -2.07474351e-01,  8.31158683e-02, -2.03911960e-01],\n",
       "        [ 3.98201384e-02, -8.03454965e-03,  2.39790026e-02, ...,\n",
       "         -7.18549127e-04,  2.66963355e-02, -4.30646613e-02],\n",
       "        [ 3.98201384e-02, -8.03454965e-03,  2.39790026e-02, ...,\n",
       "         -7.18549127e-04,  2.66963355e-02, -4.30646613e-02],\n",
       "        [ 3.98201384e-02, -8.03454965e-03,  2.39790026e-02, ...,\n",
       "         -7.18549127e-04,  2.66963355e-02, -4.30646613e-02],\n",
       "        [ 3.98201384e-02, -8.03454965e-03,  2.39790026e-02, ...,\n",
       "         -7.18549127e-04,  2.66963355e-02, -4.30646613e-02]],\n",
       "\n",
       "       [[ 1.89674050e-01,  2.61548996e-01, -3.67433839e-02, ...,\n",
       "         -2.07366899e-01, -1.05442435e-01, -2.36952081e-01],\n",
       "        [ 6.16133213e-02,  1.80511594e-01,  9.77298319e-02, ...,\n",
       "         -5.46628237e-02, -1.07340455e-01, -1.06589928e-01],\n",
       "        [ 1.53059261e-02,  6.80514947e-02,  3.14026810e-02, ...,\n",
       "         -8.92002955e-02,  1.52911525e-04, -5.65562584e-02],\n",
       "        [-4.84890305e-02, -8.41715634e-02,  1.51529670e-01, ...,\n",
       "          1.28192469e-01, -7.77286515e-02,  1.26041949e-01],\n",
       "        [-4.17212099e-02, -5.60694858e-02,  4.08860669e-02, ...,\n",
       "          8.70475471e-02,  8.92383084e-02,  1.67974353e-01],\n",
       "        [ 2.85779923e-01,  4.57767487e-01,  4.52292450e-02, ...,\n",
       "         -1.97419018e-01, -2.04659685e-01, -2.79758364e-01]]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[1](vectorizer(['Hello, world!','I am glad to meet you!'])).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **ಗಮನಿಸಿ**: ಪ್ಯಾಡಿಂಗ್ ಪ್ರಮಾಣವನ್ನು ಕಡಿಮೆ ಮಾಡಲು, ಕೆಲವು ಸಂದರ್ಭಗಳಲ್ಲಿ ಡೇಟಾಸೆಟ್‌ನ ಎಲ್ಲಾ ಕ್ರಮಗಳನ್ನು ಹೆಚ್ಚುತ್ತಿರುವ ಉದ್ದದ ಕ್ರಮದಲ್ಲಿ (ಅಥವಾ, ಹೆಚ್ಚು ನಿಖರವಾಗಿ, ಟೋಕನ್‌ಗಳ ಸಂಖ್ಯೆಯ ಕ್ರಮದಲ್ಲಿ) ವಿಂಗಡಿಸುವುದು ಅರ್ಥಪೂರ್ಣವಾಗುತ್ತದೆ. ಇದರಿಂದ ಪ್ರತಿ ಮಿನಿಬ್ಯಾಚ್ ಸಮಾನ ಉದ್ದದ ಕ್ರಮಗಳನ್ನು ಹೊಂದಿರುತ್ತದೆ ಎಂದು ಖಚಿತವಾಗುತ್ತದೆ.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ಅರ್ಥಪೂರ್ಣ ಎम्बೆಡ್ಡಿಂಗ್‌ಗಳು: Word2Vec\n",
    "\n",
    "ನಮ್ಮ ಹಿಂದಿನ ಉದಾಹರಣೆಯಲ್ಲಿ, ಎम्बೆಡ್ಡಿಂಗ್ ಲೇಯರ್ ಪದಗಳನ್ನು ವೆಕ್ಟರ್ ಪ್ರತಿನಿಧನೆಗಳಿಗೆ ನಕ್ಷೆ ಮಾಡಿಕೊಳ್ಳಲು ಕಲಿತು, ಆದರೆ ಈ ಪ್ರತಿನಿಧನೆಗಳಿಗೆ ಅರ್ಥಪೂರ್ಣ ಅರ್ಥ ಇರಲಿಲ್ಲ. ಸಮಾನಾರ್ಥಕ ಪದಗಳು ಅಥವಾ ಸಮಾನಾರ್ಥಕ ಪದಗಳು ಕೆಲವು ವೆಕ್ಟರ್ ದೂರ (ಉದಾಹರಣೆಗೆ ಯೂಕ್ಲಿಡಿಯನ್ ದೂರ) ಅರ್ಥದಲ್ಲಿ ಪರಸ್ಪರ ಹತ್ತಿರ ಇರುವ ವೆಕ್ಟರ್‌ಗಳಿಗೆ ಹೊಂದಿಕೊಳ್ಳುವಂತೆ ವೆಕ್ಟರ್ ಪ್ರತಿನಿಧನೆಯನ್ನು ಕಲಿಯುವುದು ಒಳ್ಳೆಯದು.\n",
    "\n",
    "ಅದಕ್ಕಾಗಿ, ನಾವು Word2Vec ಎಂಬ ತಂತ್ರವನ್ನು ಬಳಸಿಕೊಂಡು ದೊಡ್ಡ ಪಠ್ಯ ಸಂಗ್ರಹದ ಮೇಲೆ ನಮ್ಮ ಎम्बೆಡ್ಡಿಂಗ್ ಮಾದರಿಯನ್ನು ಪೂರ್ವಪ್ರಶಿಕ್ಷಣ ಮಾಡಬೇಕಾಗುತ್ತದೆ. ಇದು ಪದಗಳ ವಿತರಿತ ಪ್ರತಿನಿಧನೆಯನ್ನು ಉತ್ಪಾದಿಸಲು ಬಳಸುವ ಎರಡು ಪ್ರಮುಖ ವಾಸ್ತುಶಿಲ್ಪಗಳ ಮೇಲೆ ಆಧಾರಿತವಾಗಿದೆ:\n",
    "\n",
    " - **ನಿರಂತರ ಬ್ಯಾಗ್-ಆಫ್-ವರ್ಡ್ಸ್** (CBoW), ಇಲ್ಲಿ ನಾವು ಸುತ್ತಲೂ ಇರುವ ಸನ್ನಿವೇಶದಿಂದ ಒಂದು ಪದವನ್ನು ಊಹಿಸಲು ಮಾದರಿಯನ್ನು ತರಬೇತುಗೊಳಿಸುತ್ತೇವೆ. n-ಗ್ರಾಮ್ $(W_{-2},W_{-1},W_0,W_1,W_2)$ ನೀಡಿದಾಗ, ಮಾದರಿಯ ಗುರಿ $(W_{-2},W_{-1},W_1,W_2)$ ನಿಂದ $W_0$ ಅನ್ನು ಊಹಿಸುವುದು.\n",
    " - **ನಿರಂತರ ಸ್ಕಿಪ್-ಗ್ರಾಮ್** CBoW ಗೆ ವಿರುದ್ಧವಾಗಿದೆ. ಮಾದರಿ ಪ್ರಸ್ತುತ ಪದವನ್ನು ಊಹಿಸಲು ಸುತ್ತಲಿನ ಸನ್ನಿವೇಶ ಪದಗಳ ವಿಂಡೋವನ್ನು ಬಳಸುತ್ತದೆ.\n",
    "\n",
    "CBoW ವೇಗವಾಗಿ ಕಾರ್ಯನಿರ್ವಹಿಸುತ್ತದೆ, ಮತ್ತು ಸ್ಕಿಪ್-ಗ್ರಾಮ್ ನಿಧಾನವಾಗಿದ್ದರೂ ಅಪರೂಪದ ಪದಗಳನ್ನು ಪ್ರತಿನಿಧಿಸುವಲ್ಲಿ ಉತ್ತಮ ಕಾರ್ಯನಿರ್ವಹಿಸುತ್ತದೆ.\n",
    "\n",
    "![ಪದಗಳನ್ನು ವೆಕ್ಟರ್‌ಗಳಿಗೆ ಪರಿವರ್ತಿಸಲು CBoW ಮತ್ತು ಸ್ಕಿಪ್-ಗ್ರಾಮ್ ಆಲ್ಗಾರಿಥಮ್‌ಗಳನ್ನು ತೋರಿಸುವ ಚಿತ್ರ.](../../../../../translated_images/kn/example-algorithms-for-converting-words-to-vectors.fbe9207a726922f6.webp)\n",
    "\n",
    "Google News ಡೇಟಾಸೆಟ್‌ನಲ್ಲಿ ಪೂರ್ವಪ್ರಶಿಕ್ಷಿತ Word2Vec ಎम्बೆಡ್ಡಿಂಗ್‌ನೊಂದಿಗೆ ಪ್ರಯೋಗ ಮಾಡಲು, ನಾವು **gensim** ಗ್ರಂಥಾಲಯವನ್ನು ಬಳಸಬಹುದು. ಕೆಳಗೆ 'neural' ಗೆ ಅತ್ಯಂತ ಸಮಾನವಾದ ಪದಗಳನ್ನು ಕಂಡುಹಿಡಿಯಲಾಗಿದೆ.\n",
    "\n",
    "> **ಗಮನಿಸಿ:** ನೀವು ಮೊದಲ ಬಾರಿಗೆ ಪದ ವೆಕ್ಟರ್‌ಗಳನ್ನು ರಚಿಸುವಾಗ, ಅವುಗಳನ್ನು ಡೌನ್‌ಲೋಡ್ ಮಾಡಲು ಸ್ವಲ್ಪ ಸಮಯ ಬೇಕಾಗಬಹುದು!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "w2v = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neuronal -> 0.7804799675941467\n",
      "neurons -> 0.7326500415802002\n",
      "neural_circuits -> 0.7252851724624634\n",
      "neuron -> 0.7174385190010071\n",
      "cortical -> 0.6941086649894714\n",
      "brain_circuitry -> 0.6923246383666992\n",
      "synaptic -> 0.6699118614196777\n",
      "neural_circuitry -> 0.6638563275337219\n",
      "neurochemical -> 0.6555314064025879\n",
      "neuronal_activity -> 0.6531826257705688\n"
     ]
    }
   ],
   "source": [
    "for w,p in w2v.most_similar('neural'):\n",
    "    print(f\"{w} -> {p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ನಾವು ಶಬ್ದದಿಂದ ವೆಕ್ಟರ್ ಎಂಬೆಡ್ಡಿಂಗ್ ಅನ್ನು ಕೂಡ ತೆಗೆದುಹಾಕಬಹುದು, ಇದನ್ನು ವರ್ಗೀಕರಣ ಮಾದರಿಯನ್ನು ತರಬೇತಿಗೆ ಬಳಸಬಹುದು. ಎಂಬೆಡ್ಡಿಂಗ್‌ನಲ್ಲಿ 300 ಘಟಕಗಳಿವೆ, ಆದರೆ ಇಲ್ಲಿ ಸ್ಪಷ್ಟತೆಗಾಗಿ ವೆಕ್ಟರ್‌ನ ಮೊದಲ 20 ಘಟಕಗಳನ್ನು ಮಾತ್ರ ತೋರಿಸಲಾಗಿದೆ:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01226807,  0.06225586,  0.10693359,  0.05810547,  0.23828125,\n",
       "        0.03686523,  0.05151367, -0.20703125,  0.01989746,  0.10058594,\n",
       "       -0.03759766, -0.1015625 , -0.15820312, -0.08105469, -0.0390625 ,\n",
       "       -0.05053711,  0.16015625,  0.2578125 ,  0.10058594, -0.25976562],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v['play'][:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ಸಾಮಾಂತಿಕ ಎम्बೆಡ್ಡಿಂಗ್‌ಗಳ ಮಹತ್ವದ ವಿಷಯವೆಂದರೆ ನೀವು ಅರ್ಥಾಧಾರಿತವಾಗಿ ವೆಕ್ಟರ್ ಎನ್‌ಕೋಡಿಂಗ್ ಅನ್ನು ನಿಯಂತ್ರಿಸಬಹುದು. ಉದಾಹರಣೆಗೆ, ನಾವು *king* ಮತ್ತು *woman* ಎಂಬ ಪದಗಳ ವೆಕ್ಟರ್ ಪ್ರತಿನಿಧಾನದಷ್ಟು ಹತ್ತಿರವಾಗಿರುವ ಮತ್ತು *man* ಎಂಬ ಪದದಿಂದ ಸಾಧ್ಯವಾದಷ್ಟು ದೂರವಾಗಿರುವ ಪದವನ್ನು ಹುಡುಕಲು ಕೇಳಬಹುದು:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('queen', 0.7118192911148071)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar(positive=['king','woman'],negative=['man'])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "ಮೇಲಿನ ಉದಾಹರಣೆಯಲ್ಲಿ ಕೆಲವು ಆಂತರಿಕ GenSym ಮಾಯಾಜಾಲವನ್ನು ಬಳಸಲಾಗಿದೆ, ಆದರೆ ಮೂಲ ತರ್ಕವು ನಿಜವಾಗಿಯೂ ಸರಳವಾಗಿದೆ. ಎम्बೆಡ್ಡಿಂಗ್‌ಗಳ ಬಗ್ಗೆ ಒಂದು ಆಸಕ್ತಿದಾಯಕ ವಿಷಯವೆಂದರೆ ನೀವು ಎम्बೆಡ್ಡಿಂಗ್ ವೆಕ್ಟರ್‌ಗಳ ಮೇಲೆ ಸಾಮಾನ್ಯ ವೆಕ್ಟರ್ ಕಾರ್ಯಾಚರಣೆಗಳನ್ನು ನಡೆಸಬಹುದು, ಮತ್ತು ಅದು ಪದಗಳ **ಅರ್ಥಗಳ** ಮೇಲೆ ಕಾರ್ಯಾಚರಣೆಗಳನ್ನು ಪ್ರತಿಬಿಂಬಿಸುತ್ತದೆ. ಮೇಲಿನ ಉದಾಹರಣೆಯನ್ನು ವೆಕ್ಟರ್ ಕಾರ್ಯಾಚರಣೆಗಳ ರೂಪದಲ್ಲಿ ವ್ಯಕ್ತಪಡಿಸಬಹುದು: ನಾವು **KING-MAN+WOMAN** ಗೆ ಹೊಂದಿಕೆಯಾಗುವ ವೆಕ್ಟರ್ ಅನ್ನು ಲೆಕ್ಕಹಾಕುತ್ತೇವೆ (ಕಾರ್ಯಾಚರಣೆಗಳು `+` ಮತ್ತು `-` ಸಂಬಂಧಿತ ಪದಗಳ ವೆಕ್ಟರ್ ಪ್ರತಿನಿಧಿಗಳ ಮೇಲೆ ನಡೆಸಲಾಗುತ್ತವೆ), ಮತ್ತು ನಂತರ ಆ ವೆಕ್ಟರ್‌ಗೆ ನಿಕಟವಾದ ಪದವನ್ನು ನಿಘಂಟಿನಲ್ಲಿ ಹುಡುಕುತ್ತೇವೆ:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'queen'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the vector corresponding to kind-man+woman\n",
    "qvec = w2v['king']-1.7*w2v['man']+1.7*w2v['woman']\n",
    "# find the index of the closest embedding vector \n",
    "d = np.sum((w2v.vectors-qvec)**2,axis=1)\n",
    "min_idx = np.argmin(d)\n",
    "# find the corresponding word\n",
    "w2v.index_to_key[min_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NOTE**: ನಾವು *man* ಮತ್ತು *woman* ವೆಕ್ಟರ್‌ಗಳಿಗೆ ಸಣ್ಣ ಗುಣಾಂಕಗಳನ್ನು ಸೇರಿಸಬೇಕಾಯಿತು - ಅವುಗಳನ್ನು ತೆಗೆದುಹಾಕಿ ಏನಾಗುತ್ತದೆ ಎಂದು ಪ್ರಯತ್ನಿಸಿ.\n",
    "\n",
    "ಅತ್ಯಂತ ಸಮೀಪದ ವೆಕ್ಟರ್ ಅನ್ನು ಕಂಡುಹಿಡಿಯಲು, ನಾವು TensorFlow ಯಂತ್ರವನ್ನು ಬಳಸಿಕೊಂಡು ನಮ್ಮ ವೆಕ್ಟರ್ ಮತ್ತು ಶಬ್ದಕೋಶದಲ್ಲಿರುವ ಎಲ್ಲಾ ವೆಕ್ಟರ್‌ಗಳ ನಡುವಿನ ದೂರಗಳ ವೆಕ್ಟರ್ ಅನ್ನು ಲೆಕ್ಕಹಾಕುತ್ತೇವೆ, ನಂತರ `argmin` ಬಳಸಿ ಕನಿಷ್ಠ ಶಬ್ದದ ಸೂಚ್ಯಂಕವನ್ನು ಹುಡುಕುತ್ತೇವೆ.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec ಪದಾರ್ಥಗಳ ಅರ್ಥವನ್ನು ವ್ಯಕ್ತಪಡಿಸುವ ಉತ್ತಮ ವಿಧಾನವಾಗಿದ್ದರೂ, ಇದಕ್ಕೆ ಹಲವಾರು ದುರ್ಬಲತೆಗಳಿವೆ, ಅವುಗಳಲ್ಲಿ ಪ್ರಮುಖವಾದವುಗಳು:\n",
    "\n",
    "* CBoW ಮತ್ತು skip-gram ಮಾದರಿಗಳು ಎರಡೂ **ಭವಿಷ್ಯವಾಣಿ ಆಧಾರಿತ ಎम्बೆಡ್ಡಿಂಗ್‌ಗಳು**, ಮತ್ತು ಅವುಗಳು ಕೇವಲ ಸ್ಥಳೀಯ ಸಂದರ್ಭವನ್ನು ಮಾತ್ರ ಪರಿಗಣಿಸುತ್ತವೆ. Word2Vec ಜಾಗತಿಕ ಸಂದರ್ಭವನ್ನು ಉಪಯೋಗಿಸುವುದಿಲ್ಲ.\n",
    "* Word2Vec ಪದದ **ರೂಪಶಾಸ್ತ್ರ**ವನ್ನು ಪರಿಗಣಿಸುವುದಿಲ್ಲ, ಅಂದರೆ ಪದದ ಅರ್ಥವು ಪದದ ವಿವಿಧ ಭಾಗಗಳ ಮೇಲೆ, ಉದಾಹರಣೆಗೆ ಮೂಲ ಪದದ ಮೇಲೆ ಅವಲಂಬಿತವಾಗಿರಬಹುದು ಎಂಬ ಸಂಗತಿಯನ್ನು ಗಮನದಲ್ಲಿಟ್ಟುಕೊಳ್ಳುವುದಿಲ್ಲ.\n",
    "\n",
    "**FastText** ಎರಡನೇ ಮಿತಿಯನ್ನು ಮೀರಿ ಹೋಗಲು ಪ್ರಯತ್ನಿಸುತ್ತದೆ, ಮತ್ತು Word2Vec ಮೇಲೆ ನಿರ್ಮಿತವಾಗಿದ್ದು, ಪ್ರತಿ ಪದ ಮತ್ತು ಅದರೊಳಗಿನ ಅಕ್ಷರ n-ಗ್ರಾಮ್‌ಗಳಿಗಾಗಿ ವೆಕ್ಟರ್ ಪ್ರತಿನಿಧಾನಗಳನ್ನು ಕಲಿಯುತ್ತದೆ. ಪ್ರತಿನಿಧಾನದ ಮೌಲ್ಯಗಳನ್ನು ಪ್ರತಿ ತರಬೇತಿ ಹಂತದಲ್ಲಿ ಸರಾಸರಿ ಮಾಡಿ ಒಂದು ವೆಕ್ಟರ್ ಆಗಿ ರೂಪಿಸುತ್ತದೆ. ಇದರಿಂದ ಪೂರ್ವಪ್ರಶಿಕ್ಷಣದಲ್ಲಿ ಹೆಚ್ಚಿನ ಗಣನೆ ಅಗತ್ಯವಿದ್ದರೂ, ಪದ ಎम्बೆಡ್ಡಿಂಗ್‌ಗಳು ಉಪಪದ ಮಾಹಿತಿಯನ್ನು ಸಂಕೇತಗೊಳಿಸಲು ಸಾಧ್ಯವಾಗುತ್ತದೆ.\n",
    "\n",
    "ಮತ್ತೊಂದು ವಿಧಾನ, **GloVe**, ಪದ-ಸಂದರ್ಭ ಮ್ಯಾಟ್ರಿಕ್ಸ್‌ನ ಘಟಕೀಕರಣದ ಆಧಾರದ ಮೇಲೆ ಪದ ಎम्बೆಡ್ಡಿಂಗ್‌ಗಳಿಗೆ ವಿಭಿನ್ನ ವಿಧಾನವನ್ನು ಬಳಸುತ್ತದೆ. ಮೊದಲು, ಇದು ವಿವಿಧ ಸಂದರ್ಭಗಳಲ್ಲಿ ಪದಗಳ ಸಂಭವಗಳ ಸಂಖ್ಯೆಯನ್ನು ಎಣಿಸುವ ದೊಡ್ಡ ಮ್ಯಾಟ್ರಿಕ್ಸ್ ಅನ್ನು ನಿರ್ಮಿಸುತ್ತದೆ, ನಂತರ ಈ ಮ್ಯಾಟ್ರಿಕ್ಸ್ ಅನ್ನು ಕಡಿಮೆ ಆಯಾಮಗಳಲ್ಲಿ ಪ್ರತಿನಿಧಿಸಲು ಪ್ರಯತ್ನಿಸುತ್ತದೆ, ಪುನರ್ ನಿರ್ಮಾಣ ನಷ್ಟವನ್ನು ಕನಿಷ್ಠಗೊಳಿಸುವ ರೀತಿಯಲ್ಲಿ.\n",
    "\n",
    "gensim ಗ್ರಂಥಾಲಯವು ಆ ಪದ ಎम्बೆಡ್ಡಿಂಗ್‌ಗಳನ್ನು ಬೆಂಬಲಿಸುತ್ತದೆ, ಮತ್ತು ಮೇಲಿನ ಮಾದರಿ ಲೋಡ್ ಮಾಡುವ ಕೋಡ್ ಅನ್ನು ಬದಲಾಯಿಸುವ ಮೂಲಕ ನೀವು ಅವುಗಳೊಂದಿಗೆ ಪ್ರಯೋಗ ಮಾಡಬಹುದು.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras ನಲ್ಲಿ pretrained embeddings ಬಳಸುವುದು\n",
    "\n",
    "ಮೇಲಿನ ಉದಾಹರಣೆಯನ್ನು ನಾವು semantic embeddings, ಉದಾಹರಣೆಗೆ Word2Vec, ಬಳಸಿ ನಮ್ಮ embedding ಲೇಯರ್‌ನ ಮ್ಯಾಟ್ರಿಕ್ಸ್ ಅನ್ನು ಪೂರ್ವನಿರ್ಧರಿಸಲು ಬದಲಾಯಿಸಬಹುದು. pretrained embedding ಮತ್ತು ಪಠ್ಯ ಕಾರ್ಪಸ್‌ನ ಶಬ್ದಕೋಶಗಳು ಸಾಮಾನ್ಯವಾಗಿಯೂ ಹೊಂದಿಕೆಯಾಗುವುದಿಲ್ಲ, ಆದ್ದರಿಂದ ನಾವು ಒಂದನ್ನು ಆಯ್ಕೆ ಮಾಡಬೇಕಾಗುತ್ತದೆ. ಇಲ್ಲಿ ನಾವು ಎರಡು ಸಾಧ್ಯವಾದ ಆಯ್ಕೆಗಳನ್ನು ಪರಿಶೀಲಿಸುತ್ತೇವೆ: tokenizer ಶಬ್ದಕೋಶವನ್ನು ಬಳಸುವುದು ಮತ್ತು Word2Vec embeddings ನಿಂದ ಶಬ್ದಕೋಶವನ್ನು ಬಳಸುವುದು.\n",
    "\n",
    "### tokenizer ಶಬ್ದಕೋಶವನ್ನು ಬಳಸುವುದು\n",
    "\n",
    "tokenizer ಶಬ್ದಕೋಶವನ್ನು ಬಳಸುವಾಗ, ಶಬ್ದಕೋಶದ ಕೆಲವು ಪದಗಳಿಗೆ Word2Vec embeddings ಇರುತ್ತವೆ, ಮತ್ತು ಕೆಲವು ಇಲ್ಲದಿರಬಹುದು. ನಮ್ಮ ಶಬ್ದಕೋಶದ ಗಾತ್ರ `vocab_size` ಆಗಿದ್ದು, Word2Vec embedding ವೆಕ್ಟರ್ ಉದ್ದ `embed_size` ಆಗಿದ್ದರೆ, embedding ಲೇಯರ್ ಅನ್ನು `vocab_size`$\\times$`embed_size` ಆಕಾರದ ತೂಕ ಮ್ಯಾಟ್ರಿಕ್ಸ್ ಮೂಲಕ ಪ್ರತಿನಿಧಿಸಲಾಗುತ್ತದೆ. ನಾವು ಈ ಮ್ಯಾಟ್ರಿಕ್ಸ್ ಅನ್ನು ಶಬ್ದಕೋಶದ ಮೂಲಕ ಹೋಗಿ ತುಂಬಿಸುತ್ತೇವೆ:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding size: 300\n",
      "Populating matrix, this will take some time...Done, found 4551 words, 784 words missing\n"
     ]
    }
   ],
   "source": [
    "embed_size = len(w2v.get_vector('hello'))\n",
    "print(f'Embedding size: {embed_size}')\n",
    "\n",
    "vocab = vectorizer.get_vocabulary()\n",
    "W = np.zeros((vocab_size,embed_size))\n",
    "print('Populating matrix, this will take some time...',end='')\n",
    "found, not_found = 0,0\n",
    "for i,w in enumerate(vocab):\n",
    "    try:\n",
    "        W[i] = w2v.get_vector(w)\n",
    "        found+=1\n",
    "    except:\n",
    "        # W[i] = np.random.normal(0.0,0.3,size=(embed_size,))\n",
    "        not_found+=1\n",
    "\n",
    "print(f\"Done, found {found} words, {not_found} words missing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec ಶಬ್ದಕೋಶದಲ್ಲಿ ಇಲ್ಲದ ಪದಗಳಿಗಾಗಿ, ನಾವು ಅವುಗಳನ್ನು ಶೂನ್ಯಗಳಾಗಿ ಬಿಡಬಹುದು ಅಥವಾ ಯಾದೃಚ್ಛಿಕ ವೆಕ್ಟರ್ ಅನ್ನು ರಚಿಸಬಹುದು.\n",
    "\n",
    "ಈಗ ನಾವು ಪೂರ್ವಪ್ರಶಿಕ್ಷಿತ ತೂಕಗಳೊಂದಿಗೆ embedding ಲೇಯರ್ ಅನ್ನು ವ್ಯಾಖ್ಯಾನಿಸಬಹುದು:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = keras.layers.Embedding(vocab_size,embed_size,weights=[W],trainable=False)\n",
    "model = keras.models.Sequential([\n",
    "    vectorizer, emb,\n",
    "    keras.layers.Lambda(lambda x: tf.reduce_mean(x,axis=1)),\n",
    "    keras.layers.Dense(4, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ಈಗ ನಮ್ಮ ಮಾದರಿಯನ್ನು ತರಬೇತುಗೊಳಿಸೋಣ.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 10s 10ms/step - loss: 1.1075 - acc: 0.7822 - val_loss: 0.9134 - val_acc: 0.8175\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2220226ef10>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'])\n",
    "model.fit(ds_train.map(tupelize).batch(batch_size),\n",
    "          validation_data=ds_test.map(tupelize).batch(batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **ಗಮನಿಸಿ**: ನಾವು `Embedding` ಅನ್ನು ಸೃಷ್ಟಿಸುವಾಗ `trainable=False` ಅನ್ನು ಹೊಂದಿಸಿದ್ದೇವೆ, ಅಂದರೆ ನಾವು Embedding ಲೇಯರ್ ಅನ್ನು ಮರುಶಿಕ್ಷಣ ಮಾಡುತ್ತಿಲ್ಲ. ಇದರಿಂದ ನಿಖರತೆ ಸ್ವಲ್ಪ ಕಡಿಮೆಯಾಗಬಹುದು, ಆದರೆ ತರಬೇತಿಯನ್ನು ವೇಗಗೊಳಿಸುತ್ತದೆ.\n",
    "\n",
    "### embedding ಶಬ್ದಕೋಶವನ್ನು ಬಳಸುವುದು\n",
    "\n",
    "ಹಿಂದಿನ ವಿಧಾನದಲ್ಲಿ ಒಂದು ಸಮಸ್ಯೆ ಎಂದರೆ TextVectorization ಮತ್ತು Embedding ನಲ್ಲಿ ಬಳಸಿದ ಶಬ್ದಕೋಶಗಳು ವಿಭಿನ್ನವಾಗಿವೆ. ಈ ಸಮಸ್ಯೆಯನ್ನು ದಾಟಿಸಲು, ನಾವು ಕೆಳಗಿನ ಪರಿಹಾರಗಳಲ್ಲಿ ಒಂದನ್ನು ಬಳಸಬಹುದು:\n",
    "* ನಮ್ಮ ಶಬ್ದಕೋಶದ ಮೇಲೆ Word2Vec ಮಾದರಿಯನ್ನು ಮರುಶಿಕ್ಷಣ ಮಾಡುವುದು.\n",
    "* pretrained Word2Vec ಮಾದರಿಯಿಂದ ಶಬ್ದಕೋಶವನ್ನು ಪಡೆದು ನಮ್ಮ ಡೇಟಾಸೆಟ್ ಅನ್ನು ಲೋಡ್ ಮಾಡುವುದು. ಡೇಟಾಸೆಟ್ ಲೋಡ್ ಮಾಡುವಾಗ ಬಳಸುವ ಶಬ್ದಕೋಶವನ್ನು ನಿರ್ದಿಷ್ಟಪಡಿಸಬಹುದು.\n",
    "\n",
    "ಇದರಲ್ಲಿ ಎರಡನೇ ವಿಧಾನ ಸುಲಭವಾಗಿರುವಂತೆ ತೋರುತ್ತದೆ, ಆದ್ದರಿಂದ ಅದನ್ನು ಅನುಷ್ಠಾನಗೊಳಿಸೋಣ. ಮೊದಲು, Word2Vec embeddings ನಿಂದ ತೆಗೆದುಕೊಂಡ ನಿರ್ದಿಷ್ಟ ಶಬ್ದಕೋಶದೊಂದಿಗೆ `TextVectorization` ಲೇಯರ್ ಅನ್ನು ಸೃಷ್ಟಿಸುತ್ತೇವೆ:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(w2v.vocab.keys())\n",
    "vectorizer = keras.layers.experimental.preprocessing.TextVectorization(input_shape=(1,))\n",
    "vectorizer.set_vocabulary(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gensim ಶಬ್ದ ಎम्बೆಡ್ಡಿಂಗ್‌ಗಳ ಗ್ರಂಥಾಲಯದಲ್ಲಿ, `get_keras_embeddings` ಎಂಬ ಅನುಕೂಲಕರ ಫಂಕ್ಷನ್ ಇದೆ, ಇದು ಸ್ವಯಂಚಾಲಿತವಾಗಿ ನಿಮಗಾಗಿ ಸಂಬಂಧಿತ Keras ಎम्बೆಡ್ಡಿಂಗ್‌ಗಳ ಲೇಯರ್ ಅನ್ನು ರಚಿಸುತ್ತದೆ.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "938/938 [==============================] - 20s 14ms/step - loss: 1.3377 - acc: 0.4978 - val_loss: 1.2995 - val_acc: 0.5647\n",
      "Epoch 2/5\n",
      "938/938 [==============================] - 10s 10ms/step - loss: 1.2587 - acc: 0.5722 - val_loss: 1.2339 - val_acc: 0.5842\n",
      "Epoch 3/5\n",
      "938/938 [==============================] - 10s 10ms/step - loss: 1.1980 - acc: 0.5884 - val_loss: 1.1826 - val_acc: 0.5954\n",
      "Epoch 4/5\n",
      "938/938 [==============================] - 12s 13ms/step - loss: 1.1503 - acc: 0.6002 - val_loss: 1.1417 - val_acc: 0.6018\n",
      "Epoch 5/5\n",
      "938/938 [==============================] - 11s 12ms/step - loss: 1.1120 - acc: 0.6097 - val_loss: 1.1083 - val_acc: 0.6104\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2220ccb81c0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    vectorizer, \n",
    "    w2v.get_keras_embedding(train_embeddings=False),\n",
    "    keras.layers.Lambda(lambda x: tf.reduce_mean(x,axis=1)),\n",
    "    keras.layers.Dense(4, activation='softmax')\n",
    "])\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'])\n",
    "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128),epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ನಾವು ಹೆಚ್ಚಿನ ನಿಖರತೆಯನ್ನು ಕಾಣದಿರುವ ಒಂದು ಕಾರಣವೆಂದರೆ ನಮ್ಮ ಡೇಟಾಸೆಟ್‌ನ ಕೆಲವು ಪದಗಳು ಪೂರ್ವಪ್ರಶಿಕ್ಷಿತ GloVe ಶಬ್ದಕೋಶದಲ್ಲಿ ಇಲ್ಲದಿರುವುದರಿಂದ ಅವುಗಳನ್ನು ಮೂಲತಃ ನಿರ್ಲಕ್ಷಿಸಲಾಗುತ್ತದೆ. ಇದನ್ನು ಮೀರಿ ಹೋಗಲು, ನಾವು ನಮ್ಮ ಡೇಟಾಸೆಟ್ ಆಧರಿಸಿ ನಮ್ಮದೇ embeddings ಅನ್ನು ತರಬೇತಿಮಾಡಬಹುದು.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ಸಾಂದರ್ಭಿಕ ಎम्बೆಡ್ಡಿಂಗ್‌ಗಳು\n",
    "\n",
    "ಪಾರಂಪರಿಕ ಪೂರ್ವಪ್ರಶಿಕ್ಷಿತ ಎम्बೆಡ್ಡಿಂಗ್ ಪ್ರತಿನಿಧಿಗಳಾದ Word2Vec ಮುಂತಾದವುಗಳ ಪ್ರಮುಖ ಮಿತಿ ಎಂದರೆ, ಅವು ಒಂದು ಪದದ ಕೆಲವು ಅರ್ಥಗಳನ್ನು ಹಿಡಿದಿಟ್ಟುಕೊಳ್ಳಬಹುದು ಆದರೂ, ವಿಭಿನ್ನ ಅರ್ಥಗಳನ್ನು ವಿಭಿನ್ನವಾಗಿ ಗುರುತಿಸಲು ಸಾಧ್ಯವಿಲ್ಲ. ಇದರಿಂದ ನಂತರದ ಮಾದರಿಗಳಲ್ಲಿ ಸಮಸ್ಯೆಗಳು ಉಂಟಾಗಬಹುದು.\n",
    "\n",
    "ಉದಾಹರಣೆಗೆ, 'play' ಎಂಬ ಪದಕ್ಕೆ ಈ ಎರಡು ವಿಭಿನ್ನ ವಾಕ್ಯಗಳಲ್ಲಿ ವಿಭಿನ್ನ ಅರ್ಥಗಳಿವೆ:\n",
    "- ನಾನು ನಾಟಕಾಲಯದಲ್ಲಿ ಒಂದು **play** ನೋಡಲು ಹೋದೆನು.\n",
    "- ಜಾನ್ ತನ್ನ ಸ್ನೇಹಿತರೊಂದಿಗೆ **play** ಮಾಡಲು ಬಯಸುತ್ತಾನೆ.\n",
    "\n",
    "ನಾವು ಚರ್ಚಿಸಿದ ಪೂರ್ವಪ್ರಶಿಕ್ಷಿತ ಎम्बೆಡ್ಡಿಂಗ್‌ಗಳು 'play' ಪದದ ಎರಡೂ ಅರ್ಥಗಳನ್ನು ಒಂದೇ ಎम्बೆಡ್ಡಿಂಗ್‌ನಲ್ಲಿ ಪ್ರತಿನಿಧಿಸುತ್ತವೆ. ಈ ಮಿತಿಯನ್ನು ಮೀರುವುದುಗಾಗಿ, ನಾವು **ಭಾಷಾ ಮಾದರಿ** ಆಧಾರಿತ ಎम्बೆಡ್ಡಿಂಗ್‌ಗಳನ್ನು ನಿರ್ಮಿಸಬೇಕಾಗುತ್ತದೆ, ಇದು ದೊಡ್ಡ ಪಠ್ಯ ಸಂಗ್ರಹದ ಮೇಲೆ ತರಬೇತಿಗೊಳಿಸಲಾಗಿದೆ ಮತ್ತು ಪದಗಳನ್ನು ವಿಭಿನ್ನ ಸಾಂದರ್ಭಿಕಗಳಲ್ಲಿ ಹೇಗೆ ಜೋಡಿಸಬಹುದು ಎಂಬುದನ್ನು *ಬಲ್ಲದು*. ಸಾಂದರ್ಭಿಕ ಎम्बೆಡ್ಡಿಂಗ್‌ಗಳ ಬಗ್ಗೆ ಚರ್ಚಿಸುವುದು ಈ ಪಾಠದ ವ್ಯಾಪ್ತಿಗೆ ಸೇರಿಲ್ಲ, ಆದರೆ ಮುಂದಿನ ಘಟಕದಲ್ಲಿ ಭಾಷಾ ಮಾದರಿಗಳ ಬಗ್ಗೆ ಮಾತನಾಡುವಾಗ ನಾವು ಅವುಗಳತ್ತ ಮರಳುತ್ತೇವೆ.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n<!-- CO-OP TRANSLATOR DISCLAIMER START -->\n**ಅಸ್ವೀಕರಣ**:  \nಈ ದಸ್ತಾವೇಜು AI ಅನುವಾದ ಸೇವೆ [Co-op Translator](https://github.com/Azure/co-op-translator) ಬಳಸಿ ಅನುವಾದಿಸಲಾಗಿದೆ. ನಾವು ನಿಖರತೆಯಿಗಾಗಿ ಪ್ರಯತ್ನಿಸುತ್ತಿದ್ದರೂ, ಸ್ವಯಂಚಾಲಿತ ಅನುವಾದಗಳಲ್ಲಿ ತಪ್ಪುಗಳು ಅಥವಾ ಅಸತ್ಯತೆಗಳು ಇರಬಹುದು ಎಂದು ದಯವಿಟ್ಟು ಗಮನಿಸಿ. ಮೂಲ ಭಾಷೆಯಲ್ಲಿರುವ ಮೂಲ ದಸ್ತಾವೇಜನ್ನು ಅಧಿಕೃತ ಮೂಲವೆಂದು ಪರಿಗಣಿಸಬೇಕು. ಮಹತ್ವದ ಮಾಹಿತಿಗಾಗಿ, ವೃತ್ತಿಪರ ಮಾನವ ಅನುವಾದವನ್ನು ಶಿಫಾರಸು ಮಾಡಲಾಗುತ್ತದೆ. ಈ ಅನುವಾದ ಬಳಕೆಯಿಂದ ಉಂಟಾಗುವ ಯಾವುದೇ ತಪ್ಪು ಅರ್ಥಮಾಡಿಕೊಳ್ಳುವಿಕೆ ಅಥವಾ ತಪ್ಪು ವಿವರಣೆಗಳಿಗೆ ನಾವು ಹೊಣೆಗಾರರಾಗುವುದಿಲ್ಲ.\n<!-- CO-OP TRANSLATOR DISCLAIMER END -->\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0cb620c6d4b9f7a635928804c26cf22403d89d98d79684e4529119355ee6d5a5"
  },
  "kernel_info": {
   "name": "conda-env-py37_tensorflow-py"
  },
  "kernelspec": {
   "display_name": "py37_tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "coopTranslator": {
   "original_hash": "b859482be7f61d1eadc2c6a2720a37e4",
   "translation_date": "2025-11-26T01:53:30+00:00",
   "source_file": "lessons/5-NLP/14-Embeddings/EmbeddingsTF.ipynb",
   "language_code": "kn"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}