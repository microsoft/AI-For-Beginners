{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ಗಮನ ಯಂತ್ರಗಳು ಮತ್ತು ಟ್ರಾನ್ಸ್‌ಫಾರ್ಮರ್‌ಗಳು\n",
    "\n",
    "ಪುನರಾವರ್ತಿತ ಜಾಲಗಳ ಪ್ರಮುಖ ದೋಷವೆಂದರೆ ಸರಣಿಯಲ್ಲಿನ ಎಲ್ಲಾ ಪದಗಳು ಫಲಿತಾಂಶದ ಮೇಲೆ ಸಮಾನ ಪ್ರಭಾವ ಬೀರುತ್ತವೆ. ಇದರಿಂದ ನಾಮಿತ ಘಟಕ ಗುರುತಿಸುವಿಕೆ ಮತ್ತು ಯಂತ್ರ ಅನುವಾದದಂತಹ ಸರಣಿ-ನಿಂದ-ಸರಣಿ ಕಾರ್ಯಗಳಿಗೆ ಸಾಮಾನ್ಯ LSTM ಎನ್‌ಕೋಡರ್-ಡಿಕೋಡರ್ ಮಾದರಿಗಳೊಂದಿಗೆ ಅತೀಕೃಷ್ಟ ಕಾರ್ಯಕ್ಷಮತೆ ಉಂಟಾಗುತ್ತದೆ. ವಾಸ್ತವದಲ್ಲಿ, ಇನ್‌ಪುಟ್ ಸರಣಿಯ ನಿರ್ದಿಷ್ಟ ಪದಗಳು ಇತರ ಪದಗಳಿಗಿಂತ ಹೆಚ್ಚು ಪ್ರಭಾವ ಬೀರುತ್ತವೆ.\n",
    "\n",
    "ಯಂತ್ರ ಅನುವಾದದಂತಹ ಸರಣಿ-ನಿಂದ-ಸರಣಿ ಮಾದರಿಯನ್ನು ಪರಿಗಣಿಸಿ. ಇದು ಎರಡು ಪುನರಾವರ್ತಿತ ಜಾಲಗಳ ಮೂಲಕ ಅನುಷ್ಠಾನಗೊಳ್ಳುತ್ತದೆ, ಇಲ್ಲಿ ಒಂದು ಜಾಲ (**ಎನ್‌ಕೋಡರ್**) ಇನ್‌ಪುಟ್ ಸರಣಿಯನ್ನು ಗುಪ್ತ ಸ್ಥಿತಿಗೆ ಸಂಕುಚಿತಗೊಳಿಸುತ್ತದೆ, ಮತ್ತು ಇನ್ನೊಂದು, **ಡಿಕೋಡರ್**, ಈ ಗುಪ್ತ ಸ್ಥಿತಿಯನ್ನು ಅನುವಾದಿತ ಫಲಿತಾಂಶವಾಗಿ ವಿಸ್ತರಿಸುತ್ತದೆ. ಈ ವಿಧಾನದಲ್ಲಿ ಸಮಸ್ಯೆ ಏನೆಂದರೆ ಜಾಲದ ಅಂತಿಮ ಸ್ಥಿತಿಗೆ ವಾಕ್ಯದ ಆರಂಭವನ್ನು ನೆನಪಿಡಲು ಕಷ್ಟವಾಗುತ್ತದೆ, ಇದರಿಂದ ದೀರ್ಘ ವಾಕ್ಯಗಳಲ್ಲಿ ಮಾದರಿಯ ಗುಣಮಟ್ಟ ಕೆಳಮಟ್ಟಕ್ಕೆ ಇಳಿಯುತ್ತದೆ.\n",
    "\n",
    "**ಗಮನ ಯಂತ್ರಗಳು** RNNನ ಪ್ರತಿ ಔಟ್‌ಪುಟ್ ಭವಿಷ್ಯವಾಣಿ ಮೇಲೆ ಪ್ರತಿ ಇನ್‌ಪುಟ್ ವೆಕ್ಟರ್‌ನ ಸಾಂದರ್ಭಿಕ ಪ್ರಭಾವವನ್ನು ತೂಕ ನೀಡುವ ವಿಧಾನವನ್ನು ಒದಗಿಸುತ್ತವೆ. ಇದನ್ನು ಅನುಷ್ಠಾನಗೊಳಿಸುವ ವಿಧಾನವೆಂದರೆ ಇನ್‌ಪುಟ್ RNNನ ಮಧ್ಯಂತರ ಸ್ಥಿತಿಗಳ ಮತ್ತು ಔಟ್‌ಪುಟ್ RNNನ ನಡುವೆ ಶಾರ್ಟ್‌ಕಟ್‌ಗಳನ್ನು ಸೃಷ್ಟಿಸುವುದು. ಈ ರೀತಿಯಲ್ಲಿ, ಔಟ್‌ಪುಟ್ ಚಿಹ್ನೆ $y_t$ ರಚಿಸುವಾಗ, ನಾವು ಎಲ್ಲಾ ಇನ್‌ಪುಟ್ ಗುಪ್ತ ಸ್ಥಿತಿಗಳು $h_i$ಗಳನ್ನು ವಿಭಿನ್ನ ತೂಕ ಗುಣಾಂಕಗಳ $\\alpha_{t,i}$ೊಂದಿಗೆ ಪರಿಗಣಿಸುತ್ತೇವೆ.\n",
    "\n",
    "![ಚಿತ್ರವು ಎನ್‌ಕೋಡರ್/ಡಿಕೋಡರ್ ಮಾದರಿಯನ್ನು ಸೇರಿಸಿದ ಗಮನ ಪದರದೊಂದಿಗೆ ತೋರಿಸುತ್ತದೆ](../../../../../translated_images/kn/encoder-decoder-attention.7a726296894fb567.webp)\n",
    "*ಎನ್‌ಕೋಡರ್-ಡಿಕೋಡರ್ ಮಾದರಿಯೊಂದಿಗೆ ಸೇರಿಸಿದ ಗಮನ ಯಂತ್ರವನ್ನು [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) ನಲ್ಲಿ ವಿವರಿಸಲಾಗಿದೆ, [ಈ ಬ್ಲಾಗ್ ಪೋಸ್ಟ್](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html) ನಿಂದ ಉಲ್ಲೇಖಿಸಲಾಗಿದೆ*\n",
    "\n",
    "ಗಮನ ಮ್ಯಾಟ್ರಿಕ್ಸ್ $\\{\\alpha_{i,j}\\}$ ನಿರ್ದಿಷ್ಟ ಇನ್‌ಪುಟ್ ಪದಗಳು ಔಟ್‌ಪುಟ್ ಸರಣಿಯ ನಿರ್ದಿಷ್ಟ ಪದ ರಚನೆಯಲ್ಲಿ ಎಷ್ಟು ಪಾತ್ರ ವಹಿಸುತ್ತವೆ ಎಂಬುದನ್ನು ಪ್ರತಿನಿಧಿಸುತ್ತದೆ. ಕೆಳಗಿನ ಚಿತ್ರದಲ್ಲಿ ಇಂತಹ ಮ್ಯಾಟ್ರಿಕ್ಸ್ ಉದಾಹರಣೆ ನೀಡಲಾಗಿದೆ:\n",
    "\n",
    "![ಚಿತ್ರವು RNNsearch-50 ಮೂಲಕ ಕಂಡುಬಂದ ಮಾದರಿ ಹೊಂದಾಣಿಕೆಯನ್ನು ತೋರಿಸುತ್ತದೆ, Bahdanau - arviz.org ನಿಂದ ತೆಗೆದುಕೊಂಡಿದೆ](../../../../../translated_images/kn/bahdanau-fig3.09ba2d37f202a6af.webp)\n",
    "\n",
    "*ಚಿತ್ರ [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) (ಚಿತ್ರ 3) ನಿಂದ ತೆಗೆದುಕೊಂಡಿದೆ*\n",
    "\n",
    "ಗಮನ ಯಂತ್ರಗಳು ನೈಸರ್ಗಿಕ ಭಾಷಾ ಪ್ರಕ್ರಿಯೆಯಲ್ಲಿ ಇತ್ತೀಚಿನ ಅಥವಾ ಸಮೀಪದ ಇತ್ತೀಚಿನ ಅತ್ಯುತ್ತಮ ಸ್ಥಿತಿಗೆ ಬಹುಮಟ್ಟಿಗೆ ಕಾರಣವಾಗಿವೆ. ಆದರೆ ಗಮನವನ್ನು ಸೇರಿಸುವುದರಿಂದ ಮಾದರಿ ಪರಿಮಾಣಗಳು ಬಹಳಷ್ಟು ಹೆಚ್ಚಾಗುತ್ತವೆ, ಇದರಿಂದ RNNಗಳೊಂದಿಗೆ ಪ್ರಮಾಣವರ್ಧನೆ ಸಮಸ್ಯೆಗಳು ಉಂಟಾಗಿವೆ. RNNಗಳನ್ನು ಪ್ರಮಾಣವರ್ಧಿಸಲು ಪ್ರಮುಖ ಅಡ್ಡಿ ಎಂದರೆ ಮಾದರಿಗಳ ಪುನರಾವರ್ತಿತ ಸ್ವಭಾವವು ತರಬೇತಿಯನ್ನು ಬ್ಯಾಚ್ ಮತ್ತು ಸಮಾಂತರಗೊಳಿಸುವುದನ್ನು ಕಷ್ಟಕರವಾಗಿಸುತ್ತದೆ. RNNನಲ್ಲಿ ಸರಣಿಯ ಪ್ರತಿ ಅಂಶವನ್ನು ಕ್ರಮಬದ್ಧವಾಗಿ ಪ್ರಕ್ರಿಯೆಗೊಳಿಸಬೇಕಾಗುತ್ತದೆ, ಆದ್ದರಿಂದ ಅದನ್ನು ಸುಲಭವಾಗಿ ಸಮಾಂತರಗೊಳಿಸಲಾಗುವುದಿಲ್ಲ.\n",
    "\n",
    "ಈ ಅಡ್ಡಿಯೊಂದಿಗೆ ಗಮನ ಯಂತ್ರಗಳ ಅಳವಡಿಕೆಯಿಂದ ನಾವು ಇಂದು BERT ರಿಂದ OpenGPT3 ವರೆಗೆ ಬಳಸುವ ಅತ್ಯುತ್ತಮ ಟ್ರಾನ್ಸ್‌ಫಾರ್ಮರ್ ಮಾದರಿಗಳನ್ನು ಸೃಷ್ಟಿಸಿದ್ದೇವೆ.\n",
    "\n",
    "## ಟ್ರಾನ್ಸ್‌ಫಾರ್ಮರ್ ಮಾದರಿಗಳು\n",
    "\n",
    "ಪ್ರತಿ ಹಿಂದಿನ ಭವಿಷ್ಯವಾಣಿಯ ಸಾಂದರ್ಭಿಕತೆಯನ್ನು ಮುಂದಿನ ಮೌಲ್ಯಮಾಪನ ಹಂತಕ್ಕೆ ಕಳುಹಿಸುವ ಬದಲು, **ಟ್ರಾನ್ಸ್‌ಫಾರ್ಮರ್ ಮಾದರಿಗಳು** **ಸ್ಥಾನಿಕ ಎನ್‌ಕೋಡಿಂಗ್‌ಗಳು** ಮತ್ತು **ಗಮನ** ಬಳಸಿ ನೀಡಲಾದ ಪಠ್ಯದ ವಿಂಡೋದಲ್ಲಿ ಇನ್‌ಪುಟ್‌ನ ಸಾಂದರ್ಭಿಕತೆಯನ್ನು ಹಿಡಿಯುತ್ತವೆ. ಕೆಳಗಿನ ಚಿತ್ರವು ಸ್ಥಳೀಯ ಎನ್‌ಕೋಡಿಂಗ್‌ಗಳು ಮತ್ತು ಗಮನವು ನೀಡಲಾದ ವಿಂಡೋದಲ್ಲಿ ಸಾಂದರ್ಭಿಕತೆಯನ್ನು ಹೇಗೆ ಹಿಡಿಯಬಹುದು ಎಂಬುದನ್ನು ತೋರಿಸುತ್ತದೆ.\n",
    "\n",
    "![ಟ್ರಾನ್ಸ್‌ಫಾರ್ಮರ್ ಮಾದರಿಗಳಲ್ಲಿ ಮೌಲ್ಯಮಾಪನಗಳು ಹೇಗೆ ನಡೆಯುತ್ತವೆ ಎಂಬುದನ್ನು ತೋರಿಸುವ ಅನಿಮೇಟೆಡ್ GIF](../../../../../lessons/5-NLP/18-Transformers/images/transformer-animated-explanation.gif)\n",
    "\n",
    "ಪ್ರತಿ ಇನ್‌ಪುಟ್ ಸ್ಥಾನವನ್ನು ಪ್ರತಿ ಔಟ್‌ಪುಟ್ ಸ್ಥಾನಕ್ಕೆ ಸ್ವತಂತ್ರವಾಗಿ ನಕ್ಷೆ ಮಾಡಲಾಗುವುದರಿಂದ, ಟ್ರಾನ್ಸ್‌ಫಾರ್ಮರ್‌ಗಳು RNNಗಳಿಗಿಂತ ಉತ್ತಮವಾಗಿ ಸಮಾಂತರಗೊಳಿಸಬಹುದು, ಇದರಿಂದ ಬಹಳ ದೊಡ್ಡ ಮತ್ತು ಹೆಚ್ಚು ಅಭಿವ್ಯಕ್ತ ಭಾಷಾ ಮಾದರಿಗಳನ್ನು ಸೃಷ್ಟಿಸಲು ಸಾಧ್ಯವಾಗುತ್ತದೆ. ಪ್ರತಿ ಗಮನ ತಲೆ ಪದಗಳ ನಡುವಿನ ವಿಭಿನ್ನ ಸಂಬಂಧಗಳನ್ನು ಕಲಿಯಲು ಬಳಸಬಹುದು, ಇದು ನಂತರದ ನೈಸರ್ಗಿಕ ಭಾಷಾ ಪ್ರಕ್ರಿಯೆ ಕಾರ್ಯಗಳನ್ನು ಸುಧಾರಿಸುತ್ತದೆ.\n",
    "\n",
    "## ಸರಳ ಟ್ರಾನ್ಸ್‌ಫಾರ್ಮರ್ ಮಾದರಿ ನಿರ್ಮಾಣ\n",
    "\n",
    "Kerasನಲ್ಲಿ ನಿರ್ಮಿತ ಟ್ರಾನ್ಸ್‌ಫಾರ್ಮರ್ ಪದರವಿಲ್ಲ, ಆದರೆ ನಾವು ನಮ್ಮದೇ ಮಾದರಿಯನ್ನು ನಿರ್ಮಿಸಬಹುದು. ಹಿಂದಿನಂತೆ, ನಾವು AG News ಡೇಟಾಸೆಟ್‌ನ ಪಠ್ಯ ವರ್ಗೀಕರಣದ ಮೇಲೆ ಗಮನಹರಿಸುವೆವು, ಆದರೆ ಟ್ರಾನ್ಸ್‌ಫಾರ್ಮರ್ ಮಾದರಿಗಳು ಹೆಚ್ಚು ಕಠಿಣ NLP ಕಾರ್ಯಗಳಲ್ಲಿ ಉತ್ತಮ ಫಲಿತಾಂಶ ತೋರಿಸುತ್ತವೆ ಎಂದು ಹೇಳುವುದು ಮುಖ್ಯ.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "\n",
    "ds_train, ds_test = tfds.load('ag_news_subset').values()\n",
    "\n",
    "def extract_text(x):\n",
    "    return x['title']+' '+x['description']\n",
    "\n",
    "def tupelize(x):\n",
    "    return (extract_text(x),x['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ಕೇರಾಸ್‌ನಲ್ಲಿ ಹೊಸ ಲೇಯರ್‌ಗಳು `Layer` ಕ್ಲಾಸ್ ಅನ್ನು subclass ಮಾಡಬೇಕು ಮತ್ತು `call` ಮೆಥಡ್ ಅನ್ನು ಅನುಷ್ಠಾನಗೊಳಿಸಬೇಕು. **ಸ್ಥಾನಿಕ ಎಂಬೆಡ್ಡಿಂಗ್** ಲೇಯರ್‌ನಿಂದ ಪ್ರಾರಂಭಿಸೋಣ. ನಾವು [ಕೇರಾಸ್ ಅಧಿಕೃತ ಡಾಕ್ಯುಮೆಂಟೇಶನ್‌ನಿಂದ ಕೆಲವು ಕೋಡ್](https://keras.io/examples/nlp/text_classification_with_transformer/) ಬಳಸಲಿದ್ದೇವೆ. ನಾವು ಎಲ್ಲಾ ಇನ್‌ಪುಟ್ ಸರಣಿಗಳನ್ನು `maxlen` ಉದ್ದಕ್ಕೆ ಪ್ಯಾಡ್ ಮಾಡಲಾಗಿದೆ ಎಂದು فرض ಮಾಡುತ್ತೇವೆ.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(keras.layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = keras.layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "        self.maxlen = maxlen\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = self.maxlen\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x+positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ಈ ಲೇಯರ್ ಎರಡು `Embedding` ಲೇಯರ್‌ಗಳಿಂದ ಕೂಡಿದೆ: ಟೋಕನ್‌ಗಳನ್ನು ಎम्बೆಡ್ ಮಾಡಲು (ನಾವು ಹಿಂದಿನಂತೆ ಚರ್ಚಿಸಿದ ರೀತಿಯಲ್ಲಿ) ಮತ್ತು ಟೋಕನ್ ಸ್ಥಾನಗಳನ್ನು ಎम्बೆಡ್ ಮಾಡಲು. ಟೋಕನ್ ಸ್ಥಾನಗಳನ್ನು 0 ರಿಂದ `maxlen` ವರೆಗೆ ನೈಸರ್ಗಿಕ ಸಂಖ್ಯೆಗಳ ಸರಣಿಯಾಗಿ `tf.range` ಬಳಸಿ ರಚಿಸಲಾಗುತ್ತದೆ, ನಂತರ ಅದನ್ನು ಎम्बೆಡಿಂಗ್ ಲೇಯರ್ ಮೂಲಕ ಹಾದುಹೋಗುತ್ತದೆ. ಎರಡು ಫಲಿತಾಂಶ ಎम्बೆಡಿಂಗ್ ವೆಕ್ಟರ್‌ಗಳನ್ನು ಸೇರಿಸಲಾಗುತ್ತದೆ, ಇದರಿಂದ `maxlen`$\\times$`embed_dim` ಆಕಾರದ ಸ್ಥಾನಾನುಕ್ರಮಿತ ಎम्बೆಡಿಂಗ್ ಪ್ರತಿನಿಧಾನ ಸೃಷ್ಟಿಯಾಗುತ್ತದೆ.\n",
    "\n",
    "<img src=\"../../../../../translated_images/kn/pos-embedding.e41ce9b6cf6078af.webp\" width=\"40%\"/>\n",
    "\n",
    "ಈಗ, ಟ್ರಾನ್ಸ್‌ಫಾರ್ಮರ್ ಬ್ಲಾಕ್ ಅನ್ನು ಅನುಷ್ಠಾನಗೊಳಿಸೋಣ. ಇದು ಹಿಂದಿನದಾಗಿ ವ್ಯಾಖ್ಯಾನಿಸಿದ ಎम्बೆಡಿಂಗ್ ಲೇಯರ್‌ನ ಔಟ್‌ಪುಟ್ ಅನ್ನು ತೆಗೆದುಕೊಳ್ಳುತ್ತದೆ:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim, name='attn')\n",
    "        self.ffn = keras.Sequential(\n",
    "            [keras.layers.Dense(ff_dim, activation=\"relu\"), keras.layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = keras.layers.Dropout(rate)\n",
    "        self.dropout2 = keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer ಸ್ಥಾನಾನುಕ್ರಮಿತ ಇನ್‌ಪುಟ್‌ಗೆ `MultiHeadAttention` ಅನ್ನು ಅನ್ವಯಿಸಿ, `maxlen`$\\times$`embed_dim` ಆಯಾಮದ ಗಮನ ವೆಕ್ಟರ್ ಅನ್ನು ಉತ್ಪಾದಿಸುತ್ತದೆ, ಇದನ್ನು ನಂತರ ಇನ್‌ಪುಟ್ ಜೊತೆಗೆ ಮಿಶ್ರಣ ಮಾಡಿ `LayerNormalization` ಬಳಸಿ ಸಾಮಾನ್ಯೀಕರಿಸಲಾಗುತ್ತದೆ.\n",
    "\n",
    "> **Note**: `LayerNormalization` ಅನ್ನು ಈ ಕಲಿಕೆ ಮಾರ್ಗದ *ಕಂಪ್ಯೂಟರ್ ವೀಷನ್* ಭಾಗದಲ್ಲಿ ಚರ್ಚಿಸಲಾದ `BatchNormalization` ಗೆ ಹೋಲಿಸಬಹುದು, ಆದರೆ ಇದು ಪ್ರತಿ ತರಬೇತಿ ಮಾದರಿಗಾಗಿ ಹಿಂದಿನ ಲೇಯರ್‌ನ ಔಟ್‌ಪುಟ್‌ಗಳನ್ನು ಸ್ವತಂತ್ರವಾಗಿ ಸಾಮಾನ್ಯೀಕರಿಸಿ ಅವುಗಳನ್ನು [-1..1] ಶ್ರೇಣಿಗೆ ತರುತ್ತದೆ.\n",
    "\n",
    "ಈ ಲೇಯರ್‌ನ ಔಟ್‌ಪುಟ್ ನಂತರ `Dense` ನೆಟ್‌ವರ್ಕ್ (ನಮ್ಮ ಪ್ರಕರಣದಲ್ಲಿ - ಎರಡು ಲೇಯರ್ ಪರ್ಸೆಪ್ಟ್ರಾನ್) ಮೂಲಕ ಸಾಗಿಸಲಾಗುತ್ತದೆ, ಮತ್ತು ಫಲಿತಾಂಶವನ್ನು ಅಂತಿಮ ಔಟ್‌ಪುಟ್‌ಗೆ ಸೇರಿಸಲಾಗುತ್ತದೆ (ಅದು ಮತ್ತೆ ಸಾಮಾನ್ಯೀಕರಣಕ್ಕೆ ಒಳಗಾಗುತ್ತದೆ).\n",
    "\n",
    "<img src=\"../../../../../translated_images/kn/transformer-layer.905e14747ca4e7d5.webp\" width=\"30%\" />\n",
    "\n",
    "ಈಗ, ನಾವು ಸಂಪೂರ್ಣ ಟ್ರಾನ್ಸ್‌ಫಾರ್ಮರ್ ಮಾದರಿಯನ್ನು ವ್ಯಾಖ್ಯಾನಿಸಲು ಸಿದ್ಧರಾಗಿದ್ದೇವೆ:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "text_vectorization (TextVect (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "token_and_position_embedding (None, 256, 32)           648192    \n",
      "_________________________________________________________________\n",
      "transformer_block (Transform (None, 256, 32)           10656     \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 20)                660       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 84        \n",
      "=================================================================\n",
      "Total params: 659,592\n",
      "Trainable params: 659,592\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 32  # Embedding size for each token\n",
    "num_heads = 2  # Number of attention heads\n",
    "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
    "maxlen = 256\n",
    "vocab_size = 20000\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.experimental.preprocessing.TextVectorization(max_tokens=vocab_size,output_sequence_length=maxlen, input_shape=(1,)),\n",
    "    TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim),\n",
    "    TransformerBlock(embed_dim, num_heads, ff_dim),\n",
    "    keras.layers.GlobalAveragePooling1D(),\n",
    "    keras.layers.Dropout(0.1),\n",
    "    keras.layers.Dense(20, activation=\"relu\"),\n",
    "    keras.layers.Dropout(0.1),\n",
    "    keras.layers.Dense(4, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokenizer\n",
      "938/938 [==============================] - 45s 39ms/step - loss: 0.4978 - acc: 0.8068 - val_loss: 0.2808 - val_acc: 0.9124\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9c2427a0d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Training tokenizer')\n",
    "model.layers[0].adapt(ds_train.map(extract_text))\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'], optimizer='adam')\n",
    "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT ಟ್ರಾನ್ಸ್‌ಫಾರ್ಮರ್ ಮಾದರಿಗಳು\n",
    "\n",
    "**BERT** (Bidirectional Encoder Representations from Transformers) ಒಂದು ಬಹಳ ದೊಡ್ಡ ಬಹುಮಟ್ಟದ ಟ್ರಾನ್ಸ್‌ಫಾರ್ಮರ್ ನೆಟ್‌ವರ್ಕ್ ಆಗಿದ್ದು, *BERT-base* ಗೆ 12 ಲೇಯರ್‌ಗಳು ಮತ್ತು *BERT-large* ಗೆ 24 ಲೇಯರ್‌ಗಳಿವೆ. ಈ ಮಾದರಿಯನ್ನು ಮೊದಲು ದೊಡ್ಡ ಪಠ್ಯ ಡೇಟಾ (ವಿಕಿಪೀಡಿಯಾ + ಪುಸ್ತಕಗಳು) ಮೇಲೆ ಅನ್‌ಸೂಪರ್ವೈಸ್‌ಡ್ ತರಬೇತಿಯಲ್ಲಿ (ವಾಕ್ಯದಲ್ಲಿ ಮಸ್ಕ್ ಮಾಡಲಾದ ಪದಗಳನ್ನು ಊಹಿಸುವ ಮೂಲಕ) ಪೂರ್ವ-ತರಬೇತಿ ಮಾಡಲಾಗುತ್ತದೆ. ಪೂರ್ವ-ತರಬೇತಿ ಸಮಯದಲ್ಲಿ, ಮಾದರಿ ಭಾಷೆಯ ಅರ್ಥಮಾಡಿಕೊಳ್ಳುವ ಮಹತ್ವದ ಮಟ್ಟವನ್ನು ಅಳವಡಿಸಿಕೊಂಡು, ನಂತರ ಇತರ ಡೇಟಾಸೆಟ್‌ಗಳೊಂದಿಗೆ ಫೈನ್ ಟ್ಯೂನಿಂಗ್ ಮೂಲಕ ಉಪಯೋಗಿಸಬಹುದು. ಈ ಪ್ರಕ್ರಿಯೆಯನ್ನು **ಟ್ರಾನ್ಸ್‌ಫರ್ ಲರ್ನಿಂಗ್** ಎಂದು ಕರೆಯುತ್ತಾರೆ.\n",
    "\n",
    "![picture from http://jalammar.github.io/illustrated-bert/](../../../../../translated_images/kn/jalammarBERT-language-modeling-masked-lm.34f113ea5fec4362.webp)\n",
    "\n",
    "ಟ್ರಾನ್ಸ್‌ಫಾರ್ಮರ್ ವಾಸ್ತುಶಿಲ್ಪಗಳ ಹಲವು ಬದಲಾವಣೆಗಳಿವೆ, ಅವುಗಳಲ್ಲಿ BERT, DistilBERT, BigBird, OpenGPT3 ಮತ್ತು ಇನ್ನಷ್ಟು ಫೈನ್ ಟ್ಯೂನಿಂಗ್ ಮಾಡಬಹುದಾದವುಗಳಾಗಿವೆ.\n",
    "\n",
    "ನಮ್ಮ ಪರಂಪರাগত ಕ್ರಮ ವರ್ಗೀಕರಣ ಸಮಸ್ಯೆಯನ್ನು ಪರಿಹರಿಸಲು ಪೂರ್ವ-ತರಬೇತಿಗೊಂಡ BERT ಮಾದರಿಯನ್ನು ಹೇಗೆ ಬಳಸಬಹುದು ಎಂದು ನೋಡೋಣ. ನಾವು [ಅಧಿಕೃತ ಡಾಕ್ಯುಮೆಂಟೇಶನ್](https://www.tensorflow.org/text/tutorials/classify_text_with_bert) ನಿಂದ ಆಲೋಚನೆ ಮತ್ತು ಕೆಲವು ಕೋಡ್ ಅನ್ನು ತೆಗೆದುಕೊಳ್ಳುತ್ತೇವೆ.\n",
    "\n",
    "ಪೂರ್ವ-ತರಬೇತಿಗೊಂಡ ಮಾದರಿಗಳನ್ನು ಲೋಡ್ ಮಾಡಲು, ನಾವು **Tensorflow hub** ಅನ್ನು ಬಳಸುತ್ತೇವೆ. ಮೊದಲು, BERT-ನಿರ್ದಿಷ್ಟ ವೆಕ್ಟರೈಜರ್ ಅನ್ನು ಲೋಡ್ ಮಾಡೋಣ:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_41180/4216669875.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow_text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow_hub\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mhub\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhub\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mKerasLayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_text'"
     ]
    }
   ],
   "source": [
    "import tensorflow_text \n",
    "import tensorflow_hub as hub\n",
    "vectorizer = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_type_ids': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       " array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "       dtype=int32)>,\n",
       " 'input_word_ids': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       " array([[  101,  1045,  2293, 19081,   102,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]], dtype=int32)>,\n",
       " 'input_mask': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       " array([[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "       dtype=int32)>}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer(['I love transformers'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ಮೂಲ ನೆಟ್‌ವರ್ಕ್ ತರಬೇತಿ ಪಡೆದಿದ್ದ ಅದೇ ವೆಕ್ಟರೈಜರ್ ಅನ್ನು ಬಳಸುವುದು ಬಹುಮುಖ್ಯ. ಜೊತೆಗೆ, BERT ವೆಕ್ಟರೈಜರ್ ಮೂರು ಘಟಕಗಳನ್ನು ನೀಡುತ್ತದೆ:\n",
    "* `input_word_ids`, ಇದು ಇನ್‌ಪುಟ್ ವಾಕ್ಯದ ಟೋಕನ್ ಸಂಖ್ಯೆಗಳ ಸರಣಿಯಾಗಿದೆ\n",
    "* `input_mask`, ಇದು ಸರಣಿಯ ಯಾವ ಭಾಗವು ನಿಜವಾದ ಇನ್‌ಪುಟ್ ಅನ್ನು ಹೊಂದಿದೆ ಮತ್ತು ಯಾವುದು ಪ್ಯಾಡಿಂಗ್ ಆಗಿದೆ ಎಂದು ತೋರಿಸುತ್ತದೆ. ಇದು `Masking` ಲೇಯರ್ ನಿರ್ಮಿಸುವ ಮಾಸ್ಕ್‌ಗೆ ಸಮಾನವಾಗಿದೆ\n",
    "* `input_type_ids` ಭಾಷಾ ಮಾದರೀಕರಣ ಕಾರ್ಯಗಳಿಗೆ ಬಳಸಲಾಗುತ್ತದೆ ಮತ್ತು ಒಂದು ಸರಣಿಯಲ್ಲಿ ಎರಡು ಇನ್‌ಪುಟ್ ವಾಕ್ಯಗಳನ್ನು ನಿರ್ದಿಷ್ಟಪಡಿಸಲು ಅನುಮತಿಸುತ್ತದೆ.\n",
    "\n",
    "ಆಮೇಲೆ, ನಾವು BERT ವೈಶಿಷ್ಟ್ಯ ಎಕ್ಸ್ಟ್ರಾಕ್ಟರ್ ಅನ್ನು ಸ್ಥಾಪಿಸಬಹುದು:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = hub.KerasLayer('https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pooled_output -> (1, 128)\n",
      "encoder_outputs -> 4\n",
      "sequence_output -> (1, 128, 128)\n",
      "default -> (1, 128)\n"
     ]
    }
   ],
   "source": [
    "z = bert(vectorizer(['I love transformers']))\n",
    "for i,x in z.items():\n",
    "    print(f\"{i} -> { len(x) if isinstance(x, list) else x.shape }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ಹೀಗಾಗಿ, BERT ಲೇಯರ್ ಹಲವಾರು ಉಪಯುಕ್ತ ಫಲಿತಾಂಶಗಳನ್ನು ನೀಡುತ್ತದೆ:\n",
    "* `pooled_output` ಎಂದರೆ ಸರಣಿಯಲ್ಲಿನ ಎಲ್ಲಾ ಟೋಕನ್‌ಗಳ ಸರಾಸರಿ ಫಲಿತಾಂಶ. ಇದನ್ನು ಸಂಪೂರ್ಣ ನೆಟ್‌ವರ್ಕ್‌ನ ಬುದ್ಧಿವಂತ ಸಾಂದರ್ಭಿಕ ಎम्बೆಡ್ಡಿಂಗ್ ಎಂದು ನೋಡಬಹುದು. ಇದು ನಮ್ಮ ಹಿಂದಿನ ಮಾದರಿಯಲ್ಲಿ `GlobalAveragePooling1D` ಲೇಯರ್‌ನ ಔಟ್‌ಪುಟ್‌ಗೆ ಸಮಾನವಾಗಿದೆ.\n",
    "* `sequence_output` ಎಂದರೆ ಕೊನೆಯ ಟ್ರಾನ್ಸ್‌ಫಾರ್ಮರ್ ಲೇಯರ್‌ನ ಔಟ್‌ಪುಟ್ (ನಮ್ಮ ಮೇಲಿನ ಮಾದರಿಯಲ್ಲಿ `TransformerBlock` ಔಟ್‌ಪುಟ್‌ಗೆ ಹೊಂದಿಕೆಯಾಗುತ್ತದೆ)\n",
    "* `encoder_outputs` ಎಂದರೆ ಎಲ್ಲಾ ಟ್ರಾನ್ಸ್‌ಫಾರ್ಮರ್ ಲೇಯರ್‌ಗಳ ಔಟ್‌ಪುಟ್‌ಗಳು. ನಾವು 4-ಲೇಯರ್ BERT ಮಾದರಿಯನ್ನು ಲೋಡ್ ಮಾಡಿದ್ದೇವೆ (ನೀವು ಹೆಸರುದಿಂದ ಅಂದಾಜಿಸಬಹುದು, ಇದರಲ್ಲಿ `4_H` ಇದೆ), ಅದರಲ್ಲಿ 4 ಟೆನ್ಸರ್‌ಗಳಿವೆ. ಕೊನೆಯದು `sequence_output` ಜೊತೆಗೆ ಒಂದೇ.\n",
    "\n",
    "ಈಗ ನಾವು ಎಂಡ್-ಟು-ಎಂಡ್ ವರ್ಗೀಕರಣ ಮಾದರಿಯನ್ನು ವ್ಯಾಖ್ಯಾನಿಸುವೆವು. ನಾವು *ಫಂಕ್ಷನಲ್ ಮಾದರಿ ವ್ಯಾಖ್ಯಾನ* ಬಳಸುತ್ತೇವೆ, ಅಂದರೆ ನಾವು ಮಾದರಿ ಇನ್‌ಪುಟ್ ಅನ್ನು ವ್ಯಾಖ್ಯಾನಿಸಿ, ನಂತರ ಅದರ ಔಟ್‌ಪುಟ್ ಲೆಕ್ಕಿಸಲು ಸರಣಿಯ ಅಭಿವ್ಯಕ್ತಿಗಳನ್ನು ನೀಡುತ್ತೇವೆ. ನಾವು BERT ಮಾದರಿ ತೂಕಗಳನ್ನು ತರಬೇತಿಗೆ ಅಸಾಧ್ಯವಾಗಿಸುವೆವು ಮತ್ತು ಕೇವಲ ಅಂತಿಮ ವರ್ಗೀಕರಣಕಾರಿಯನ್ನು ತರಬೇತಿಗೆ ಒಳಪಡಿಸುವೆವು:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer (KerasLayer)        {'input_type_ids': ( 0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer_1 (KerasLayer)      {'pooled_output': (N 4782465     keras_layer[0][0]                \n",
      "                                                                 keras_layer[0][1]                \n",
      "                                                                 keras_layer[0][2]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 128)          0           keras_layer_1[0][5]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 4)            516         dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 4,782,981\n",
      "Trainable params: 516\n",
      "Non-trainable params: 4,782,465\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inp = keras.Input(shape=(),dtype=tf.string)\n",
    "x = vectorizer(inp)\n",
    "x = bert(x)\n",
    "x = keras.layers.Dropout(0.1)(x['pooled_output'])\n",
    "out = keras.layers.Dense(4,activation='softmax')(x)\n",
    "model = keras.models.Model(inp,out)\n",
    "bert.trainable = False\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 528s 559ms/step - loss: 0.8056 - acc: 0.6983 - val_loss: 0.5953 - val_acc: 0.7888\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9bb1e36d00>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'], optimizer='adam')\n",
    "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ಶಿಕ್ಷಣಾರ್ಹ ಪರಿಮಾಣಗಳು ಕಡಿಮೆ ಇದ್ದರೂ, ಪ್ರಕ್ರಿಯೆ ನಿಧಾನವಾಗಿದೆ, ಏಕೆಂದರೆ BERT ವೈಶಿಷ್ಟ್ಯ ಸಂಗ್ರಾಹಕ ಗಣನಾತ್ಮಕವಾಗಿ ಭಾರವಾಗಿದೆ. ತರಬೇತಿ ಕೊರತೆ ಅಥವಾ ಮಾದರಿ ಪರಿಮಾಣಗಳ ಕೊರತೆಯಿಂದ ನಾವು ಸಮಂಜಸವಾದ ನಿಖರತೆಯನ್ನು ಸಾಧಿಸಲು ಸಾಧ್ಯವಾಗಲಿಲ್ಲ ಎಂದು ತೋರುತ್ತದೆ.\n",
    "\n",
    "BERT ತೂಕಗಳನ್ನು ಅನ್ಫ್ರೀಜ್ ಮಾಡಿ ಅದನ್ನು ಸಹ ತರಬೇತಿಮಾಡಿ ನೋಡೋಣ. ಇದಕ್ಕೆ ತುಂಬಾ ಕಡಿಮೆ ಕಲಿಕೆ ದರ ಬೇಕಾಗುತ್ತದೆ, ಮತ್ತು **warmup** ಬಳಸಿ ಹೆಚ್ಚು ಜಾಗರೂಕ ತರಬೇತಿ ತಂತ್ರವನ್ನು ಅನುಸರಿಸಬೇಕು, **AdamW** ಆಪ್ಟಿಮೈಜರ್ ಬಳಸಿ. ಆಪ್ಟಿಮೈಜರ್ ಸೃಷ್ಟಿಸಲು ನಾವು `tf-models-official` ಪ್ಯಾಕೇಜ್ ಬಳಸುತ್ತೇವೆ:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer (KerasLayer)        {'input_type_ids': ( 0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer_1 (KerasLayer)      {'pooled_output': (N 4782465     keras_layer[0][0]                \n",
      "                                                                 keras_layer[0][1]                \n",
      "                                                                 keras_layer[0][2]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 128)          0           keras_layer_1[0][5]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 4)            516         dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 4,782,981\n",
      "Trainable params: 4,782,980\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n",
      "938/938 [==============================] - 629s 664ms/step - loss: 0.6344 - acc: 0.7658 - val_loss: 0.4876 - val_acc: 0.8247\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9bb0bd0070>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from official.nlp import optimization \n",
    "bert.trainable=True\n",
    "model.summary()\n",
    "epochs = 3\n",
    "opt = optimization.create_optimizer(\n",
    "    init_lr=3e-5,\n",
    "    num_train_steps=epochs*len(ds_train),\n",
    "    num_warmup_steps=0.1*epochs*len(ds_train),\n",
    "    optimizer_type='adamw')\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'], optimizer=opt)\n",
    "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ನೀವು ನೋಡಬಹುದು, ತರಬೇತಿ ಬಹಳ ನಿಧಾನವಾಗಿ ನಡೆಯುತ್ತಿದೆ - ಆದರೆ ನೀವು ಪ್ರಯೋಗ ಮಾಡಿ, ಕೆಲವು epochs (5-10) ತರಬೇತಿ ಮಾಡಿ, ನಾವು ಹಿಂದಿನ ವಿಧಾನಗಳೊಂದಿಗೆ ಹೋಲಿಸಿದಾಗ ಉತ್ತಮ ಫಲಿತಾಂಶವನ್ನು ಪಡೆಯಬಹುದೇ ಎಂದು ನೋಡಬಹುದು.\n",
    "\n",
    "## Huggingface Transformers ಗ್ರಂಥಾಲಯ\n",
    "\n",
    "ಮತ್ತೊಂದು ಬಹಳ ಸಾಮಾನ್ಯ (ಮತ್ತು ಸ್ವಲ್ಪ ಸರಳ) ವಿಧಾನ Transformer ಮಾದರಿಗಳನ್ನು ಬಳಸಲು [HuggingFace ಪ್ಯಾಕೇಜ್](https://github.com/huggingface/) ಆಗಿದ್ದು, ಇದು ವಿಭಿನ್ನ NLP ಕಾರ್ಯಗಳಿಗೆ ಸರಳ ನಿರ್ಮಾಣ ಘಟಕಗಳನ್ನು ಒದಗಿಸುತ್ತದೆ. ಇದು Tensorflow ಮತ್ತು PyTorch ಎರಡಕ್ಕೂ ಲಭ್ಯವಿದೆ, ಮತ್ತೊಂದು ಬಹಳ ಜನಪ್ರಿಯ ನ್ಯೂರಲ್ ನೆಟ್‌ವರ್ಕ್ ಫ್ರೇಮ್ವರ್ಕ್.\n",
    "\n",
    "> **Note**: ನೀವು Transformers ಗ್ರಂಥಾಲಯ ಹೇಗೆ ಕೆಲಸ ಮಾಡುತ್ತದೆ ಎಂದು ನೋಡಲು ಆಸಕ್ತಿ ಇಲ್ಲದಿದ್ದರೆ - ನೀವು ಈ ನೋಟ್ಬುಕ್‌ನ ಕೊನೆಯನ್ನು ತಲುಪಬಹುದು, ಏಕೆಂದರೆ ನಾವು ಮೇಲಿನಂತೆ ಮಾಡಿದ್ದರಿಂದ ಬಹುಮಟ್ಟಿಗೆ ವಿಭಿನ್ನವಾದ ಏನನ್ನೂ ನೀವು ನೋಡಲಾರಿರಿ. ನಾವು ಬೇರೆ ಗ್ರಂಥಾಲಯ ಮತ್ತು ಬಹುಮಟ್ಟಿಗೆ ದೊಡ್ಡ ಮಾದರಿಯನ್ನು ಬಳಸಿಕೊಂಡು BERT ಮಾದರಿಯನ್ನು ತರಬೇತಿಗೊಳಿಸುವ ಅದೇ ಹಂತಗಳನ್ನು ಪುನರಾವರ್ತಿಸುತ್ತೇವೆ. ಆದ್ದರಿಂದ, ಪ್ರಕ್ರಿಯೆ ಕೆಲವು ದೀರ್ಘ ತರಬೇತಿಯನ್ನು ಒಳಗೊಂಡಿದೆ, ಆದ್ದರಿಂದ ನೀವು ಕೇವಲ ಕೋಡ್ ಅನ್ನು ನೋಡಬಹುದು.\n",
    "\n",
    "ನಮ್ಮ ಸಮಸ್ಯೆಯನ್ನು [Huggingface Transformers](http://huggingface.co) ಬಳಸಿ ಹೇಗೆ ಪರಿಹರಿಸಬಹುದು ಎಂದು ನೋಡೋಣ.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ಮೊದಲನೆಯದಾಗಿ ನಾವು ಬಳಸಲಿರುವ ಮಾದರಿಯನ್ನು ಆಯ್ಕೆ ಮಾಡಬೇಕಾಗುತ್ತದೆ. ಕೆಲವು ಒಳಗೊಂಡಿರುವ ಮಾದರಿಗಳ ಜೊತೆಗೆ, Huggingface ನಲ್ಲಿ [ಆನ್ಲೈನ್ ಮಾದರಿ ಸಂಗ್ರಹಾಲಯ](https://huggingface.co/models) ಇದೆ, ಅಲ್ಲಿ ಸಮುದಾಯದಿಂದ ಪೂರ್ವ-ಪ್ರಶಿಕ್ಷಿತ ಅನೇಕ ಮಾದರಿಗಳನ್ನು ನೀವು ಕಂಡುಹಿಡಿಯಬಹುದು. ಆ ಎಲ್ಲಾ ಮಾದರಿಗಳನ್ನು ಕೇವಲ ಮಾದರಿ ಹೆಸರನ್ನು ನೀಡುವುದರಿಂದ ಲೋಡ್ ಮಾಡಿ ಬಳಸಬಹುದು. ಮಾದರಿಗಾಗಿ ಅಗತ್ಯವಿರುವ ಎಲ್ಲಾ ಬೈನರಿ ಫೈಲ್‌ಗಳು ಸ್ವಯಂಚಾಲಿತವಾಗಿ ಡೌನ್‌ಲೋಡ್ ಆಗುತ್ತವೆ.\n",
    "\n",
    "ಕೆಲವು ಸಂದರ್ಭಗಳಲ್ಲಿ ನೀವು ನಿಮ್ಮ ಸ್ವಂತ ಮಾದರಿಗಳನ್ನು ಲೋಡ್ ಮಾಡಬೇಕಾಗಬಹುದು, ಆ ಸಂದರ್ಭದಲ್ಲಿ ಟೋಕನೈಜರ್‌ಗಾಗಿ ಪ್ಯಾರಾಮೀಟರ್‌ಗಳು, ಮಾದರಿ ಪ್ಯಾರಾಮೀಟರ್‌ಗಳೊಂದಿಗೆ `config.json` ಫೈಲ್, ಬೈನರಿ ತೂಕಗಳು ಮುಂತಾದ ಎಲ್ಲಾ ಸಂಬಂಧಿತ ಫೈಲ್‌ಗಳನ್ನು ಹೊಂದಿರುವ ಡೈರೆಕ್ಟರಿಯನ್ನು ನೀವು ಸೂಚಿಸಬಹುದು.\n",
    "\n",
    "ಮಾದರಿ ಹೆಸರಿನಿಂದ, ನಾವು ಮಾದರಿಯ ಜೊತೆಗೆ ಟೋಕನೈಜರ್ ಅನ್ನು ಕೂಡ ಉದ್ಘಾಟಿಸಬಹುದು. ಟೋಕನೈಜರ್‌ನಿಂದ ಪ್ರಾರಂಭಿಸೋಣ:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "# To load the model from Internet repository using model name. \n",
    "# Use this if you are running from your own copy of the notebooks\n",
    "bert_model = 'bert-base-uncased' \n",
    "\n",
    "# To load the model from the directory on disk. Use this for Microsoft Learn module, because we have\n",
    "# prepared all required files for you.\n",
    "#bert_model = './bert'\n",
    "\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(bert_model)\n",
    "\n",
    "MAX_SEQ_LEN = 128\n",
    "PAD_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "UNK_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tokenizer` ವಸ್ತುವಿನಲ್ಲಿ ಪಠ್ಯವನ್ನು ನೇರವಾಗಿ ಎನ್‌ಕೋಡ್ ಮಾಡಲು ಬಳಸಬಹುದಾದ `encode` ಫಂಕ್ಷನ್ ಇದೆ:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 23435, 12314, 2003, 1037, 2307, 7705, 2005, 17953, 2361, 102]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('Tensorflow is a great framework for NLP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ನಾವು ಮಾದರಿಗೆ ಪಾಸ್ ಮಾಡಲು ಸೂಕ್ತವಾದ ರೀತಿಯಲ್ಲಿ ಸರಣಿಯನ್ನು ಎನ್‌ಕೋಡ್ ಮಾಡಲು ಟೋಕನೈಜರ್ ಅನ್ನು ಬಳಸಬಹುದು, ಅಂದರೆ `token_ids`, `input_mask` ಕ್ಷೇತ್ರಗಳನ್ನು ಸೇರಿಸುವುದು ಇತ್ಯಾದಿ. ನಾವು `return_tensors='tf'` ಆರ್ಗ್ಯುಮೆಂಟ್ ಅನ್ನು ನೀಡುವ ಮೂಲಕ ಟೆನ್ಸರ್‌ಫ್ಲೋ ಟೆನ್ಸರ್‌ಗಳನ್ನು ಬೇಕೆಂದು ಸೂಚಿಸಬಹುದು:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[ 101, 7592, 1010, 2045,  102]], dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[0, 0, 0, 0, 0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[1, 1, 1, 1, 1]], dtype=int32)>}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(['Hello, there'],return_tensors='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ನಮ್ಮ ಪ್ರಕರಣದಲ್ಲಿ, ನಾವು `bert-base-uncased` ಎಂಬ ಪೂರ್ವ-ಪ್ರಶಿಕ್ಷಿತ BERT ಮಾದರಿಯನ್ನು ಬಳಸಲಿದ್ದೇವೆ. *Uncased* ಎಂದರೆ ಮಾದರಿ ಕೇಸ್-ಅಸಂವೇದನಶೀಲವಾಗಿದೆ.\n",
    "\n",
    "ಮಾದರಿಯನ್ನು ತರಬೇತುಗೊಳಿಸುವಾಗ, ನಮಗೆ ಟೋಕನೈಜ್ ಮಾಡಿದ ಕ್ರಮವನ್ನು ಇನ್‌ಪುಟ್ ಆಗಿ ನೀಡಬೇಕಾಗುತ್ತದೆ, ಆದ್ದರಿಂದ ನಾವು ಡೇಟಾ ಪ್ರಕ್ರಿಯೆ ಪೈಪ್‌ಲೈನ್ ಅನ್ನು ವಿನ್ಯಾಸಗೊಳಿಸುವೆವು. `tokenizer.encode` ಒಂದು Python ಫಂಕ್ಷನ್ ಆಗಿರುವುದರಿಂದ, ನಾವು ಹಿಂದಿನ ಘಟಕದಲ್ಲಿ ಮಾಡಿದಂತೆ ಅದನ್ನು `py_function` ಬಳಸಿ ಕರೆಮಾಡುವ ವಿಧಾನವನ್ನು ಬಳಸುತ್ತೇವೆ:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(x):\n",
    "    return tokenizer.encode(x.numpy().decode('utf-8'),return_tensors='tf',padding='max_length',max_length=MAX_SEQ_LEN,truncation=True)[0]\n",
    "\n",
    "def process_fn(x):\n",
    "    s = x['title']+' '+x['description']\n",
    "    e = tf.py_function(process,inp=[s],Tout=(tf.int32))\n",
    "    e.set_shape(MAX_SEQ_LEN)\n",
    "    return e,x['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ಈಗ ನಾವು `BertForSequenceClassfication` ಪ್ಯಾಕೇಜ್ ಬಳಸಿ ನಿಜವಾದ ಮಾದರಿಯನ್ನು ಲೋಡ್ ಮಾಡಬಹುದು. ಇದು ನಮ್ಮ ಮಾದರಿಯಲ್ಲಿ ವರ್ಗೀಕರಣಕ್ಕಾಗಿ ಅಗತ್ಯವಿರುವ ವಾಸ್ತುಶಿಲ್ಪವನ್ನು, ಅಂತಿಮ ವರ್ಗೀಕರಣಕಾರಿಯನ್ನು ಸೇರಿಸಿ, ಹೊಂದಿರುವುದನ್ನು ಖಚಿತಪಡಿಸುತ್ತದೆ. ನೀವು ಅಂತಿಮ ವರ್ಗೀಕರಣಕಾರಿಯ ತೂಕಗಳು ಪ್ರಾರಂಭವಾಗಿಲ್ಲ ಎಂಬ ಎಚ್ಚರಿಕೆ ಸಂದೇಶವನ್ನು ಕಾಣುತ್ತೀರಿ, ಮತ್ತು ಮಾದರಿಗೆ ಪೂರ್ವ-ಪ್ರಶಿಕ್ಷಣ ಅಗತ್ಯವಿದೆ - ಅದು ಸಂಪೂರ್ಣವಾಗಿ ಸರಿಯಾಗಿದೆ, ಏಕೆಂದರೆ ನಾವು ಅದೇ ಮಾಡಬೇಕಾಗಿದೆ!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = transformers.TFBertForSequenceClassification.from_pretrained(bert_model,num_labels=4,output_attentions=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  109482240 \n",
      "_________________________________________________________________\n",
      "dropout_75 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  3076      \n",
      "=================================================================\n",
      "Total params: 109,485,316\n",
      "Trainable params: 109,485,316\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ನೀವು `summary()` ನಿಂದ ನೋಡಬಹುದು, ಮಾದರಿಯಲ್ಲಿ ಸುಮಾರು 110 ಮಿಲಿಯನ್ ಪ್ಯಾರಾಮೀಟರ್‌ಗಳಿವೆ! ಸಾಮಾನ್ಯವಾಗಿ, ನಾವು ಸಣ್ಣ ಡೇಟಾಸೆಟ್‌ನಲ್ಲಿ ಸರಳ ವರ್ಗೀಕರಣ ಕಾರ್ಯವನ್ನು ಮಾಡಲು ಬಯಸಿದರೆ, ನಾವು BERT ಮೂಲ ಲೇಯರ್ ಅನ್ನು ತರಬೇತಿಗೆ ಒಳಪಡಿಸಲು ಬಯಸುವುದಿಲ್ಲ:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  109482240 \n",
      "_________________________________________________________________\n",
      "dropout_75 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  3076      \n",
      "=================================================================\n",
      "Total params: 109,485,316\n",
      "Trainable params: 3,076\n",
      "Non-trainable params: 109,482,240\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.layers[0].trainable = False\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ಈಗ ನಾವು ತರಬೇತಿ ಪ್ರಾರಂಭಿಸಲು ಸಿದ್ಧರಾಗಿದ್ದೇವೆ!\n",
    "\n",
    "> **Note**: ಸಂಪೂರ್ಣ ಪ್ರಮಾಣದ BERT ಮಾದರಿಯನ್ನು ತರಬೇತಿಗೊಳಿಸುವುದು ಬಹಳ ಸಮಯ ತೆಗೆದುಕೊಳ್ಳಬಹುದು! ಆದ್ದರಿಂದ ನಾವು ಮೊದಲ 32 ಬ್ಯಾಚ್‌ಗಳಿಗಾಗಿ ಮಾತ್ರ ತರಬೇತಿಗೊಳಿಸುವೆವು. ಇದು ಮಾದರಿ ತರಬೇತಿ ಹೇಗೆ ಹೊಂದಿಸಲಾಗುತ್ತದೆ ಎಂಬುದನ್ನು ತೋರಿಸಲು ಮಾತ್ರ. ನೀವು ಸಂಪೂರ್ಣ ಪ್ರಮಾಣದ ತರಬೇತಿಯನ್ನು ಪ್ರಯತ್ನಿಸಲು ಆಸಕ್ತರಾಗಿದ್ದರೆ - `steps_per_epoch` ಮತ್ತು `validation_steps` ಪರಿಮಾಣಗಳನ್ನು ತೆಗೆದುಹಾಕಿ, ಮತ್ತು ಕಾಯಲು ಸಿದ್ಧರಾಗಿ!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 142s 4s/step - loss: 1.3896 - acc: 0.2500 - val_loss: 1.3863 - val_acc: 0.2480\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f1d40a4b6a0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile('adam','sparse_categorical_crossentropy',['acc'])\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "model.fit(ds_train.map(process_fn).batch(32),validation_data=ds_test.map(process_fn).batch(32),steps_per_epoch=32,validation_steps=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ನೀವು ಪುನರಾವೃತ್ತಿಗಳ ಸಂಖ್ಯೆಯನ್ನು ಹೆಚ್ಚಿಸಿ ಸಾಕಷ್ಟು ಸಮಯ ಕಾಯುತ್ತೀರಾ ಮತ್ತು ಹಲವಾರು ಎಪೋಕ್‌ಗಳಿಗಾಗಿ ತರಬೇತಿ ನೀಡುತ್ತೀರಾ, BERT ವರ್ಗೀಕರಣವು ಅತ್ಯುತ್ತಮ ನಿಖರತೆಯನ್ನು ನೀಡುತ್ತದೆ ಎಂದು ನಿರೀಕ್ಷಿಸಬಹುದು! ಇದಕ್ಕೆ ಕಾರಣ BERT ಈಗಾಗಲೇ ಭಾಷೆಯ ರಚನೆಯನ್ನು ಚೆನ್ನಾಗಿ ಅರ್ಥಮಾಡಿಕೊಂಡಿದೆ, ಮತ್ತು ನಾವು ಕೇವಲ ಅಂತಿಮ ವರ್ಗೀಕರಣೆಯನ್ನು ಸೂಕ್ಷ್ಮವಾಗಿ ಹೊಂದಿಸಬೇಕಾಗುತ್ತದೆ. ಆದಾಗ್ಯೂ, BERT ಒಂದು ದೊಡ್ಡ ಮಾದರಿಯಾಗಿರುವುದರಿಂದ, ಸಂಪೂರ್ಣ ತರಬೇತಿ ಪ್ರಕ್ರಿಯೆಗೆ ಹೆಚ್ಚು ಸಮಯ ಬೇಕಾಗುತ್ತದೆ ಮತ್ತು ಗಂಭೀರ ಗಣನ ಶಕ್ತಿಯನ್ನು ಅಗತ್ಯವಿರುತ್ತದೆ! (GPU, ಮತ್ತು ಆದ್ಯತೆಯಿಂದ ಒಂದುಕ್ಕಿಂತ ಹೆಚ್ಚು).\n",
    "\n",
    "> **Note:** ನಮ್ಮ ಉದಾಹರಣೆಯಲ್ಲಿ, ನಾವು ಅತ್ಯಂತ ಸಣ್ಣ ಪೂರ್ವ-ತರಬೇತಿಗೊಂಡ BERT ಮಾದರಿಗಳಲ್ಲಿ ಒಂದನ್ನು ಬಳಸುತ್ತಿದ್ದೇವೆ. ಉತ್ತಮ ಫಲಿತಾಂಶಗಳನ್ನು ನೀಡುವ ಸಾಧ್ಯತೆ ಇರುವ ದೊಡ್ಡ ಮಾದರಿಗಳು ಇವೆ.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ಸಾರಾಂಶ\n",
    "\n",
    "ಈ ಘಟಕದಲ್ಲಿ, ನಾವು **ಟ್ರಾನ್ಸ್‌ಫಾರ್ಮರ್‌ಗಳು** ಆಧಾರಿತ ಅತ್ಯಾಧುನಿಕ ಮಾದರಿ ವಾಸ್ತುಶಿಲ್ಪಗಳನ್ನು ನೋಡಿದ್ದೇವೆ. ನಾವು ಅವುಗಳನ್ನು ನಮ್ಮ ಪಠ್ಯ ವರ್ಗೀಕರಣ ಕಾರ್ಯಕ್ಕೆ ಅನ್ವಯಿಸಿದ್ದೇವೆ, ಆದರೆ ಅದೇ ರೀತಿಯಲ್ಲಿ, BERT ಮಾದರಿಗಳನ್ನು ಘಟಕ ಹೊರತೆಗೆಯುವಿಕೆ, ಪ್ರಶ್ನೆ ಉತ್ತರಿಸುವಿಕೆ ಮತ್ತು ಇತರ NLP ಕಾರ್ಯಗಳಿಗೆ ಬಳಸಬಹುದು.\n",
    "\n",
    "ಟ್ರಾನ್ಸ್‌ಫಾರ್ಮರ್ ಮಾದರಿಗಳು NLP ನಲ್ಲಿ ಪ್ರಸ್ತುತ ಅತ್ಯುತ್ತಮ ತಂತ್ರಜ್ಞಾನವನ್ನು ಪ್ರತಿನಿಧಿಸುತ್ತವೆ, ಮತ್ತು ಬಹುತೇಕ ಸಂದರ್ಭಗಳಲ್ಲಿ ಕಸ್ಟಮ್ NLP ಪರಿಹಾರಗಳನ್ನು ಅನುಷ್ಠಾನಗೊಳಿಸುವಾಗ ನೀವು ಮೊದಲನೆಯದಾಗಿ ಪ್ರಯೋಗಿಸಬೇಕಾದ ಪರಿಹಾರವಾಗಿರಬೇಕು. ಆದಾಗ್ಯೂ, ಈ ಘಟಕದಲ್ಲಿ ಚರ್ಚಿಸಲಾದ ಪುನರಾವರ್ತಿತ ನ್ಯೂರಲ್ ನೆಟ್‌ವರ್ಕ್‌ಗಳ ಮೂಲಭೂತ ತತ್ವಗಳನ್ನು ಅರ್ಥಮಾಡಿಕೊಳ್ಳುವುದು ಅತ್ಯಂತ ಮುಖ್ಯ, ನೀವು ಉನ್ನತ ಮಟ್ಟದ ನ್ಯೂರಲ್ ಮಾದರಿಗಳನ್ನು ನಿರ್ಮಿಸಲು ಬಯಸಿದರೆ.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n<!-- CO-OP TRANSLATOR DISCLAIMER START -->\n**ಅಸ್ವೀಕರಣ**:  \nಈ ದಸ್ತಾವೇಜು [Co-op Translator](https://github.com/Azure/co-op-translator) ಎಂಬ AI ಅನುವಾದ ಸೇವೆಯನ್ನು ಬಳಸಿ ಅನುವಾದಿಸಲಾಗಿದೆ. ನಾವು ಶುದ್ಧತೆಯತ್ತ ಪ್ರಯತ್ನಿಸುತ್ತಿದ್ದರೂ, ಸ್ವಯಂಚಾಲಿತ ಅನುವಾದಗಳಲ್ಲಿ ತಪ್ಪುಗಳು ಅಥವಾ ಅಸತ್ಯತೆಗಳು ಇರಬಹುದು ಎಂದು ದಯವಿಟ್ಟು ಗಮನಿಸಿ. ಮೂಲ ಭಾಷೆಯಲ್ಲಿರುವ ಮೂಲ ದಸ್ತಾವೇಜನ್ನು ಅಧಿಕೃತ ಮೂಲವೆಂದು ಪರಿಗಣಿಸಬೇಕು. ಮಹತ್ವದ ಮಾಹಿತಿಗಾಗಿ, ವೃತ್ತಿಪರ ಮಾನವ ಅನುವಾದವನ್ನು ಶಿಫಾರಸು ಮಾಡಲಾಗುತ್ತದೆ. ಈ ಅನುವಾದ ಬಳಕೆಯಿಂದ ಉಂಟಾಗುವ ಯಾವುದೇ ತಪ್ಪು ಅರ್ಥಮಾಡಿಕೊಳ್ಳುವಿಕೆ ಅಥವಾ ತಪ್ಪು ವಿವರಣೆಗಳಿಗೆ ನಾವು ಹೊಣೆಗಾರರಾಗುವುದಿಲ್ಲ.\n<!-- CO-OP TRANSLATOR DISCLAIMER END -->\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0cb620c6d4b9f7a635928804c26cf22403d89d98d79684e4529119355ee6d5a5"
  },
  "kernelspec": {
   "display_name": "py38_tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "coopTranslator": {
   "original_hash": "ab59c532409774988ab875f2260e8e53",
   "translation_date": "2025-11-26T02:28:41+00:00",
   "source_file": "lessons/5-NLP/18-Transformers/TransformersTF.ipynb",
   "language_code": "kn"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}