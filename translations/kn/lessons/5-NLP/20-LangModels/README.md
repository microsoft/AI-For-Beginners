# ಪೂರ್ವ-ಪ್ರಶಿಕ್ಷಿತ ದೊಡ್ಡ ಭಾಷಾ ಮಾದರಿಗಳು

ನಮ್ಮ ಹಿಂದಿನ ಎಲ್ಲಾ ಕಾರ್ಯಗಳಲ್ಲಿ, ನಾವು ಲೇಬಲ್ ಮಾಡಲಾದ ಡೇಟಾಸೆಟ್ ಬಳಸಿ ನಿರ್ದಿಷ್ಟ ಕಾರ್ಯವನ್ನು ನಿರ್ವಹಿಸಲು ನ್ಯೂರಲ್ ನೆಟ್‌ವರ್ಕ್ ಅನ್ನು ತರಬೇತುಗೊಳಿಸುತ್ತಿದ್ದೆವು. BERT ಮುಂತಾದ ದೊಡ್ಡ ಟ್ರಾನ್ಸ್‌ಫಾರ್ಮರ್ ಮಾದರಿಗಳೊಂದಿಗೆ, ನಾವು ಸ್ವಯಂ-ನಿರೀಕ್ಷಿತ ರೀತಿಯಲ್ಲಿ ಭಾಷಾ ಮಾದರಿಯನ್ನು ನಿರ್ಮಿಸಲು ಭಾಷಾ ಮಾದರಿಕರಣವನ್ನು ಬಳಸುತ್ತೇವೆ, ನಂತರ ಅದನ್ನು ನಿರ್ದಿಷ್ಟ ಡೊಮೇನ್-ಸ್ಪೆಸಿಫಿಕ್ ತರಬೇತಿನೊಂದಿಗೆ ನಿರ್ದಿಷ್ಟ ಡೌನ್‌ಸ್ಟ್ರೀಮ್ ಕಾರ್ಯಕ್ಕೆ ವಿಶೇಷಗೊಳಿಸಲಾಗುತ್ತದೆ. ಆದಾಗ್ಯೂ, ದೊಡ್ಡ ಭಾಷಾ ಮಾದರಿಗಳು ಯಾವುದೇ ಡೊಮೇನ್-ಸ್ಪೆಸಿಫಿಕ್ ತರಬೇತಿನಿಲ್ಲದೆ ಹಲವಾರು ಕಾರ್ಯಗಳನ್ನು ಪರಿಹರಿಸಬಹುದು ಎಂದು ತೋರಿಸಲಾಗಿದೆ. ಅದನ್ನು ಮಾಡಲು ಸಾಮರ್ಥ್ಯವಿರುವ ಮಾದರಿಗಳ ಕುಟುಂಬವನ್ನು **GPT** ಎಂದು ಕರೆಯುತ್ತಾರೆ: ಜನರೇಟಿವ್ ಪೂರ್ವ-ಪ್ರಶಿಕ್ಷಿತ ಟ್ರಾನ್ಸ್‌ಫಾರ್ಮರ್.

## [ಪೂರ್ವ-ಲೆಕ್ಚರ್ ಕ್ವಿಜ್](https://ff-quizzes.netlify.app/en/ai/quiz/39)

## ಪಠ್ಯ ಉತ್ಪಾದನೆ ಮತ್ತು ಪರ್ಪ್ಲೆಕ್ಸಿಟಿ

ನ್ಯೂರಲ್ ನೆಟ್‌ವರ್ಕ್ ಡೌನ್‌ಸ್ಟ್ರೀಮ್ ತರಬೇತಿನಿಲ್ಲದೆ ಸಾಮಾನ್ಯ ಕಾರ್ಯಗಳನ್ನು ಮಾಡಬಲ್ಲದು ಎಂಬ ಕಲ್ಪನೆ [Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) ಕಾಗದದಲ್ಲಿ ಪ್ರಸ್ತುತಪಡಿಸಲಾಗಿದೆ. ಮುಖ್ಯ ಕಲ್ಪನೆ ಎಂದರೆ, ಅನೇಕ ಇತರ ಕಾರ್ಯಗಳನ್ನು **ಪಠ್ಯ ಉತ್ಪಾದನೆ** ಬಳಸಿ ಮಾದರೀಕರಿಸಬಹುದು, ಏಕೆಂದರೆ ಪಠ್ಯವನ್ನು ಅರ್ಥಮಾಡಿಕೊಳ್ಳುವುದು ಅಂದರೆ ಅದನ್ನು ಉತ್ಪಾದಿಸಲು ಸಾಧ್ಯವಾಗುವುದು. ಮಾದರಿ ಮಾನವ ಜ್ಞಾನವನ್ನು ಒಳಗೊಂಡಿರುವ ಅಪಾರ ಪ್ರಮಾಣದ ಪಠ್ಯದಲ್ಲಿ ತರಬೇತಿಗೊಳಿಸಲಾಗಿರುವುದರಿಂದ, ಅದು ವಿವಿಧ ವಿಷಯಗಳ ಬಗ್ಗೆ ಜ್ಞಾನ ಹೊಂದಿರುತ್ತದೆ.

> ಪಠ್ಯವನ್ನು ಅರ್ಥಮಾಡಿಕೊಳ್ಳುವುದು ಮತ್ತು ಉತ್ಪಾದಿಸಲು ಸಾಧ್ಯವಾಗುವುದು ನಮ್ಮ ಸುತ್ತಲೂ ಇರುವ ಜಗತ್ತಿನ ಬಗ್ಗೆ ಏನೋ ತಿಳಿದಿರುವುದನ್ನು ಸೂಚಿಸುತ್ತದೆ. ಜನರು ಬಹುಮಟ್ಟಿಗೆ ಓದುವುದರಿಂದ ಕಲಿಯುತ್ತಾರೆ, ಮತ್ತು GPT ನೆಟ್‌ವರ್ಕ್ ಕೂಡ ಈ ದೃಷ್ಟಿಯಿಂದ ಅದೇ ರೀತಿಯದು.

ಪಠ್ಯ ಉತ್ಪಾದನೆ ನೆಟ್‌ವರ್ಕ್ ಮುಂದಿನ ಪದದ $$P(w_N)$$ ಸಾಧ್ಯತೆಯನ್ನು ಊಹಿಸುವ ಮೂಲಕ ಕಾರ್ಯನಿರ್ವಹಿಸುತ್ತದೆ. ಆದಾಗ್ಯೂ, ಮುಂದಿನ ಪದದ ನಿರಪೇಕ್ಷ ಸಾಧ್ಯತೆ ಪಠ್ಯ ಸಂಗ್ರಹದಲ್ಲಿ ಆ ಪದದ ಆವರ್ತನೆಯ ಸಮಾನವಾಗಿರುತ್ತದೆ. GPT ನಮಗೆ ಹಿಂದಿನ ಪದಗಳನ್ನು ನೀಡಿದಾಗ ಮುಂದಿನ ಪದದ **ಶರತುಗತ ಸಾಧ್ಯತೆ** $$P(w_N | w_{n-1}, ..., w_0)$$ ನೀಡಲು ಸಾಧ್ಯ.

> ಸಾಧ್ಯತೆಗಳ ಬಗ್ಗೆ ಹೆಚ್ಚಿನ ಮಾಹಿತಿಗಾಗಿ ನಮ್ಮ [ಡೇಟಾ ಸೈನ್ಸ್ ಫಾರ್ ಬಿಗಿನರ್ಸ್ ಪಠ್ಯಕ್ರಮ](https://github.com/microsoft/Data-Science-For-Beginners/tree/main/1-Introduction/04-stats-and-probability) ಓದಿ.

ಭಾಷೆ ಉತ್ಪಾದಿಸುವ ಮಾದರಿಯ ಗುಣಮಟ್ಟವನ್ನು **ಪರ್ಪ್ಲೆಕ್ಸಿಟಿ** ಬಳಸಿ ವ್ಯಾಖ್ಯಾನಿಸಬಹುದು. ಇದು ಯಾವುದೇ ಕಾರ್ಯ-ನಿರ್ದಿಷ್ಟ ಡೇಟಾಸೆಟ್ ಇಲ್ಲದೆ ಮಾದರಿಯ ಗುಣಮಟ್ಟವನ್ನು ಅಳೆಯಲು ಅನುಕೂಲವಾಗುವ ಆಂತರಿಕ ಮಾನದಂಡ. ಇದು *ವಾಕ್ಯದ ಸಾಧ್ಯತೆ* ಎಂಬ ಕಲ್ಪನೆ ಆಧಾರಿತವಾಗಿದೆ - ಮಾದರಿ ವಾಸ್ತವಿಕವಾಗಿರುವ ಸಾಧ್ಯತೆ ಇರುವ ವಾಕ್ಯಕ್ಕೆ ಹೆಚ್ಚಿನ ಸಾಧ್ಯತೆ ನೀಡುತ್ತದೆ (ಅಂದರೆ, ಮಾದರಿ ಅದರಿಂದ **ಪರ್ಪ್ಲೆಕ್ಸ್ಡ್** ಆಗುವುದಿಲ್ಲ), ಮತ್ತು ಅರ್ಥವಿಲ್ಲದ ವಾಕ್ಯಗಳಿಗೆ ಕಡಿಮೆ ಸಾಧ್ಯತೆ ನೀಡುತ್ತದೆ (ಉದಾ: *Can it does what?*). ನಾವು ನಮ್ಮ ಮಾದರಿಗೆ ವಾಸ್ತವಿಕ ಪಠ್ಯ ಸಂಗ್ರಹದಿಂದ ವಾಕ್ಯಗಳನ್ನು ನೀಡಿದಾಗ, ಅವುಗಳಿಗೆ ಹೆಚ್ಚಿನ ಸಾಧ್ಯತೆ ಮತ್ತು ಕಡಿಮೆ **ಪರ್ಪ್ಲೆಕ್ಸಿಟಿ** ಇರಬೇಕೆಂದು ನಿರೀಕ್ಷಿಸುತ್ತೇವೆ. ಗಣಿತೀಯವಾಗಿ, ಇದು ಪರೀಕ್ಷಾ ಸೆಟ್‌ನ ಸಾಮಾನ್ಯೀಕೃತ ಪ್ರತಿಕೂಲ ಸಾಧ್ಯತೆಯಾಗಿ ವ್ಯಾಖ್ಯಾನಿಸಲಾಗಿದೆ:
$$
\mathrm{Perplexity}(W) = \sqrt[N]{1\over P(W_1,...,W_N)}
$$ 

**ನೀವು [Hugging Face ನಿಂದ GPT-ಚಾಲಿತ ಪಠ್ಯ ಸಂಪಾದಕ](https://transformer.huggingface.co/doc/gpt2-large) ಬಳಸಿ ಪಠ್ಯ ಉತ್ಪಾದನೆ ಪ್ರಯೋಗ ಮಾಡಬಹುದು**. ಈ ಸಂಪಾದಕದಲ್ಲಿ, ನೀವು ನಿಮ್ಮ ಪಠ್ಯವನ್ನು ಬರೆಯಲು ಪ್ರಾರಂಭಿಸುತ್ತೀರಿ, ಮತ್ತು **[TAB]** ಒತ್ತಿದಾಗ ನಿಮಗೆ ಹಲವು ಪೂರ್ಣಗೊಳಿಸುವ ಆಯ್ಕೆಗಳು ನೀಡಲಾಗುತ್ತವೆ. ಅವು ತುಂಬಾ ಚಿಕ್ಕದಾಗಿದ್ದರೆ ಅಥವಾ ನೀವು ತೃಪ್ತರಾಗದಿದ್ದರೆ - ಮತ್ತೆ [TAB] ಒತ್ತಿ, ಮತ್ತು ನಿಮಗೆ ಹೆಚ್ಚಿನ ಆಯ್ಕೆಗಳು, ಉದ್ದವಾದ ಪಠ್ಯ ಭಾಗಗಳೂ ಲಭ್ಯವಾಗುತ್ತವೆ.

## GPT ಒಂದು ಕುಟುಂಬ

GPT ಒಂದು ಏಕೈಕ ಮಾದರಿ ಅಲ್ಲ, ಬದಲಾಗಿ [OpenAI](https://openai.com) ಅಭಿವೃದ್ಧಿಪಡಿಸಿ ತರಬೇತಿಗೊಳಿಸಿದ ಮಾದರಿಗಳ ಸಂಗ್ರಹ.

GPT ಮಾದರಿಗಳಲ್ಲಿ ನಾವು ಹೊಂದಿರುವವು:

| [GPT-2](https://huggingface.co/docs/transformers/model_doc/gpt2#openai-gpt2) | [GPT 3](https://openai.com/research/language-models-are-few-shot-learners) | [GPT-4](https://openai.com/gpt-4) |
| -- | -- | -- |
| 1.5 ಬಿಲಿಯನ್ ಪರಿಮಾಣಗಳವರೆಗೆ ಭಾಷಾ ಮಾದರಿ. | 175 ಬಿಲಿಯನ್ ಪರಿಮಾಣಗಳವರೆಗೆ ಭಾಷಾ ಮಾದರಿ | 100 ಟ್ರಿಲಿಯನ್ ಪರಿಮಾಣಗಳು ಮತ್ತು ಚಿತ್ರ ಮತ್ತು ಪಠ್ಯ ಇನ್‌ಪುಟ್‌ಗಳನ್ನು ಸ್ವೀಕರಿಸಿ ಪಠ್ಯವನ್ನು ಔಟ್‌ಪುಟ್ ಮಾಡುತ್ತದೆ. |

GPT-3 ಮತ್ತು GPT-4 ಮಾದರಿಗಳು [Microsoft Azure ನಿಂದ ಜ್ಞಾನ ಸೇವೆಯಾಗಿ](https://azure.microsoft.com/en-us/services/cognitive-services/openai-service/#overview?WT.mc_id=academic-77998-cacaste) ಮತ್ತು [OpenAI API](https://openai.com/api/) ಆಗಿ ಲಭ್ಯವಿವೆ.

## ಪ್ರಾಂಪ್ಟ್ ಎಂಜಿನಿಯರಿಂಗ್

GPT ಭಾಷೆ ಮತ್ತು ಕೋಡ್ ಅರ್ಥಮಾಡಿಕೊಳ್ಳಲು ಅಪಾರ ಪ್ರಮಾಣದ ಡೇಟಾದ ಮೇಲೆ ತರಬೇತಿಗೊಳಿಸಲ್ಪಟ್ಟಿರುವುದರಿಂದ, ಅವು ಇನ್‌ಪುಟ್‌ಗಳಿಗೆ (ಪ್ರಾಂಪ್ಟ್‌ಗಳಿಗೆ) ಪ್ರತಿಕ್ರಿಯೆ ನೀಡುತ್ತವೆ. ಪ್ರಾಂಪ್ಟ್‌ಗಳು GPT ಇನ್‌ಪುಟ್‌ಗಳು ಅಥವಾ ಪ್ರಶ್ನೆಗಳು, ಅವು ಮಾದರಿಗಳಿಗೆ ಮುಂದಿನ ಕಾರ್ಯಗಳನ್ನು ಪೂರ್ಣಗೊಳಿಸಲು ಸೂಚನೆಗಳನ್ನು ನೀಡುತ್ತವೆ. ಬಯಸಿದ ಫಲಿತಾಂಶವನ್ನು ಪಡೆಯಲು, ನೀವು ಅತ್ಯಂತ ಪರಿಣಾಮಕಾರಿ ಪ್ರಾಂಪ್ಟ್ ಬೇಕಾಗುತ್ತದೆ, ಅದು ಸರಿಯಾದ ಪದಗಳು, ಸ್ವರೂಪಗಳು, ವಾಕ್ಯಗಳು ಅಥವಾ ಸಂಕೇತಗಳನ್ನು ಆಯ್ಕೆ ಮಾಡುವುದು. ಈ ವಿಧಾನವನ್ನು [ಪ್ರಾಂಪ್ಟ್ ಎಂಜಿನಿಯರಿಂಗ್](https://learn.microsoft.com/en-us/shows/ai-show/the-basics-of-prompt-engineering-with-azure-openai-service?WT.mc_id=academic-77998-bethanycheum) ಎಂದು ಕರೆಯುತ್ತಾರೆ.

[ಈ ಡಾಕ್ಯುಮೆಂಟೇಶನ್](https://learn.microsoft.com/en-us/semantic-kernel/prompt-engineering/?WT.mc_id=academic-77998-bethanycheum) ನಿಮಗೆ ಪ್ರಾಂಪ್ಟ್ ಎಂಜಿನಿಯರಿಂಗ್ ಬಗ್ಗೆ ಹೆಚ್ಚಿನ ಮಾಹಿತಿಯನ್ನು ನೀಡುತ್ತದೆ.

## ✍️ ಉದಾಹರಣೆ ನೋಟ್ಬುಕ್: [OpenAI-GPT ಜೊತೆಗೆ ಆಟವಾಡುವುದು](GPT-PyTorch.ipynb)

ಕೆಳಗಿನ ನೋಟ್ಬುಕ್‌ಗಳಲ್ಲಿ ನಿಮ್ಮ ಅಧ್ಯಯನವನ್ನು ಮುಂದುವರಿಸಿ:

* [OpenAI-GPT ಮತ್ತು Hugging Face Transformers ಬಳಸಿ ಪಠ್ಯ ಉತ್ಪಾದನೆ](GPT-PyTorch.ipynb)

## ಸಾರಾಂಶ

ಹೊಸ ಸಾಮಾನ್ಯ ಪೂರ್ವ-ಪ್ರಶಿಕ್ಷಿತ ಭಾಷಾ ಮಾದರಿಗಳು ಕೇವಲ ಭಾಷಾ ರಚನೆಯನ್ನು ಮಾದರೀಕರಿಸುವುದಲ್ಲ, ಸಹಜ ಭಾಷೆಯ ಅಪಾರ ಪ್ರಮಾಣವನ್ನು ಒಳಗೊಂಡಿರುತ್ತವೆ. ಆದ್ದರಿಂದ, ಅವುಗಳನ್ನು ಶೂನ್ಯ-ಶಾಟ್ ಅಥವಾ ಕೆಲವು-ಶಾಟ್ ಸೆಟ್ಟಿಂಗ್‌ಗಳಲ್ಲಿ ಕೆಲವು NLP ಕಾರ್ಯಗಳನ್ನು ಪರಿಣಾಮಕಾರಿಯಾಗಿ ಪರಿಹರಿಸಲು ಬಳಸಬಹುದು.

## [ಪೋಸ್ಟ್-ಲೆಕ್ಚರ್ ಕ್ವಿಜ್](https://ff-quizzes.netlify.app/en/ai/quiz/40)

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**ಅಸ್ವೀಕರಣ**:  
ಈ ದಸ್ತಾವೇಜು AI ಅನುವಾದ ಸೇವೆ [Co-op Translator](https://github.com/Azure/co-op-translator) ಬಳಸಿ ಅನುವಾದಿಸಲಾಗಿದೆ. ನಾವು ನಿಖರತೆಯಿಗಾಗಿ ಪ್ರಯತ್ನಿಸುತ್ತಿದ್ದರೂ, ಸ್ವಯಂಚಾಲಿತ ಅನುವಾದಗಳಲ್ಲಿ ದೋಷಗಳು ಅಥವಾ ಅಸತ್ಯತೆಗಳು ಇರಬಹುದು ಎಂದು ದಯವಿಟ್ಟು ಗಮನಿಸಿ. ಮೂಲ ಭಾಷೆಯಲ್ಲಿರುವ ಮೂಲ ದಸ್ತಾವೇಜನ್ನು ಅಧಿಕೃತ ಮೂಲವಾಗಿ ಪರಿಗಣಿಸಬೇಕು. ಮಹತ್ವದ ಮಾಹಿತಿಗಾಗಿ, ವೃತ್ತಿಪರ ಮಾನವ ಅನುವಾದವನ್ನು ಶಿಫಾರಸು ಮಾಡಲಾಗುತ್ತದೆ. ಈ ಅನುವಾದ ಬಳಕೆಯಿಂದ ಉಂಟಾಗುವ ಯಾವುದೇ ತಪ್ಪು ಅರ್ಥಮಾಡಿಕೊಳ್ಳುವಿಕೆ ಅಥವಾ ತಪ್ಪು ವಿವರಣೆಗಳಿಗೆ ನಾವು ಹೊಣೆಗಾರರಾಗುವುದಿಲ್ಲ.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->