{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ಪಠ್ಯ ವರ್ಗೀಕರಣ ಕಾರ್ಯ\n",
    "\n",
    "ಈ ಘಟಕದಲ್ಲಿ, ನಾವು ಸರಳ ಪಠ್ಯ ವರ್ಗೀಕರಣ ಕಾರ್ಯದಿಂದ ಪ್ರಾರಂಭಿಸುವೆವು, ಇದು **[AG_NEWS](http://www.di.unipi.it/~gulli/AG_corpus_of_news_articles.html)** ಡೇಟಾಸೆಟ್ ಆಧಾರಿತವಾಗಿದೆ: ನಾವು ಸುದ್ದಿಯ ಶೀರ್ಷಿಕೆಗಳನ್ನು 4 ವರ್ಗಗಳಲ್ಲಿ ಒಂದಾಗಿ ವರ್ಗೀಕರಿಸುವೆವು: ವಿಶ್ವ, ಕ್ರೀಡೆ, ವ್ಯವಹಾರ ಮತ್ತು ವಿಜ್ಞಾನ/ತಂತ್ರಜ್ಞಾನ.\n",
    "\n",
    "## ಡೇಟಾಸೆಟ್\n",
    "\n",
    "ಡೇಟಾಸೆಟ್ ಅನ್ನು ಲೋಡ್ ಮಾಡಲು, ನಾವು **[TensorFlow Datasets](https://www.tensorflow.org/datasets)** API ಅನ್ನು ಬಳಸುವೆವು.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# In this tutorial, we will be training a lot of models. In order to use GPU memory cautiously,\n",
    "# we will set tensorflow option to grow GPU memory allocation when required.\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "if len(physical_devices)>0:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "dataset = tfds.load('ag_news_subset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ನಾವು ಈಗ `dataset['train']` ಮತ್ತು `dataset['test']` ಅನ್ನು ಕ್ರಮವಾಗಿ ಬಳಸಿ ಡೇಟಾಸೆಟ್‌ನ ತರಬೇತಿ ಮತ್ತು ಪರೀಕ್ಷಾ ಭಾಗಗಳನ್ನು ಪ್ರವೇಶಿಸಬಹುದು:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train dataset = 120000\n",
      "Length of test dataset = 7600\n"
     ]
    }
   ],
   "source": [
    "ds_train = dataset['train']\n",
    "ds_test = dataset['test']\n",
    "\n",
    "print(f\"Length of train dataset = {len(ds_train)}\")\n",
    "print(f\"Length of test dataset = {len(ds_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ನಮ್ಮ ಡೇಟಾಸೆಟ್‌ನ ಮೊದಲ 10 ಹೊಸ ಶೀರ್ಷಿಕೆಗಳನ್ನು ಮುದ್ರಿಸೋಣ:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 (Sci/Tech) -> b'AMD Debuts Dual-Core Opteron Processor' b'AMD #39;s new dual-core Opteron chip is designed mainly for corporate computing applications, including databases, Web services, and financial transactions.'\n",
      "1 (Sports) -> b\"Wood's Suspension Upheld (Reuters)\" b'Reuters - Major League Baseball\\\\Monday announced a decision on the appeal filed by Chicago Cubs\\\\pitcher Kerry Wood regarding a suspension stemming from an\\\\incident earlier this season.'\n",
      "2 (Business) -> b'Bush reform may have blue states seeing red' b'President Bush #39;s  quot;revenue-neutral quot; tax reform needs losers to balance its winners, and people claiming the federal deduction for state and local taxes may be in administration planners #39; sights, news reports say.'\n",
      "3 (Sci/Tech) -> b\"'Halt science decline in schools'\" b'Britain will run out of leading scientists unless science education is improved, says Professor Colin Pillinger.'\n",
      "1 (Sports) -> b'Gerrard leaves practice' b'London, England (Sports Network) - England midfielder Steven Gerrard injured his groin late in Thursday #39;s training session, but is hopeful he will be ready for Saturday #39;s World Cup qualifier against Austria.'\n"
     ]
    }
   ],
   "source": [
    "classes = ['World', 'Sports', 'Business', 'Sci/Tech']\n",
    "\n",
    "for i,x in zip(range(5),ds_train):\n",
    "    print(f\"{x['label']} ({classes[x['label']]}) -> {x['title']} {x['description']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ಪಠ್ಯ ವೆಕ್ಟರೀಕರಣ\n",
    "\n",
    "ಈಗ ನಮಗೆ ಪಠ್ಯವನ್ನು ಟೆನ್ಸರ್‌ಗಳಾಗಿ ಪ್ರತಿನಿಧಿಸಬಹುದಾದ **ಸಂಖ್ಯೆಗಳಾಗಿ** ಪರಿವರ್ತಿಸಲು ಅಗತ್ಯವಿದೆ. ನಾವು ಪದಮಟ್ಟದ ಪ್ರತಿನಿಧಾನವನ್ನು ಬಯಸಿದರೆ, ನಾವು ಎರಡು ಕೆಲಸಗಳನ್ನು ಮಾಡಬೇಕಾಗುತ್ತದೆ:\n",
    "\n",
    "* ಪಠ್ಯವನ್ನು **ಟೋಕನ್‌ಗಳಾಗಿ** ವಿಭಜಿಸಲು **ಟೋಕನೈಜರ್** ಅನ್ನು ಬಳಸಬೇಕು.\n",
    "* ಆ ಟೋಕನ್‌ಗಳ **ಶಬ್ದಕೋಶ**ವನ್ನು ನಿರ್ಮಿಸಬೇಕು.\n",
    "\n",
    "### ಶಬ್ದಕೋಶದ ಗಾತ್ರವನ್ನು ಮಿತಿಗೊಳಿಸುವುದು\n",
    "\n",
    "AG News ಡೇಟಾಸೆಟ್ ಉದಾಹರಣೆಯಲ್ಲಿ, ಶಬ್ದಕೋಶದ ಗಾತ್ರ ಬಹಳ ದೊಡ್ಡದು, 100k ಕ್ಕಿಂತ ಹೆಚ್ಚು ಪದಗಳಿವೆ. ಸಾಮಾನ್ಯವಾಗಿ, ಪಠ್ಯದಲ್ಲಿ ಅಪರೂಪವಾಗಿ ಕಾಣುವ ಪದಗಳು ಬೇಕಾಗುವುದಿಲ್ಲ — ಕೆಲವೇ ವಾಕ್ಯಗಳಲ್ಲಿ ಮಾತ್ರ ಅವು ಇರುತ್ತವೆ ಮತ್ತು ಮಾದರಿ ಅವುಗಳಿಂದ ಕಲಿಯುವುದಿಲ್ಲ. ಆದ್ದರಿಂದ, ವೆಕ್ಟರೈಜರ್ ನಿರ್ಮಾಪಕಕ್ಕೆ ಒಂದು ಆರ್ಗ್ಯುಮೆಂಟ್ ನೀಡುವ ಮೂಲಕ ಶಬ್ದಕೋಶದ ಗಾತ್ರವನ್ನು ಕಡಿಮೆ ಸಂಖ್ಯೆಗೆ ಮಿತಿಗೊಳಿಸುವುದು ಅರ್ಥಪೂರ್ಣ.\n",
    "\n",
    "ಈ ಎರಡೂ ಹಂತಗಳನ್ನು **TextVectorization** ಲೇಯರ್ ಬಳಸಿ ನಿರ್ವಹಿಸಬಹುದು. ಈಗ ನಾವು ವೆಕ್ಟರೈಜರ್ ವಸ್ತುವನ್ನು ಸೃಷ್ಟಿಸಿ, ನಂತರ ಎಲ್ಲಾ ಪಠ್ಯವನ್ನು ಪರಿಶೀಲಿಸಿ ಶಬ್ದಕೋಶವನ್ನು ನಿರ್ಮಿಸಲು `adapt` ವಿಧಾನವನ್ನು ಕರೆಸೋಣ:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50000\n",
    "vectorizer = keras.layers.experimental.preprocessing.TextVectorization(max_tokens=vocab_size)\n",
    "vectorizer.adapt(ds_train.take(500).map(lambda x: x['title']+' '+x['description']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **ಗಮನಿಸಿ** ನಾವು ಸಂಪೂರ್ಣ ಡೇಟಾಸೆಟ್‌ನ ಒಂದು ಉಪಸಮೂಹವನ್ನು ಮಾತ್ರ ಶಬ್ದಕೋಶವನ್ನು ನಿರ್ಮಿಸಲು ಬಳಸುತ್ತಿದ್ದೇವೆ. ನಾವು ಇದನ್ನು ಕಾರ್ಯಾಚರಣೆಯ ಸಮಯವನ್ನು ವೇಗಗೊಳಿಸಲು ಮತ್ತು ನಿಮ್ಮನ್ನು ಕಾಯದಿರಿಸಲು ಮಾಡುತ್ತೇವೆ. ಆದಾಗ್ಯೂ, ಸಂಪೂರ್ಣ ಡೇಟಾಸೆಟ್‌ನ ಕೆಲವು ಪದಗಳು ಶಬ್ದಕೋಶದಲ್ಲಿ ಸೇರಿಸಲಾಗದಿರುವ ಸಾಧ್ಯತೆ ಇದೆ ಮತ್ತು ತರಬೇತಿಯ ಸಮಯದಲ್ಲಿ ಅವುಗಳನ್ನು ನಿರ್ಲಕ್ಷಿಸಲಾಗುತ್ತದೆ. ಆದ್ದರಿಂದ, ಸಂಪೂರ್ಣ ಶಬ್ದಕೋಶದ ಗಾತ್ರವನ್ನು ಬಳಸಿಕೊಂಡು `adapt` ಸಮಯದಲ್ಲಿ ಸಂಪೂರ್ಣ ಡೇಟಾಸೆಟ್ ಮೂಲಕ ಓಡುವುದು ಅಂತಿಮ ನಿಖರತೆಯನ್ನು ಹೆಚ್ಚಿಸಬಹುದು, ಆದರೆ ಬಹಳಷ್ಟು ಅಲ್ಲ.\n",
    "\n",
    "ಈಗ ನಾವು ನಿಜವಾದ ಶಬ್ದಕೋಶವನ್ನು ಪ್ರವೇಶಿಸಬಹುದು:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '[UNK]', 'the', 'to', 'a', 'in', 'of', 'and', 'on', 'for']\n",
      "Length of vocabulary: 5335\n"
     ]
    }
   ],
   "source": [
    "vocab = vectorizer.get_vocabulary()\n",
    "vocab_size = len(vocab)\n",
    "print(vocab[:10])\n",
    "print(f\"Length of vocabulary: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ವೆಕ್ಟರೈಸರ್ ಬಳಸಿ, ನಾವು ಯಾವುದೇ ಪಠ್ಯವನ್ನು ಸುಲಭವಾಗಿ ಸಂಖ್ಯೆಗಳ ಸಮೂಹವಾಗಿ ಎನ್‌ಕೋಡ್ ಮಾಡಬಹುದು:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(7,), dtype=int64, numpy=array([ 112, 3695,    3,  304,   11, 1041,    1], dtype=int64)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer('I love to play with my words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ಪದಗಳ ಬ್ಯಾಗ್ ಪಠ್ಯ ಪ್ರತಿನಿಧಾನ\n",
    "\n",
    "ಪದಗಳು ಅರ್ಥವನ್ನು ಪ್ರತಿನಿಧಿಸುವುದರಿಂದ, ಕೆಲವೊಮ್ಮೆ ನಾವು ವಾಕ್ಯದ ಕ್ರಮವನ್ನು ಪರಿಗಣಿಸದೆ, ಪ್ರತ್ಯೇಕ ಪದಗಳನ್ನು ನೋಡಿಕೊಂಡು ಪಠ್ಯದ ಅರ್ಥವನ್ನು ತಿಳಿದುಕೊಳ್ಳಬಹುದು. ಉದಾಹರಣೆಗೆ, ಸುದ್ದಿಗಳನ್ನು ವರ್ಗೀಕರಿಸುವಾಗ, *ಹವಾಮಾನ* ಮತ್ತು *ಹಿಮ* ಎಂಬ ಪದಗಳು *ಹವಾಮಾನ ಮುನ್ಸೂಚನೆ* ಅನ್ನು ಸೂಚಿಸುವ ಸಾಧ್ಯತೆ ಇರುತ್ತದೆ, ಆದರೆ *ಸ್ಟಾಕ್ಸ್* ಮತ್ತು *ಡಾಲರ್* ಎಂಬ ಪದಗಳು *ಆರ್ಥಿಕ ಸುದ್ದಿ* ಗೆ ಸೇರಬಹುದು.\n",
    "\n",
    "**ಪದಗಳ ಬ್ಯಾಗ್** (BoW) ವೆಕ್ಟರ್ ಪ್ರತಿನಿಧಾನವು ಅತಿ ಸರಳವಾಗಿ ಅರ್ಥಮಾಡಿಕೊಳ್ಳಬಹುದಾದ ಪರಂಪರাগত ವೆಕ್ಟರ್ ಪ್ರತಿನಿಧಾನವಾಗಿದೆ. ಪ್ರತಿ ಪದವು ಒಂದು ವೆಕ್ಟರ್ ಸೂಚ್ಯಂಕಕ್ಕೆ ಜೋಡಿಸಲಾಗುತ್ತದೆ, ಮತ್ತು ಒಂದು ವೆಕ್ಟರ್ ಅಂಶವು ನೀಡಲಾದ ದಾಖಲೆಗಳಲ್ಲಿ ಪ್ರತಿ ಪದದ ಸಂಭವನಗಳ ಸಂಖ್ಯೆಯನ್ನು ಹೊಂದಿರುತ್ತದೆ.\n",
    "\n",
    "![ಪದಗಳ ಬ್ಯಾಗ್ ವೆಕ್ಟರ್ ಪ್ರತಿನಿಧಾನವನ್ನು ಮೆಮೊರಿಯಲ್ಲಿ ಹೇಗೆ ಪ್ರತಿನಿಧಿಸಲಾಗುತ್ತದೆ ಎಂಬುದನ್ನು ತೋರಿಸುವ ಚಿತ್ರ.](../../../../../translated_images/kn/bag-of-words-example.606fc1738f1d7ba9.webp) \n",
    "\n",
    "> **Note**: BoW ಅನ್ನು ಪಠ್ಯದ ಪ್ರತ್ಯೇಕ ಪದಗಳ ಒನ್-ಹಾಟ್-ಎನ್‌ಕೋಡ್ ಮಾಡಿದ ವೆಕ್ಟರ್‌ಗಳ ಮೊತ್ತವೆಂದು ಕೂಡ ಪರಿಗಣಿಸಬಹುದು.\n",
    "\n",
    "ಕೆಳಗಿನ ಉದಾಹರಣೆಯಲ್ಲಿ Scikit Learn ಪೈಥಾನ್ ಲೈಬ್ರರಿಯನ್ನು ಬಳಸಿ ಪದಗಳ ಬ್ಯಾಗ್ ಪ್ರತಿನಿಧಾನವನ್ನು ಹೇಗೆ ರಚಿಸುವುದು ಎಂಬುದನ್ನು ತೋರಿಸಲಾಗಿದೆ:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 2, 0, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "sc_vectorizer = CountVectorizer()\n",
    "corpus = [\n",
    "        'I like hot dogs.',\n",
    "        'The dog ran fast.',\n",
    "        'Its hot outside.',\n",
    "    ]\n",
    "sc_vectorizer.fit_transform(corpus)\n",
    "sc_vectorizer.transform(['My dog likes hot dogs on a hot day.']).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ನಾವು ಮೇಲ್ನೋಟದಲ್ಲಿ ವ್ಯಾಖ್ಯಾನಿಸಿದ ಕೇರಾಸ್ ವೆಕ್ಟರೈಜರ್ ಅನ್ನು ಕೂಡ ಬಳಸಬಹುದು, ಪ್ರತಿ ಪದ ಸಂಖ್ಯೆಯನ್ನು ಒನ್-ಹಾಟ್ ಎನ್‌ಕೋಡಿಂಗ್‌ಗೆ ಪರಿವರ್ತಿಸಿ ಮತ್ತು ಆ ಎಲ್ಲಾ ವೆಕ್ಟರ್‌ಗಳನ್ನು ಸೇರಿಸುವ ಮೂಲಕ:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 5., 0., ..., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def to_bow(text):\n",
    "    return tf.reduce_sum(tf.one_hot(vectorizer(text),vocab_size),axis=0)\n",
    "\n",
    "to_bow('My dog likes hot dogs on a hot day.').numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **ಗಮನಿಸಿ**: ನೀವು ಹಿಂದಿನ ಉದಾಹರಣೆಯಿಂದ ಫಲಿತಾಂಶ ಭಿನ್ನವಾಗಿರುವುದನ್ನು ಆಶ್ಚರ್ಯಪಡಬಹುದು. ಕಾರಣವೆಂದರೆ Keras ಉದಾಹರಣೆಯಲ್ಲಿ ವೆಕ್ಟರ್‌ನ ಉದ್ದವು ಶಬ್ದಕೋಶದ ಗಾತ್ರಕ್ಕೆ ಹೊಂದಿಕೆಯಾಗಿದ್ದು, ಅದು ಸಂಪೂರ್ಣ AG News ಡೇಟಾಸೆಟ್‌ನಿಂದ ನಿರ್ಮಿಸಲ್ಪಟ್ಟಿದೆ, ಆದರೆ Scikit Learn ಉದಾಹರಣೆಯಲ್ಲಿ ನಾವು ಶಬ್ದಕೋಶವನ್ನು ಉದಾಹರಣೆಯ ಪಠ್ಯದಿಂದ ತಕ್ಷಣ ನಿರ್ಮಿಸಿದ್ದೇವೆ.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BoW ವರ್ಗೀಕರಣೆಯನ್ನು ತರಬೇತುಗೊಳಿಸುವುದು\n",
    "\n",
    "ನಾವು ಈಗ ನಮ್ಮ ಪಠ್ಯದ ಬ್ಯಾಗ್-ಆಫ್-ವರ್ಡ್ಸ್ ಪ್ರತಿನಿಧಾನವನ್ನು ಹೇಗೆ ನಿರ್ಮಿಸುವುದನ್ನು ಕಲಿತಿದ್ದೇವೆ, ಅದನ್ನು ಬಳಸುವ ವರ್ಗೀಕರಣೆಯನ್ನು ತರಬೇತುಗೊಳಿಸೋಣ. ಮೊದಲು, ನಮ್ಮ ಡೇಟಾಸೆಟ್ ಅನ್ನು ಬ್ಯಾಗ್-ಆಫ್-ವರ್ಡ್ಸ್ ಪ್ರತಿನಿಧಾನಕ್ಕೆ ಪರಿವರ್ತಿಸಬೇಕಾಗುತ್ತದೆ. ಇದನ್ನು ಕೆಳಗಿನಂತೆ `map` ಫಂಕ್ಷನ್ ಬಳಸಿ ಸಾಧಿಸಬಹುದು:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "ds_train_bow = ds_train.map(lambda x: (to_bow(x['title']+x['description']),x['label'])).batch(batch_size)\n",
    "ds_test_bow = ds_test.map(lambda x: (to_bow(x['title']+x['description']),x['label'])).batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ಈಗ ನಾವು ಒಂದು ಸರಳ ವರ್ಗೀಕರಣ ನ್ಯೂರಲ್ ನೆಟ್‌ವರ್ಕ್ ಅನ್ನು ವ್ಯಾಖ್ಯಾನಿಸೋಣ, ಇದರಲ್ಲಿ ಒಂದು ಲೀನಿಯರ್ ಲೇಯರ್ ಇದೆ. ಇನ್‌ಪುಟ್ ಗಾತ್ರವು `vocab_size` ಆಗಿದ್ದು, ಔಟ್‌ಪುಟ್ ಗಾತ್ರವು ವರ್ಗಗಳ ಸಂಖ್ಯೆಗೆ (4) ಹೊಂದಿಕೆಯಾಗುತ್ತದೆ. ನಾವು ವರ್ಗೀಕರಣ ಕಾರ್ಯವನ್ನು ಪರಿಹರಿಸುತ್ತಿದ್ದೇವೆ ಆದ್ದರಿಂದ, ಅಂತಿಮ ಸಕ್ರಿಯತೆ ಕಾರ್ಯ **softmax** ಆಗಿದೆ:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 66s 70ms/step - loss: 0.6144 - acc: 0.8427 - val_loss: 0.4416 - val_acc: 0.8697\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20c70a947f0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(4,activation='softmax',input_shape=(vocab_size,))\n",
    "])\n",
    "model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['acc'])\n",
    "model.fit(ds_train_bow,validation_data=ds_test_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ನಾವು 4 ವರ್ಗಗಳನ್ನು ಹೊಂದಿರುವುದರಿಂದ, 80% ಕ್ಕಿಂತ ಹೆಚ್ಚು ಶುದ್ಧತೆ ಒಳ್ಳೆಯ ಫಲಿತಾಂಶವಾಗಿದೆ.\n",
    "\n",
    "## ಒಂದು ನೆಟ್‌ವರ್ಕ್ ಆಗಿ ವರ್ಗೀಕರಿಸುವ ಯಂತ್ರವನ್ನು ತರಬೇತುಗೊಳಿಸುವುದು\n",
    "\n",
    "ವ್ಯಕ್ತಿಗತಕಾರಕವೂ Keras ಲೇಯರ್ ಆಗಿರುವುದರಿಂದ, ಅದನ್ನು ಒಳಗೊಂಡಿರುವ ನೆಟ್‌ವರ್ಕ್ ಅನ್ನು ನಾವು ವ್ಯಾಖ್ಯಾನಿಸಿ, ಅಂತ್ಯದಿಂದ ಅಂತ್ಯಕ್ಕೆ ತರಬೇತುಗೊಳಿಸಬಹುದು. ಈ ರೀತಿಯಲ್ಲಿ ನಾವು `map` ಬಳಸಿ ಡೇಟಾಸೆಟ್ ಅನ್ನು ವ್ಯಕ್ತಿಗತಗೊಳಿಸುವ ಅಗತ್ಯವಿಲ್ಲ, ನಾವು ಮೂಲ ಡೇಟಾಸೆಟ್ ಅನ್ನು ನೇರವಾಗಿ ನೆಟ್‌ವರ್ಕ್‌ನ ಇನ್‌ಪುಟ್‌ಗೆ ನೀಡಬಹುದು.\n",
    "\n",
    "> **Note**: ನಮಗೆ ಇನ್ನೂ ಡಿಕ್ಷನರಿಗಳಿಂದ (ಹಾಗು `title`, `description` ಮತ್ತು `label` ಮುಂತಾದ) ಕ್ಷೇತ್ರಗಳನ್ನು ಟ್ಯೂಪಲ್‌ಗಳಿಗೆ ಪರಿವರ್ತಿಸಲು ನಮ್ಮ ಡೇಟಾಸೆಟ್‌ಗೆ ಮ್ಯಾಪ್‌ಗಳನ್ನು ಅನ್ವಯಿಸಬೇಕಾಗುತ್ತದೆ. ಆದಾಗ್ಯೂ, ಡಿಸ್ಕ್‌ನಿಂದ ಡೇಟಾವನ್ನು ಲೋಡ್ ಮಾಡುವಾಗ, ನಾವು ಮೊದಲಿನಿಂದಲೇ ಅಗತ್ಯವಿರುವ ರಚನೆಯೊಂದಿಗೆ ಡೇಟಾಸೆಟ್ ಅನ್ನು ನಿರ್ಮಿಸಬಹುದು.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization (TextVec  (None, None)             0         \n",
      " torization)                                                     \n",
      "                                                                 \n",
      " tf.one_hot (TFOpLambda)     (None, None, 5335)        0         \n",
      "                                                                 \n",
      " tf.math.reduce_sum (TFOpLam  (None, 5335)             0         \n",
      " bda)                                                            \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 4)                 21344     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21,344\n",
      "Trainable params: 21,344\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "938/938 [==============================] - 73s 77ms/step - loss: 0.6057 - acc: 0.8414 - val_loss: 0.4202 - val_acc: 0.8736\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20c721521f0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_text(x):\n",
    "    return x['title']+' '+x['description']\n",
    "\n",
    "def tupelize(x):\n",
    "    return (extract_text(x),x['label'])\n",
    "\n",
    "inp = keras.Input(shape=(1,),dtype=tf.string)\n",
    "x = vectorizer(inp)\n",
    "x = tf.reduce_sum(tf.one_hot(x,vocab_size),axis=1)\n",
    "out = keras.layers.Dense(4,activation='softmax')(x)\n",
    "model = keras.models.Model(inp,out)\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['acc'])\n",
    "model.fit(ds_train.map(tupelize).batch(batch_size),validation_data=ds_test.map(tupelize).batch(batch_size))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ಬಿಗ್ರಾಮ್, ಟ್ರಿಗ್ರಾಮ್ ಮತ್ತು ಎನ್-ಗ್ರಾಮ್‌ಗಳು\n",
    "\n",
    "ಬ್ಯಾಗ್-ಆಫ್-ವರ್ಡ್ಸ್ ವಿಧಾನದ ಒಂದು ಮಿತಿ ಎಂದರೆ ಕೆಲವು ಪದಗಳು ಬಹುಪದ ಅಭಿವ್ಯಕ್ತಿಗಳ ಭಾಗವಾಗಿರುತ್ತವೆ, ಉದಾಹರಣೆಗೆ, 'ಹಾಟ್ ಡಾಗ್' ಎಂಬ ಪದವು 'ಹಾಟ್' ಮತ್ತು 'ಡಾಗ್' ಎಂಬ ಪದಗಳಿಗಿಂತ ಸಂಪೂರ್ಣ ವಿಭಿನ್ನ ಅರ್ಥ ಹೊಂದಿದೆ. ನಾವು 'ಹಾಟ್' ಮತ್ತು 'ಡಾಗ್' ಪದಗಳನ್ನು ಯಾವಾಗಲೂ ಒಂದೇ ವೆಕ್ಟರ್‌ಗಳ ಮೂಲಕ ಪ್ರತಿನಿಧಿಸಿದರೆ, ಅದು ನಮ್ಮ ಮಾದರಿಯನ್ನು ಗೊಂದಲಕ್ಕೆ ಒಳಪಡಿಸಬಹುದು.\n",
    "\n",
    "ಇದನ್ನು ಪರಿಹರಿಸಲು, **ಎನ್-ಗ್ರಾಮ್ ಪ್ರತಿನಿಧಾನಗಳು** ಡಾಕ್ಯುಮೆಂಟ್ ವರ್ಗೀಕರಣ ವಿಧಾನಗಳಲ್ಲಿ ಸಾಮಾನ್ಯವಾಗಿ ಬಳಸಲಾಗುತ್ತವೆ, ಇಲ್ಲಿ ಪ್ರತಿ ಪದ, ದ್ವಿಪದ ಅಥವಾ ತ್ರಿಪದದ ಆವರ್ತನೆ ತರಬೇತಿ ವರ್ಗೀಕರಿಸುವವರಿಗೆ ಉಪಯುಕ್ತ ಲಕ್ಷಣವಾಗಿರುತ್ತದೆ. ಉದಾಹರಣೆಗೆ, ಬಿಗ್ರಾಮ್ ಪ್ರತಿನಿಧಾನಗಳಲ್ಲಿ, ಮೂಲ ಪದಗಳ ಜೊತೆಗೆ ಎಲ್ಲಾ ಪದ ಜೋಡಿಗಳನ್ನು ಶಬ್ದಕೋಶಕ್ಕೆ ಸೇರಿಸಲಾಗುತ್ತದೆ.\n",
    "\n",
    "ಕೆಳಗಿನ ಉದಾಹರಣೆಯಲ್ಲಿ Scikit Learn ಬಳಸಿ ಬಿಗ್ರಾಮ್ ಬ್ಯಾಗ್ ಆಫ್ ವರ್ಡ್ ಪ್ರತಿನಿಧಾನವನ್ನು ಹೇಗೆ ರಚಿಸುವುದು ಎಂಬುದನ್ನು ತೋರಿಸಲಾಗಿದೆ:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:\n",
      " {'i': 7, 'like': 11, 'hot': 4, 'dogs': 2, 'i like': 8, 'like hot': 12, 'hot dogs': 5, 'the': 16, 'dog': 0, 'ran': 14, 'fast': 3, 'the dog': 17, 'dog ran': 1, 'ran fast': 15, 'its': 9, 'outside': 13, 'its hot': 10, 'hot outside': 6}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 1, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_vectorizer = CountVectorizer(ngram_range=(1, 2), token_pattern=r'\\b\\w+\\b', min_df=1)\n",
    "corpus = [\n",
    "        'I like hot dogs.',\n",
    "        'The dog ran fast.',\n",
    "        'Its hot outside.',\n",
    "    ]\n",
    "bigram_vectorizer.fit_transform(corpus)\n",
    "print(\"Vocabulary:\\n\",bigram_vectorizer.vocabulary_)\n",
    "bigram_vectorizer.transform(['My dog likes hot dogs on a hot day.']).toarray()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n-ಗ್ರಾಮ್ ವಿಧಾನದ ಪ್ರಮುಖ ದೋಷವೆಂದರೆ ಶಬ್ದಕೋಶದ ಗಾತ್ರ ಅತ್ಯಂತ ವೇಗವಾಗಿ ಹೆಚ್ಚಾಗುತ್ತದೆ. ಪ್ರಾಯೋಗಿಕವಾಗಿ, ನಾವು n-ಗ್ರಾಮ್ ಪ್ರತಿನಿಧಾನವನ್ನು *ಎಂಬೆಡ್ಡಿಂಗ್‌ಗಳು* ಎಂಬ ಆಯಾಮ ಕಡಿತ ತಂತ್ರಜ್ಞಾನದೊಂದಿಗೆ ಸಂಯೋಜಿಸಬೇಕಾಗುತ್ತದೆ, ಇದನ್ನು ನಾವು ಮುಂದಿನ ಘಟಕದಲ್ಲಿ ಚರ್ಚಿಸುವೆವು.\n",
    "\n",
    "ನಮ್ಮ **AG News** ಡೇಟಾಸೆಟ್‌ನಲ್ಲಿ n-ಗ್ರಾಮ್ ಪ್ರತಿನಿಧಾನವನ್ನು ಬಳಸಲು, ನಾವು `TextVectorization` ನಿರ್ಮಾಪಕಕ್ಕೆ `ngrams` ಪರಿಮಾಣವನ್ನು ಪಾಸ್ ಮಾಡಬೇಕಾಗುತ್ತದೆ. ಬಿಗ್ರಾಮ್ ಶಬ್ದಕೋಶದ ಉದ್ದವು **ಗಣನೀಯವಾಗಿ ದೊಡ್ಡದು**, ನಮ್ಮ ಪ್ರಕರಣದಲ್ಲಿ ಇದು 1.3 ಮಿಲಿಯನ್ ಟೋಕನ್‌ಗಳಿಗಿಂತ ಹೆಚ್ಚು! ಆದ್ದರಿಂದ, ಬಿಗ್ರಾಮ್ ಟೋಕನ್‌ಗಳ ಸಂಖ್ಯೆಯನ್ನು ಸಹ ಯುಕ್ತಿಯುತ ಸಂಖ್ಯೆಯಿಂದ ಮಿತಿಗೊಳಿಸುವುದು ಸೂಕ್ತ.\n",
    "\n",
    "ಮೇಲಿನ ಕೋಡ್ ಅನ್ನು ಬಳಸಿ ವರ್ಗೀಕರಣಕಾರಿಯನ್ನು ತರಬೇತುಗೊಳಿಸಬಹುದು, ಆದರೆ ಅದು ಬಹಳಷ್ಟು ಮೆಮೊರಿ ಅಸಮರ್ಥವಾಗಿರುತ್ತದೆ. ಮುಂದಿನ ಘಟಕದಲ್ಲಿ, ನಾವು ಎಂಬೆಡ್ಡಿಂಗ್‌ಗಳನ್ನು ಬಳಸಿ ಬಿಗ್ರಾಮ್ ವರ್ಗೀಕರಣಕಾರಿಯನ್ನು ತರಬೇತುಗೊಳಿಸುವೆವು. ಈ ನಡುವೆ, ನೀವು ಈ ನೋಟ್ಬುಕ್‌ನಲ್ಲಿ ಬಿಗ್ರಾಮ್ ವರ್ಗೀಕರಣಕಾರಿಯ ತರಬೇತಿಯನ್ನು ಪ್ರಯೋಗಿಸಿ, ಹೆಚ್ಚಿನ ನಿಖರತೆಯನ್ನು ಪಡೆಯಬಹುದೇ ಎಂದು ನೋಡಬಹುದು.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ಸ್ವಯಂಚಾಲಿತವಾಗಿ BoW ವೆಕ್ಟರ್‌ಗಳನ್ನು ಲೆಕ್ಕಹಾಕುವುದು\n",
    "\n",
    "ಮೇಲಿನ ಉದಾಹರಣೆಯಲ್ಲಿ ನಾವು ಪ್ರತ್ಯೇಕ ಪದಗಳ ಒನ್-ಹಾಟ್ ಎನ್‌ಕೋಡಿಂಗ್‌ಗಳನ್ನು ಸೇರಿಸುವ ಮೂಲಕ ಕೈಯಿಂದ BoW ವೆಕ್ಟರ್‌ಗಳನ್ನು ಲೆಕ್ಕಹಾಕಿದ್ದೇವೆ. ಆದರೆ, TensorFlow ನ ಇತ್ತೀಚಿನ ಆವೃತ್ತಿ `output_mode='count'` ಪರಾಮೀಟರ್ ಅನ್ನು ವೆಕ್ಟರೈಜರ್ ನಿರ್ಮಾಪಕಕ್ಕೆ ನೀಡುವ ಮೂಲಕ BoW ವೆಕ್ಟರ್‌ಗಳನ್ನು ಸ್ವಯಂಚಾಲಿತವಾಗಿ ಲೆಕ್ಕಹಾಕಲು ಅನುಮತಿಸುತ್ತದೆ. ಇದರಿಂದ ನಮ್ಮ ಮಾದರಿಯನ್ನು ವ್ಯಾಖ್ಯಾನಿಸುವುದು ಮತ್ತು ತರಬೇತುಗೊಳಿಸುವುದು ಬಹಳ ಸುಲಭವಾಗುತ್ತದೆ:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training vectorizer\n",
      "938/938 [==============================] - 7s 7ms/step - loss: 0.5929 - acc: 0.8486 - val_loss: 0.4168 - val_acc: 0.8772\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20c725217c0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.experimental.preprocessing.TextVectorization(max_tokens=vocab_size,output_mode='count'),\n",
    "    keras.layers.Dense(4,input_shape=(vocab_size,), activation='softmax')\n",
    "])\n",
    "print(\"Training vectorizer\")\n",
    "model.layers[0].adapt(ds_train.take(500).map(extract_text))\n",
    "model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['acc'])\n",
    "model.fit(ds_train.map(tupelize).batch(batch_size),validation_data=ds_test.map(tupelize).batch(batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ಪದದ ಆವರ್ತನೆ - ಪ್ರತಿಕೂಲ ದಾಖಲೆ ಆವರ್ತನೆ (TF-IDF)\n",
    "\n",
    "BoW ಪ್ರತಿನಿಧಾನದಲ್ಲಿ, ಪದಗಳ ಸಂಭವಗಳನ್ನು ಪದದ ಸ್ವಭಾವವನ್ನು ಪರಿಗಣಿಸದೆ ಒಂದೇ ತಂತ್ರವನ್ನು ಬಳಸಿ ತೂಕ ನೀಡಲಾಗುತ್ತದೆ. ಆದರೆ, *a* ಮತ್ತು *in* ಹೀಗೆ ಸಾಮಾನ್ಯವಾಗಿ ಬರುತ್ತಿರುವ ಪದಗಳು ವಿಶೇಷ ಪದಗಳಿಗಿಂತ ವರ್ಗೀಕರಣಕ್ಕೆ ಕಡಿಮೆ ಮಹತ್ವದವು ಎಂಬುದು ಸ್ಪಷ್ಟವಾಗಿದೆ. ಬಹುತೇಕ NLP ಕಾರ್ಯಗಳಲ್ಲಿ ಕೆಲವು ಪದಗಳು ಇತರರಿಗಿಂತ ಹೆಚ್ಚು ಪ್ರಾಸಂಗಿಕವಾಗಿರುತ್ತವೆ.\n",
    "\n",
    "**TF-IDF** ಎಂದರೆ **term frequency - inverse document frequency**. ಇದು bag-of-words ನ ಒಂದು ಬದಲಾವಣೆ, ಇಲ್ಲಿ ಒಂದು ದಾಖಲೆದಲ್ಲಿ ಪದದ ಕಾಣಿಕೆ 0/1 ಬೈನರಿ ಮೌಲ್ಯ ಬದಲಾಗಿ, ಪದದ ಸಂಭವದ ಆವರ್ತನೆಗೆ ಸಂಬಂಧಿಸಿದ ಫ್ಲೋಟಿಂಗ್-ಪಾಯಿಂಟ್ ಮೌಲ್ಯವನ್ನು ಬಳಸಲಾಗುತ್ತದೆ.\n",
    "\n",
    "ವೈಜ್ಞಾನಿಕವಾಗಿ, ದಾಖಲೆ $j$ ಯಲ್ಲಿನ ಪದ $i$ ಯ ತೂಕ $w_{ij}$ ಅನ್ನು ಹೀಗೆ ವ್ಯಾಖ್ಯಾನಿಸಲಾಗುತ್ತದೆ:\n",
    "$$\n",
    "w_{ij} = tf_{ij}\\times\\log({N\\over df_i})\n",
    "$$\n",
    "ಇಲ್ಲಿ\n",
    "* $tf_{ij}$ ಎಂದರೆ $j$ ನಲ್ಲಿ $i$ ಪದದ ಸಂಭವಗಳ ಸಂಖ್ಯೆ, ಅಂದರೆ ನಾವು ಹಿಂದಿನ BoW ಮೌಲ್ಯವನ್ನು ನೋಡಿದ್ದೇವೆ\n",
    "* $N$ ಎಂದರೆ ಸಂಗ್ರಹದಲ್ಲಿನ ದಾಖಲೆಗಳ ಸಂಖ್ಯೆ\n",
    "* $df_i$ ಎಂದರೆ ಸಂಪೂರ್ಣ ಸಂಗ್ರಹದಲ್ಲಿ ಪದ $i$ ಹೊಂದಿರುವ ದಾಖಲೆಗಳ ಸಂಖ್ಯೆ\n",
    "\n",
    "TF-IDF ಮೌಲ್ಯ $w_{ij}$ ಒಂದು ಪದವು ದಾಖಲೆಗಳಲ್ಲಿ ಎಷ್ಟು ಬಾರಿ ಬರುತ್ತದೆ ಎಂಬುದರ ಅನುಪಾತದಲ್ಲಿ ಹೆಚ್ಚಾಗುತ್ತದೆ ಮತ್ತು ಆ ಪದವನ್ನು ಹೊಂದಿರುವ ದಾಖಲೆಗಳ ಸಂಖ್ಯೆಯಿಂದ ಕಡಿಮೆ ಮಾಡಲಾಗುತ್ತದೆ, ಇದು ಕೆಲವು ಪದಗಳು ಇತರರಿಗಿಂತ ಹೆಚ್ಚು ಬಾರಿ ಬರುತ್ತವೆ ಎಂಬುದನ್ನು ಸರಿಹೊಂದಿಸಲು ಸಹಾಯ ಮಾಡುತ್ತದೆ. ಉದಾಹರಣೆಗೆ, ಒಂದು ಪದವು ಸಂಗ್ರಹದಲ್ಲಿನ *ಪ್ರತಿ* ದಾಖಲೆದಲ್ಲಿಯೂ ಇದ್ದರೆ, $df_i=N$, ಆಗ $w_{ij}=0$ ಆಗಿ ಆ ಪದಗಳನ್ನು ಸಂಪೂರ್ಣವಾಗಿ ನಿರ್ಲಕ್ಷಿಸಲಾಗುತ್ತದೆ.\n",
    "\n",
    "ನೀವು ಸುಲಭವಾಗಿ Scikit Learn ಬಳಸಿ ಪಠ್ಯದ TF-IDF ವೆಕ್ಟರೈಜೆಶನ್ ಸೃಷ್ಟಿಸಬಹುದು:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.43381609, 0.        , 0.43381609, 0.        , 0.65985664,\n",
       "        0.43381609, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,2))\n",
    "vectorizer.fit_transform(corpus)\n",
    "vectorizer.transform(['My dog likes hot dogs on a hot day.']).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras ನಲ್ಲಿ, `TextVectorization` ಲೇಯರ್ ಸ್ವಯಂಚಾಲಿತವಾಗಿ TF-IDF ಆವರ್ತನೆಗಳನ್ನು ಲೆಕ್ಕಹಾಕಬಹುದು `output_mode='tf-idf'` ಪರಿಮಾಣವನ್ನು ನೀಡುವ ಮೂಲಕ. ಮೇಲಿನ ಕೋಡ್ ಅನ್ನು ಪುನರಾವರ್ತಿಸೋಣ ಮತ್ತು TF-IDF ಬಳಕೆ ನಿಖರತೆಯನ್ನು ಹೆಚ್ಚಿಸುವುದೇ ಎಂದು ನೋಡೋಣ:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training vectorizer\n",
      "938/938 [==============================] - 12s 12ms/step - loss: 0.4197 - acc: 0.8662 - val_loss: 0.3432 - val_acc: 0.8849\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20c729dfd30>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.experimental.preprocessing.TextVectorization(max_tokens=vocab_size,output_mode='tf-idf'),\n",
    "    keras.layers.Dense(4,input_shape=(vocab_size,), activation='softmax')\n",
    "])\n",
    "print(\"Training vectorizer\")\n",
    "model.layers[0].adapt(ds_train.take(500).map(extract_text))\n",
    "model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['acc'])\n",
    "model.fit(ds_train.map(tupelize).batch(batch_size),validation_data=ds_test.map(tupelize).batch(batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ಸಮಾರೋಪ\n",
    "\n",
    "TF-IDF ಪ್ರತಿನಿಧಿಗಳು ವಿಭಿನ್ನ ಪದಗಳಿಗೆ ಆವರ್ತನ ತೂಕಗಳನ್ನು ನೀಡಿದರೂ, ಅವು ಅರ್ಥ ಅಥವಾ ಕ್ರಮವನ್ನು ಪ್ರತಿನಿಧಿಸಲು ಸಾಧ್ಯವಿಲ್ಲ. 1935 ರಲ್ಲಿ ಪ್ರಸಿದ್ಧ ಭಾಷಾಶಾಸ್ತ್ರಜ್ಞ ಜೆ. ಆರ್. ಫರ್ಥ್ ಹೇಳಿದಂತೆ, \"ಒಂದು ಪದದ ಸಂಪೂರ್ಣ ಅರ್ಥ ಯಾವಾಗಲೂ ಸಾಂದರ್ಭಿಕವಾಗಿರುತ್ತದೆ, ಮತ್ತು ಸಾಂದರ್ಭಿಕತೆಯ ಹೊರಗಿನ ಅರ್ಥ ಅಧ್ಯಯನವನ್ನು ಗಂಭೀರವಾಗಿ ತೆಗೆದುಕೊಳ್ಳಲಾಗುವುದಿಲ್ಲ.\" ನಾವು ಈ ಕೋರ್ಸ್‌ನಲ್ಲಿ ನಂತರ ಭಾಷಾ ಮಾದರೀಕರಣವನ್ನು ಬಳಸಿ ಪಠ್ಯದಿಂದ ಸಾಂದರ್ಭಿಕ ಮಾಹಿತಿಯನ್ನು ಹೇಗೆ ಹಿಡಿಯುವುದು ಎಂಬುದನ್ನು ಕಲಿಯುತ್ತೇವೆ.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n<!-- CO-OP TRANSLATOR DISCLAIMER START -->\n**ಅಸ್ವೀಕರಣ**:  \nಈ ದಸ್ತಾವೇಜು AI ಅನುವಾದ ಸೇವೆ [Co-op Translator](https://github.com/Azure/co-op-translator) ಬಳಸಿ ಅನುವಾದಿಸಲಾಗಿದೆ. ನಾವು ನಿಖರತೆಯಿಗಾಗಿ ಪ್ರಯತ್ನಿಸುತ್ತಿದ್ದರೂ, ಸ್ವಯಂಚಾಲಿತ ಅನುವಾದಗಳಲ್ಲಿ ತಪ್ಪುಗಳು ಅಥವಾ ಅಸತ್ಯತೆಗಳು ಇರಬಹುದು ಎಂದು ದಯವಿಟ್ಟು ಗಮನಿಸಿ. ಮೂಲ ಭಾಷೆಯಲ್ಲಿರುವ ಮೂಲ ದಸ್ತಾವೇಜನ್ನು ಅಧಿಕೃತ ಮೂಲವೆಂದು ಪರಿಗಣಿಸಬೇಕು. ಮಹತ್ವದ ಮಾಹಿತಿಗಾಗಿ, ವೃತ್ತಿಪರ ಮಾನವ ಅನುವಾದವನ್ನು ಶಿಫಾರಸು ಮಾಡಲಾಗುತ್ತದೆ. ಈ ಅನುವಾದ ಬಳಕೆಯಿಂದ ಉಂಟಾಗುವ ಯಾವುದೇ ತಪ್ಪು ಅರ್ಥಮಾಡಿಕೊಳ್ಳುವಿಕೆ ಅಥವಾ ತಪ್ಪು ವಿವರಣೆಗಳಿಗೆ ನಾವು ಹೊಣೆಗಾರರಾಗುವುದಿಲ್ಲ.\n<!-- CO-OP TRANSLATOR DISCLAIMER END -->\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0cb620c6d4b9f7a635928804c26cf22403d89d98d79684e4529119355ee6d5a5"
  },
  "kernel_info": {
   "name": "conda-env-py37_tensorflow-py"
  },
  "kernelspec": {
   "display_name": "py37_tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "coopTranslator": {
   "original_hash": "19b43951d55b377a76209c24c1f017e4",
   "translation_date": "2025-11-26T01:42:09+00:00",
   "source_file": "lessons/5-NLP/13-TextRep/TextRepresentationTF.ipynb",
   "language_code": "kn"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}