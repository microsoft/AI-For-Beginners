{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ενσωματώσεις\n",
    "\n",
    "Στο προηγούμενο παράδειγμά μας, εργαστήκαμε με διανύσματα υψηλής διάστασης τύπου bag-of-words με μήκος `vocab_size`, και μετατρέψαμε ρητά διανύσματα χαμηλής διάστασης θέσης σε αραιή αναπαράσταση one-hot. Αυτή η αναπαράσταση one-hot δεν είναι αποδοτική από άποψη μνήμης. Επιπλέον, κάθε λέξη αντιμετωπίζεται ανεξάρτητα από τις άλλες, οπότε τα κωδικοποιημένα διανύσματα one-hot δεν εκφράζουν τις σημασιολογικές ομοιότητες μεταξύ των λέξεων.\n",
    "\n",
    "Σε αυτήν την ενότητα, θα συνεχίσουμε να εξερευνούμε το σύνολο δεδομένων **News AG**. Για να ξεκινήσουμε, ας φορτώσουμε τα δεδομένα και ας πάρουμε κάποιους ορισμούς από την προηγούμενη ενότητα.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "\n",
    "ds_train, ds_test = tfds.load('ag_news_subset').values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Τι είναι ένα embedding;\n",
    "\n",
    "Η ιδέα του **embedding** είναι να αναπαραστήσουμε λέξεις χρησιμοποιώντας χαμηλότερης διάστασης πυκνά διανύσματα που αντανακλούν τη σημασιολογική έννοια της λέξης. Αργότερα θα συζητήσουμε πώς να δημιουργήσουμε ουσιαστικά word embeddings, αλλά προς το παρόν ας σκεφτούμε τα embeddings ως έναν τρόπο να μειώσουμε τη διαστατικότητα ενός διανύσματος λέξης.\n",
    "\n",
    "Έτσι, ένα embedding layer παίρνει μια λέξη ως είσοδο και παράγει ένα διανύσμα εξόδου με καθορισμένο `embedding_size`. Με μια έννοια, είναι πολύ παρόμοιο με ένα `Dense` layer, αλλά αντί να παίρνει ένα one-hot encoded διάνυσμα ως είσοδο, μπορεί να πάρει έναν αριθμό λέξης.\n",
    "\n",
    "Χρησιμοποιώντας ένα embedding layer ως το πρώτο layer στο δίκτυό μας, μπορούμε να μεταβούμε από το μοντέλο bag-of-words σε ένα μοντέλο **embedding bag**, όπου πρώτα μετατρέπουμε κάθε λέξη στο κείμενό μας στο αντίστοιχο embedding και στη συνέχεια υπολογίζουμε κάποια συνάρτηση συσσωμάτωσης πάνω σε όλα αυτά τα embeddings, όπως `sum`, `average` ή `max`.\n",
    "\n",
    "![Εικόνα που δείχνει έναν ταξινομητή embedding για πέντε λέξεις ακολουθίας.](../../../../../translated_images/el/embedding-classifier-example.b77f021a7ee67eee.webp)\n",
    "\n",
    "Το νευρωνικό δίκτυο ταξινομητή μας αποτελείται από τα εξής layers:\n",
    "\n",
    "* Το layer `TextVectorization`, το οποίο παίρνει μια συμβολοσειρά ως είσοδο και παράγει έναν tensor αριθμών token. Θα καθορίσουμε ένα λογικό μέγεθος λεξιλογίου `vocab_size` και θα αγνοήσουμε λέξεις που χρησιμοποιούνται λιγότερο συχνά. Το σχήμα εισόδου θα είναι 1, και το σχήμα εξόδου θα είναι $n$, καθώς θα πάρουμε $n$ tokens ως αποτέλεσμα, καθένα από τα οποία περιέχει αριθμούς από 0 έως `vocab_size`.\n",
    "* Το layer `Embedding`, το οποίο παίρνει $n$ αριθμούς και μειώνει κάθε αριθμό σε ένα πυκνό διάνυσμα ενός δεδομένου μήκους (100 στο παράδειγμά μας). Έτσι, το tensor εισόδου με σχήμα $n$ θα μετατραπεί σε ένα tensor $n\\times 100$.\n",
    "* Το layer συσσωμάτωσης, το οποίο παίρνει τον μέσο όρο αυτού του tensor κατά τον πρώτο άξονα, δηλαδή θα υπολογίσει τον μέσο όρο όλων των $n$ tensors εισόδου που αντιστοιχούν σε διαφορετικές λέξεις. Για να υλοποιήσουμε αυτό το layer, θα χρησιμοποιήσουμε ένα layer `Lambda` και θα περάσουμε σε αυτό τη συνάρτηση για τον υπολογισμό του μέσου όρου. Η έξοδος θα έχει σχήμα 100 και θα είναι η αριθμητική αναπαράσταση ολόκληρης της ακολουθίας εισόδου.\n",
    "* Τελικός γραμμικός ταξινομητής `Dense`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization (TextVec  (None, None)             0         \n",
      " torization)                                                     \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, None, 100)         3000000   \n",
      "                                                                 \n",
      " lambda (Lambda)             (None, 100)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4)                 404       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,000,404\n",
      "Trainable params: 3,000,404\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 30000\n",
    "batch_size = 128\n",
    "\n",
    "vectorizer = keras.layers.experimental.preprocessing.TextVectorization(max_tokens=vocab_size,input_shape=(1,))\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    vectorizer,    \n",
    "    keras.layers.Embedding(vocab_size,100),\n",
    "    keras.layers.Lambda(lambda x: tf.reduce_mean(x,axis=1)),\n",
    "    keras.layers.Dense(4, activation='softmax')\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Στην εκτύπωση `summary`, στη στήλη **output shape**, η πρώτη διάσταση του tensor `None` αντιστοιχεί στο μέγεθος του minibatch, και η δεύτερη αντιστοιχεί στο μήκος της ακολουθίας των tokens. Όλες οι ακολουθίες tokens στο minibatch έχουν διαφορετικά μήκη. Θα συζητήσουμε πώς να το διαχειριστούμε στην επόμενη ενότητα.\n",
    "\n",
    "Τώρα ας εκπαιδεύσουμε το δίκτυο:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training vectorizer\n",
      "938/938 [==============================] - 20s 20ms/step - loss: 0.7891 - acc: 0.8155 - val_loss: 0.4470 - val_acc: 0.8642\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22255515100>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_text(x):\n",
    "    return x['title']+' '+x['description']\n",
    "\n",
    "def tupelize(x):\n",
    "    return (extract_text(x),x['label'])\n",
    "\n",
    "print(\"Training vectorizer\")\n",
    "vectorizer.adapt(ds_train.take(500).map(extract_text))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'])\n",
    "model.fit(ds_train.map(tupelize).batch(batch_size),validation_data=ds_test.map(tupelize).batch(batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "> **Σημείωση** ότι δημιουργούμε τον vectorizer βασισμένο σε ένα υποσύνολο των δεδομένων. Αυτό γίνεται για να επιταχυνθεί η διαδικασία, και μπορεί να οδηγήσει σε μια κατάσταση όπου δεν υπάρχουν όλα τα tokens από το κείμενό μας στο λεξιλόγιο. Σε αυτή την περίπτωση, αυτά τα tokens θα αγνοηθούν, κάτι που μπορεί να οδηγήσει σε ελαφρώς χαμηλότερη ακρίβεια. Ωστόσο, στην πραγματική ζωή, ένα υποσύνολο κειμένου συχνά δίνει μια καλή εκτίμηση λεξιλογίου.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Αντιμετώπιση διαφορετικών μεγεθών ακολουθιών μεταβλητών\n",
    "\n",
    "Ας κατανοήσουμε πώς γίνεται η εκπαίδευση σε μικροπαρτίδες. Στο παραπάνω παράδειγμα, ο tensor εισόδου έχει διάσταση 1, και χρησιμοποιούμε μικροπαρτίδες μήκους 128, έτσι ώστε το πραγματικό μέγεθος του tensor να είναι $128 \\times 1$. Ωστόσο, ο αριθμός των tokens σε κάθε πρόταση είναι διαφορετικός. Εάν εφαρμόσουμε το layer `TextVectorization` σε μία μόνο είσοδο, ο αριθμός των tokens που επιστρέφονται είναι διαφορετικός, ανάλογα με το πώς γίνεται η τοκενοποίηση του κειμένου:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 1 45], shape=(2,), dtype=int64)\n",
      "tf.Tensor([ 112 1271    1    3 1747  158], shape=(6,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer('Hello, world!'))\n",
    "print(vectorizer('I am glad to meet you!'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ωστόσο, όταν εφαρμόζουμε τον vectorizer σε αρκετές ακολουθίες, πρέπει να παράγει έναν τανυστή ορθογώνιου σχήματος, οπότε γεμίζει τα μη χρησιμοποιημένα στοιχεία με το PAD token (το οποίο στην περίπτωσή μας είναι μηδέν):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 6), dtype=int64, numpy=\n",
       "array([[   1,   45,    0,    0,    0,    0],\n",
       "       [ 112, 1271,    1,    3, 1747,  158]], dtype=int64)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer(['Hello, world!','I am glad to meet you!'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Εδώ μπορούμε να δούμε τις ενσωματώσεις:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 1.53059261e-02,  6.80514947e-02,  3.14026810e-02, ...,\n",
       "         -8.92002955e-02,  1.52911525e-04, -5.65562584e-02],\n",
       "        [ 2.57456154e-01,  2.79364467e-01, -2.03605562e-01, ...,\n",
       "         -2.07474351e-01,  8.31158683e-02, -2.03911960e-01],\n",
       "        [ 3.98201384e-02, -8.03454965e-03,  2.39790026e-02, ...,\n",
       "         -7.18549127e-04,  2.66963355e-02, -4.30646613e-02],\n",
       "        [ 3.98201384e-02, -8.03454965e-03,  2.39790026e-02, ...,\n",
       "         -7.18549127e-04,  2.66963355e-02, -4.30646613e-02],\n",
       "        [ 3.98201384e-02, -8.03454965e-03,  2.39790026e-02, ...,\n",
       "         -7.18549127e-04,  2.66963355e-02, -4.30646613e-02],\n",
       "        [ 3.98201384e-02, -8.03454965e-03,  2.39790026e-02, ...,\n",
       "         -7.18549127e-04,  2.66963355e-02, -4.30646613e-02]],\n",
       "\n",
       "       [[ 1.89674050e-01,  2.61548996e-01, -3.67433839e-02, ...,\n",
       "         -2.07366899e-01, -1.05442435e-01, -2.36952081e-01],\n",
       "        [ 6.16133213e-02,  1.80511594e-01,  9.77298319e-02, ...,\n",
       "         -5.46628237e-02, -1.07340455e-01, -1.06589928e-01],\n",
       "        [ 1.53059261e-02,  6.80514947e-02,  3.14026810e-02, ...,\n",
       "         -8.92002955e-02,  1.52911525e-04, -5.65562584e-02],\n",
       "        [-4.84890305e-02, -8.41715634e-02,  1.51529670e-01, ...,\n",
       "          1.28192469e-01, -7.77286515e-02,  1.26041949e-01],\n",
       "        [-4.17212099e-02, -5.60694858e-02,  4.08860669e-02, ...,\n",
       "          8.70475471e-02,  8.92383084e-02,  1.67974353e-01],\n",
       "        [ 2.85779923e-01,  4.57767487e-01,  4.52292450e-02, ...,\n",
       "         -1.97419018e-01, -2.04659685e-01, -2.79758364e-01]]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[1](vectorizer(['Hello, world!','I am glad to meet you!'])).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Σημείωση**: Για να ελαχιστοποιηθεί η ποσότητα της συμπλήρωσης, σε ορισμένες περιπτώσεις έχει νόημα να ταξινομηθούν όλες οι ακολουθίες στο σύνολο δεδομένων με τη σειρά αύξησης του μήκους (ή, πιο συγκεκριμένα, του αριθμού των τοκεν). Αυτό θα διασφαλίσει ότι κάθε minibatch περιέχει ακολουθίες παρόμοιου μήκους.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Σημασιολογικές ενσωματώσεις: Word2Vec\n",
    "\n",
    "Στο προηγούμενο παράδειγμα, το επίπεδο ενσωμάτωσης έμαθε να αντιστοιχεί λέξεις σε διανυσματικές αναπαραστάσεις, ωστόσο αυτές οι αναπαραστάσεις δεν είχαν σημασιολογική έννοια. Θα ήταν χρήσιμο να μάθουμε μια διανυσματική αναπαράσταση όπου παρόμοιες λέξεις ή συνώνυμα αντιστοιχούν σε διανύσματα που βρίσκονται κοντά μεταξύ τους με βάση κάποια διανυσματική απόσταση (για παράδειγμα, την ευκλείδεια απόσταση).\n",
    "\n",
    "Για να το πετύχουμε αυτό, πρέπει να προεκπαιδεύσουμε το μοντέλο ενσωμάτωσης σε μια μεγάλη συλλογή κειμένων χρησιμοποιώντας μια τεχνική όπως το [Word2Vec](https://en.wikipedia.org/wiki/Word2vec). Βασίζεται σε δύο κύριες αρχιτεκτονικές που χρησιμοποιούνται για την παραγωγή μιας κατανεμημένης αναπαράστασης των λέξεων:\n",
    "\n",
    " - **Συνεχές bag-of-words** (CBoW), όπου εκπαιδεύουμε το μοντέλο να προβλέπει μια λέξη από το περιβάλλον της. Δεδομένου του ngram $(W_{-2},W_{-1},W_0,W_1,W_2)$, ο στόχος του μοντέλου είναι να προβλέψει το $W_0$ από $(W_{-2},W_{-1},W_1,W_2)$.\n",
    " - **Συνεχές skip-gram**, το οποίο είναι αντίθετο του CBoW. Το μοντέλο χρησιμοποιεί το παράθυρο των λέξεων του περιβάλλοντος για να προβλέψει την τρέχουσα λέξη.\n",
    "\n",
    "Το CBoW είναι πιο γρήγορο, ενώ το skip-gram, αν και πιο αργό, αποδίδει καλύτερα στην αναπαράσταση σπάνιων λέξεων.\n",
    "\n",
    "![Εικόνα που δείχνει τους αλγόριθμους CBoW και Skip-Gram για τη μετατροπή λέξεων σε διανύσματα.](../../../../../translated_images/el/example-algorithms-for-converting-words-to-vectors.fbe9207a726922f6.webp)\n",
    "\n",
    "Για να πειραματιστούμε με την ενσωμάτωση Word2Vec που έχει προεκπαιδευτεί στο σύνολο δεδομένων Google News, μπορούμε να χρησιμοποιήσουμε τη βιβλιοθήκη **gensim**. Παρακάτω βρίσκουμε τις λέξεις που είναι πιο παρόμοιες με τη λέξη 'neural'.\n",
    "\n",
    "> **Σημείωση:** Όταν δημιουργείτε για πρώτη φορά διανύσματα λέξεων, η λήψη τους μπορεί να πάρει αρκετό χρόνο!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "w2v = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neuronal -> 0.7804799675941467\n",
      "neurons -> 0.7326500415802002\n",
      "neural_circuits -> 0.7252851724624634\n",
      "neuron -> 0.7174385190010071\n",
      "cortical -> 0.6941086649894714\n",
      "brain_circuitry -> 0.6923246383666992\n",
      "synaptic -> 0.6699118614196777\n",
      "neural_circuitry -> 0.6638563275337219\n",
      "neurochemical -> 0.6555314064025879\n",
      "neuronal_activity -> 0.6531826257705688\n"
     ]
    }
   ],
   "source": [
    "for w,p in w2v.most_similar('neural'):\n",
    "    print(f\"{w} -> {p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Μπορούμε επίσης να εξαγάγουμε την ενσωμάτωση διανύσματος από τη λέξη, για να χρησιμοποιηθεί στην εκπαίδευση του μοντέλου ταξινόμησης. Η ενσωμάτωση έχει 300 συνιστώσες, αλλά εδώ δείχνουμε μόνο τις πρώτες 20 συνιστώσες του διανύσματος για λόγους σαφήνειας:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01226807,  0.06225586,  0.10693359,  0.05810547,  0.23828125,\n",
       "        0.03686523,  0.05151367, -0.20703125,  0.01989746,  0.10058594,\n",
       "       -0.03759766, -0.1015625 , -0.15820312, -0.08105469, -0.0390625 ,\n",
       "       -0.05053711,  0.16015625,  0.2578125 ,  0.10058594, -0.25976562],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v['play'][:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Το σπουδαίο με τις σημασιολογικές ενσωματώσεις είναι ότι μπορείτε να χειριστείτε την κωδικοποίηση του διανύσματος βάσει της σημασιολογίας. Για παράδειγμα, μπορούμε να ζητήσουμε να βρούμε μια λέξη της οποίας η διανυσματική αναπαράσταση είναι όσο το δυνατόν πιο κοντά στις λέξεις *βασιλιάς* και *γυναίκα*, και όσο το δυνατόν πιο μακριά από τη λέξη *άνδρας*:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('queen', 0.7118192911148071)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar(positive=['king','woman'],negative=['man'])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Ένα παράδειγμα παραπάνω χρησιμοποιεί κάποια εσωτερική μαγεία του GenSym, αλλά η υποκείμενη λογική είναι στην πραγματικότητα αρκετά απλή. Ένα ενδιαφέρον στοιχείο σχετικά με τα embeddings είναι ότι μπορείτε να εκτελέσετε κανονικές διανυσματικές πράξεις σε διανύσματα embeddings, και αυτό θα αντικατοπτρίζει πράξεις στις **σημασίες** των λέξεων. Το παραπάνω παράδειγμα μπορεί να εκφραστεί με όρους διανυσματικών πράξεων: υπολογίζουμε το διάνυσμα που αντιστοιχεί στο **ΒΑΣΙΛΙΑΣ-ΑΝΔΡΑΣ+ΓΥΝΑΙΚΑ** (οι πράξεις `+` και `-` εκτελούνται στις διανυσματικές αναπαραστάσεις των αντίστοιχων λέξεων), και στη συνέχεια βρίσκουμε τη λέξη στο λεξικό που είναι πιο κοντά σε αυτό το διάνυσμα:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'queen'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the vector corresponding to kind-man+woman\n",
    "qvec = w2v['king']-1.7*w2v['man']+1.7*w2v['woman']\n",
    "# find the index of the closest embedding vector \n",
    "d = np.sum((w2v.vectors-qvec)**2,axis=1)\n",
    "min_idx = np.argmin(d)\n",
    "# find the corresponding word\n",
    "w2v.index_to_key[min_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **ΣΗΜΕΙΩΣΗ**: Χρειάστηκε να προσθέσουμε μικρούς συντελεστές στους διανύσματα *man* και *woman* - δοκιμάστε να τους αφαιρέσετε για να δείτε τι συμβαίνει.\n",
    "\n",
    "Για να βρούμε το κοντινότερο διάνυσμα, χρησιμοποιούμε τη μηχανή του TensorFlow για να υπολογίσουμε ένα διάνυσμα αποστάσεων μεταξύ του δικού μας διανύσματος και όλων των διανυσμάτων στο λεξιλόγιο, και στη συνέχεια βρίσκουμε τον δείκτη της ελάχιστης λέξης χρησιμοποιώντας `argmin`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ενώ το Word2Vec φαίνεται να είναι ένας εξαιρετικός τρόπος για να εκφράσει κανείς τη σημασιολογία των λέξεων, έχει αρκετά μειονεκτήματα, όπως τα εξής:\n",
    "\n",
    "* Τα μοντέλα CBoW και skip-gram είναι **προβλεπτικά embeddings**, και λαμβάνουν υπόψη μόνο το τοπικό πλαίσιο. Το Word2Vec δεν αξιοποιεί το παγκόσμιο πλαίσιο.\n",
    "* Το Word2Vec δεν λαμβάνει υπόψη τη **μορφολογία** των λέξεων, δηλαδή το γεγονός ότι η σημασία μιας λέξης μπορεί να εξαρτάται από διαφορετικά μέρη της λέξης, όπως η ρίζα.\n",
    "\n",
    "Το **FastText** προσπαθεί να ξεπεράσει τον δεύτερο περιορισμό και βασίζεται στο Word2Vec, μαθαίνοντας διανυσματικές αναπαραστάσεις για κάθε λέξη και για τα n-grams χαρακτήρων που βρίσκονται μέσα σε κάθε λέξη. Οι τιμές αυτών των αναπαραστάσεων στη συνέχεια υπολογίζονται κατά μέσο όρο σε ένα διάνυσμα σε κάθε βήμα εκπαίδευσης. Παρόλο που αυτό προσθέτει αρκετό επιπλέον υπολογιστικό φόρτο κατά την προεκπαίδευση, επιτρέπει στα embeddings λέξεων να κωδικοποιούν πληροφορίες υπολέξεων.\n",
    "\n",
    "Μια άλλη μέθοδος, το **GloVe**, χρησιμοποιεί μια διαφορετική προσέγγιση για τα embeddings λέξεων, βασισμένη στη παραγοντοποίηση του πίνακα λέξεων-πλαισίων. Αρχικά, δημιουργεί έναν μεγάλο πίνακα που μετρά τον αριθμό εμφανίσεων λέξεων σε διαφορετικά πλαίσια και στη συνέχεια προσπαθεί να αναπαραστήσει αυτόν τον πίνακα σε χαμηλότερες διαστάσεις με τρόπο που να ελαχιστοποιεί την απώλεια ανακατασκευής.\n",
    "\n",
    "Η βιβλιοθήκη gensim υποστηρίζει αυτά τα embeddings λέξεων, και μπορείτε να πειραματιστείτε μαζί τους αλλάζοντας τον κώδικα φόρτωσης του μοντέλου παραπάνω.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Χρήση προεκπαιδευμένων ενσωματώσεων στο Keras\n",
    "\n",
    "Μπορούμε να τροποποιήσουμε το παραπάνω παράδειγμα ώστε να προετοιμάσουμε τη μήτρα στο επίπεδο ενσωμάτωσης μας με σημασιολογικές ενσωματώσεις, όπως το Word2Vec. Τα λεξιλόγια της προεκπαιδευμένης ενσωμάτωσης και του κειμενικού σώματος πιθανότατα δεν θα ταιριάζουν, οπότε πρέπει να επιλέξουμε ένα. Εδώ εξετάζουμε τις δύο πιθανές επιλογές: τη χρήση του λεξιλογίου του tokenizer και τη χρήση του λεξιλογίου από τις ενσωματώσεις Word2Vec.\n",
    "\n",
    "### Χρήση λεξιλογίου tokenizer\n",
    "\n",
    "Όταν χρησιμοποιούμε το λεξιλόγιο του tokenizer, ορισμένες από τις λέξεις του λεξιλογίου θα έχουν αντίστοιχες ενσωματώσεις Word2Vec, ενώ κάποιες θα λείπουν. Δεδομένου ότι το μέγεθος του λεξιλογίου μας είναι `vocab_size`, και το μήκος του διανύσματος ενσωμάτωσης Word2Vec είναι `embed_size`, το επίπεδο ενσωμάτωσης θα αναπαρίσταται από μια μήτρα βαρών με σχήμα `vocab_size`$\\times$`embed_size`. Θα γεμίσουμε αυτήν τη μήτρα περνώντας μέσα από το λεξιλόγιο:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding size: 300\n",
      "Populating matrix, this will take some time...Done, found 4551 words, 784 words missing\n"
     ]
    }
   ],
   "source": [
    "embed_size = len(w2v.get_vector('hello'))\n",
    "print(f'Embedding size: {embed_size}')\n",
    "\n",
    "vocab = vectorizer.get_vocabulary()\n",
    "W = np.zeros((vocab_size,embed_size))\n",
    "print('Populating matrix, this will take some time...',end='')\n",
    "found, not_found = 0,0\n",
    "for i,w in enumerate(vocab):\n",
    "    try:\n",
    "        W[i] = w2v.get_vector(w)\n",
    "        found+=1\n",
    "    except:\n",
    "        # W[i] = np.random.normal(0.0,0.3,size=(embed_size,))\n",
    "        not_found+=1\n",
    "\n",
    "print(f\"Done, found {found} words, {not_found} words missing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Για λέξεις που δεν υπάρχουν στο λεξιλόγιο του Word2Vec, μπορούμε είτε να τις αφήσουμε ως μηδενικά, είτε να δημιουργήσουμε έναν τυχαίο διάνυσμα.\n",
    "\n",
    "Τώρα μπορούμε να ορίσουμε ένα embedding layer με προκαθορισμένα βάρη:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = keras.layers.Embedding(vocab_size,embed_size,weights=[W],trainable=False)\n",
    "model = keras.models.Sequential([\n",
    "    vectorizer, emb,\n",
    "    keras.layers.Lambda(lambda x: tf.reduce_mean(x,axis=1)),\n",
    "    keras.layers.Dense(4, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 10s 10ms/step - loss: 1.1075 - acc: 0.7822 - val_loss: 0.9134 - val_acc: 0.8175\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2220226ef10>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'])\n",
    "model.fit(ds_train.map(tupelize).batch(batch_size),\n",
    "          validation_data=ds_test.map(tupelize).batch(batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Σημείωση**: Παρατηρήστε ότι ορίζουμε `trainable=False` κατά τη δημιουργία του `Embedding`, που σημαίνει ότι δεν επανεκπαιδεύουμε το Embedding layer. Αυτό μπορεί να προκαλέσει ελαφρώς χαμηλότερη ακρίβεια, αλλά επιταχύνει την εκπαίδευση.\n",
    "\n",
    "### Χρήση λεξιλογίου ενσωμάτωσης\n",
    "\n",
    "Ένα πρόβλημα με την προηγούμενη προσέγγιση είναι ότι τα λεξιλόγια που χρησιμοποιούνται στο TextVectorization και στο Embedding είναι διαφορετικά. Για να ξεπεράσουμε αυτό το πρόβλημα, μπορούμε να χρησιμοποιήσουμε μία από τις παρακάτω λύσεις:\n",
    "* Επανεκπαίδευση του μοντέλου Word2Vec στο δικό μας λεξιλόγιο.\n",
    "* Φόρτωση του dataset μας με το λεξιλόγιο από το προεκπαιδευμένο μοντέλο Word2Vec. Τα λεξιλόγια που χρησιμοποιούνται για τη φόρτωση του dataset μπορούν να καθοριστούν κατά τη φόρτωση.\n",
    "\n",
    "Η δεύτερη προσέγγιση φαίνεται πιο εύκολη, οπότε ας την υλοποιήσουμε. Πρώτα απ' όλα, θα δημιουργήσουμε ένα `TextVectorization` layer με το καθορισμένο λεξιλόγιο, το οποίο προέρχεται από τις ενσωματώσεις Word2Vec:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(w2v.vocab.keys())\n",
    "vectorizer = keras.layers.experimental.preprocessing.TextVectorization(input_shape=(1,))\n",
    "vectorizer.set_vocabulary(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Η βιβλιοθήκη ενσωματώσεων λέξεων gensim περιέχει μια βολική συνάρτηση, `get_keras_embeddings`, η οποία θα δημιουργήσει αυτόματα το αντίστοιχο επίπεδο ενσωματώσεων Keras για εσάς.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "938/938 [==============================] - 20s 14ms/step - loss: 1.3377 - acc: 0.4978 - val_loss: 1.2995 - val_acc: 0.5647\n",
      "Epoch 2/5\n",
      "938/938 [==============================] - 10s 10ms/step - loss: 1.2587 - acc: 0.5722 - val_loss: 1.2339 - val_acc: 0.5842\n",
      "Epoch 3/5\n",
      "938/938 [==============================] - 10s 10ms/step - loss: 1.1980 - acc: 0.5884 - val_loss: 1.1826 - val_acc: 0.5954\n",
      "Epoch 4/5\n",
      "938/938 [==============================] - 12s 13ms/step - loss: 1.1503 - acc: 0.6002 - val_loss: 1.1417 - val_acc: 0.6018\n",
      "Epoch 5/5\n",
      "938/938 [==============================] - 11s 12ms/step - loss: 1.1120 - acc: 0.6097 - val_loss: 1.1083 - val_acc: 0.6104\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2220ccb81c0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    vectorizer, \n",
    "    w2v.get_keras_embedding(train_embeddings=False),\n",
    "    keras.layers.Lambda(lambda x: tf.reduce_mean(x,axis=1)),\n",
    "    keras.layers.Dense(4, activation='softmax')\n",
    "])\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'])\n",
    "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128),epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ένας από τους λόγους που δεν βλέπουμε μεγαλύτερη ακρίβεια είναι επειδή ορισμένες λέξεις από το σύνολο δεδομένων μας λείπουν από το προεκπαιδευμένο λεξιλόγιο του GloVe και, επομένως, ουσιαστικά αγνοούνται. Για να ξεπεράσουμε αυτό, μπορούμε να εκπαιδεύσουμε τις δικές μας ενσωματώσεις βασισμένες στο σύνολο δεδομένων μας.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ενσωματώσεις με βάση τα συμφραζόμενα\n",
    "\n",
    "Ένας βασικός περιορισμός των παραδοσιακών προεκπαιδευμένων αναπαραστάσεων ενσωμάτωσης, όπως το Word2Vec, είναι το γεγονός ότι, παρόλο που μπορούν να αποτυπώσουν κάποια σημασία μιας λέξης, δεν μπορούν να διακρίνουν μεταξύ διαφορετικών νοημάτων. Αυτό μπορεί να προκαλέσει προβλήματα στα μοντέλα που χρησιμοποιούνται στη συνέχεια.\n",
    "\n",
    "Για παράδειγμα, η λέξη «play» έχει διαφορετική σημασία στις παρακάτω δύο προτάσεις:\n",
    "- Πήγα σε μια **παράσταση** στο θέατρο.\n",
    "- Ο Τζον θέλει να **παίξει** με τους φίλους του.\n",
    "\n",
    "Οι προεκπαιδευμένες ενσωματώσεις που αναφέραμε αναπαριστούν και τις δύο σημασίες της λέξης «play» με την ίδια ενσωμάτωση. Για να ξεπεράσουμε αυτόν τον περιορισμό, πρέπει να δημιουργήσουμε ενσωματώσεις βασισμένες στο **γλωσσικό μοντέλο**, το οποίο έχει εκπαιδευτεί σε ένα μεγάλο σώμα κειμένου και *γνωρίζει* πώς οι λέξεις μπορούν να συνδυαστούν σε διαφορετικά συμφραζόμενα. Η συζήτηση για τις ενσωματώσεις με βάση τα συμφραζόμενα ξεφεύγει από το πλαίσιο αυτού του μαθήματος, αλλά θα επανέλθουμε σε αυτές όταν μιλήσουμε για τα γλωσσικά μοντέλα στην επόμενη ενότητα.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Αποποίηση ευθύνης**:  \nΑυτό το έγγραφο έχει μεταφραστεί χρησιμοποιώντας την υπηρεσία αυτόματης μετάφρασης AI [Co-op Translator](https://github.com/Azure/co-op-translator). Παρόλο που καταβάλλουμε κάθε προσπάθεια για ακρίβεια, παρακαλούμε να έχετε υπόψη ότι οι αυτόματες μεταφράσεις ενδέχεται να περιέχουν σφάλματα ή ανακρίβειες. Το πρωτότυπο έγγραφο στη μητρική του γλώσσα θα πρέπει να θεωρείται η αυθεντική πηγή. Για κρίσιμες πληροφορίες, συνιστάται επαγγελματική ανθρώπινη μετάφραση. Δεν φέρουμε ευθύνη για τυχόν παρεξηγήσεις ή εσφαλμένες ερμηνείες που προκύπτουν από τη χρήση αυτής της μετάφρασης.\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0cb620c6d4b9f7a635928804c26cf22403d89d98d79684e4529119355ee6d5a5"
  },
  "kernel_info": {
   "name": "conda-env-py37_tensorflow-py"
  },
  "kernelspec": {
   "display_name": "py37_tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "coopTranslator": {
   "original_hash": "b859482be7f61d1eadc2c6a2720a37e4",
   "translation_date": "2025-08-29T10:55:29+00:00",
   "source_file": "lessons/5-NLP/14-Embeddings/EmbeddingsTF.ipynb",
   "language_code": "el"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}