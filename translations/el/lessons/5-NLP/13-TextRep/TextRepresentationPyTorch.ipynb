{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Εργασία ταξινόμησης κειμένου\n",
    "\n",
    "Όπως έχουμε αναφέρει, θα επικεντρωθούμε σε μια απλή εργασία ταξινόμησης κειμένου βασισμένη στο dataset **AG_NEWS**, το οποίο αφορά την ταξινόμηση τίτλων ειδήσεων σε μία από τις 4 κατηγορίες: Κόσμος, Αθλητικά, Επιχειρήσεις και Επιστήμη/Τεχνολογία.\n",
    "\n",
    "## Το Dataset\n",
    "\n",
    "Αυτό το dataset είναι ενσωματωμένο στο module [`torchtext`](https://github.com/pytorch/text), οπότε μπορούμε να το έχουμε εύκολα πρόσβαση.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "import os\n",
    "import collections\n",
    "os.makedirs('./data',exist_ok=True)\n",
    "train_dataset, test_dataset = torchtext.datasets.AG_NEWS(root='./data')\n",
    "classes = ['World', 'Sports', 'Business', 'Sci/Tech']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Εδώ, `train_dataset` και `test_dataset` περιέχουν συλλογές που επιστρέφουν ζεύγη ετικέτας (αριθμός κατηγορίας) και κειμένου αντίστοιχα, για παράδειγμα:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,\n",
       " \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(train_dataset)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Λοιπόν, ας εκτυπώσουμε τους πρώτους 10 νέους τίτλους από το σύνολο δεδομένων μας:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Sci/Tech** -> Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\band of ultra-cynics, are seeing green again.\n",
      "**Sci/Tech** -> Carlyle Looks Toward Commercial Aerospace (Reuters) Reuters - Private investment firm Carlyle Group,\\which has a reputation for making well-timed and occasionally\\controversial plays in the defense industry, has quietly placed\\its bets on another part of the market.\n",
      "**Sci/Tech** -> Oil and Economy Cloud Stocks' Outlook (Reuters) Reuters - Soaring crude prices plus worries\\about the economy and the outlook for earnings are expected to\\hang over the stock market next week during the depth of the\\summer doldrums.\n",
      "**Sci/Tech** -> Iraq Halts Oil Exports from Main Southern Pipeline (Reuters) Reuters - Authorities have halted oil export\\flows from the main pipeline in southern Iraq after\\intelligence showed a rebel militia could strike\\infrastructure, an oil official said on Saturday.\n",
      "**Sci/Tech** -> Oil prices soar to all-time record, posing new menace to US economy (AFP) AFP - Tearaway world oil prices, toppling records and straining wallets, present a new economic menace barely three months before the US presidential elections.\n"
     ]
    }
   ],
   "source": [
    "for i,x in zip(range(5),train_dataset):\n",
    "    print(f\"**{classes[x[0]]}** -> {x[1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Επειδή τα σύνολα δεδομένων είναι επαναλήπτες, αν θέλουμε να χρησιμοποιήσουμε τα δεδομένα πολλές φορές πρέπει να τα μετατρέψουμε σε λίστα:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = torchtext.datasets.AG_NEWS(root='./data')\n",
    "train_dataset = list(train_dataset)\n",
    "test_dataset = list(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Τοκενισμός\n",
    "\n",
    "Τώρα πρέπει να μετατρέψουμε το κείμενο σε **αριθμούς** που μπορούν να αναπαρασταθούν ως τανυστές. Αν θέλουμε αναπαράσταση σε επίπεδο λέξεων, πρέπει να κάνουμε δύο πράγματα:\n",
    "* να χρησιμοποιήσουμε έναν **τοκενάιζερ** για να χωρίσουμε το κείμενο σε **τοκενς**\n",
    "* να δημιουργήσουμε ένα **λεξιλόγιο** από αυτά τα τοκενς.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['he', 'said', 'hello']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = torchtext.data.utils.get_tokenizer('basic_english')\n",
    "tokenizer('He said: hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = collections.Counter()\n",
    "for (label, line) in train_dataset:\n",
    "    counter.update(tokenizer(line))\n",
    "vocab = torchtext.vocab.vocab(counter, min_freq=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Χρησιμοποιώντας το λεξιλόγιο, μπορούμε εύκολα να κωδικοποιήσουμε την συμβολοσειρά που έχει υποστεί τοκενισμό σε ένα σύνολο αριθμών:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size if 95810\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[599, 3279, 97, 1220, 329, 225, 7368]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(vocab)\n",
    "print(f\"Vocab size if {vocab_size}\")\n",
    "\n",
    "stoi = vocab.get_stoi() # dict to convert tokens to indices\n",
    "\n",
    "def encode(x):\n",
    "    return [stoi[s] for s in tokenizer(x)]\n",
    "\n",
    "encode('I love to play with my words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Αναπαράσταση κειμένου με Bag of Words\n",
    "\n",
    "Επειδή οι λέξεις αντιπροσωπεύουν νόημα, μερικές φορές μπορούμε να κατανοήσουμε το νόημα ενός κειμένου απλώς κοιτάζοντας τις μεμονωμένες λέξεις, ανεξάρτητα από τη σειρά τους μέσα στην πρόταση. Για παράδειγμα, όταν ταξινομούμε ειδήσεις, λέξεις όπως *καιρός*, *χιόνι* είναι πιθανό να υποδεικνύουν *πρόγνωση καιρού*, ενώ λέξεις όπως *μετοχές*, *δολάριο* θα μπορούσαν να σχετίζονται με *οικονομικές ειδήσεις*.\n",
    "\n",
    "Η **αναπαράσταση Bag of Words** (BoW) είναι η πιο συχνά χρησιμοποιούμενη παραδοσιακή μέθοδος αναπαράστασης με διανύσματα. Κάθε λέξη συνδέεται με έναν δείκτη διανύσματος, και το στοιχείο του διανύσματος περιέχει τον αριθμό εμφανίσεων μιας λέξης σε ένα συγκεκριμένο έγγραφο.\n",
    "\n",
    "![Εικόνα που δείχνει πώς η αναπαράσταση διανύσματος Bag of Words αποθηκεύεται στη μνήμη.](../../../../../translated_images/el/bag-of-words-example.606fc1738f1d7ba9.webp) \n",
    "\n",
    "> **Note**: Μπορείτε επίσης να σκεφτείτε το BoW ως το άθροισμα όλων των διανυσμάτων one-hot-encoded για τις μεμονωμένες λέξεις του κειμένου.\n",
    "\n",
    "Παρακάτω είναι ένα παράδειγμα για το πώς να δημιουργήσετε μια αναπαράσταση Bag of Words χρησιμοποιώντας τη βιβλιοθήκη Scikit Learn της Python:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 2, 0, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "corpus = [\n",
    "        'I like hot dogs.',\n",
    "        'The dog ran fast.',\n",
    "        'Its hot outside.',\n",
    "    ]\n",
    "vectorizer.fit_transform(corpus)\n",
    "vectorizer.transform(['My dog likes hot dogs on a hot day.']).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Για να υπολογίσουμε το διάνυσμα bag-of-words από την αναπαράσταση διανυσμάτων του συνόλου δεδομένων AG_NEWS, μπορούμε να χρησιμοποιήσουμε την ακόλουθη συνάρτηση:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 1., 2.,  ..., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(vocab)\n",
    "\n",
    "def to_bow(text,bow_vocab_size=vocab_size):\n",
    "    res = torch.zeros(bow_vocab_size,dtype=torch.float32)\n",
    "    for i in encode(text):\n",
    "        if i<bow_vocab_size:\n",
    "            res[i] += 1\n",
    "    return res\n",
    "\n",
    "print(to_bow(train_dataset[0][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Σημείωση:** Εδώ χρησιμοποιούμε τη μεταβλητή `vocab_size` σε παγκόσμιο επίπεδο για να καθορίσουμε το προεπιλεγμένο μέγεθος του λεξιλογίου. Δεδομένου ότι συχνά το μέγεθος του λεξιλογίου είναι αρκετά μεγάλο, μπορούμε να περιορίσουμε το μέγεθός του στις πιο συχνές λέξεις. Δοκιμάστε να μειώσετε την τιμή του `vocab_size` και να εκτελέσετε τον παρακάτω κώδικα, και δείτε πώς επηρεάζει την ακρίβεια. Θα πρέπει να περιμένετε κάποια πτώση στην ακρίβεια, αλλά όχι δραματική, προς όφελος της υψηλότερης απόδοσης.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Εκπαίδευση ταξινομητή BoW\n",
    "\n",
    "Τώρα που μάθαμε πώς να δημιουργούμε την αναπαράσταση Bag-of-Words για το κείμενό μας, ας εκπαιδεύσουμε έναν ταξινομητή πάνω σε αυτήν. Πρώτα, πρέπει να μετατρέψουμε το σύνολο δεδομένων μας για εκπαίδευση με τέτοιο τρόπο ώστε όλες οι αναπαραστάσεις διανυσμάτων θέσης να μετατραπούν σε αναπαράσταση bag-of-words. Αυτό μπορεί να επιτευχθεί περνώντας τη συνάρτηση `bowify` ως παράμετρο `collate_fn` στον τυπικό torch `DataLoader`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import numpy as np \n",
    "\n",
    "# this collate function gets list of batch_size tuples, and needs to \n",
    "# return a pair of label-feature tensors for the whole minibatch\n",
    "def bowify(b):\n",
    "    return (\n",
    "            torch.LongTensor([t[0]-1 for t in b]),\n",
    "            torch.stack([to_bow(t[1]) for t in b])\n",
    "    )\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, collate_fn=bowify, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, collate_fn=bowify, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Τώρα ας ορίσουμε ένα απλό νευρωνικό δίκτυο ταξινόμησης που περιέχει ένα γραμμικό επίπεδο. Το μέγεθος του διανύσματος εισόδου είναι ίσο με το `vocab_size`, και το μέγεθος εξόδου αντιστοιχεί στον αριθμό των κατηγοριών (4). Επειδή λύνουμε ένα πρόβλημα ταξινόμησης, η τελική συνάρτηση ενεργοποίησης είναι η `LogSoftmax()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = torch.nn.Sequential(torch.nn.Linear(vocab_size,4),torch.nn.LogSoftmax(dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Τώρα θα ορίσουμε τον τυπικό βρόχο εκπαίδευσης του PyTorch. Επειδή το σύνολο δεδομένων μας είναι αρκετά μεγάλο, για τον σκοπό της διδασκαλίας μας θα εκπαιδεύσουμε μόνο για μία εποχή, και μερικές φορές ακόμη και για λιγότερο από μία εποχή (ορίζοντας την παράμετρο `epoch_size` μπορούμε να περιορίσουμε την εκπαίδευση). Θα αναφέρουμε επίσης τη συσσωρευμένη ακρίβεια εκπαίδευσης κατά τη διάρκεια της εκπαίδευσης· η συχνότητα αναφοράς καθορίζεται χρησιμοποιώντας την παράμετρο `report_freq`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(net,dataloader,lr=0.01,optimizer=None,loss_fn = torch.nn.NLLLoss(),epoch_size=None, report_freq=200):\n",
    "    optimizer = optimizer or torch.optim.Adam(net.parameters(),lr=lr)\n",
    "    net.train()\n",
    "    total_loss,acc,count,i = 0,0,0,0\n",
    "    for labels,features in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        out = net(features)\n",
    "        loss = loss_fn(out,labels) #cross_entropy(out,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss+=loss\n",
    "        _,predicted = torch.max(out,1)\n",
    "        acc+=(predicted==labels).sum()\n",
    "        count+=len(labels)\n",
    "        i+=1\n",
    "        if i%report_freq==0:\n",
    "            print(f\"{count}: acc={acc.item()/count}\")\n",
    "        if epoch_size and count>epoch_size:\n",
    "            break\n",
    "    return total_loss.item()/count, acc.item()/count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200: acc=0.8028125\n",
      "6400: acc=0.8371875\n",
      "9600: acc=0.8534375\n",
      "12800: acc=0.85765625\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.026090790722161722, 0.8620069296375267)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_epoch(net,train_loader,epoch_size=15000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Διγράμματα, Τριγράμματα και Ν-γράμματα\n",
    "\n",
    "Ένας περιορισμός της προσέγγισης με τσάντα λέξεων είναι ότι ορισμένες λέξεις αποτελούν μέρος εκφράσεων πολλαπλών λέξεων. Για παράδειγμα, η λέξη 'hot dog' έχει εντελώς διαφορετική σημασία από τις λέξεις 'hot' και 'dog' σε άλλες περιπτώσεις. Εάν εκπροσωπούμε τις λέξεις 'hot' και 'dog' πάντα με τους ίδιους διανύσματα, μπορεί να προκαλέσει σύγχυση στο μοντέλο μας.\n",
    "\n",
    "Για να αντιμετωπιστεί αυτό, οι **αναπαραστάσεις Ν-γραμμάτων** χρησιμοποιούνται συχνά σε μεθόδους ταξινόμησης εγγράφων, όπου η συχνότητα κάθε λέξης, διλέξης ή τριλέξης αποτελεί χρήσιμο χαρακτηριστικό για την εκπαίδευση ταξινομητών. Στην αναπαράσταση διγράμματος, για παράδειγμα, προσθέτουμε όλα τα ζεύγη λέξεων στο λεξιλόγιο, επιπλέον των αρχικών λέξεων.\n",
    "\n",
    "Παρακάτω είναι ένα παράδειγμα για το πώς να δημιουργήσετε μια αναπαράσταση τσάντας λέξεων με διγράμματα χρησιμοποιώντας το Scikit Learn:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:\n",
      " {'i': 7, 'like': 11, 'hot': 4, 'dogs': 2, 'i like': 8, 'like hot': 12, 'hot dogs': 5, 'the': 16, 'dog': 0, 'ran': 14, 'fast': 3, 'the dog': 17, 'dog ran': 1, 'ran fast': 15, 'its': 9, 'outside': 13, 'its hot': 10, 'hot outside': 6}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 1, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_vectorizer = CountVectorizer(ngram_range=(1, 2), token_pattern=r'\\b\\w+\\b', min_df=1)\n",
    "corpus = [\n",
    "        'I like hot dogs.',\n",
    "        'The dog ran fast.',\n",
    "        'Its hot outside.',\n",
    "    ]\n",
    "bigram_vectorizer.fit_transform(corpus)\n",
    "print(\"Vocabulary:\\n\",bigram_vectorizer.vocabulary_)\n",
    "bigram_vectorizer.transform(['My dog likes hot dogs on a hot day.']).toarray()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Το κύριο μειονέκτημα της προσέγγισης N-gram είναι ότι το μέγεθος του λεξιλογίου αρχίζει να αυξάνεται εξαιρετικά γρήγορα. Στην πράξη, χρειάζεται να συνδυάσουμε την αναπαράσταση N-gram με κάποιες τεχνικές μείωσης διαστάσεων, όπως οι *ενσωματώσεις* (embeddings), τις οποίες θα συζητήσουμε στην επόμενη ενότητα.\n",
    "\n",
    "Για να χρησιμοποιήσουμε την αναπαράσταση N-gram στο σύνολο δεδομένων μας **AG News**, πρέπει να δημιουργήσουμε ένα ειδικό λεξιλόγιο ngram:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram vocabulary length =  1308842\n"
     ]
    }
   ],
   "source": [
    "counter = collections.Counter()\n",
    "for (label, line) in train_dataset:\n",
    "    l = tokenizer(line)\n",
    "    counter.update(torchtext.data.utils.ngrams_iterator(l,ngrams=2))\n",
    "    \n",
    "bi_vocab = torchtext.vocab.vocab(counter, min_freq=1)\n",
    "\n",
    "print(\"Bigram vocabulary length = \",len(bi_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Θα μπορούσαμε να χρησιμοποιήσουμε τον ίδιο κώδικα όπως παραπάνω για να εκπαιδεύσουμε τον ταξινομητή, ωστόσο, αυτό θα ήταν πολύ αναποτελεσματικό από άποψη μνήμης. Στην επόμενη ενότητα, θα εκπαιδεύσουμε έναν ταξινομητή διγράμματος χρησιμοποιώντας ενσωματώσεις.\n",
    "\n",
    "> **Σημείωση:** Μπορείτε να κρατήσετε μόνο εκείνα τα ngrams που εμφανίζονται στο κείμενο περισσότερες φορές από τον καθορισμένο αριθμό. Αυτό θα διασφαλίσει ότι τα σπάνια διγράμματα θα παραλειφθούν και θα μειώσει σημαντικά τη διαστατικότητα. Για να το πετύχετε αυτό, ορίστε την παράμετρο `min_freq` σε υψηλότερη τιμή και παρατηρήστε την αλλαγή στο μήκος του λεξιλογίου.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Συχνότητα Όρων Αντίστροφη Συχνότητα Εγγράφων TF-IDF\n",
    "\n",
    "Στην αναπαράσταση BoW, οι εμφανίσεις λέξεων έχουν ίσο βάρος, ανεξάρτητα από την ίδια τη λέξη. Ωστόσο, είναι προφανές ότι οι συχνές λέξεις, όπως *ένα*, *σε*, κ.λπ., είναι πολύ λιγότερο σημαντικές για την ταξινόμηση, σε σύγκριση με εξειδικευμένους όρους. Στην πραγματικότητα, σε πολλές εργασίες NLP, ορισμένες λέξεις είναι πιο σχετικές από άλλες.\n",
    "\n",
    "**TF-IDF** σημαίνει **συχνότητα όρων–αντίστροφη συχνότητα εγγράφων**. Είναι μια παραλλαγή του bag of words, όπου αντί για μια δυαδική τιμή 0/1 που υποδεικνύει την εμφάνιση μιας λέξης σε ένα έγγραφο, χρησιμοποιείται μια τιμή κινητής υποδιαστολής, η οποία σχετίζεται με τη συχνότητα εμφάνισης της λέξης στο σύνολο των εγγράφων.\n",
    "\n",
    "Πιο επίσημα, το βάρος $w_{ij}$ μιας λέξης $i$ στο έγγραφο $j$ ορίζεται ως:\n",
    "$$\n",
    "w_{ij} = tf_{ij}\\times\\log({N\\over df_i})\n",
    "$$\n",
    "όπου\n",
    "* $tf_{ij}$ είναι ο αριθμός εμφανίσεων της $i$ στο $j$, δηλαδή η τιμή BoW που έχουμε δει προηγουμένως\n",
    "* $N$ είναι ο αριθμός των εγγράφων στη συλλογή\n",
    "* $df_i$ είναι ο αριθμός των εγγράφων που περιέχουν τη λέξη $i$ σε ολόκληρη τη συλλογή\n",
    "\n",
    "Η τιμή TF-IDF $w_{ij}$ αυξάνεται αναλογικά με τον αριθμό των φορών που μια λέξη εμφανίζεται σε ένα έγγραφο και μειώνεται με βάση τον αριθμό των εγγράφων στη συλλογή που περιέχουν τη λέξη, κάτι που βοηθά να προσαρμοστεί το γεγονός ότι ορισμένες λέξεις εμφανίζονται πιο συχνά από άλλες. Για παράδειγμα, αν η λέξη εμφανίζεται σε *κάθε* έγγραφο της συλλογής, τότε $df_i=N$, και $w_{ij}=0$, και αυτοί οι όροι θα αγνοηθούν πλήρως.\n",
    "\n",
    "Μπορείτε εύκολα να δημιουργήσετε την αναπαράσταση TF-IDF ενός κειμένου χρησιμοποιώντας το Scikit Learn:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.43381609, 0.        , 0.43381609, 0.        , 0.65985664,\n",
       "        0.43381609, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,2))\n",
    "vectorizer.fit_transform(corpus)\n",
    "vectorizer.transform(['My dog likes hot dogs on a hot day.']).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Συμπέρασμα\n",
    "\n",
    "Παρόλο που οι αναπαραστάσεις TF-IDF παρέχουν βάρος συχνότητας σε διαφορετικές λέξεις, δεν μπορούν να αποδώσουν το νόημα ή τη σειρά. Όπως είπε ο διάσημος γλωσσολόγος J. R. Firth το 1935, «Το πλήρες νόημα μιας λέξης είναι πάντα συμφραζόμενο, και καμία μελέτη του νοήματος εκτός συμφραζομένων δεν μπορεί να ληφθεί σοβαρά υπόψη». Αργότερα στο μάθημα, θα μάθουμε πώς να αποτυπώνουμε συμφραζόμενες πληροφορίες από κείμενο χρησιμοποιώντας μοντελοποίηση γλώσσας.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Αποποίηση Ευθύνης**:  \nΑυτό το έγγραφο έχει μεταφραστεί χρησιμοποιώντας την υπηρεσία αυτόματης μετάφρασης [Co-op Translator](https://github.com/Azure/co-op-translator). Παρόλο που καταβάλλουμε κάθε προσπάθεια για ακρίβεια, παρακαλούμε να έχετε υπόψη ότι οι αυτόματες μεταφράσεις ενδέχεται να περιέχουν λάθη ή ανακρίβειες. Το πρωτότυπο έγγραφο στη μητρική του γλώσσα θα πρέπει να θεωρείται η αυθεντική πηγή. Για κρίσιμες πληροφορίες, συνιστάται επαγγελματική ανθρώπινη μετάφραση. Δεν φέρουμε ευθύνη για τυχόν παρεξηγήσεις ή εσφαλμένες ερμηνείες που προκύπτουν από τη χρήση αυτής της μετάφρασης.\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "16af2a8bbb083ea23e5e41c7f5787656b2ce26968575d8763f2c4b17f9cd711f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('py38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "coopTranslator": {
   "original_hash": "7b9040985e748e4e2d4c689892456ad7",
   "translation_date": "2025-08-29T11:00:28+00:00",
   "source_file": "lessons/5-NLP/13-TextRep/TextRepresentationPyTorch.ipynb",
   "language_code": "el"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}