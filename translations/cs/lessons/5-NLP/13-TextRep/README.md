<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "4522e22e150be0845e03aa41209a39d5",
  "translation_date": "2025-08-25T21:49:38+00:00",
  "source_file": "lessons/5-NLP/13-TextRep/README.md",
  "language_code": "cs"
}
-->
# Reprezentace textu jako tenzorÅ¯

## [KvÃ­z pÅ™ed pÅ™ednÃ¡Å¡kou](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/113)

## Klasifikace textu

V prvnÃ­ ÄÃ¡sti tÃ©to sekce se zamÄ›Å™Ã­me na Ãºkol **klasifikace textu**. PouÅ¾ijeme dataset [AG News](https://www.kaggle.com/amananandrai/ag-news-classification-dataset), kterÃ½ obsahuje zpravodajskÃ© ÄlÃ¡nky, napÅ™Ã­klad:

* Kategorie: VÄ›da/Technika  
* Titulek: Ky. spoleÄnost zÃ­skala grant na studium peptidÅ¯ (AP)  
* Text: AP - SpoleÄnost zaloÅ¾enÃ¡ vÃ½zkumnÃ­kem chemie na UniverzitÄ› v Louisville zÃ­skala grant na vÃ½voj...

NaÅ¡Ã­m cÃ­lem bude klasifikovat zpravodajskÃ½ ÄlÃ¡nek do jednÃ© z kategoriÃ­ na zÃ¡kladÄ› textu.

## Reprezentace textu

Pokud chceme Å™eÅ¡it Ãºkoly z oblasti zpracovÃ¡nÃ­ pÅ™irozenÃ©ho jazyka (NLP) pomocÃ­ neuronovÃ½ch sÃ­tÃ­, potÅ™ebujeme zpÅ¯sob, jak reprezentovat text jako tenzory. PoÄÃ­taÄe jiÅ¾ reprezentujÃ­ textovÃ© znaky jako ÄÃ­sla, kterÃ¡ mapujÃ­ na fonty na obrazovce, pomocÃ­ kÃ³dovÃ¡nÃ­, jako je ASCII nebo UTF-8.

<img alt="ObrÃ¡zek zobrazujÃ­cÃ­ diagram mapujÃ­cÃ­ znak na ASCII a binÃ¡rnÃ­ reprezentaci" src="images/ascii-character-map.png" width="50%"/>

> [Zdroj obrÃ¡zku](https://www.seobility.net/en/wiki/ASCII)

Jako lidÃ© rozumÃ­me, co kaÅ¾dÃ½ znak **znamenÃ¡**, a jak vÅ¡echny znaky dohromady tvoÅ™Ã­ slova ve vÄ›tÄ›. PoÄÃ­taÄe vÅ¡ak samy o sobÄ› takovÃ© porozumÄ›nÃ­ nemajÃ­ a neuronovÃ¡ sÃ­Å¥ se musÃ­ vÃ½znam nauÄit bÄ›hem trÃ©novÃ¡nÃ­.

Proto mÅ¯Å¾eme pouÅ¾Ã­t rÅ¯znÃ© pÅ™Ã­stupy pÅ™i reprezentaci textu:

* **Reprezentace na Ãºrovni znakÅ¯**, kdy reprezentujeme text tak, Å¾e kaÅ¾dÃ½ znak povaÅ¾ujeme za ÄÃ­slo. Pokud mÃ¡me *C* rÅ¯znÃ½ch znakÅ¯ v naÅ¡em textovÃ©m korpusu, slovo *Hello* by bylo reprezentovÃ¡no jako tenzor o rozmÄ›rech 5x*C*. KaÅ¾dÃ© pÃ­smeno by odpovÃ­dalo sloupci tenzoru v one-hot kÃ³dovÃ¡nÃ­.  
* **Reprezentace na Ãºrovni slov**, kdy vytvoÅ™Ã­me **slovnÃ­k** vÅ¡ech slov v naÅ¡em textu a potÃ© reprezentujeme slova pomocÃ­ one-hot kÃ³dovÃ¡nÃ­. Tento pÅ™Ã­stup je o nÄ›co lepÅ¡Ã­, protoÅ¾e jednotlivÃ¡ pÃ­smena sama o sobÄ› nemajÃ­ velkÃ½ vÃ½znam, a pouÅ¾itÃ­m vyÅ¡Å¡Ã­ch sÃ©mantickÃ½ch konceptÅ¯ - slov - Ãºkol pro neuronovou sÃ­Å¥ zjednoduÅ¡Ã­me. NicmÃ©nÄ›, vzhledem k velkÃ© velikosti slovnÃ­ku musÃ­me pracovat s vysoce dimenzionÃ¡lnÃ­mi Å™Ã­dkÃ½mi tenzory.

Bez ohledu na zvolenou reprezentaci musÃ­me nejprve pÅ™evÃ©st text na sekvenci **tokenÅ¯**, pÅ™iÄemÅ¾ token mÅ¯Å¾e bÃ½t znak, slovo nebo nÄ›kdy i ÄÃ¡st slova. PotÃ© token pÅ™evedeme na ÄÃ­slo, obvykle pomocÃ­ **slovnÃ­ku**, a toto ÄÃ­slo lze do neuronovÃ© sÃ­tÄ› pÅ™edat pomocÃ­ one-hot kÃ³dovÃ¡nÃ­.

## N-Gramy

V pÅ™irozenÃ©m jazyce lze pÅ™esnÃ½ vÃ½znam slov urÄit pouze v kontextu. NapÅ™Ã­klad vÃ½znamy *neuronovÃ¡ sÃ­Å¥* a *rybÃ¡Å™skÃ¡ sÃ­Å¥* jsou zcela odliÅ¡nÃ©. JednÃ­m ze zpÅ¯sobÅ¯, jak toto zohlednit, je vytvoÅ™it model zaloÅ¾enÃ½ na dvojicÃ­ch slov a povaÅ¾ovat dvojice slov za samostatnÃ© tokeny slovnÃ­ku. TÃ­mto zpÅ¯sobem bude vÄ›ta *RÃ¡d chodÃ­m na ryby* reprezentovÃ¡na nÃ¡sledujÃ­cÃ­ sekvencÃ­ tokenÅ¯: *RÃ¡d chodÃ­m*, *chodÃ­m na*, *na ryby*. ProblÃ©mem tohoto pÅ™Ã­stupu je, Å¾e velikost slovnÃ­ku vÃ½raznÄ› narÅ¯stÃ¡ a kombinace jako *na ryby* a *na nÃ¡kupy* jsou reprezentovÃ¡ny rÅ¯znÃ½mi tokeny, kterÃ© nesdÃ­lejÃ­ Å¾Ã¡dnou sÃ©mantickou podobnost, pÅ™estoÅ¾e obsahujÃ­ stejnÃ© sloveso.

V nÄ›kterÃ½ch pÅ™Ã­padech mÅ¯Å¾eme zvÃ¡Å¾it pouÅ¾itÃ­ tri-gramÅ¯ â€“ kombinacÃ­ tÅ™Ã­ slov. Tento pÅ™Ã­stup se proto Äasto nazÃ½vÃ¡ **n-gramy**. DÃ¡vÃ¡ takÃ© smysl pouÅ¾Ã­vat n-gramy s reprezentacÃ­ na Ãºrovni znakÅ¯, kdy n-gramy pÅ™ibliÅ¾nÄ› odpovÃ­dajÃ­ rÅ¯znÃ½m slabikÃ¡m.

## Bag-of-Words a TF/IDF

PÅ™i Å™eÅ¡enÃ­ ÃºkolÅ¯, jako je klasifikace textu, potÅ™ebujeme bÃ½t schopni reprezentovat text jako jeden vektor pevnÃ© velikosti, kterÃ½ pouÅ¾ijeme jako vstup pro koneÄnÃ½ hustÃ½ klasifikÃ¡tor. JednÃ­m z nejjednoduÅ¡Å¡Ã­ch zpÅ¯sobÅ¯, jak to udÄ›lat, je kombinovat vÅ¡echny jednotlivÃ© reprezentace slov, napÅ™Ã­klad jejich seÄtenÃ­m. Pokud seÄteme one-hot kÃ³dovÃ¡nÃ­ kaÅ¾dÃ©ho slova, zÃ­skÃ¡me vektor frekvencÃ­, kterÃ½ ukazuje, kolikrÃ¡t se kaÅ¾dÃ© slovo v textu objevuje. TakovÃ¡ reprezentace textu se nazÃ½vÃ¡ **bag-of-words** (BoW).

<img src="images/bow.png" width="90%"/>

> ObrÃ¡zek od autora

BoW v podstatÄ› reprezentuje, kterÃ¡ slova se v textu objevujÃ­ a v jakÃ©m mnoÅ¾stvÃ­, coÅ¾ mÅ¯Å¾e bÃ½t dobrÃ½m ukazatelem toho, o Äem text je. NapÅ™Ã­klad zpravodajskÃ½ ÄlÃ¡nek o politice pravdÄ›podobnÄ› obsahuje slova jako *prezident* a *zemÄ›*, zatÃ­mco vÄ›deckÃ¡ publikace by mohla obsahovat slova jako *urychlovaÄ*, *objeveno* atd. Frekvence slov tedy v mnoha pÅ™Ã­padech mohou bÃ½t dobrÃ½m indikÃ¡torem obsahu textu.

ProblÃ©mem BoW je, Å¾e urÄitÃ¡ bÄ›Å¾nÃ¡ slova, jako *a*, *je* atd., se objevujÃ­ ve vÄ›tÅ¡inÄ› textÅ¯ a majÃ­ nejvyÅ¡Å¡Ã­ frekvence, ÄÃ­mÅ¾ zastÃ­rajÃ­ slova, kterÃ¡ jsou skuteÄnÄ› dÅ¯leÅ¾itÃ¡. MÅ¯Å¾eme snÃ­Å¾it dÅ¯leÅ¾itost tÄ›chto slov tÃ­m, Å¾e vezmeme v Ãºvahu frekvenci, s jakou se slova objevujÃ­ v celÃ© kolekci dokumentÅ¯. To je hlavnÃ­ myÅ¡lenka pÅ™Ã­stupu TF/IDF, kterÃ½ je podrobnÄ›ji popsÃ¡n v pÅ™iloÅ¾enÃ½ch poznÃ¡mkovÃ½ch blocÃ­ch k tÃ©to lekci.

NicmÃ©nÄ› Å¾Ã¡dnÃ½ z tÄ›chto pÅ™Ã­stupÅ¯ nedokÃ¡Å¾e plnÄ› zohlednit **sÃ©mantiku** textu. K tomu potÅ™ebujeme vÃ½konnÄ›jÅ¡Ã­ modely neuronovÃ½ch sÃ­tÃ­, o kterÃ½ch budeme diskutovat pozdÄ›ji v tÃ©to sekci.

## âœï¸ CviÄenÃ­: Reprezentace textu

PokraÄujte ve svÃ©m uÄenÃ­ v nÃ¡sledujÃ­cÃ­ch poznÃ¡mkovÃ½ch blocÃ­ch:

* [Reprezentace textu s PyTorch](../../../../../lessons/5-NLP/13-TextRep/TextRepresentationPyTorch.ipynb)  
* [Reprezentace textu s TensorFlow](../../../../../lessons/5-NLP/13-TextRep/TextRepresentationTF.ipynb)

## ZÃ¡vÄ›r

Doposud jsme studovali techniky, kterÃ© mohou pÅ™idat vÃ¡hu frekvencÃ­m rÅ¯znÃ½ch slov. Tyto techniky vÅ¡ak nejsou schopny reprezentovat vÃ½znam nebo poÅ™adÃ­. Jak slavnÃ½ lingvista J. R. Firth Å™ekl v roce 1935: "ÃšplnÃ½ vÃ½znam slova je vÅ¾dy kontextovÃ½ a Å¾Ã¡dnÃ¡ studie vÃ½znamu oddÄ›lenÃ¡ od kontextu nemÅ¯Å¾e bÃ½t brÃ¡na vÃ¡Å¾nÄ›." PozdÄ›ji v kurzu se nauÄÃ­me, jak zachytit kontextovÃ© informace z textu pomocÃ­ jazykovÃ©ho modelovÃ¡nÃ­.

## ğŸš€ VÃ½zva

VyzkouÅ¡ejte dalÅ¡Ã­ cviÄenÃ­ s pouÅ¾itÃ­m bag-of-words a rÅ¯znÃ½ch datovÃ½ch modelÅ¯. MÅ¯Å¾ete se inspirovat touto [soutÄ›Å¾Ã­ na Kaggle](https://www.kaggle.com/competitions/word2vec-nlp-tutorial/overview/part-1-for-beginners-bag-of-words).

## [KvÃ­z po pÅ™ednÃ¡Å¡ce](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/213)

## PÅ™ehled a samostudium

ProcviÄte si svÃ© dovednosti s technikami vklÃ¡dÃ¡nÃ­ textu a bag-of-words na [Microsoft Learn](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-pytorch/?WT.mc_id=academic-77998-cacaste)

## [Ãškol: PoznÃ¡mkovÃ© bloky](assignment.md)

**ProhlÃ¡Å¡enÃ­:**  
Tento dokument byl pÅ™eloÅ¾en pomocÃ­ sluÅ¾by pro automatickÃ½ pÅ™eklad [Co-op Translator](https://github.com/Azure/co-op-translator). AÄkoli se snaÅ¾Ã­me o pÅ™esnost, mÄ›jte prosÃ­m na pamÄ›ti, Å¾e automatickÃ© pÅ™eklady mohou obsahovat chyby nebo nepÅ™esnosti. PÅ¯vodnÃ­ dokument v jeho pÅ¯vodnÃ­m jazyce by mÄ›l bÃ½t povaÅ¾ovÃ¡n za autoritativnÃ­ zdroj. Pro dÅ¯leÅ¾itÃ© informace doporuÄujeme profesionÃ¡lnÃ­ lidskÃ½ pÅ™eklad. NeodpovÃ­dÃ¡me za Å¾Ã¡dnÃ¡ nedorozumÄ›nÃ­ nebo nesprÃ¡vnÃ© interpretace vyplÃ½vajÃ­cÃ­ z pouÅ¾itÃ­ tohoto pÅ™ekladu.