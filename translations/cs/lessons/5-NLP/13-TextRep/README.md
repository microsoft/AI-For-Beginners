# Reprezentace textu jako tenzory

## [KvÃ­z pÅ™ed pÅ™ednÃ¡Å¡kou](https://ff-quizzes.netlify.app/en/ai/quiz/25)

## Klasifikace textu

V prvnÃ­ ÄÃ¡sti tÃ©to sekce se zamÄ›Å™Ã­me na Ãºkol **klasifikace textu**. PouÅ¾ijeme dataset [AG News](https://www.kaggle.com/amananandrai/ag-news-classification-dataset), kterÃ½ obsahuje zpravodajskÃ© ÄlÃ¡nky, napÅ™Ã­klad:

* Kategorie: VÄ›da/Technologie  
* Titulek: Ky. SpoleÄnost zÃ­skala grant na studium peptidÅ¯ (AP)  
* Text: AP - SpoleÄnost zaloÅ¾enÃ¡ chemickÃ½m vÃ½zkumnÃ­kem z University of Louisville zÃ­skala grant na vÃ½voj...

NaÅ¡Ã­m cÃ­lem bude klasifikovat zpravodajskÃ½ ÄlÃ¡nek do jednÃ© z kategoriÃ­ na zÃ¡kladÄ› textu.

## Reprezentace textu

Pokud chceme Å™eÅ¡it Ãºkoly z oblasti zpracovÃ¡nÃ­ pÅ™irozenÃ©ho jazyka (NLP) pomocÃ­ neuronovÃ½ch sÃ­tÃ­, potÅ™ebujeme zpÅ¯sob, jak reprezentovat text jako tenzory. PoÄÃ­taÄe jiÅ¾ reprezentujÃ­ textovÃ© znaky jako ÄÃ­sla, kterÃ¡ mapujÃ­ na fonty na obrazovce pomocÃ­ kÃ³dovÃ¡nÃ­, jako je ASCII nebo UTF-8.

<img alt="ObrÃ¡zek zobrazujÃ­cÃ­ diagram mapujÃ­cÃ­ znak na ASCII a binÃ¡rnÃ­ reprezentaci" src="../../../../../translated_images/cs/ascii-character-map.18ed6aa7f3b0a7ff.webp" width="50%"/>

> [Zdroj obrÃ¡zku](https://www.seobility.net/en/wiki/ASCII)

Jako lidÃ© rozumÃ­me tomu, co kaÅ¾dÃ½ znak **reprezentuje**, a jak vÅ¡echny znaky dohromady tvoÅ™Ã­ slova ve vÄ›tÄ›. PoÄÃ­taÄe vÅ¡ak samy o sobÄ› takovÃ© porozumÄ›nÃ­ nemajÃ­, a neuronovÃ¡ sÃ­Å¥ se musÃ­ vÃ½znam nauÄit bÄ›hem trÃ©ninku.

Proto mÅ¯Å¾eme pouÅ¾Ã­t rÅ¯znÃ© pÅ™Ã­stupy pÅ™i reprezentaci textu:

* **Reprezentace na Ãºrovni znakÅ¯**, kdy text reprezentujeme tak, Å¾e kaÅ¾dÃ½ znak povaÅ¾ujeme za ÄÃ­slo. Pokud mÃ¡me *C* rÅ¯znÃ½ch znakÅ¯ v naÅ¡em textovÃ©m korpusu, slovo *Hello* by bylo reprezentovÃ¡no jako tenzor 5x*C*. KaÅ¾dÃ© pÃ­smeno by odpovÃ­dalo sloupci tenzoru v jednorozmÄ›rnÃ©m kÃ³dovÃ¡nÃ­ (one-hot encoding).  
* **Reprezentace na Ãºrovni slov**, kdy vytvoÅ™Ã­me **slovnÃ­k** vÅ¡ech slov v naÅ¡em textu a potÃ© slova reprezentujeme pomocÃ­ jednorozmÄ›rnÃ©ho kÃ³dovÃ¡nÃ­. Tento pÅ™Ã­stup je o nÄ›co lepÅ¡Ã­, protoÅ¾e jednotlivÃ© znaky samy o sobÄ› nemajÃ­ velkÃ½ vÃ½znam, a pouÅ¾itÃ­m vyÅ¡Å¡Ã­ch sÃ©mantickÃ½ch konceptÅ¯ - slov - Ãºkol pro neuronovou sÃ­Å¥ zjednoduÅ¡ujeme. NicmÃ©nÄ› vzhledem k velkÃ© velikosti slovnÃ­ku musÃ­me pracovat s vysoce dimenzionÃ¡lnÃ­mi Å™Ã­dkÃ½mi tenzory.

Bez ohledu na zpÅ¯sob reprezentace musÃ­me nejprve pÅ™evÃ©st text na sekvenci **tokenÅ¯**, pÅ™iÄemÅ¾ jeden token mÅ¯Å¾e bÃ½t znak, slovo nebo nÄ›kdy i ÄÃ¡st slova. PotÃ© token pÅ™evedeme na ÄÃ­slo, obvykle pomocÃ­ **slovnÃ­ku**, a toto ÄÃ­slo mÅ¯Å¾e bÃ½t pÅ™edÃ¡no neuronovÃ© sÃ­ti pomocÃ­ jednorozmÄ›rnÃ©ho kÃ³dovÃ¡nÃ­.

## N-Gramy

V pÅ™irozenÃ©m jazyce lze pÅ™esnÃ½ vÃ½znam slov urÄit pouze v kontextu. NapÅ™Ã­klad vÃ½znamy *neuronovÃ¡ sÃ­Å¥* a *rybÃ¡Å™skÃ¡ sÃ­Å¥* jsou zcela odliÅ¡nÃ©. JednÃ­m ze zpÅ¯sobÅ¯, jak toto zohlednit, je vytvoÅ™it model na zÃ¡kladÄ› dvojic slov a povaÅ¾ovat dvojice slov za samostatnÃ© tokeny slovnÃ­ku. TÃ­mto zpÅ¯sobem bude vÄ›ta *RÃ¡d chodÃ­m na ryby* reprezentovÃ¡na nÃ¡sledujÃ­cÃ­ sekvencÃ­ tokenÅ¯: *RÃ¡d chodÃ­m*, *chodÃ­m na*, *na ryby*. ProblÃ©mem tohoto pÅ™Ã­stupu je, Å¾e velikost slovnÃ­ku se vÃ½raznÄ› zvÄ›tÅ¡uje a kombinace jako *na ryby* a *na nÃ¡kupy* jsou reprezentovÃ¡ny rÅ¯znÃ½mi tokeny, kterÃ© nesdÃ­lejÃ­ Å¾Ã¡dnou sÃ©mantickou podobnost, pÅ™estoÅ¾e obsahujÃ­ stejnÃ© sloveso.

V nÄ›kterÃ½ch pÅ™Ã­padech mÅ¯Å¾eme zvÃ¡Å¾it pouÅ¾itÃ­ tri-gramÅ¯ -- kombinacÃ­ tÅ™Ã­ slov. Tento pÅ™Ã­stup se proto Äasto nazÃ½vÃ¡ **n-gramy**. TakÃ© mÃ¡ smysl pouÅ¾Ã­vat n-gramy s reprezentacÃ­ na Ãºrovni znakÅ¯, kdy n-gramy pÅ™ibliÅ¾nÄ› odpovÃ­dajÃ­ rÅ¯znÃ½m slabikÃ¡m.

## Bag-of-Words a TF/IDF

PÅ™i Å™eÅ¡enÃ­ ÃºkolÅ¯, jako je klasifikace textu, potÅ™ebujeme bÃ½t schopni reprezentovat text jednÃ­m vektorem pevnÃ© velikosti, kterÃ½ pouÅ¾ijeme jako vstup pro koneÄnÃ½ hustÃ½ klasifikÃ¡tor. JednÃ­m z nejjednoduÅ¡Å¡Ã­ch zpÅ¯sobÅ¯, jak toho dosÃ¡hnout, je kombinovat vÅ¡echny jednotlivÃ© reprezentace slov, napÅ™Ã­klad jejich seÄtenÃ­m. Pokud seÄteme jednorozmÄ›rnÃ© kÃ³dovÃ¡nÃ­ kaÅ¾dÃ©ho slova, zÃ­skÃ¡me vektor frekvencÃ­, kterÃ½ ukazuje, kolikrÃ¡t se kaÅ¾dÃ© slovo v textu objevuje. TakovÃ¡ reprezentace textu se nazÃ½vÃ¡ **bag of words** (BoW).

<img src="../../../../../translated_images/cs/bow.3811869cff59368d.webp" width="90%"/>

> ObrÃ¡zek od autora

BoW v podstatÄ› reprezentuje, kterÃ¡ slova se v textu objevujÃ­ a v jakÃ©m mnoÅ¾stvÃ­, coÅ¾ mÅ¯Å¾e bÃ½t dobrÃ½m indikÃ¡torem toho, o Äem text je. NapÅ™Ã­klad zpravodajskÃ½ ÄlÃ¡nek o politice pravdÄ›podobnÄ› obsahuje slova jako *prezident* a *zemÄ›*, zatÃ­mco vÄ›deckÃ¡ publikace by mohla obsahovat slova jako *kolider*, *objeveno* atd. Frekvence slov tak mohou bÃ½t v mnoha pÅ™Ã­padech dobrÃ½m indikÃ¡torem obsahu textu.

ProblÃ©mem BoW je, Å¾e urÄitÃ¡ bÄ›Å¾nÃ¡ slova, jako *a*, *je* atd., se objevujÃ­ ve vÄ›tÅ¡inÄ› textÅ¯ a majÃ­ nejvyÅ¡Å¡Ã­ frekvence, coÅ¾ mÅ¯Å¾e zakrÃ½t slova, kterÃ¡ jsou skuteÄnÄ› dÅ¯leÅ¾itÃ¡. MÅ¯Å¾eme snÃ­Å¾it dÅ¯leÅ¾itost tÄ›chto slov tÃ­m, Å¾e vezmeme v Ãºvahu frekvenci, s jakou se slova objevujÃ­ v celÃ© kolekci dokumentÅ¯. To je hlavnÃ­ myÅ¡lenka pÅ™Ã­stupu TF/IDF, kterÃ½ je podrobnÄ›ji popsÃ¡n v pÅ™iloÅ¾enÃ½ch noteboocÃ­ch k tÃ©to lekci.

NicmÃ©nÄ› Å¾Ã¡dnÃ½ z tÄ›chto pÅ™Ã­stupÅ¯ nemÅ¯Å¾e plnÄ› zohlednit **sÃ©mantiku** textu. K tomu potÅ™ebujeme vÃ½konnÄ›jÅ¡Ã­ modely neuronovÃ½ch sÃ­tÃ­, kterÃ© probereme pozdÄ›ji v tÃ©to sekci.

## âœï¸ CviÄenÃ­: Reprezentace textu

PokraÄujte ve svÃ©m uÄenÃ­ v nÃ¡sledujÃ­cÃ­ch noteboocÃ­ch:

* [Reprezentace textu s PyTorch](TextRepresentationPyTorch.ipynb)  
* [Reprezentace textu s TensorFlow](TextRepresentationTF.ipynb)  

## ZÃ¡vÄ›r

Doposud jsme studovali techniky, kterÃ© mohou pÅ™idat vÃ¡hu frekvencÃ­m rÅ¯znÃ½ch slov. Ty vÅ¡ak nejsou schopny reprezentovat vÃ½znam nebo poÅ™adÃ­. Jak slavnÃ½ lingvista J. R. Firth Å™ekl v roce 1935: "ÃšplnÃ½ vÃ½znam slova je vÅ¾dy kontextovÃ½ a Å¾Ã¡dnÃ¡ studie vÃ½znamu bez kontextu nemÅ¯Å¾e bÃ½t brÃ¡na vÃ¡Å¾nÄ›." PozdÄ›ji v kurzu se nauÄÃ­me, jak zachytit kontextovÃ© informace z textu pomocÃ­ jazykovÃ©ho modelovÃ¡nÃ­.

## ğŸš€ VÃ½zva

VyzkouÅ¡ejte dalÅ¡Ã­ cviÄenÃ­ s bag-of-words a rÅ¯znÃ½mi datovÃ½mi modely. MÅ¯Å¾ete se inspirovat touto [soutÄ›Å¾Ã­ na Kaggle](https://www.kaggle.com/competitions/word2vec-nlp-tutorial/overview/part-1-for-beginners-bag-of-words)

## [KvÃ­z po pÅ™ednÃ¡Å¡ce](https://ff-quizzes.netlify.app/en/ai/quiz/26)

## PÅ™ehled & Samostudium

ProcviÄte si svÃ© dovednosti s textovÃ½mi embeddingy a technikami bag-of-words na [Microsoft Learn](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-pytorch/?WT.mc_id=academic-77998-cacaste)

## [Ãškol: Notebooks](assignment.md)

---

