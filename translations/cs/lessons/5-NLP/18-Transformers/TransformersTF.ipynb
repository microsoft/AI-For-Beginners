{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mechanismy pozornosti a transformery\n",
    "\n",
    "Jednou z hlavních nevýhod rekurentních sítí je, že všechna slova v sekvenci mají stejný vliv na výsledek. To vede k suboptimálnímu výkonu u standardních modelů LSTM encoder-decoder pro úlohy typu sekvence na sekvenci, jako je rozpoznávání pojmenovaných entit nebo strojový překlad. Ve skutečnosti mají konkrétní slova ve vstupní sekvenci často větší vliv na výstupy než jiná.\n",
    "\n",
    "Zvažme model sekvence na sekvenci, například strojový překlad. Tento model je implementován pomocí dvou rekurentních sítí, kde jedna síť (**encoder**) zkomprimuje vstupní sekvenci do skrytého stavu a druhá síť (**decoder**) tento skrytý stav rozvine do přeloženého výsledku. Problém tohoto přístupu spočívá v tom, že konečný stav sítě má potíže s uchováním informací z počátku věty, což vede k nízké kvalitě modelu u dlouhých vět.\n",
    "\n",
    "**Mechanismy pozornosti** poskytují způsob, jak vážit kontextuální vliv jednotlivých vstupních vektorů na každou výstupní predikci RNN. Toho je dosaženo vytvořením zkratek mezi mezistavy vstupní RNN a výstupní RNN. Tímto způsobem, při generování výstupního symbolu $y_t$, bereme v úvahu všechny skryté stavy vstupu $h_i$ s různými váhovými koeficienty $\\alpha_{t,i}$. \n",
    "\n",
    "![Obrázek zobrazující model encoder/decoder s aditivní vrstvou pozornosti](../../../../../translated_images/cs/encoder-decoder-attention.7a726296894fb567.webp)\n",
    "*Model encoder-decoder s mechanismem aditivní pozornosti podle [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf), citováno z [tohoto blogového příspěvku](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)*\n",
    "\n",
    "Matice pozornosti $\\{\\alpha_{i,j}\\}$ reprezentuje míru, do jaké určitá vstupní slova ovlivňují generování konkrétního slova ve výstupní sekvenci. Níže je příklad takové matice:\n",
    "\n",
    "![Obrázek zobrazující ukázkové zarovnání nalezené modelem RNNsearch-50, převzato z Bahdanau - arviz.org](../../../../../translated_images/cs/bahdanau-fig3.09ba2d37f202a6af.webp)\n",
    "\n",
    "*Obrázek převzatý z [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) (Obr. 3)*\n",
    "\n",
    "Mechanismy pozornosti jsou zodpovědné za velkou část současného nebo téměř současného stavu techniky v oblasti zpracování přirozeného jazyka. Přidání pozornosti však výrazně zvyšuje počet parametrů modelu, což vedlo k problémům se škálováním u RNN. Klíčovým omezením škálování RNN je, že rekurentní povaha těchto modelů ztěžuje dávkování a paralelizaci tréninku. V RNN musí být každý prvek sekvence zpracován v sekvenčním pořadí, což znamená, že paralelizace není snadná.\n",
    "\n",
    "Přijetí mechanismů pozornosti v kombinaci s tímto omezením vedlo k vytvoření dnes známých modelů Transformer, které představují současný stav techniky, od BERT po OpenGPT3.\n",
    "\n",
    "## Modely Transformer\n",
    "\n",
    "Namísto předávání kontextu každé předchozí predikce do dalšího kroku hodnocení používají **modely Transformer** **poziční kódování** a **pozornost**, aby zachytily kontext daného vstupu v rámci poskytnutého textového okna. Obrázek níže ukazuje, jak poziční kódování s pozorností dokáže zachytit kontext v rámci daného okna.\n",
    "\n",
    "![Animovaný GIF zobrazující, jak jsou hodnocení prováděna v modelech Transformer.](../../../../../lessons/5-NLP/18-Transformers/images/transformer-animated-explanation.gif) \n",
    "\n",
    "Protože každá vstupní pozice je mapována nezávisle na každou výstupní pozici, transformery umožňují lepší paralelizaci než RNN, což umožňuje vytvářet mnohem větší a expresivnější jazykové modely. Každá hlava pozornosti může být použita k učení různých vztahů mezi slovy, což zlepšuje výsledky úloh zpracování přirozeného jazyka.\n",
    "\n",
    "## Vytvoření jednoduchého modelu Transformer\n",
    "\n",
    "Keras neobsahuje vestavěnou vrstvu Transformer, ale můžeme si ji vytvořit sami. Stejně jako dříve se zaměříme na klasifikaci textu z datasetu AG News, ale stojí za zmínku, že modely Transformer dosahují nejlepších výsledků u složitějších úloh zpracování přirozeného jazyka.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "\n",
    "ds_train, ds_test = tfds.load('ag_news_subset').values()\n",
    "\n",
    "def extract_text(x):\n",
    "    return x['title']+' '+x['description']\n",
    "\n",
    "def tupelize(x):\n",
    "    return (extract_text(x),x['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nové vrstvy v Keras by měly dědit třídu `Layer` a implementovat metodu `call`. Začněme s vrstvou **Positional Embedding**. Použijeme [nějaký kód z oficiální dokumentace Keras](https://keras.io/examples/nlp/text_classification_with_transformer/). Budeme předpokládat, že všechny vstupní sekvence doplníme na délku `maxlen`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(keras.layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = keras.layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "        self.maxlen = maxlen\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = self.maxlen\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x+positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tato vrstva se skládá ze dvou vrstev `Embedding`: pro vkládání tokenů (způsobem, který jsme již diskutovali) a pozic tokenů. Pozice tokenů jsou vytvořeny jako posloupnost přirozených čísel od 0 do `maxlen` pomocí `tf.range`, a poté jsou předány přes vrstvu embedding. Dva výsledné vektorové embeddingy jsou následně sečteny, čímž vznikne pozičně vložená reprezentace vstupu o tvaru `maxlen`$\\times$`embed_dim`.\n",
    "\n",
    "Nyní implementujeme blok transformátoru. Ten bude přijímat výstup z dříve definované vrstvy embedding:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim, name='attn')\n",
    "        self.ffn = keras.Sequential(\n",
    "            [keras.layers.Dense(ff_dim, activation=\"relu\"), keras.layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = keras.layers.Dropout(rate)\n",
    "        self.dropout2 = keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nyní jsme připraveni definovat kompletní model transformátoru:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "text_vectorization (TextVect (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "token_and_position_embedding (None, 256, 32)           648192    \n",
      "_________________________________________________________________\n",
      "transformer_block (Transform (None, 256, 32)           10656     \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 20)                660       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 84        \n",
      "=================================================================\n",
      "Total params: 659,592\n",
      "Trainable params: 659,592\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 32  # Embedding size for each token\n",
    "num_heads = 2  # Number of attention heads\n",
    "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
    "maxlen = 256\n",
    "vocab_size = 20000\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.experimental.preprocessing.TextVectorization(max_tokens=vocab_size,output_sequence_length=maxlen, input_shape=(1,)),\n",
    "    TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim),\n",
    "    TransformerBlock(embed_dim, num_heads, ff_dim),\n",
    "    keras.layers.GlobalAveragePooling1D(),\n",
    "    keras.layers.Dropout(0.1),\n",
    "    keras.layers.Dense(20, activation=\"relu\"),\n",
    "    keras.layers.Dropout(0.1),\n",
    "    keras.layers.Dense(4, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokenizer\n",
      "938/938 [==============================] - 45s 39ms/step - loss: 0.4978 - acc: 0.8068 - val_loss: 0.2808 - val_acc: 0.9124\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9c2427a0d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Training tokenizer')\n",
    "model.layers[0].adapt(ds_train.map(extract_text))\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'], optimizer='adam')\n",
    "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT Transformer Modely\n",
    "\n",
    "**BERT** (Bidirectional Encoder Representations from Transformers) je velmi rozsáhlá vícevrstvá transformátorová síť s 12 vrstvami pro *BERT-base* a 24 vrstvami pro *BERT-large*. Model je nejprve předtrénován na velkém korpusu textových dat (WikiPedia + knihy) pomocí nesupervizovaného učení (předpovídání maskovaných slov ve větě). Během předtrénování model získává významnou úroveň porozumění jazyku, kterou lze následně využít s jinými datovými sadami pomocí jemného ladění. Tento proces se nazývá **transferové učení**.\n",
    "\n",
    "![obrázek z http://jalammar.github.io/illustrated-bert/](../../../../../translated_images/cs/jalammarBERT-language-modeling-masked-lm.34f113ea5fec4362.webp)\n",
    "\n",
    "Existuje mnoho variant architektur Transformerů, včetně BERT, DistilBERT, BigBird, OpenGPT3 a dalších, které lze jemně doladit.\n",
    "\n",
    "Podívejme se, jak můžeme použít předtrénovaný model BERT k řešení našeho tradičního problému klasifikace sekvencí. Využijeme myšlenku a část kódu z [oficiální dokumentace](https://www.tensorflow.org/text/tutorials/classify_text_with_bert).\n",
    "\n",
    "Pro načtení předtrénovaných modelů použijeme **Tensorflow hub**. Nejprve načtěme vektorizační nástroj specifický pro BERT:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_41180/4216669875.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow_text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow_hub\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mhub\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhub\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mKerasLayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_text'"
     ]
    }
   ],
   "source": [
    "import tensorflow_text \n",
    "import tensorflow_hub as hub\n",
    "vectorizer = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_type_ids': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       " array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "       dtype=int32)>,\n",
       " 'input_word_ids': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       " array([[  101,  1045,  2293, 19081,   102,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]], dtype=int32)>,\n",
       " 'input_mask': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       " array([[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "       dtype=int32)>}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer(['I love transformers'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Je důležité použít stejný vektorizér, jaký byl použit při trénování původní sítě. BERT vektorizér vrací tři komponenty:\n",
    "* `input_word_ids`, což je sekvence čísel tokenů pro vstupní větu\n",
    "* `input_mask`, která ukazuje, která část sekvence obsahuje skutečný vstup a která je vyplněna paddingem. Je podobná masce vytvořené vrstvou `Masking`\n",
    "* `input_type_ids` se používá pro úlohy jazykového modelování a umožňuje specifikovat dvě vstupní věty v jedné sekvenci.\n",
    "\n",
    "Poté můžeme vytvořit instanci extraktoru funkcí BERT:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = hub.KerasLayer('https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pooled_output -> (1, 128)\n",
      "encoder_outputs -> 4\n",
      "sequence_output -> (1, 128, 128)\n",
      "default -> (1, 128)\n"
     ]
    }
   ],
   "source": [
    "z = bert(vectorizer(['I love transformers']))\n",
    "for i,x in z.items():\n",
    "    print(f\"{i} -> { len(x) if isinstance(x, list) else x.shape }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Takže BERT vrstva vrací několik užitečných výsledků:\n",
    "* `pooled_output` je výsledkem zprůměrování všech tokenů v sekvenci. Můžete si jej představit jako inteligentní sémantické vnoření celé sítě. Je ekvivalentní výstupu vrstvy `GlobalAveragePooling1D` v našem předchozím modelu.\n",
    "* `sequence_output` je výstup poslední transformátorové vrstvy (odpovídá výstupu `TransformerBlock` v našem modelu výše).\n",
    "* `encoder_outputs` jsou výstupy všech transformátorových vrstev. Protože jsme nahráli 4-vrstvý BERT model (jak pravděpodobně odhadnete z názvu, který obsahuje `4_H`), má 4 tenzory. Poslední z nich je stejný jako `sequence_output`.\n",
    "\n",
    "Nyní definujeme end-to-end klasifikační model. Použijeme *funkční definici modelu*, kdy definujeme vstup modelu a poté poskytneme sérii výrazů pro výpočet jeho výstupu. Také nastavíme váhy BERT modelu jako netrénovatelné a budeme trénovat pouze finální klasifikátor:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer (KerasLayer)        {'input_type_ids': ( 0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer_1 (KerasLayer)      {'pooled_output': (N 4782465     keras_layer[0][0]                \n",
      "                                                                 keras_layer[0][1]                \n",
      "                                                                 keras_layer[0][2]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 128)          0           keras_layer_1[0][5]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 4)            516         dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 4,782,981\n",
      "Trainable params: 516\n",
      "Non-trainable params: 4,782,465\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inp = keras.Input(shape=(),dtype=tf.string)\n",
    "x = vectorizer(inp)\n",
    "x = bert(x)\n",
    "x = keras.layers.Dropout(0.1)(x['pooled_output'])\n",
    "out = keras.layers.Dense(4,activation='softmax')(x)\n",
    "model = keras.models.Model(inp,out)\n",
    "bert.trainable = False\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 528s 559ms/step - loss: 0.8056 - acc: 0.6983 - val_loss: 0.5953 - val_acc: 0.7888\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9bb1e36d00>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'], optimizer='adam')\n",
    "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Přestože je zde jen málo trénovatelných parametrů, proces je poměrně pomalý, protože extraktor příznaků BERT je výpočetně náročný. Vypadá to, že jsme nebyli schopni dosáhnout rozumné přesnosti, buď kvůli nedostatku tréninku, nebo kvůli nedostatku parametrů modelu.\n",
    "\n",
    "Zkusme odemknout váhy BERT a trénovat je také. To vyžaduje velmi malou rychlost učení a také opatrnější tréninkovou strategii s **warmup**, za použití optimalizátoru **AdamW**. K vytvoření optimalizátoru použijeme balíček `tf-models-official`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer (KerasLayer)        {'input_type_ids': ( 0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer_1 (KerasLayer)      {'pooled_output': (N 4782465     keras_layer[0][0]                \n",
      "                                                                 keras_layer[0][1]                \n",
      "                                                                 keras_layer[0][2]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 128)          0           keras_layer_1[0][5]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 4)            516         dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 4,782,981\n",
      "Trainable params: 4,782,980\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n",
      "938/938 [==============================] - 629s 664ms/step - loss: 0.6344 - acc: 0.7658 - val_loss: 0.4876 - val_acc: 0.8247\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9bb0bd0070>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from official.nlp import optimization \n",
    "bert.trainable=True\n",
    "model.summary()\n",
    "epochs = 3\n",
    "opt = optimization.create_optimizer(\n",
    "    init_lr=3e-5,\n",
    "    num_train_steps=epochs*len(ds_train),\n",
    "    num_warmup_steps=0.1*epochs*len(ds_train),\n",
    "    optimizer_type='adamw')\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'], optimizer=opt)\n",
    "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jak vidíte, trénink probíhá poměrně pomalu - ale možná budete chtít experimentovat a trénovat model několik epoch (5-10) a zjistit, zda dosáhnete nejlepšího výsledku ve srovnání s přístupy, které jsme použili dříve.\n",
    "\n",
    "## Knihovna Huggingface Transformers\n",
    "\n",
    "Dalším velmi běžným (a o něco jednodušším) způsobem použití modelů Transformer je [balíček HuggingFace](https://github.com/huggingface/), který poskytuje jednoduché stavební bloky pro různé úlohy NLP. Je dostupný jak pro Tensorflow, tak pro PyTorch, další velmi populární framework pro neuronové sítě.\n",
    "\n",
    "> **Note**: Pokud vás nezajímá, jak knihovna Transformers funguje - můžete přeskočit na konec tohoto notebooku, protože neuvidíte nic zásadně odlišného od toho, co jsme udělali výše. Budeme opakovat stejné kroky při trénování modelu BERT pomocí jiné knihovny a podstatně většího modelu. Proces tedy zahrnuje poměrně dlouhý trénink, takže možná budete chtít jen projít kód. \n",
    "\n",
    "Podívejme se, jak lze náš problém vyřešit pomocí [Huggingface Transformers](http://huggingface.co).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "První věc, kterou musíme udělat, je vybrat model, který budeme používat. Kromě několika vestavěných modelů obsahuje Huggingface [online repozitář modelů](https://huggingface.co/models), kde můžete najít mnoho dalších předtrénovaných modelů od komunity. Všechny tyto modely lze načíst a použít jednoduše zadáním názvu modelu. Všechny potřebné binární soubory pro model budou automaticky staženy.\n",
    "\n",
    "V určitých situacích budete potřebovat načíst vlastní modely, v takovém případě můžete specifikovat adresář, který obsahuje všechny relevantní soubory, včetně parametrů pro tokenizer, soubor `config.json` s parametry modelu, binární váhy atd.\n",
    "\n",
    "Z názvu modelu můžeme vytvořit jak model, tak tokenizer. Začněme s tokenizerem:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "# To load the model from Internet repository using model name. \n",
    "# Use this if you are running from your own copy of the notebooks\n",
    "bert_model = 'bert-base-uncased' \n",
    "\n",
    "# To load the model from the directory on disk. Use this for Microsoft Learn module, because we have\n",
    "# prepared all required files for you.\n",
    "#bert_model = './bert'\n",
    "\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(bert_model)\n",
    "\n",
    "MAX_SEQ_LEN = 128\n",
    "PAD_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "UNK_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objekt `tokenizer` obsahuje funkci `encode`, kterou lze přímo použít k zakódování textu:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 23435, 12314, 2003, 1037, 2307, 7705, 2005, 17953, 2361, 102]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('Tensorflow is a great framework for NLP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Můžeme také použít tokenizer k zakódování sekvence způsobem vhodným pro předání modelu, tj. včetně polí `token_ids`, `input_mask` atd. Můžeme také specifikovat, že chceme Tensorflow tensory poskytnutím argumentu `return_tensors='tf'`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[ 101, 7592, 1010, 2045,  102]], dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[0, 0, 0, 0, 0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[1, 1, 1, 1, 1]], dtype=int32)>}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(['Hello, there'],return_tensors='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "V našem případě budeme používat předtrénovaný model BERT nazvaný `bert-base-uncased`. *Uncased* znamená, že model nerozlišuje velká a malá písmena.\n",
    "\n",
    "Při trénování modelu je potřeba poskytnout tokenizovanou sekvenci jako vstup, a proto navrhneme datový zpracovatelský proces. Jelikož `tokenizer.encode` je funkce v Pythonu, použijeme stejný přístup jako v předchozí jednotce, kdy ji voláme pomocí `py_function`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(x):\n",
    "    return tokenizer.encode(x.numpy().decode('utf-8'),return_tensors='tf',padding='max_length',max_length=MAX_SEQ_LEN,truncation=True)[0]\n",
    "\n",
    "def process_fn(x):\n",
    "    s = x['title']+' '+x['description']\n",
    "    e = tf.py_function(process,inp=[s],Tout=(tf.int32))\n",
    "    e.set_shape(MAX_SEQ_LEN)\n",
    "    return e,x['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nyní můžeme načíst skutečný model pomocí balíčku `BertForSequenceClassification`. To zajišťuje, že náš model již má požadovanou architekturu pro klasifikaci, včetně finálního klasifikátoru. Uvidíte varovnou zprávu, která uvádí, že váhy finálního klasifikátoru nejsou inicializovány a model by vyžadoval předtrénování - to je naprosto v pořádku, protože přesně to se chystáme udělat!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = transformers.TFBertForSequenceClassification.from_pretrained(bert_model,num_labels=4,output_attentions=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  109482240 \n",
      "_________________________________________________________________\n",
      "dropout_75 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  3076      \n",
      "=================================================================\n",
      "Total params: 109,485,316\n",
      "Trainable params: 109,485,316\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jak můžete vidět z `summary()`, model obsahuje téměř 110 milionů parametrů! Pravděpodobně, pokud chceme jednoduchý klasifikační úkol na relativně malém datasetu, nechceme trénovat základní vrstvu BERT:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  109482240 \n",
      "_________________________________________________________________\n",
      "dropout_75 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  3076      \n",
      "=================================================================\n",
      "Total params: 109,485,316\n",
      "Trainable params: 3,076\n",
      "Non-trainable params: 109,482,240\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.layers[0].trainable = False\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nyní jsme připraveni začít s tréninkem!\n",
    "\n",
    "> **Poznámka**: Trénování plnohodnotného modelu BERT může být velmi časově náročné! Proto ho budeme trénovat pouze na prvních 32 dávkách. Toto slouží pouze k ukázce, jak nastavit trénink modelu. Pokud máte zájem vyzkoušet plnohodnotné trénování - stačí odstranit parametry `steps_per_epoch` a `validation_steps` a připravit se na čekání!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 142s 4s/step - loss: 1.3896 - acc: 0.2500 - val_loss: 1.3863 - val_acc: 0.2480\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f1d40a4b6a0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile('adam','sparse_categorical_crossentropy',['acc'])\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "model.fit(ds_train.map(process_fn).batch(32),validation_data=ds_test.map(process_fn).batch(32),steps_per_epoch=32,validation_steps=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pokud zvýšíte počet iterací, počkáte dostatečně dlouho a provedete trénink po několik epoch, můžete očekávat, že klasifikace pomocí BERT nám poskytne nejlepší přesnost! Je to proto, že BERT už velmi dobře rozumí struktuře jazyka, a my potřebujeme pouze doladit finální klasifikátor. Nicméně, protože BERT je velký model, celý proces tréninku trvá dlouho a vyžaduje značnou výpočetní sílu! (GPU, a ideálně více než jedno).\n",
    "\n",
    "> **Note:** V našem příkladu jsme používali jeden z nejmenších předtrénovaných modelů BERT. Existují větší modely, které pravděpodobně přinesou lepší výsledky.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shrnutí\n",
    "\n",
    "V této jednotce jsme se seznámili s nejnovějšími modelovými architekturami založenými na **transformerech**. Použili jsme je pro úlohu klasifikace textu, ale podobně mohou být BERT modely využity pro extrakci entit, odpovídání na otázky a další úlohy z oblasti NLP.\n",
    "\n",
    "Modely založené na transformerech představují současný špičkový standard v NLP a ve většině případů by měly být první volbou, se kterou začnete experimentovat při implementaci vlastních řešení NLP. Nicméně pochopení základních principů rekurentních neuronových sítí, o kterých jsme hovořili v tomto modulu, je nesmírně důležité, pokud chcete vytvářet pokročilé neuronové modely.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Prohlášení**:  \nTento dokument byl přeložen pomocí služby pro automatický překlad [Co-op Translator](https://github.com/Azure/co-op-translator). Ačkoli se snažíme o přesnost, mějte na paměti, že automatické překlady mohou obsahovat chyby nebo nepřesnosti. Původní dokument v jeho původním jazyce by měl být považován za autoritativní zdroj. Pro důležité informace doporučujeme profesionální lidský překlad. Neodpovídáme za žádná nedorozumění nebo nesprávné interpretace vyplývající z použití tohoto překladu.\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0cb620c6d4b9f7a635928804c26cf22403d89d98d79684e4529119355ee6d5a5"
  },
  "kernelspec": {
   "display_name": "py38_tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "coopTranslator": {
   "original_hash": "ab59c532409774988ab875f2260e8e53",
   "translation_date": "2025-08-29T16:06:04+00:00",
   "source_file": "lessons/5-NLP/18-Transformers/TransformersTF.ipynb",
   "language_code": "cs"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}