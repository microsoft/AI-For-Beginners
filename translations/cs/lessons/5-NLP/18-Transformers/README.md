<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "7e617f0b8de85a43957a853aba09bfeb",
  "translation_date": "2025-08-25T22:00:17+00:00",
  "source_file": "lessons/5-NLP/18-Transformers/README.md",
  "language_code": "cs"
}
-->
# Mechanismy pozornosti a transformery

## [KvÃ­z pÅ™ed pÅ™ednÃ¡Å¡kou](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/118)

JednÃ­m z nejdÅ¯leÅ¾itÄ›jÅ¡Ã­ch problÃ©mÅ¯ v oblasti NLP je **strojovÃ½ pÅ™eklad**, coÅ¾ je klÃ­ÄovÃ½ Ãºkol, na kterÃ©m stojÃ­ nÃ¡stroje jako Google PÅ™ekladaÄ. V tÃ©to sekci se zamÄ›Å™Ã­me na strojovÃ½ pÅ™eklad, nebo obecnÄ›ji na jakÃ½koli Ãºkol typu *sekvence na sekvenci* (takÃ© nazÃ½vanÃ½ **pÅ™evod vÄ›t**).

U RNN je sekvence na sekvenci implementovÃ¡na dvÄ›ma rekurentnÃ­mi sÃ­tÄ›mi, kde jedna sÃ­Å¥, **enkodÃ©r**, zkomprimuje vstupnÃ­ sekvenci do skrytÃ©ho stavu, zatÃ­mco druhÃ¡ sÃ­Å¥, **dekodÃ©r**, tento skrytÃ½ stav rozvine do pÅ™eloÅ¾enÃ©ho vÃ½sledku. Tento pÅ™Ã­stup mÃ¡ vÅ¡ak nÄ›kolik problÃ©mÅ¯:

* KoneÄnÃ½ stav enkodÃ©ru mÃ¡ problÃ©m si zapamatovat zaÄÃ¡tek vÄ›ty, coÅ¾ zpÅ¯sobuje nÃ­zkou kvalitu modelu u dlouhÃ½ch vÄ›t.
* VÅ¡echna slova v sekvenci majÃ­ stejnÃ½ vliv na vÃ½sledek. Ve skuteÄnosti vÅ¡ak konkrÃ©tnÃ­ slova ve vstupnÃ­ sekvenci Äasto majÃ­ vÄ›tÅ¡Ã­ vliv na vÃ½stupnÃ­ sekvenci neÅ¾ jinÃ¡.

**Mechanismy pozornosti** poskytujÃ­ zpÅ¯sob, jak vÃ¡Å¾it kontextuÃ¡lnÃ­ vliv kaÅ¾dÃ©ho vstupnÃ­ho vektoru na kaÅ¾dou vÃ½stupnÃ­ predikci RNN. To se implementuje vytvoÅ™enÃ­m zkratek mezi mezistavy vstupnÃ­ RNN a vÃ½stupnÃ­ RNN. TÃ­mto zpÅ¯sobem pÅ™i generovÃ¡nÃ­ vÃ½stupnÃ­ho symbolu y<sub>t</sub> zohlednÃ­me vÅ¡echny skrytÃ© stavy vstupu h<sub>i</sub> s rÅ¯znÃ½mi vÃ¡hovÃ½mi koeficienty Î±<sub>t,i</sub>.

![ObrÃ¡zek ukazujÃ­cÃ­ model enkodÃ©r/dekodÃ©r s aditivnÃ­ vrstvou pozornosti](../../../../../translated_images/encoder-decoder-attention.7a726296894fb567aa2898c94b17b3289087f6705c11907df8301df9e5eeb3de.cs.png)

> Model enkodÃ©r-dekodÃ©r s aditivnÃ­m mechanismem pozornosti podle [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf), citovÃ¡no z [tohoto blogovÃ©ho pÅ™Ã­spÄ›vku](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)

Matice pozornosti {Î±<sub>i,j</sub>} by reprezentovala mÃ­ru, jakou urÄitÃ¡ vstupnÃ­ slova ovlivÅˆujÃ­ generovÃ¡nÃ­ konkrÃ©tnÃ­ho slova ve vÃ½stupnÃ­ sekvenci. NÃ­Å¾e je pÅ™Ã­klad takovÃ© matice:

![ObrÃ¡zek ukazujÃ­cÃ­ pÅ™Ã­klad zarovnÃ¡nÃ­ nalezenÃ©ho RNNsearch-50, pÅ™evzato z Bahdanau - arviz.org](../../../../../translated_images/bahdanau-fig3.09ba2d37f202a6af11de6c82d2d197830ba5f4528d9ea430eb65fd3a75065973.cs.png)

> ObrÃ¡zek z [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) (Obr. 3)

Mechanismy pozornosti jsou zodpovÄ›dnÃ© za velkou ÄÃ¡st souÄasnÃ©ho nebo tÃ©mÄ›Å™ souÄasnÃ©ho stavu techniky v NLP. PÅ™idÃ¡nÃ­ pozornosti vÅ¡ak vÃ½raznÄ› zvyÅ¡uje poÄet parametrÅ¯ modelu, coÅ¾ vedlo k problÃ©mÅ¯m se Å¡kÃ¡lovÃ¡nÃ­m u RNN. KlÃ­ÄovÃ½m omezenÃ­m Å¡kÃ¡lovÃ¡nÃ­ RNN je, Å¾e rekurentnÃ­ povaha modelÅ¯ ztÄ›Å¾uje dÃ¡vkovÃ¡nÃ­ a paralelizaci trÃ©ninku. V RNN musÃ­ bÃ½t kaÅ¾dÃ½ prvek sekvence zpracovÃ¡n v sekvenÄnÃ­m poÅ™adÃ­, coÅ¾ znamenÃ¡, Å¾e jej nelze snadno paralelizovat.

![EnkodÃ©r DekodÃ©r s PozornostÃ­](../../../../../lessons/5-NLP/18-Transformers/images/EncDecAttention.gif)

> ObrÃ¡zek z [Google Blogu](https://research.googleblog.com/2016/09/a-neural-network-for-machine.html)

PÅ™ijetÃ­ mechanismÅ¯ pozornosti v kombinaci s tÃ­mto omezenÃ­m vedlo k vytvoÅ™enÃ­ souÄasnÃ½ch Å¡piÄkovÃ½ch modelÅ¯ Transformer, kterÃ© dnes znÃ¡me a pouÅ¾Ã­vÃ¡me, jako jsou BERT a Open-GPT3.

## Modely Transformer

Jednou z hlavnÃ­ch myÅ¡lenek za transformery je vyhnout se sekvenÄnÃ­ povaze RNN a vytvoÅ™it model, kterÃ½ je paralelizovatelnÃ½ bÄ›hem trÃ©ninku. Toho je dosaÅ¾eno implementacÃ­ dvou myÅ¡lenek:

* poziÄnÃ­ kÃ³dovÃ¡nÃ­
* pouÅ¾itÃ­ mechanismu self-attention k zachycenÃ­ vzorcÅ¯ namÃ­sto RNN (nebo CNN) (proto se ÄlÃ¡nek, kterÃ½ pÅ™edstavuje transformery, nazÃ½vÃ¡ *[Attention is all you need](https://arxiv.org/abs/1706.03762)*)

### PoziÄnÃ­ kÃ³dovÃ¡nÃ­/vklÃ¡dÃ¡nÃ­

MyÅ¡lenka poziÄnÃ­ho kÃ³dovÃ¡nÃ­ je nÃ¡sledujÃ­cÃ­.  
1. PÅ™i pouÅ¾itÃ­ RNN je relativnÃ­ pozice tokenÅ¯ reprezentovÃ¡na poÄtem krokÅ¯, a proto nemusÃ­ bÃ½t explicitnÄ› reprezentovÃ¡na.  
2. Jakmile vÅ¡ak pÅ™ejdeme k pozornosti, potÅ™ebujeme znÃ¡t relativnÃ­ pozice tokenÅ¯ v rÃ¡mci sekvence.  
3. Abychom zÃ­skali poziÄnÃ­ kÃ³dovÃ¡nÃ­, rozÅ¡Ã­Å™Ã­me naÅ¡i sekvenci tokenÅ¯ o sekvenci pozic tokenÅ¯ v sekvenci (tj. sekvenci ÄÃ­sel 0,1, ...).  
4. PotÃ© smÃ­chÃ¡me pozici tokenu s vektorovÃ½m vklÃ¡dÃ¡nÃ­m tokenu. K transformaci pozice (celÃ©ho ÄÃ­sla) na vektor mÅ¯Å¾eme pouÅ¾Ã­t rÅ¯znÃ© pÅ™Ã­stupy:

* TrÃ©novatelnÃ© vklÃ¡dÃ¡nÃ­, podobnÃ© vklÃ¡dÃ¡nÃ­ tokenÅ¯. Toto je pÅ™Ã­stup, kterÃ½ zde zvaÅ¾ujeme. PouÅ¾ijeme vrstvy vklÃ¡dÃ¡nÃ­ na tokeny i jejich pozice, coÅ¾ vede k vektorÅ¯m vklÃ¡dÃ¡nÃ­ stejnÃ© dimenze, kterÃ© potÃ© seÄteme.
* FixnÃ­ funkce poziÄnÃ­ho kÃ³dovÃ¡nÃ­, jak bylo navrÅ¾eno v pÅ¯vodnÃ­m ÄlÃ¡nku.

<img src="images/pos-embedding.png" width="50%"/>

> ObrÃ¡zek od autora

VÃ½sledkem poziÄnÃ­ho vklÃ¡dÃ¡nÃ­ je, Å¾e vklÃ¡dÃ¡me jak pÅ¯vodnÃ­ token, tak jeho pozici v rÃ¡mci sekvence.

### Multi-Head Self-Attention

DÃ¡le potÅ™ebujeme zachytit urÄitÃ© vzorce v rÃ¡mci naÅ¡Ã­ sekvence. K tomu transformery pouÅ¾Ã­vajÃ­ mechanismus **self-attention**, coÅ¾ je v podstatÄ› pozornost aplikovanÃ¡ na stejnou sekvenci jako vstup a vÃ½stup. Aplikace self-attention nÃ¡m umoÅ¾Åˆuje zohlednit **kontext** v rÃ¡mci vÄ›ty a vidÄ›t, kterÃ¡ slova jsou vzÃ¡jemnÄ› propojenÃ¡. NapÅ™Ã­klad nÃ¡m umoÅ¾Åˆuje vidÄ›t, na kterÃ¡ slova odkazujÃ­ zÃ¡jmena jako *to*, a takÃ© zohlednit kontext:

![](../../../../../translated_images/CoreferenceResolution.861924d6d384a7d68d8d0039d06a71a151f18a796b8b1330239d3590bd4947eb.cs.png)

> ObrÃ¡zek z [Google Blogu](https://research.googleblog.com/2017/08/transformer-novel-neural-network.html)

V transformerech pouÅ¾Ã­vÃ¡me **Multi-Head Attention**, abychom sÃ­ti dali schopnost zachytit nÄ›kolik rÅ¯znÃ½ch typÅ¯ zÃ¡vislostÃ­, napÅ™. dlouhodobÃ© vs. krÃ¡tkodobÃ© vztahy mezi slovy, koreference vs. nÄ›co jinÃ©ho atd.

[TensorFlow Notebook](../../../../../lessons/5-NLP/18-Transformers/TransformersTF.ipynb) obsahuje vÃ­ce podrobnostÃ­ o implementaci vrstev transformeru.

### Pozornost EnkodÃ©r-DekodÃ©r

V transformerech se pozornost pouÅ¾Ã­vÃ¡ na dvou mÃ­stech:

* K zachycenÃ­ vzorcÅ¯ ve vstupnÃ­m textu pomocÃ­ self-attention
* K provedenÃ­ sekvenÄnÃ­ho pÅ™ekladu - jednÃ¡ se o vrstvu pozornosti mezi enkodÃ©rem a dekodÃ©rem.

Pozornost enkodÃ©r-dekodÃ©r je velmi podobnÃ¡ mechanismu pozornosti pouÅ¾Ã­vanÃ©mu v RNN, jak bylo popsÃ¡no na zaÄÃ¡tku tÃ©to sekce. Tento animovanÃ½ diagram vysvÄ›tluje roli pozornosti enkodÃ©r-dekodÃ©r.

![AnimovanÃ½ GIF ukazujÃ­cÃ­, jak jsou provÃ¡dÄ›na hodnocenÃ­ v modelech transformer.](../../../../../lessons/5-NLP/18-Transformers/images/transformer-animated-explanation.gif)

ProtoÅ¾e kaÅ¾dÃ¡ vstupnÃ­ pozice je mapovÃ¡na nezÃ¡visle na kaÅ¾dou vÃ½stupnÃ­ pozici, transformery mohou lÃ©pe paralelizovat neÅ¾ RNN, coÅ¾ umoÅ¾Åˆuje mnohem vÄ›tÅ¡Ã­ a expresivnÄ›jÅ¡Ã­ jazykovÃ© modely. KaÅ¾dÃ¡ hlava pozornosti mÅ¯Å¾e bÃ½t pouÅ¾ita k uÄenÃ­ rÅ¯znÃ½ch vztahÅ¯ mezi slovy, coÅ¾ zlepÅ¡uje nÃ¡slednÃ© Ãºkoly zpracovÃ¡nÃ­ pÅ™irozenÃ©ho jazyka.

## BERT

**BERT** (Bidirectional Encoder Representations from Transformers) je velmi velkÃ¡ vÃ­cevrstvÃ¡ sÃ­Å¥ transformerÅ¯ s 12 vrstvami pro *BERT-base* a 24 pro *BERT-large*. Model je nejprve pÅ™edtrÃ©novÃ¡n na velkÃ©m korpusu textovÃ½ch dat (WikiPedia + knihy) pomocÃ­ nesupervizovanÃ©ho trÃ©ninku (predikce maskovanÃ½ch slov ve vÄ›tÄ›). BÄ›hem pÅ™edtrÃ©novÃ¡nÃ­ model absorbuje vÃ½znamnÃ© ÃºrovnÄ› porozumÄ›nÃ­ jazyku, kterÃ© lze nÃ¡slednÄ› vyuÅ¾Ã­t s jinÃ½mi datovÃ½mi sadami pomocÃ­ doladÄ›nÃ­. Tento proces se nazÃ½vÃ¡ **transfer learning**.

![obrÃ¡zek z http://jalammar.github.io/illustrated-bert/](../../../../../translated_images/jalammarBERT-language-modeling-masked-lm.34f113ea5fec4362e39ee4381aab7cad06b5465a0b5f053a0f2aa05fbe14e746.cs.png)

> ObrÃ¡zek [zdroj](http://jalammar.github.io/illustrated-bert/)

## âœï¸ CviÄenÃ­: Transformery

PokraÄujte ve svÃ©m uÄenÃ­ v nÃ¡sledujÃ­cÃ­ch noteboocÃ­ch:

* [Transformery v PyTorch](../../../../../lessons/5-NLP/18-Transformers/TransformersPyTorch.ipynb)
* [Transformery v TensorFlow](../../../../../lessons/5-NLP/18-Transformers/TransformersTF.ipynb)

## ZÃ¡vÄ›r

V tÃ©to lekci jste se nauÄili o Transformerech a Mechanismech pozornosti, coÅ¾ jsou klÃ­ÄovÃ© nÃ¡stroje v NLP. Existuje mnoho variant architektur Transformer, vÄetnÄ› BERT, DistilBERT, BigBird, OpenGPT3 a dalÅ¡Ã­ch, kterÃ© lze doladit. BalÃ­Äek [HuggingFace](https://github.com/huggingface/) poskytuje repozitÃ¡Å™ pro trÃ©nink mnoha z tÄ›chto architektur s PyTorch i TensorFlow.

## ğŸš€ VÃ½zva

## [KvÃ­z po pÅ™ednÃ¡Å¡ce](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/218)

## PÅ™ehled a samostudium

* [BlogovÃ½ pÅ™Ã­spÄ›vek](https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/), vysvÄ›tlujÃ­cÃ­ klasickÃ½ ÄlÃ¡nek [Attention is all you need](https://arxiv.org/abs/1706.03762) o transformerech.
* [SÃ©rie blogovÃ½ch pÅ™Ã­spÄ›vkÅ¯](https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452) o transformerech, podrobnÄ› vysvÄ›tlujÃ­cÃ­ architekturu.

## [Ãškol](assignment.md)

**ProhlÃ¡Å¡enÃ­:**  
Tento dokument byl pÅ™eloÅ¾en pomocÃ­ sluÅ¾by pro automatickÃ½ pÅ™eklad [Co-op Translator](https://github.com/Azure/co-op-translator). AÄkoli se snaÅ¾Ã­me o pÅ™esnost, mÄ›jte prosÃ­m na pamÄ›ti, Å¾e automatickÃ© pÅ™eklady mohou obsahovat chyby nebo nepÅ™esnosti. PÅ¯vodnÃ­ dokument v jeho pÅ¯vodnÃ­m jazyce by mÄ›l bÃ½t povaÅ¾ovÃ¡n za autoritativnÃ­ zdroj. Pro dÅ¯leÅ¾itÃ© informace se doporuÄuje profesionÃ¡lnÃ­ lidskÃ½ pÅ™eklad. NeodpovÃ­dÃ¡me za Å¾Ã¡dnÃ¡ nedorozumÄ›nÃ­ nebo nesprÃ¡vnÃ© interpretace vyplÃ½vajÃ­cÃ­ z pouÅ¾itÃ­ tohoto pÅ™ekladu.