# Mechanismy pozornosti a transformery

## [KvÃ­z pÅ™ed pÅ™ednÃ¡Å¡kou](https://ff-quizzes.netlify.app/en/ai/quiz/35)

JednÃ­m z nejdÅ¯leÅ¾itÄ›jÅ¡Ã­ch problÃ©mÅ¯ v oblasti NLP je **strojovÃ½ pÅ™eklad**, coÅ¾ je klÃ­ÄovÃ½ Ãºkol, na kterÃ©m jsou zaloÅ¾eny nÃ¡stroje jako Google Translate. V tÃ©to ÄÃ¡sti se zamÄ›Å™Ã­me na strojovÃ½ pÅ™eklad, nebo obecnÄ›ji na jakÃ½koli Ãºkol typu *sekvence na sekvenci* (kterÃ½ se takÃ© nazÃ½vÃ¡ **pÅ™evod vÄ›t**).

U RNN je sekvence na sekvenci implementovÃ¡na dvÄ›ma rekurentnÃ­mi sÃ­tÄ›mi, kde jedna sÃ­Å¥, **enkodÃ©r**, zkomprimuje vstupnÃ­ sekvenci do skrytÃ©ho stavu, zatÃ­mco druhÃ¡ sÃ­Å¥, **dekodÃ©r**, rozvine tento skrytÃ½ stav do pÅ™eloÅ¾enÃ©ho vÃ½sledku. Tento pÅ™Ã­stup mÃ¡ vÅ¡ak nÄ›kolik problÃ©mÅ¯:

* KoneÄnÃ½ stav enkodÃ©ru mÃ¡ problÃ©m si zapamatovat zaÄÃ¡tek vÄ›ty, coÅ¾ zpÅ¯sobuje nÃ­zkou kvalitu modelu u dlouhÃ½ch vÄ›t.
* VÅ¡echna slova v sekvenci majÃ­ stejnÃ½ vliv na vÃ½sledek. Ve skuteÄnosti vÅ¡ak konkrÃ©tnÃ­ slova ve vstupnÃ­ sekvenci Äasto majÃ­ vÄ›tÅ¡Ã­ vliv na vÃ½stupy neÅ¾ jinÃ¡.

**Mechanismy pozornosti** poskytujÃ­ zpÅ¯sob, jak vÃ¡Å¾it kontextuÃ¡lnÃ­ vliv kaÅ¾dÃ©ho vstupnÃ­ho vektoru na kaÅ¾dou vÃ½stupnÃ­ predikci RNN. Implementuje se to vytvoÅ™enÃ­m zkratek mezi mezistavy vstupnÃ­ RNN a vÃ½stupnÃ­ RNN. TÃ­mto zpÅ¯sobem pÅ™i generovÃ¡nÃ­ vÃ½stupnÃ­ho symbolu y<sub>t</sub> zohlednÃ­me vÅ¡echny skrytÃ© stavy vstupu h<sub>i</sub>, s rÅ¯znÃ½mi vÃ¡hovÃ½mi koeficienty &alpha;<sub>t,i</sub>.

![ObrÃ¡zek zobrazujÃ­cÃ­ model enkodÃ©r/dekodÃ©r s vrstvou aditivnÃ­ pozornosti](../../../../../translated_images/cs/encoder-decoder-attention.7a726296894fb567.webp)

> Model enkodÃ©r-dekodÃ©r s mechanismem aditivnÃ­ pozornosti podle [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf), citovÃ¡no z [tohoto blogovÃ©ho pÅ™Ã­spÄ›vku](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)

Matice pozornosti {&alpha;<sub>i,j</sub>} by reprezentovala mÃ­ru, jakou urÄitÃ¡ vstupnÃ­ slova ovlivÅˆujÃ­ generovÃ¡nÃ­ danÃ©ho slova ve vÃ½stupnÃ­ sekvenci. NÃ­Å¾e je pÅ™Ã­klad takovÃ© matice:

![ObrÃ¡zek zobrazujÃ­cÃ­ vzorovÃ© zarovnÃ¡nÃ­ nalezenÃ© RNNsearch-50, pÅ™evzato z Bahdanau - arviz.org](../../../../../translated_images/cs/bahdanau-fig3.09ba2d37f202a6af.webp)

> ObrÃ¡zek z [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) (Obr.3)

Mechanismy pozornosti jsou zodpovÄ›dnÃ© za velkou ÄÃ¡st souÄasnÃ©ho nebo tÃ©mÄ›Å™ souÄasnÃ©ho stavu umÄ›nÃ­ v NLP. PÅ™idÃ¡nÃ­ pozornosti vÅ¡ak vÃ½raznÄ› zvyÅ¡uje poÄet parametrÅ¯ modelu, coÅ¾ vedlo k problÃ©mÅ¯m se Å¡kÃ¡lovÃ¡nÃ­m u RNN. KlÃ­ÄovÃ½m omezenÃ­m Å¡kÃ¡lovÃ¡nÃ­ RNN je, Å¾e rekurentnÃ­ povaha modelÅ¯ ztÄ›Å¾uje dÃ¡vkovÃ¡nÃ­ a paralelizaci trÃ©ninku. V RNN musÃ­ bÃ½t kaÅ¾dÃ½ prvek sekvence zpracovÃ¡n v sekvenÄnÃ­m poÅ™adÃ­, coÅ¾ znamenÃ¡, Å¾e jej nelze snadno paralelizovat.

![EnkodÃ©r DekodÃ©r s PozornostÃ­](../../../../../lessons/5-NLP/18-Transformers/images/EncDecAttention.gif)

> ObrÃ¡zek z [Google Blogu](https://research.googleblog.com/2016/09/a-neural-network-for-machine.html)

PÅ™ijetÃ­ mechanismÅ¯ pozornosti v kombinaci s tÃ­mto omezenÃ­m vedlo k vytvoÅ™enÃ­ nynÃ­ Å¡piÄkovÃ½ch modelÅ¯ Transformer, kterÃ© dnes znÃ¡me a pouÅ¾Ã­vÃ¡me, jako jsou BERT a Open-GPT3.

## Modely Transformer

Jednou z hlavnÃ­ch myÅ¡lenek za transformery je vyhnout se sekvenÄnÃ­ povaze RNN a vytvoÅ™it model, kterÃ½ je paralelizovatelnÃ½ bÄ›hem trÃ©ninku. Toho je dosaÅ¾eno implementacÃ­ dvou myÅ¡lenek:

* poziÄnÃ­ kÃ³dovÃ¡nÃ­
* pouÅ¾itÃ­ mechanismu vlastnÃ­ pozornosti k zachycenÃ­ vzorcÅ¯ mÃ­sto RNN (nebo CNN) (proto se ÄlÃ¡nek, kterÃ½ pÅ™edstavuje transformery, nazÃ½vÃ¡ *[Attention is all you need](https://arxiv.org/abs/1706.03762)*)

### PoziÄnÃ­ kÃ³dovÃ¡nÃ­/embedding

MyÅ¡lenka poziÄnÃ­ho kÃ³dovÃ¡nÃ­ je nÃ¡sledujÃ­cÃ­. 
1. PÅ™i pouÅ¾itÃ­ RNN je relativnÃ­ pozice tokenÅ¯ reprezentovÃ¡na poÄtem krokÅ¯, a proto ji nenÃ­ tÅ™eba explicitnÄ› reprezentovat. 
2. Jakmile vÅ¡ak pÅ™ejdeme k pozornosti, potÅ™ebujeme znÃ¡t relativnÃ­ pozice tokenÅ¯ v rÃ¡mci sekvence. 
3. Abychom zÃ­skali poziÄnÃ­ kÃ³dovÃ¡nÃ­, rozÅ¡Ã­Å™Ã­me naÅ¡i sekvenci tokenÅ¯ o sekvenci pozic tokenÅ¯ v sekvenci (tj. sekvenci ÄÃ­sel 0,1, ...).
4. PotÃ© smÃ­chÃ¡me pozici tokenu s vektorem embeddingu tokenu. K transformaci pozice (celÃ©ho ÄÃ­sla) na vektor mÅ¯Å¾eme pouÅ¾Ã­t rÅ¯znÃ© pÅ™Ã­stupy:

* TrÃ©novatelnÃ½ embedding, podobnÄ› jako embedding tokenÅ¯. Tento pÅ™Ã­stup zde zvaÅ¾ujeme. Aplikujeme vrstvy embeddingu na tokeny i jejich pozice, coÅ¾ vede k embeddingovÃ½m vektorÅ¯m stejnÃ½ch rozmÄ›rÅ¯, kterÃ© potÃ© seÄteme.
* FixnÃ­ funkce poziÄnÃ­ho kÃ³dovÃ¡nÃ­, jak bylo navrÅ¾eno v pÅ¯vodnÃ­m ÄlÃ¡nku.

<img src="../../../../../translated_images/cs/pos-embedding.e41ce9b6cf6078af.webp" width="50%"/>

> ObrÃ¡zek od autora

VÃ½sledek, kterÃ½ zÃ­skÃ¡me s poziÄnÃ­m embeddingem, zahrnuje jak pÅ¯vodnÃ­ token, tak jeho pozici v rÃ¡mci sekvence.

### Multi-Head Self-Attention

DÃ¡le potÅ™ebujeme zachytit urÄitÃ© vzorce v rÃ¡mci naÅ¡Ã­ sekvence. K tomu transformery pouÅ¾Ã­vajÃ­ mechanismus **vlastnÃ­ pozornosti**, coÅ¾ je v podstatÄ› pozornost aplikovanÃ¡ na stejnou sekvenci jako vstup a vÃ½stup. Aplikace vlastnÃ­ pozornosti nÃ¡m umoÅ¾Åˆuje zohlednit **kontext** v rÃ¡mci vÄ›ty a vidÄ›t, kterÃ¡ slova jsou vzÃ¡jemnÄ› propojenÃ¡. NapÅ™Ã­klad nÃ¡m umoÅ¾Åˆuje vidÄ›t, na kterÃ¡ slova odkazujÃ­ koreference, jako *to*, a takÃ© zohlednit kontext:

![](../../../../../translated_images/cs/CoreferenceResolution.861924d6d384a7d6.webp)

> ObrÃ¡zek z [Google Blogu](https://research.googleblog.com/2017/08/transformer-novel-neural-network.html)

V transformerech pouÅ¾Ã­vÃ¡me **Multi-Head Attention**, abychom sÃ­ti dali schopnost zachytit nÄ›kolik rÅ¯znÃ½ch typÅ¯ zÃ¡vislostÃ­, napÅ™. dlouhodobÃ© vs. krÃ¡tkodobÃ© vztahy mezi slovy, koreference vs. nÄ›co jinÃ©ho atd.

[TensorFlow Notebook](TransformersTF.ipynb) obsahuje vÃ­ce podrobnostÃ­ o implementaci vrstev transformeru.

### Pozornost mezi enkodÃ©rem a dekodÃ©rem

V transformerech se pozornost pouÅ¾Ã­vÃ¡ na dvou mÃ­stech:

* K zachycenÃ­ vzorcÅ¯ ve vstupnÃ­m textu pomocÃ­ vlastnÃ­ pozornosti
* K provÃ¡dÄ›nÃ­ pÅ™ekladÅ¯ sekvencÃ­ - jednÃ¡ se o vrstvu pozornosti mezi enkodÃ©rem a dekodÃ©rem.

Pozornost mezi enkodÃ©rem a dekodÃ©rem je velmi podobnÃ¡ mechanismu pozornosti pouÅ¾Ã­vanÃ©mu v RNN, jak bylo popsÃ¡no na zaÄÃ¡tku tÃ©to ÄÃ¡sti. Tento animovanÃ½ diagram vysvÄ›tluje roli pozornosti mezi enkodÃ©rem a dekodÃ©rem.

![AnimovanÃ½ GIF ukazujÃ­cÃ­, jak jsou provÃ¡dÄ›na hodnocenÃ­ v modelech transformeru.](../../../../../lessons/5-NLP/18-Transformers/images/transformer-animated-explanation.gif)

ProtoÅ¾e kaÅ¾dÃ¡ vstupnÃ­ pozice je mapovÃ¡na nezÃ¡visle na kaÅ¾dou vÃ½stupnÃ­ pozici, transformery mohou lÃ©pe paralelizovat neÅ¾ RNN, coÅ¾ umoÅ¾Åˆuje mnohem vÄ›tÅ¡Ã­ a expresivnÄ›jÅ¡Ã­ jazykovÃ© modely. KaÅ¾dÃ¡ hlava pozornosti mÅ¯Å¾e bÃ½t pouÅ¾ita k uÄenÃ­ rÅ¯znÃ½ch vztahÅ¯ mezi slovy, coÅ¾ zlepÅ¡uje nÃ¡slednÃ© Ãºkoly zpracovÃ¡nÃ­ pÅ™irozenÃ©ho jazyka.

## BERT

**BERT** (Bidirectional Encoder Representations from Transformers) je velmi velkÃ¡ vÃ­cevstvÃ¡ sÃ­Å¥ transformeru s 12 vrstvami pro *BERT-base* a 24 pro *BERT-large*. Model je nejprve pÅ™edtrÃ©novÃ¡n na velkÃ©m korpusu textovÃ½ch dat (WikiPedia + knihy) pomocÃ­ nesupervizovanÃ©ho trÃ©ninku (predikce maskovanÃ½ch slov ve vÄ›tÄ›). BÄ›hem pÅ™edtrÃ©novÃ¡nÃ­ model absorbuje vÃ½znamnÃ© ÃºrovnÄ› porozumÄ›nÃ­ jazyku, kterÃ© lze nÃ¡slednÄ› vyuÅ¾Ã­t s jinÃ½mi datovÃ½mi sadami pomocÃ­ jemnÃ©ho ladÄ›nÃ­. Tento proces se nazÃ½vÃ¡ **transfer learning**.

![obrÃ¡zek z http://jalammar.github.io/illustrated-bert/](../../../../../translated_images/cs/jalammarBERT-language-modeling-masked-lm.34f113ea5fec4362.webp)

> ObrÃ¡zek [zdroj](http://jalammar.github.io/illustrated-bert/)

## âœï¸ CviÄenÃ­: Transformery

PokraÄujte ve svÃ©m uÄenÃ­ v nÃ¡sledujÃ­cÃ­ch noteboocÃ­ch:

* [Transformery v PyTorch](TransformersPyTorch.ipynb)
* [Transformery v TensorFlow](TransformersTF.ipynb)

## ZÃ¡vÄ›r

V tÃ©to lekci jste se nauÄili o transformerech a mechanismech pozornosti, coÅ¾ jsou zÃ¡kladnÃ­ nÃ¡stroje v NLP. Existuje mnoho variant architektur transformeru, vÄetnÄ› BERT, DistilBERT, BigBird, OpenGPT3 a dalÅ¡Ã­ch, kterÃ© lze jemnÄ› ladit. BalÃ­Äek [HuggingFace](https://github.com/huggingface/) poskytuje repozitÃ¡Å™ pro trÃ©novÃ¡nÃ­ mnoha z tÄ›chto architektur s pouÅ¾itÃ­m PyTorch i TensorFlow.

## ğŸš€ VÃ½zva

## [KvÃ­z po pÅ™ednÃ¡Å¡ce](https://ff-quizzes.netlify.app/en/ai/quiz/36)

## PÅ™ehled & Samostudium

* [BlogovÃ½ pÅ™Ã­spÄ›vek](https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/), vysvÄ›tlujÃ­cÃ­ klasickÃ½ ÄlÃ¡nek [Attention is all you need](https://arxiv.org/abs/1706.03762) o transformerech.
* [SÃ©rie blogovÃ½ch pÅ™Ã­spÄ›vkÅ¯](https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452) o transformerech, vysvÄ›tlujÃ­cÃ­ architekturu podrobnÄ›.

## [Ãškol](assignment.md)

---

