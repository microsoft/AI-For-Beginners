{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vektory slov\n",
    "\n",
    "V našem předchozím příkladu jsme pracovali s vysoko-dimenzionálními vektory bag-of-words o délce `vocab_size` a explicitně jsme převáděli nízko-dimenzionální poziční reprezentace na řídké jednorozměrné reprezentace. Tato jednorozměrná reprezentace není paměťově efektivní. Navíc je každé slovo považováno za nezávislé na ostatních, takže jednorozměrné kódování nevyjadřuje sémantické podobnosti mezi slovy.\n",
    "\n",
    "V této části budeme pokračovat v průzkumu datasetu **News AG**. Nejprve načteme data a získáme některé definice z předchozí části.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "\n",
    "ds_train, ds_test = tfds.load('ag_news_subset').values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Co je to embedding?\n",
    "\n",
    "Myšlenka **embeddingu** spočívá v reprezentaci slov pomocí nízkodimenzionálních hustých vektorů, které odrážejí sémantický význam slova. Později si povíme, jak vytvořit smysluplné word embeddings, ale prozatím si embeddingy představme jako způsob, jak snížit dimenzionalitu vektorů slov.\n",
    "\n",
    "Embedding vrstva tedy přijímá slovo jako vstup a produkuje výstupní vektor o specifikované velikosti `embedding_size`. V jistém smyslu je velmi podobná vrstvě `Dense`, ale místo toho, aby přijímala jednorozměrný vektor jako vstup, dokáže přijmout číslo odpovídající slovu.\n",
    "\n",
    "Použitím embedding vrstvy jako první vrstvy v naší síti můžeme přejít od modelu bag-of-words k modelu **embedding bag**, kde nejprve převedeme každé slovo v našem textu na odpovídající embedding a poté vypočítáme nějakou agregační funkci nad všemi těmito embeddingy, například `sum`, `average` nebo `max`.\n",
    "\n",
    "![Obrázek ukazující embedding klasifikátor pro pět slov v sekvenci.](../../../../../translated_images/cs/embedding-classifier-example.b77f021a7ee67eee.webp)\n",
    "\n",
    "Naše neuronová síť klasifikátoru se skládá z následujících vrstev:\n",
    "\n",
    "* Vrstva `TextVectorization`, která přijímá řetězec jako vstup a produkuje tenzor čísel tokenů. Určíme rozumnou velikost slovníku `vocab_size` a ignorujeme méně často používaná slova. Vstupní tvar bude 1 a výstupní tvar bude $n$, protože získáme $n$ tokenů jako výsledek, přičemž každý z nich obsahuje čísla od 0 do `vocab_size`.\n",
    "* Vrstva `Embedding`, která přijímá $n$ čísel a redukuje každé číslo na hustý vektor o dané délce (v našem příkladu 100). Tím se vstupní tenzor tvaru $n$ transformuje na tenzor $n\\times 100$.\n",
    "* Agregační vrstva, která vypočítá průměr tohoto tenzoru podél první osy, tj. vypočítá průměr všech $n$ vstupních tenzorů odpovídajících různým slovům. K implementaci této vrstvy použijeme vrstvu `Lambda` a předáme jí funkci pro výpočet průměru. Výstup bude mít tvar 100 a bude představovat číselnou reprezentaci celé vstupní sekvence.\n",
    "* Konečný lineární klasifikátor `Dense`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization (TextVec  (None, None)             0         \n",
      " torization)                                                     \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, None, 100)         3000000   \n",
      "                                                                 \n",
      " lambda (Lambda)             (None, 100)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4)                 404       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,000,404\n",
      "Trainable params: 3,000,404\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 30000\n",
    "batch_size = 128\n",
    "\n",
    "vectorizer = keras.layers.experimental.preprocessing.TextVectorization(max_tokens=vocab_size,input_shape=(1,))\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    vectorizer,    \n",
    "    keras.layers.Embedding(vocab_size,100),\n",
    "    keras.layers.Lambda(lambda x: tf.reduce_mean(x,axis=1)),\n",
    "    keras.layers.Dense(4, activation='softmax')\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "V přehledu, ve sloupci **tvar výstupu**, první rozměr tenzoru `None` odpovídá velikosti minibatch, zatímco druhý odpovídá délce sekvence tokenů. Všechny sekvence tokenů v minibatch mají různé délky. O tom, jak s tím pracovat, si povíme v další části.\n",
    "\n",
    "Teď pojďme natrénovat síť:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training vectorizer\n",
      "938/938 [==============================] - 20s 20ms/step - loss: 0.7891 - acc: 0.8155 - val_loss: 0.4470 - val_acc: 0.8642\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22255515100>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_text(x):\n",
    "    return x['title']+' '+x['description']\n",
    "\n",
    "def tupelize(x):\n",
    "    return (extract_text(x),x['label'])\n",
    "\n",
    "print(\"Training vectorizer\")\n",
    "vectorizer.adapt(ds_train.take(500).map(extract_text))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'])\n",
    "model.fit(ds_train.map(tupelize).batch(batch_size),validation_data=ds_test.map(tupelize).batch(batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "> **Poznámka** že vytváříme vektorizér na základě podmnožiny dat. To se provádí za účelem urychlení procesu, což může vést k situaci, kdy ne všechny tokeny z našeho textu budou přítomny ve slovníku. V takovém případě budou tyto tokeny ignorovány, což může vést k mírně nižší přesnosti. Nicméně, v reálném životě podmnožina textu často poskytuje dobrý odhad slovníku.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Práce s různými délkami sekvencí proměnných\n",
    "\n",
    "Pojďme si vysvětlit, jak probíhá trénink v minibatchích. V uvedeném příkladu má vstupní tensor rozměr 1 a používáme minibatche o délce 128, takže skutečná velikost tensoru je $128 \\times 1$. Počet tokenů v každé větě je však odlišný. Pokud aplikujeme vrstvu `TextVectorization` na jeden vstup, počet vrácených tokenů se liší v závislosti na způsobu tokenizace textu:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 1 45], shape=(2,), dtype=int64)\n",
      "tf.Tensor([ 112 1271    1    3 1747  158], shape=(6,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer('Hello, world!'))\n",
    "print(vectorizer('I am glad to meet you!'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nicméně, když aplikujeme vektorizér na několik sekvencí, musí vytvořit tenzor obdélníkového tvaru, takže nevyužité prvky vyplní tokenem PAD (který je v našem případě nula):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 6), dtype=int64, numpy=\n",
       "array([[   1,   45,    0,    0,    0,    0],\n",
       "       [ 112, 1271,    1,    3, 1747,  158]], dtype=int64)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer(['Hello, world!','I am glad to meet you!'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zde můžeme vidět vnoření:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 1.53059261e-02,  6.80514947e-02,  3.14026810e-02, ...,\n",
       "         -8.92002955e-02,  1.52911525e-04, -5.65562584e-02],\n",
       "        [ 2.57456154e-01,  2.79364467e-01, -2.03605562e-01, ...,\n",
       "         -2.07474351e-01,  8.31158683e-02, -2.03911960e-01],\n",
       "        [ 3.98201384e-02, -8.03454965e-03,  2.39790026e-02, ...,\n",
       "         -7.18549127e-04,  2.66963355e-02, -4.30646613e-02],\n",
       "        [ 3.98201384e-02, -8.03454965e-03,  2.39790026e-02, ...,\n",
       "         -7.18549127e-04,  2.66963355e-02, -4.30646613e-02],\n",
       "        [ 3.98201384e-02, -8.03454965e-03,  2.39790026e-02, ...,\n",
       "         -7.18549127e-04,  2.66963355e-02, -4.30646613e-02],\n",
       "        [ 3.98201384e-02, -8.03454965e-03,  2.39790026e-02, ...,\n",
       "         -7.18549127e-04,  2.66963355e-02, -4.30646613e-02]],\n",
       "\n",
       "       [[ 1.89674050e-01,  2.61548996e-01, -3.67433839e-02, ...,\n",
       "         -2.07366899e-01, -1.05442435e-01, -2.36952081e-01],\n",
       "        [ 6.16133213e-02,  1.80511594e-01,  9.77298319e-02, ...,\n",
       "         -5.46628237e-02, -1.07340455e-01, -1.06589928e-01],\n",
       "        [ 1.53059261e-02,  6.80514947e-02,  3.14026810e-02, ...,\n",
       "         -8.92002955e-02,  1.52911525e-04, -5.65562584e-02],\n",
       "        [-4.84890305e-02, -8.41715634e-02,  1.51529670e-01, ...,\n",
       "          1.28192469e-01, -7.77286515e-02,  1.26041949e-01],\n",
       "        [-4.17212099e-02, -5.60694858e-02,  4.08860669e-02, ...,\n",
       "          8.70475471e-02,  8.92383084e-02,  1.67974353e-01],\n",
       "        [ 2.85779923e-01,  4.57767487e-01,  4.52292450e-02, ...,\n",
       "         -1.97419018e-01, -2.04659685e-01, -2.79758364e-01]]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[1](vectorizer(['Hello, world!','I am glad to meet you!'])).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Poznámka**: Aby se minimalizovalo množství doplňování, v některých případech má smysl seřadit všechny sekvence v datové sadě podle rostoucí délky (nebo přesněji podle počtu tokenů). To zajistí, že každý minibatch bude obsahovat sekvence podobné délky.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sémantické vektorizace: Word2Vec\n",
    "\n",
    "V našem předchozím příkladu se vrstva pro vektorizaci naučila mapovat slova na jejich vektorové reprezentace, avšak tyto reprezentace neměly sémantický význam. Bylo by užitečné naučit se vektorovou reprezentaci, kde podobná slova nebo synonyma odpovídají vektorům, které jsou si blízké podle určité vzdálenosti mezi vektory (například euklidovské vzdálenosti).\n",
    "\n",
    "Abychom toho dosáhli, musíme náš model pro vektorizaci předtrénovat na velké kolekci textů pomocí techniky, jako je [Word2Vec](https://en.wikipedia.org/wiki/Word2vec). Tato metoda je založena na dvou hlavních architekturách, které se používají k vytvoření distribuované reprezentace slov:\n",
    "\n",
    " - **Continuous bag-of-words** (CBoW), kde trénujeme model tak, aby předpovídal slovo na základě okolního kontextu. Pokud máme n-gram $(W_{-2},W_{-1},W_0,W_1,W_2)$, cílem modelu je předpovědět $W_0$ na základě $(W_{-2},W_{-1},W_1,W_2)$.\n",
    " - **Continuous skip-gram** je opakem CBoW. Model využívá okolní okno kontextových slov k předpovědi aktuálního slova.\n",
    "\n",
    "CBoW je rychlejší, zatímco skip-gram je pomalejší, ale lépe reprezentuje méně častá slova.\n",
    "\n",
    "![Obrázek ukazující algoritmy CBoW a Skip-Gram pro převod slov na vektory.](../../../../../translated_images/cs/example-algorithms-for-converting-words-to-vectors.fbe9207a726922f6.webp)\n",
    "\n",
    "Pro experimentování s Word2Vec vektorizací předtrénovanou na datasetu Google News můžeme použít knihovnu **gensim**. Níže najdeme slova nejpodobnější slovu 'neural'.\n",
    "\n",
    "> **Note:** Když poprvé vytváříte slovní vektory, jejich stahování může chvíli trvat!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "w2v = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neuronal -> 0.7804799675941467\n",
      "neurons -> 0.7326500415802002\n",
      "neural_circuits -> 0.7252851724624634\n",
      "neuron -> 0.7174385190010071\n",
      "cortical -> 0.6941086649894714\n",
      "brain_circuitry -> 0.6923246383666992\n",
      "synaptic -> 0.6699118614196777\n",
      "neural_circuitry -> 0.6638563275337219\n",
      "neurochemical -> 0.6555314064025879\n",
      "neuronal_activity -> 0.6531826257705688\n"
     ]
    }
   ],
   "source": [
    "for w,p in w2v.most_similar('neural'):\n",
    "    print(f\"{w} -> {p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Můžeme také extrahovat vektorové zakotvení ze slova, které bude použito při trénování klasifikačního modelu. Zakotvení má 300 komponent, ale zde ukazujeme pouze prvních 20 komponent vektoru pro přehlednost:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01226807,  0.06225586,  0.10693359,  0.05810547,  0.23828125,\n",
       "        0.03686523,  0.05151367, -0.20703125,  0.01989746,  0.10058594,\n",
       "       -0.03759766, -0.1015625 , -0.15820312, -0.08105469, -0.0390625 ,\n",
       "       -0.05053711,  0.16015625,  0.2578125 ,  0.10058594, -0.25976562],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v['play'][:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skvělá věc na sémantických vnořeních je, že můžete manipulovat s vektorovým kódováním na základě sémantiky. Například můžeme požádat o nalezení slova, jehož vektorová reprezentace je co nejblíže slovům *král* a *žena*, a co nejdále od slova *muž*:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('queen', 0.7118192911148071)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar(positive=['king','woman'],negative=['man'])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Příklad výše používá určité interní kouzlo GenSym, ale základní logika je ve skutečnosti docela jednoduchá. Zajímavou věcí na vnořeních je, že můžete provádět běžné operace s vektory na vektorových reprezentacích vnoření, což odráží operace na **významech** slov. Příklad výše lze vyjádřit pomocí vektorových operací: vypočítáme vektor odpovídající **KRÁL-MUŽ+ŽENA** (operace `+` a `-` jsou prováděny na vektorových reprezentacích odpovídajících slov) a poté najdeme nejbližší slovo ve slovníku k tomuto vektoru:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'queen'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the vector corresponding to kind-man+woman\n",
    "qvec = w2v['king']-1.7*w2v['man']+1.7*w2v['woman']\n",
    "# find the index of the closest embedding vector \n",
    "d = np.sum((w2v.vectors-qvec)**2,axis=1)\n",
    "min_idx = np.argmin(d)\n",
    "# find the corresponding word\n",
    "w2v.index_to_key[min_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NOTE**: Museli jsme přidat malé koeficienty k vektorům *man* a *woman* – zkuste je odstranit a podívejte se, co se stane.\n",
    "\n",
    "K nalezení nejbližšího vektoru používáme TensorFlow mechanismus k výpočtu vektoru vzdáleností mezi naším vektorem a všemi vektory ve slovníku, a poté najdeme index minimálního slova pomocí `argmin`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zatímco Word2Vec se zdá být skvělým způsobem, jak vyjádřit sémantiku slov, má mnoho nevýhod, včetně následujících:\n",
    "\n",
    "* Modely CBoW i skip-gram jsou **prediktivní vektorizace**, které berou v úvahu pouze lokální kontext. Word2Vec nevyužívá globální kontext.\n",
    "* Word2Vec nezohledňuje **morfologii** slov, tj. skutečnost, že význam slova může záviset na různých částech slova, jako je kořen.\n",
    "\n",
    "**FastText** se snaží překonat druhé omezení a staví na Word2Vec tím, že se učí vektorové reprezentace pro každé slovo a n-gramy znaků obsažené v každém slově. Hodnoty těchto reprezentací jsou pak při každém kroku trénování zprůměrovány do jednoho vektoru. I když to přidává spoustu dodatečných výpočtů během předtrénování, umožňuje to vektorům slov zakódovat informace o podslovech.\n",
    "\n",
    "Další metoda, **GloVe**, používá odlišný přístup k vektorizaci slov, založený na faktorizaci matice slovního kontextu. Nejprve vytvoří velkou matici, která počítá počet výskytů slov v různých kontextech, a poté se snaží tuto matici reprezentovat v nižších dimenzích tak, aby minimalizovala ztrátu při rekonstrukci.\n",
    "\n",
    "Knihovna gensim podporuje tyto vektorizace slov a můžete s nimi experimentovat změnou kódu pro načítání modelu výše.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Použití předtrénovaných embeddingů v Keras\n",
    "\n",
    "Můžeme upravit výše uvedený příklad tak, aby byla matice v naší embedding vrstvě předem naplněna sémantickými embeddingy, jako je Word2Vec. Slovníky předtrénovaného embeddingu a textového korpusu se pravděpodobně nebudou shodovat, takže si musíme vybrat jeden. Zde prozkoumáme dvě možné možnosti: použití slovníku tokenizeru a použití slovníku z embeddingů Word2Vec.\n",
    "\n",
    "### Použití slovníku tokenizeru\n",
    "\n",
    "Při použití slovníku tokenizeru budou některá slova ze slovníku mít odpovídající embeddingy Word2Vec, zatímco jiná budou chybět. Vzhledem k tomu, že velikost našeho slovníku je `vocab_size` a délka vektorů embeddingu Word2Vec je `embed_size`, bude embedding vrstva reprezentována váhovou maticí tvaru `vocab_size`$\\times$`embed_size`. Tuto matici naplníme tak, že projdeme slovník:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding size: 300\n",
      "Populating matrix, this will take some time...Done, found 4551 words, 784 words missing\n"
     ]
    }
   ],
   "source": [
    "embed_size = len(w2v.get_vector('hello'))\n",
    "print(f'Embedding size: {embed_size}')\n",
    "\n",
    "vocab = vectorizer.get_vocabulary()\n",
    "W = np.zeros((vocab_size,embed_size))\n",
    "print('Populating matrix, this will take some time...',end='')\n",
    "found, not_found = 0,0\n",
    "for i,w in enumerate(vocab):\n",
    "    try:\n",
    "        W[i] = w2v.get_vector(w)\n",
    "        found+=1\n",
    "    except:\n",
    "        # W[i] = np.random.normal(0.0,0.3,size=(embed_size,))\n",
    "        not_found+=1\n",
    "\n",
    "print(f\"Done, found {found} words, {not_found} words missing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pro slova, která nejsou přítomna ve slovníku Word2Vec, můžeme buď ponechat jejich hodnoty jako nuly, nebo vygenerovat náhodný vektor.\n",
    "\n",
    "Nyní můžeme definovat vrstvu pro vkládání s předem naučenými váhami:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = keras.layers.Embedding(vocab_size,embed_size,weights=[W],trainable=False)\n",
    "model = keras.models.Sequential([\n",
    "    vectorizer, emb,\n",
    "    keras.layers.Lambda(lambda x: tf.reduce_mean(x,axis=1)),\n",
    "    keras.layers.Dense(4, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 10s 10ms/step - loss: 1.1075 - acc: 0.7822 - val_loss: 0.9134 - val_acc: 0.8175\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2220226ef10>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'])\n",
    "model.fit(ds_train.map(tupelize).batch(batch_size),\n",
    "          validation_data=ds_test.map(tupelize).batch(batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Poznámka**: Všimněte si, že jsme nastavili `trainable=False` při vytváření `Embedding`, což znamená, že vrstvu Embedding nebudeme znovu trénovat. To může způsobit mírně nižší přesnost, ale urychluje to proces trénování.\n",
    "\n",
    "### Použití slovníku pro embedding\n",
    "\n",
    "Jedním z problémů předchozího přístupu je, že slovníky použité v TextVectorization a Embedding jsou odlišné. Abychom tento problém překonali, můžeme použít jednu z následujících možností:\n",
    "* Znovu natrénovat Word2Vec model na našem slovníku.\n",
    "* Načíst náš dataset se slovníkem z předtrénovaného Word2Vec modelu. Slovníky použité k načtení datasetu lze specifikovat během načítání.\n",
    "\n",
    "Druhý přístup se zdá být jednodušší, takže ho implementujeme. Nejprve vytvoříme vrstvu `TextVectorization` se specifikovaným slovníkem, který je převzat z Word2Vec embeddingů:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(w2v.vocab.keys())\n",
    "vectorizer = keras.layers.experimental.preprocessing.TextVectorization(input_shape=(1,))\n",
    "vectorizer.set_vocabulary(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knihovna word embeddings Gensim obsahuje praktickou funkci `get_keras_embeddings`, která automaticky vytvoří odpovídající vrstvu embeddings pro Keras.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "938/938 [==============================] - 20s 14ms/step - loss: 1.3377 - acc: 0.4978 - val_loss: 1.2995 - val_acc: 0.5647\n",
      "Epoch 2/5\n",
      "938/938 [==============================] - 10s 10ms/step - loss: 1.2587 - acc: 0.5722 - val_loss: 1.2339 - val_acc: 0.5842\n",
      "Epoch 3/5\n",
      "938/938 [==============================] - 10s 10ms/step - loss: 1.1980 - acc: 0.5884 - val_loss: 1.1826 - val_acc: 0.5954\n",
      "Epoch 4/5\n",
      "938/938 [==============================] - 12s 13ms/step - loss: 1.1503 - acc: 0.6002 - val_loss: 1.1417 - val_acc: 0.6018\n",
      "Epoch 5/5\n",
      "938/938 [==============================] - 11s 12ms/step - loss: 1.1120 - acc: 0.6097 - val_loss: 1.1083 - val_acc: 0.6104\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2220ccb81c0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    vectorizer, \n",
    "    w2v.get_keras_embedding(train_embeddings=False),\n",
    "    keras.layers.Lambda(lambda x: tf.reduce_mean(x,axis=1)),\n",
    "    keras.layers.Dense(4, activation='softmax')\n",
    "])\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'])\n",
    "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128),epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jedním z důvodů, proč nevidíme vyšší přesnost, je to, že některá slova z našeho datového souboru chybí v předtrénovaném slovníku GloVe, a proto jsou v podstatě ignorována. Abychom to překonali, můžeme na základě našeho datového souboru natrénovat vlastní vektory slov.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kontextuální vektory slov\n",
    "\n",
    "Jedním z hlavních omezení tradičních předtrénovaných reprezentací vektorů slov, jako je Word2Vec, je skutečnost, že i když dokážou zachytit určitý význam slova, nedokážou rozlišit mezi různými významy. To může způsobovat problémy v následných modelech.\n",
    "\n",
    "Například slovo 'play' má odlišný význam v těchto dvou větách:\n",
    "- Šel jsem na **hru** do divadla.\n",
    "- John si chce **hrát** se svými přáteli.\n",
    "\n",
    "Předtrénované vektory, o kterých jsme mluvili, reprezentují oba významy slova 'play' stejným způsobem. Abychom toto omezení překonali, musíme vytvářet vektory založené na **jazykovém modelu**, který je natrénován na velkém korpusu textu a *rozumí*, jak mohou být slova spojována v různých kontextech. Diskuze o kontextuálních vektorech slov přesahuje rámec tohoto tutoriálu, ale vrátíme se k nim při probírání jazykových modelů v další části.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Prohlášení**:  \nTento dokument byl přeložen pomocí služby pro automatický překlad [Co-op Translator](https://github.com/Azure/co-op-translator). Ačkoli se snažíme o přesnost, mějte prosím na paměti, že automatické překlady mohou obsahovat chyby nebo nepřesnosti. Původní dokument v jeho původním jazyce by měl být považován za autoritativní zdroj. Pro důležité informace doporučujeme profesionální lidský překlad. Neodpovídáme za žádná nedorozumění nebo nesprávné interpretace vyplývající z použití tohoto překladu.\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0cb620c6d4b9f7a635928804c26cf22403d89d98d79684e4529119355ee6d5a5"
  },
  "kernel_info": {
   "name": "conda-env-py37_tensorflow-py"
  },
  "kernelspec": {
   "display_name": "py37_tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "coopTranslator": {
   "original_hash": "b859482be7f61d1eadc2c6a2720a37e4",
   "translation_date": "2025-08-29T16:28:35+00:00",
   "source_file": "lessons/5-NLP/14-Embeddings/EmbeddingsTF.ipynb",
   "language_code": "cs"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}