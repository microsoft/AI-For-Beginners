# Vektorizace slov (Embeddings)

## [Kv√≠z p≈ôed p≈ôedn√°≈°kou](https://ff-quizzes.netlify.app/en/ai/quiz/27)

P≈ôi tr√©nov√°n√≠ klasifik√°tor≈Ø zalo≈æen√Ωch na BoW nebo TF/IDF jsme pracovali s vysoko-dimenzion√°ln√≠mi vektory bag-of-words o d√©lce `vocab_size` a explicitnƒõ jsme p≈ôev√°dƒõli n√≠zko-dimenzion√°ln√≠ poziƒçn√≠ reprezentace na ≈ô√≠dk√© one-hot reprezentace. Tato one-hot reprezentace v≈°ak nen√≠ pamƒõ≈•ovƒõ efektivn√≠. Nav√≠c je s ka≈æd√Ωm slovem zach√°zeno nez√°visle na ostatn√≠ch, co≈æ znamen√°, ≈æe one-hot k√≥dovan√© vektory nevyjad≈ôuj√≠ ≈æ√°dnou s√©mantickou podobnost mezi slovy.

My≈°lenka **vektorizace slov (embedding)** spoƒç√≠v√° v reprezentaci slov pomoc√≠ n√≠zko-dimenzion√°ln√≠ch hust√Ωch vektor≈Ø, kter√© nƒõjak√Ωm zp≈Øsobem odr√°≈æej√≠ s√©mantick√Ω v√Ωznam slova. Pozdƒõji si uk√°≈æeme, jak vytvo≈ôit smyslupln√© vektorizace slov, ale prozat√≠m si je p≈ôedstavme jako zp≈Øsob, jak sn√≠≈æit dimenzionalitu slovn√≠ho vektoru.

Vrstva embedding tedy p≈ôijme slovo jako vstup a vytvo≈ô√≠ v√Ωstupn√≠ vektor o specifikovan√© velikosti `embedding_size`. V jist√©m smyslu je to velmi podobn√© vrstvƒõ `Linear`, ale m√≠sto toho, aby p≈ôij√≠mala one-hot k√≥dovan√Ω vektor, dok√°≈æe p≈ôijmout ƒç√≠slo slova jako vstup, co≈æ n√°m umo≈æ≈àuje vyhnout se vytv√°≈ôen√≠ velk√Ωch one-hot k√≥dovan√Ωch vektor≈Ø.

Pou≈æit√≠m vrstvy embedding jako prvn√≠ vrstvy v na≈°√≠ klasifikaƒçn√≠ s√≠ti m≈Ø≈æeme p≈ôej√≠t od modelu bag-of-words k modelu **embedding bag**, kde nejprve p≈ôevedeme ka≈æd√© slovo v textu na odpov√≠daj√≠c√≠ embedding a pot√© vypoƒç√≠t√°me nƒõjakou agregaƒçn√≠ funkci nad v≈°emi tƒõmito embeddingy, nap≈ô√≠klad `sum`, `average` nebo `max`.

![Obr√°zek zn√°zor≈àuj√≠c√≠ klasifik√°tor s embeddingy pro pƒõt slov v sekvenci.](../../../../../translated_images/cs/embedding-classifier-example.b77f021a7ee67eee.webp)

> Obr√°zek od autora

## ‚úçÔ∏è Cviƒçen√≠: Vektorizace slov

Pokraƒçujte ve studiu v n√°sleduj√≠c√≠ch notebooc√≠ch:
* [Embeddings s PyTorch](EmbeddingsPyTorch.ipynb)
* [Embeddings s TensorFlow](EmbeddingsTF.ipynb)

## S√©mantick√© vektorizace: Word2Vec

Zat√≠mco vrstva embedding se nauƒçila mapovat slova na vektorovou reprezentaci, tato reprezentace nutnƒõ neobsahuje mnoho s√©mantick√©ho v√Ωznamu. Bylo by u≈æiteƒçn√© nauƒçit se vektorovou reprezentaci tak, aby podobn√° slova nebo synonyma odpov√≠dala vektor≈Øm, kter√© jsou si bl√≠zk√© z hlediska nƒõjak√© vektorov√© vzd√°lenosti (nap≈ô. Euklidovsk√© vzd√°lenosti).

K tomu je t≈ôeba p≈ôedtr√©novat model embedding na velk√© kolekci text≈Ø specifick√Ωm zp≈Øsobem. Jedn√≠m ze zp≈Øsob≈Ø tr√©nov√°n√≠ s√©mantick√Ωch embedding≈Ø je metoda [Word2Vec](https://en.wikipedia.org/wiki/Word2vec). Ta je zalo≈æena na dvou hlavn√≠ch architektur√°ch, kter√© se pou≈æ√≠vaj√≠ k vytvo≈ôen√≠ distribuovan√© reprezentace slov:

 - **Continuous bag-of-words** (CBoW) ‚Äî v t√©to architektu≈ôe tr√©nujeme model, aby p≈ôedpovƒõdƒõl slovo na z√°kladƒõ okoln√≠ho kontextu. Pro dan√Ω n-gram $(W_{-2},W_{-1},W_0,W_1,W_2)$ je c√≠lem modelu p≈ôedpovƒõdƒõt $W_0$ na z√°kladƒõ $(W_{-2},W_{-1},W_1,W_2)$.
 - **Continuous skip-gram** je opakem CBoW. Model pou≈æ√≠v√° okoln√≠ okno kontextov√Ωch slov k p≈ôedpovƒõdi aktu√°ln√≠ho slova.

CBoW je rychlej≈°√≠, zat√≠mco skip-gram je pomalej≈°√≠, ale l√©pe reprezentuje m√©nƒõ ƒçast√° slova.

![Obr√°zek zn√°zor≈àuj√≠c√≠ algoritmy CBoW a Skip-Gram pro p≈ôevod slov na vektory.](../../../../../translated_images/cs/example-algorithms-for-converting-words-to-vectors.fbe9207a726922f6.webp)

> Obr√°zek z [tohoto ƒçl√°nku](https://arxiv.org/pdf/1301.3781.pdf)

P≈ôedtr√©novan√© embeddingy Word2Vec (stejnƒõ jako jin√© podobn√© modely, nap≈ô√≠klad GloVe) lze tak√© pou≈æ√≠t m√≠sto vrstvy embedding v neuronov√Ωch s√≠t√≠ch. Mus√≠me se v≈°ak vypo≈ô√°dat se slovn√≠ky, proto≈æe slovn√≠k pou≈æit√Ω pro p≈ôedtr√©nov√°n√≠ Word2Vec/GloVe se pravdƒõpodobnƒõ li≈°√≠ od slovn√≠ku v na≈°em textov√©m korpusu. Pod√≠vejte se na v√Ω≈°e uveden√© notebooky, abyste zjistili, jak tento probl√©m vy≈ôe≈°it.

## Kontextov√© vektorizace

Jedn√≠m z kl√≠ƒçov√Ωch omezen√≠ tradiƒçn√≠ch p≈ôedtr√©novan√Ωch embedding≈Ø, jako je Word2Vec, je probl√©m disambiguace v√Ωznamu slov. Zat√≠mco p≈ôedtr√©novan√© embeddingy dok√°≈æou zachytit ƒç√°st v√Ωznamu slov v kontextu, ka≈æd√Ω mo≈æn√Ω v√Ωznam slova je zak√≥dov√°n do stejn√©ho embeddingu. To m≈Ø≈æe zp≈Øsobit probl√©my v n√°sledn√Ωch modelech, proto≈æe mnoho slov, nap≈ô√≠klad slovo 'play', m√° r≈Øzn√© v√Ωznamy v z√°vislosti na kontextu, ve kter√©m jsou pou≈æity.

Nap≈ô√≠klad slovo 'play' m√° v tƒõchto dvou vƒõt√°ch zcela odli≈°n√Ω v√Ωznam:

- ≈†el jsem na **hru** do divadla.
- John si chce **hr√°t** se sv√Ωmi p≈ô√°teli.

V√Ω≈°e zm√≠nƒõn√© p≈ôedtr√©novan√© embeddingy reprezentuj√≠ oba tyto v√Ωznamy slova 'play' stejn√Ωm embeddingem. Abychom toto omezen√≠ p≈ôekonali, mus√≠me vytv√°≈ôet embeddingy zalo≈æen√© na **jazykov√©m modelu**, kter√Ω je tr√©nov√°n na velk√©m korpusu textu a *v√≠*, jak lze slova skl√°dat v r≈Øzn√Ωch kontextech. Diskuze o kontextov√Ωch embeddingech je mimo rozsah tohoto tutori√°lu, ale vr√°t√≠me se k nim p≈ôi prob√≠r√°n√≠ jazykov√Ωch model≈Ø pozdƒõji v kurzu.

## Z√°vƒõr

V t√©to lekci jste se nauƒçili, jak vytvo≈ôit a pou≈æ√≠vat vrstvy embedding v TensorFlow a PyTorch, aby l√©pe odr√°≈æely s√©mantick√© v√Ωznamy slov.

## üöÄ V√Ωzva

Word2Vec byl pou≈æit pro nƒõkter√© zaj√≠mav√© aplikace, vƒçetnƒõ generov√°n√≠ text≈Ø p√≠sn√≠ a poezie. Pod√≠vejte se na [tento ƒçl√°nek](https://www.politetype.com/blog/word2vec-color-poems), kter√Ω popisuje, jak autor pou≈æil Word2Vec k generov√°n√≠ poezie. Pod√≠vejte se tak√© na [toto video od Dana Shiffmanna](https://www.youtube.com/watch?v=LSS_bos_TPI&ab_channel=TheCodingTrain), kde najdete jin√© vysvƒõtlen√≠ t√©to techniky. Pot√© zkuste tyto techniky aplikovat na sv≈Øj vlastn√≠ textov√Ω korpus, nap≈ô√≠klad z Kaggle.

## [Kv√≠z po p≈ôedn√°≈°ce](https://ff-quizzes.netlify.app/en/ai/quiz/28)

## Opakov√°n√≠ a samostudium

P≈ôeƒçtƒõte si tento ƒçl√°nek o Word2Vec: [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf)

## [√ökol: Notebooks](assignment.md)

---

