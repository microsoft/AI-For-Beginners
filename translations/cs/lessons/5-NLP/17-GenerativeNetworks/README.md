# GenerativnÃ­ sÃ­tÄ›

## [KvÃ­z pÅ™ed pÅ™ednÃ¡Å¡kou](https://ff-quizzes.netlify.app/en/ai/quiz/33)

RekurentnÃ­ neuronovÃ© sÃ­tÄ› (RNN) a jejich varianty s brÃ¡nÄ›nÃ½mi buÅˆkami, jako jsou Long Short Term Memory Cells (LSTM) a Gated Recurrent Units (GRU), poskytujÃ­ mechanismus pro modelovÃ¡nÃ­ jazyka, protoÅ¾e se dokÃ¡Å¾ou nauÄit poÅ™adÃ­ slov a pÅ™edpovÃ­dat dalÅ¡Ã­ slovo v sekvenci. To nÃ¡m umoÅ¾Åˆuje vyuÅ¾Ã­vat RNN pro **generativnÃ­ Ãºlohy**, jako je bÄ›Å¾nÃ¡ generace textu, strojovÃ½ pÅ™eklad nebo dokonce popisovÃ¡nÃ­ obrÃ¡zkÅ¯.

> âœ… Zamyslete se nad vÅ¡emi situacemi, kdy jste vyuÅ¾ili generativnÃ­ Ãºlohy, napÅ™Ã­klad pÅ™i automatickÃ©m doplÅˆovÃ¡nÃ­ textu bÄ›hem psanÃ­. Prozkoumejte svÃ© oblÃ­benÃ© aplikace a zjistÄ›te, zda vyuÅ¾Ã­vajÃ­ RNN.

V architektuÅ™e RNN, kterou jsme probÃ­rali v pÅ™edchozÃ­ kapitole, kaÅ¾dÃ¡ jednotka RNN produkovala jako vÃ½stup dalÅ¡Ã­ skrytÃ½ stav. MÅ¯Å¾eme vÅ¡ak takÃ© pÅ™idat dalÅ¡Ã­ vÃ½stup ke kaÅ¾dÃ© rekurentnÃ­ jednotce, coÅ¾ nÃ¡m umoÅ¾nÃ­ generovat **sekvenci** (kterÃ¡ mÃ¡ stejnou dÃ©lku jako pÅ¯vodnÃ­ sekvence). NavÃ­c mÅ¯Å¾eme pouÅ¾Ã­t RNN jednotky, kterÃ© nepÅ™ijÃ­majÃ­ vstup na kaÅ¾dÃ©m kroku, ale pouze poÄÃ¡teÄnÃ­ stavovÃ½ vektor, a potÃ© generujÃ­ sekvenci vÃ½stupÅ¯.

To umoÅ¾Åˆuje rÅ¯znÃ© neuronovÃ© architektury, jak je znÃ¡zornÄ›no na obrÃ¡zku nÃ­Å¾e:

![ObrÃ¡zek zobrazujÃ­cÃ­ bÄ›Å¾nÃ© vzory rekurentnÃ­ch neuronovÃ½ch sÃ­tÃ­.](../../../../../translated_images/cs/unreasonable-effectiveness-of-rnn.541ead816778f42d.webp)

> ObrÃ¡zek z blogovÃ©ho pÅ™Ã­spÄ›vku [Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) od [Andreje Karpatyho](http://karpathy.github.io/)

* **One-to-one** je tradiÄnÃ­ neuronovÃ¡ sÃ­Å¥ s jednÃ­m vstupem a jednÃ­m vÃ½stupem.
* **One-to-many** je generativnÃ­ architektura, kterÃ¡ pÅ™ijÃ­mÃ¡ jednu vstupnÃ­ hodnotu a generuje sekvenci vÃ½stupnÃ­ch hodnot. NapÅ™Ã­klad pokud chceme trÃ©novat sÃ­Å¥ pro **popisovÃ¡nÃ­ obrÃ¡zkÅ¯**, kterÃ¡ by vytvoÅ™ila textovÃ½ popis obrÃ¡zku, mÅ¯Å¾eme pouÅ¾Ã­t obrÃ¡zek jako vstup, zpracovat jej pomocÃ­ CNN pro zÃ­skÃ¡nÃ­ skrytÃ©ho stavu a potÃ© nechat rekurentnÃ­ Å™etÄ›zec generovat popis slovo po slovu.
* **Many-to-one** odpovÃ­dÃ¡ architekturÃ¡m RNN, kterÃ© jsme popsali v pÅ™edchozÃ­ kapitole, napÅ™Ã­klad klasifikace textu.
* **Many-to-many**, nebo **sekvence na sekvenci**, odpovÃ­dÃ¡ ÃºlohÃ¡m, jako je **strojovÃ½ pÅ™eklad**, kde prvnÃ­ RNN shromÃ¡Å¾dÃ­ vÅ¡echny informace ze vstupnÃ­ sekvence do skrytÃ©ho stavu a dalÅ¡Ã­ Å™etÄ›zec RNN tento stav rozvine do vÃ½stupnÃ­ sekvence.

V tÃ©to kapitole se zamÄ›Å™Ã­me na jednoduchÃ© generativnÃ­ modely, kterÃ© nÃ¡m pomohou generovat text. Pro zjednoduÅ¡enÃ­ pouÅ¾ijeme tokenizaci na Ãºrovni znakÅ¯.

Tuto RNN budeme trÃ©novat na generovÃ¡nÃ­ textu krok za krokem. Na kaÅ¾dÃ©m kroku vezmeme sekvenci znakÅ¯ o dÃ©lce `nchars` a poÅ¾Ã¡dÃ¡me sÃ­Å¥, aby pro kaÅ¾dÃ½ vstupnÃ­ znak vygenerovala dalÅ¡Ã­ vÃ½stupnÃ­ znak:

![ObrÃ¡zek zobrazujÃ­cÃ­ pÅ™Ã­klad generovÃ¡nÃ­ slova 'HELLO' pomocÃ­ RNN.](../../../../../translated_images/cs/rnn-generate.56c54afb52f9781d.webp)

PÅ™i generovÃ¡nÃ­ textu (bÄ›hem inference) zaÄÃ­nÃ¡me s nÄ›jakÃ½m **podnÄ›tem**, kterÃ½ je pÅ™edÃ¡n pÅ™es RNN buÅˆky pro vytvoÅ™enÃ­ mezistavu, a potÃ© zaÄÃ­nÃ¡ samotnÃ© generovÃ¡nÃ­. Generujeme jeden znak po druhÃ©m a pÅ™edÃ¡vÃ¡me stav a vygenerovanÃ½ znak dalÅ¡Ã­ RNN buÅˆce, aby vygenerovala dalÅ¡Ã­ znak, dokud nevygenerujeme dostatek znakÅ¯.

<img src="../../../../../translated_images/cs/rnn-generate-inf.5168dc65e0370eea.webp" width="60%"/>

> ObrÃ¡zek od autora

## âœï¸ CviÄenÃ­: GenerativnÃ­ sÃ­tÄ›

PokraÄujte ve studiu v nÃ¡sledujÃ­cÃ­ch noteboocÃ­ch:

* [GenerativnÃ­ sÃ­tÄ› s PyTorch](GenerativePyTorch.ipynb)
* [GenerativnÃ­ sÃ­tÄ› s TensorFlow](GenerativeTF.ipynb)

## MÄ›kkÃ© generovÃ¡nÃ­ textu a teplota

VÃ½stup kaÅ¾dÃ© RNN buÅˆky je pravdÄ›podobnostnÃ­ rozdÄ›lenÃ­ znakÅ¯. Pokud bychom vÅ¾dy vybrali znak s nejvyÅ¡Å¡Ã­ pravdÄ›podobnostÃ­ jako dalÅ¡Ã­ znak v generovanÃ©m textu, text by se Äasto mohl "zacyklit" mezi stejnÃ½mi sekvencemi znakÅ¯ znovu a znovu, jako v tomto pÅ™Ã­kladu:

```
today of the second the company and a second the company ...
```

Pokud se vÅ¡ak podÃ­vÃ¡me na pravdÄ›podobnostnÃ­ rozdÄ›lenÃ­ pro dalÅ¡Ã­ znak, mÅ¯Å¾e se stÃ¡t, Å¾e rozdÃ­l mezi nÄ›kolika nejvyÅ¡Å¡Ã­mi pravdÄ›podobnostmi nenÃ­ velkÃ½, napÅ™. jeden znak mÅ¯Å¾e mÃ­t pravdÄ›podobnost 0,2, jinÃ½ 0,19 atd. NapÅ™Ã­klad pÅ™i hledÃ¡nÃ­ dalÅ¡Ã­ho znaku v sekvenci '*play*' mÅ¯Å¾e bÃ½t dalÅ¡Ã­m znakem stejnÄ› dobÅ™e mezera nebo **e** (jako ve slovÄ› *player*).

To nÃ¡s vede k zÃ¡vÄ›ru, Å¾e nenÃ­ vÅ¾dy "spravedlivÃ©" vybÃ­rat znak s nejvyÅ¡Å¡Ã­ pravdÄ›podobnostÃ­, protoÅ¾e vÃ½bÄ›r druhÃ©ho nejvyÅ¡Å¡Ã­ho mÅ¯Å¾e stÃ¡le vÃ©st k smysluplnÃ©mu textu. Je rozumnÄ›jÅ¡Ã­ **vzorkovat** znaky z pravdÄ›podobnostnÃ­ho rozdÄ›lenÃ­ danÃ©ho vÃ½stupem sÃ­tÄ›. MÅ¯Å¾eme takÃ© pouÅ¾Ã­t parametr **teplota**, kterÃ½ rozprostÅ™e pravdÄ›podobnostnÃ­ rozdÄ›lenÃ­, pokud chceme pÅ™idat vÃ­ce nÃ¡hodnosti, nebo jej udÄ›lÃ¡ strmÄ›jÅ¡Ã­m, pokud chceme vÃ­ce preferovat znaky s nejvyÅ¡Å¡Ã­ pravdÄ›podobnostÃ­.

Prozkoumejte, jak je toto mÄ›kkÃ© generovÃ¡nÃ­ textu implementovÃ¡no v noteboocÃ­ch uvedenÃ½ch vÃ½Å¡e.

## ZÃ¡vÄ›r

AÄkoli generovÃ¡nÃ­ textu mÅ¯Å¾e bÃ½t uÅ¾iteÄnÃ© samo o sobÄ›, hlavnÃ­ pÅ™Ã­nosy pÅ™ichÃ¡zejÃ­ z moÅ¾nosti generovat text pomocÃ­ RNN z nÄ›jakÃ©ho poÄÃ¡teÄnÃ­ho vektorovÃ©ho znaku. NapÅ™Ã­klad generovÃ¡nÃ­ textu se pouÅ¾Ã­vÃ¡ jako souÄÃ¡st strojovÃ©ho pÅ™ekladu (sekvence na sekvenci, v tomto pÅ™Ã­padÄ› je stavovÃ½ vektor z *enkodÃ©ru* pouÅ¾it pro generovÃ¡nÃ­ nebo *dekÃ³dovÃ¡nÃ­* pÅ™eloÅ¾enÃ© zprÃ¡vy) nebo pro generovÃ¡nÃ­ textovÃ©ho popisu obrÃ¡zku (v tomto pÅ™Ã­padÄ› pochÃ¡zÃ­ vektor znakÅ¯ z extraktoru CNN).

## ğŸš€ VÃ½zva

Absolvujte nÄ›kterÃ© lekce na Microsoft Learn na toto tÃ©ma:

* GenerovÃ¡nÃ­ textu s [PyTorch](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-pytorch/6-generative-networks/?WT.mc_id=academic-77998-cacaste)/[TensorFlow](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-tensorflow/5-generative-networks/?WT.mc_id=academic-77998-cacaste)

## [KvÃ­z po pÅ™ednÃ¡Å¡ce](https://ff-quizzes.netlify.app/en/ai/quiz/34)

## PÅ™ehled a samostudium

Zde jsou nÄ›kterÃ© ÄlÃ¡nky pro rozÅ¡Ã­Å™enÃ­ vaÅ¡ich znalostÃ­:

* RÅ¯znÃ© pÅ™Ã­stupy ke generovÃ¡nÃ­ textu s MarkovovÃ½m Å™etÄ›zcem, LSTM a GPT-2: [blogovÃ½ pÅ™Ã­spÄ›vek](https://towardsdatascience.com/text-generation-gpt-2-lstm-markov-chain-9ea371820e1e)
* UkÃ¡zka generovÃ¡nÃ­ textu v [dokumentaci Keras](https://keras.io/examples/generative/lstm_character_level_text_generation/)

## [Ãškol](lab/README.md)

VidÄ›li jsme, jak generovat text znak po znaku. V laboratoÅ™i budete zkoumat generovÃ¡nÃ­ textu na Ãºrovni slov.

---

