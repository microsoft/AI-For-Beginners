<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "58bf4adb210aab53e8f78c8082040e7c",
  "translation_date": "2025-08-25T21:32:06+00:00",
  "source_file": "lessons/5-NLP/16-RNN/README.md",
  "language_code": "cs"
}
-->
# RekurentnÃ­ neuronovÃ© sÃ­tÄ›

## [KvÃ­z pÅ™ed pÅ™ednÃ¡Å¡kou](https://ff-quizzes.netlify.app/en/ai/quiz/31)

V pÅ™edchozÃ­ch sekcÃ­ch jsme pouÅ¾Ã­vali bohatÃ© sÃ©mantickÃ© reprezentace textu a jednoduchÃ½ lineÃ¡rnÃ­ klasifikÃ¡tor nad embeddingy. Tato architektura zachycuje agregovanÃ½ vÃ½znam slov ve vÄ›tÄ›, ale nezohledÅˆuje **poÅ™adÃ­** slov, protoÅ¾e operace agregace nad embeddingy tuto informaci z pÅ¯vodnÃ­ho textu odstranila. ProtoÅ¾e tyto modely nedokÃ¡Å¾ou modelovat poÅ™adÃ­ slov, nemohou Å™eÅ¡it sloÅ¾itÄ›jÅ¡Ã­ nebo nejednoznaÄnÃ© Ãºkoly, jako je generovÃ¡nÃ­ textu nebo odpovÃ­dÃ¡nÃ­ na otÃ¡zky.

Abychom zachytili vÃ½znam sekvence textu, musÃ­me pouÅ¾Ã­t jinou architekturu neuronovÃ© sÃ­tÄ›, kterÃ¡ se nazÃ½vÃ¡ **rekurentnÃ­ neuronovÃ¡ sÃ­Å¥** (RNN). V RNN prochÃ¡zÃ­me vÄ›tou sÃ­tÃ­ po jednom symbolu a sÃ­Å¥ produkuje urÄitÃ½ **stav**, kterÃ½ nÃ¡slednÄ› pÅ™edÃ¡vÃ¡me zpÄ›t do sÃ­tÄ› spolu s dalÅ¡Ã­m symbolem.

![RNN](../../../../../translated_images/rnn.27f5c29c53d727b546ad3961637a267f0fe9ec5ab01f2a26a853c92fcefbb574.cs.png)

> ObrÃ¡zek od autora

Pro danou vstupnÃ­ sekvenci tokenÅ¯ X<sub>0</sub>,...,X<sub>n</sub> vytvoÅ™Ã­ RNN sekvenci blokÅ¯ neuronovÃ© sÃ­tÄ› a trÃ©nuje tuto sekvenci end-to-end pomocÃ­ zpÄ›tnÃ© propagace. KaÅ¾dÃ½ blok sÃ­tÄ› pÅ™ijÃ­mÃ¡ dvojici (X<sub>i</sub>,S<sub>i</sub>) jako vstup a produkuje S<sub>i+1</sub> jako vÃ½sledek. KoneÄnÃ½ stav S<sub>n</sub> (nebo vÃ½stup Y<sub>n</sub>) jde do lineÃ¡rnÃ­ho klasifikÃ¡toru, kterÃ½ produkuje vÃ½sledek. VÅ¡echny bloky sÃ­tÄ› sdÃ­lejÃ­ stejnÃ© vÃ¡hy a jsou trÃ©novÃ¡ny end-to-end bÄ›hem jednÃ© zpÄ›tnÃ© propagace.

ProtoÅ¾e stavovÃ© vektory S<sub>0</sub>,...,S<sub>n</sub> prochÃ¡zejÃ­ sÃ­tÃ­, je schopna se nauÄit sekvenÄnÃ­ zÃ¡vislosti mezi slovy. NapÅ™Ã­klad kdyÅ¾ se nÄ›kde v sekvenci objevÃ­ slovo *not*, mÅ¯Å¾e se nauÄit negovat urÄitÃ© prvky ve stavovÃ©m vektoru, coÅ¾ vede k negaci.

> âœ… ProtoÅ¾e vÃ¡hy vÅ¡ech blokÅ¯ RNN na obrÃ¡zku vÃ½Å¡e jsou sdÃ­lenÃ©, stejnÃ½ obrÃ¡zek lze reprezentovat jako jeden blok (vpravo) s rekurentnÃ­ zpÄ›tnou vazbou, kterÃ¡ pÅ™edÃ¡vÃ¡ vÃ½stupnÃ­ stav sÃ­tÄ› zpÄ›t na vstup.

## Anatomie RNN buÅˆky

PodÃ­vejme se, jak je organizovÃ¡na jednoduchÃ¡ RNN buÅˆka. PÅ™ijÃ­mÃ¡ pÅ™edchozÃ­ stav S<sub>i-1</sub> a aktuÃ¡lnÃ­ symbol X<sub>i</sub> jako vstupy a musÃ­ produkovat vÃ½stupnÃ­ stav S<sub>i</sub> (a nÄ›kdy nÃ¡s takÃ© zajÃ­mÃ¡ jinÃ½ vÃ½stup Y<sub>i</sub>, jako v pÅ™Ã­padÄ› generativnÃ­ch sÃ­tÃ­).

JednoduchÃ¡ RNN buÅˆka obsahuje dvÄ› vÃ¡hovÃ© matice: jedna transformuje vstupnÃ­ symbol (nazvÄ›me ji W) a druhÃ¡ transformuje vstupnÃ­ stav (H). V tomto pÅ™Ã­padÄ› je vÃ½stup sÃ­tÄ› vypoÄÃ­tÃ¡n jako Ïƒ(WÃ—X<sub>i</sub>+HÃ—S<sub>i-1</sub>+b), kde Ïƒ je aktivaÄnÃ­ funkce a b je dodateÄnÃ½ bias.

<img alt="Anatomie RNN buÅˆky" src="images/rnn-anatomy.png" width="50%"/>

> ObrÃ¡zek od autora

V mnoha pÅ™Ã­padech jsou vstupnÃ­ tokeny pÅ™ed vstupem do RNN prohnÃ¡ny embedding vrstvou, aby se snÃ­Å¾ila dimenzionalita. V tomto pÅ™Ã­padÄ›, pokud je dimenze vstupnÃ­ch vektorÅ¯ *emb_size* a stavovÃ©ho vektoru *hid_size*, velikost W je *emb_size*Ã—*hid_size* a velikost H je *hid_size*Ã—*hid_size*.

## Long Short Term Memory (LSTM)

JednÃ­m z hlavnÃ­ch problÃ©mÅ¯ klasickÃ½ch RNN je tzv. problÃ©m **mizÃ­cÃ­ch gradientÅ¯**. ProtoÅ¾e RNN jsou trÃ©novÃ¡ny end-to-end bÄ›hem jednÃ© zpÄ›tnÃ© propagace, majÃ­ potÃ­Å¾e s propagacÃ­ chyby do prvnÃ­ch vrstev sÃ­tÄ›, a tÃ­m pÃ¡dem se sÃ­Å¥ nemÅ¯Å¾e nauÄit vztahy mezi vzdÃ¡lenÃ½mi tokeny. JednÃ­m ze zpÅ¯sobÅ¯, jak tento problÃ©m obejÃ­t, je zavedenÃ­ **explicitnÃ­ho Å™Ã­zenÃ­ stavu** pomocÃ­ tzv. **bran**. ExistujÃ­ dvÄ› znÃ¡mÃ© architektury tohoto typu: **Long Short Term Memory** (LSTM) a **Gated Relay Unit** (GRU).

![ObrÃ¡zek ukazujÃ­cÃ­ pÅ™Ã­klad buÅˆky LSTM](../../../../../lessons/5-NLP/16-RNN/images/long-short-term-memory-cell.svg)

> Zdroj obrÃ¡zku TBD

LSTM sÃ­Å¥ je organizovÃ¡na podobnÄ› jako RNN, ale existujÃ­ dva stavy, kterÃ© se pÅ™edÃ¡vajÃ­ z vrstvy do vrstvy: aktuÃ¡lnÃ­ stav C a skrytÃ½ vektor H. V kaÅ¾dÃ© jednotce je skrytÃ½ vektor H<sub>i</sub> spojen s vstupem X<sub>i</sub> a spoleÄnÄ› kontrolujÃ­, co se stane se stavem C prostÅ™ednictvÃ­m **bran**. KaÅ¾dÃ¡ brÃ¡na je neuronovÃ¡ sÃ­Å¥ se sigmoidnÃ­ aktivacÃ­ (vÃ½stup v rozmezÃ­ [0,1]), kterou lze chÃ¡pat jako bitovou masku pÅ™i nÃ¡sobenÃ­ stavovÃ½m vektorem. ExistujÃ­ nÃ¡sledujÃ­cÃ­ brÃ¡ny (zleva doprava na obrÃ¡zku vÃ½Å¡e):

* **ZapomÃ­nacÃ­ brÃ¡na** bere skrytÃ½ vektor a urÄuje, kterÃ© komponenty vektoru C je tÅ™eba zapomenout a kterÃ© pÅ™edat dÃ¡l.
* **VstupnÃ­ brÃ¡na** bere urÄitÃ© informace ze vstupnÃ­ho a skrytÃ©ho vektoru a vklÃ¡dÃ¡ je do stavu.
* **VÃ½stupnÃ­ brÃ¡na** transformuje stav pÅ™es lineÃ¡rnÃ­ vrstvu s *tanh* aktivacÃ­ a potÃ© vybÃ­rÃ¡ nÄ›kterÃ© jeho komponenty pomocÃ­ skrytÃ©ho vektoru H<sub>i</sub>, aby vytvoÅ™ila novÃ½ stav C<sub>i+1</sub>.

Komponenty stavu C lze chÃ¡pat jako urÄitÃ© pÅ™Ã­znaky, kterÃ© lze zapÃ­nat a vypÃ­nat. NapÅ™Ã­klad kdyÅ¾ v sekvenci narazÃ­me na jmÃ©no *Alice*, mÅ¯Å¾eme pÅ™edpoklÃ¡dat, Å¾e se jednÃ¡ o Å¾enskou postavu, a nastavit pÅ™Ã­znak ve stavu, Å¾e mÃ¡me ve vÄ›tÄ› Å¾enskÃ© podstatnÃ© jmÃ©no. KdyÅ¾ dÃ¡le narazÃ­me na frÃ¡zi *and Tom*, nastavÃ­me pÅ™Ã­znak, Å¾e mÃ¡me mnoÅ¾nÃ© ÄÃ­slo. ManipulacÃ­ se stavem tak mÅ¯Å¾eme ÃºdajnÄ› sledovat gramatickÃ© vlastnosti ÄÃ¡stÃ­ vÄ›ty.

> âœ… SkvÄ›lÃ½m zdrojem pro pochopenÃ­ vnitÅ™nÃ­ho fungovÃ¡nÃ­ LSTM je tento vÃ½bornÃ½ ÄlÃ¡nek [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) od Christophera Olaha.

## BidirekcionÃ¡lnÃ­ a vÃ­cevrstvÃ© RNN

Diskutovali jsme o rekurentnÃ­ch sÃ­tÃ­ch, kterÃ© fungujÃ­ jednÃ­m smÄ›rem, od zaÄÃ¡tku sekvence do konce. To vypadÃ¡ pÅ™irozenÄ›, protoÅ¾e to pÅ™ipomÃ­nÃ¡ zpÅ¯sob, jakÃ½m Äteme a poslouchÃ¡me Å™eÄ. NicmÃ©nÄ›, protoÅ¾e v mnoha praktickÃ½ch pÅ™Ã­padech mÃ¡me nÃ¡hodnÃ½ pÅ™Ã­stup ke vstupnÃ­ sekvenci, mÅ¯Å¾e mÃ­t smysl spustit rekurentnÃ­ vÃ½poÄet v obou smÄ›rech. TakovÃ© sÃ­tÄ› se nazÃ½vajÃ­ **bidirekcionÃ¡lnÃ­** RNN. PÅ™i prÃ¡ci s bidirekcionÃ¡lnÃ­ sÃ­tÃ­ bychom potÅ™ebovali dva skrytÃ© stavovÃ© vektory, jeden pro kaÅ¾dÃ½ smÄ›r.

RekurentnÃ­ sÃ­Å¥, aÅ¥ uÅ¾ jednosmÄ›rnÃ¡ nebo bidirekcionÃ¡lnÃ­, zachycuje urÄitÃ© vzory v sekvenci a mÅ¯Å¾e je uloÅ¾it do stavovÃ©ho vektoru nebo pÅ™edat do vÃ½stupu. StejnÄ› jako u konvoluÄnÃ­ch sÃ­tÃ­ mÅ¯Å¾eme na prvnÃ­ vrstvu postavit dalÅ¡Ã­ rekurentnÃ­ vrstvu, kterÃ¡ zachytÃ­ vzory vyÅ¡Å¡Ã­ ÃºrovnÄ› a stavÃ­ na vzorech niÅ¾Å¡Ã­ ÃºrovnÄ› extrahovanÃ½ch prvnÃ­ vrstvou. To nÃ¡s pÅ™ivÃ¡dÃ­ k pojmu **vÃ­cevrstvÃ© RNN**, kterÃ¡ se sklÃ¡dÃ¡ ze dvou nebo vÃ­ce rekurentnÃ­ch sÃ­tÃ­, kde vÃ½stup pÅ™edchozÃ­ vrstvy je pÅ™edÃ¡n jako vstup do dalÅ¡Ã­ vrstvy.

![ObrÃ¡zek ukazujÃ­cÃ­ vÃ­cevrstvou LSTM RNN](../../../../../translated_images/multi-layer-lstm.dd975e29bb2a59fe58b429db833932d734c81f211cad2783797a9608984acb8c.cs.jpg)

*ObrÃ¡zek z [tohoto skvÄ›lÃ©ho ÄlÃ¡nku](https://towardsdatascience.com/from-a-lstm-cell-to-a-multilayer-lstm-network-with-pytorch-2899eb5696f3) od Fernanda LÃ³peze*

## âœï¸ CviÄenÃ­: Embeddingy

PokraÄujte ve studiu v nÃ¡sledujÃ­cÃ­ch noteboocÃ­ch:

* [RNNs s PyTorch](../../../../../lessons/5-NLP/16-RNN/RNNPyTorch.ipynb)
* [RNNs s TensorFlow](../../../../../lessons/5-NLP/16-RNN/RNNTF.ipynb)

## ZÃ¡vÄ›r

V tÃ©to jednotce jsme vidÄ›li, Å¾e RNN lze pouÅ¾Ã­t pro klasifikaci sekvencÃ­, ale ve skuteÄnosti zvlÃ¡dnou mnohem vÃ­ce ÃºkolÅ¯, jako je generovÃ¡nÃ­ textu, strojovÃ½ pÅ™eklad a dalÅ¡Ã­. TÄ›mto ÃºkolÅ¯m se budeme vÄ›novat v dalÅ¡Ã­ jednotce.

## ğŸš€ VÃ½zva

ProjdÄ›te si literaturu o LSTM a zvaÅ¾te jejich aplikace:

- [Grid Long Short-Term Memory](https://arxiv.org/pdf/1507.01526v1.pdf)
- [Show, Attend and Tell: Neural Image Caption
Generation with Visual Attention](https://arxiv.org/pdf/1502.03044v2.pdf)

## [KvÃ­z po pÅ™ednÃ¡Å¡ce](https://ff-quizzes.netlify.app/en/ai/quiz/32)

## Recenze a samostudium

- [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) od Christophera Olaha.

## [Ãškol: Notebooks](assignment.md)

**ProhlÃ¡Å¡enÃ­:**  
Tento dokument byl pÅ™eloÅ¾en pomocÃ­ sluÅ¾by pro automatickÃ½ pÅ™eklad [Co-op Translator](https://github.com/Azure/co-op-translator). AÄkoli se snaÅ¾Ã­me o pÅ™esnost, mÄ›jte prosÃ­m na pamÄ›ti, Å¾e automatickÃ© pÅ™eklady mohou obsahovat chyby nebo nepÅ™esnosti. PÅ¯vodnÃ­ dokument v jeho pÅ¯vodnÃ­m jazyce by mÄ›l bÃ½t povaÅ¾ovÃ¡n za autoritativnÃ­ zdroj. Pro dÅ¯leÅ¾itÃ© informace se doporuÄuje profesionÃ¡lnÃ­ lidskÃ½ pÅ™eklad. NeodpovÃ­dÃ¡me za Å¾Ã¡dnÃ¡ nedorozumÄ›nÃ­ nebo nesprÃ¡vnÃ© interpretace vyplÃ½vajÃ­cÃ­ z pouÅ¾itÃ­ tohoto pÅ™ekladu.