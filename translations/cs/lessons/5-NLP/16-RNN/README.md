# RekurentnÃ­ neuronovÃ© sÃ­tÄ›

## [KvÃ­z pÅ™ed pÅ™ednÃ¡Å¡kou](https://ff-quizzes.netlify.app/en/ai/quiz/31)

V pÅ™edchozÃ­ch sekcÃ­ch jsme pouÅ¾Ã­vali bohatÃ© sÃ©mantickÃ© reprezentace textu a jednoduchÃ½ lineÃ¡rnÃ­ klasifikÃ¡tor nad embeddingy. Tato architektura zachycuje agregovanÃ½ vÃ½znam slov ve vÄ›tÄ›, ale nezohledÅˆuje **poÅ™adÃ­** slov, protoÅ¾e operace agregace nad embeddingy tuto informaci z pÅ¯vodnÃ­ho textu odstranila. KvÅ¯li tomu, Å¾e tyto modely nedokÃ¡Å¾ou modelovat poÅ™adÃ­ slov, nejsou schopny Å™eÅ¡it sloÅ¾itÄ›jÅ¡Ã­ nebo nejednoznaÄnÃ© Ãºkoly, jako je generovÃ¡nÃ­ textu nebo odpovÃ­dÃ¡nÃ­ na otÃ¡zky.

Abychom zachytili vÃ½znam textovÃ© sekvence, musÃ­me pouÅ¾Ã­t jinou architekturu neuronovÃ© sÃ­tÄ›, kterÃ¡ se nazÃ½vÃ¡ **rekurentnÃ­ neuronovÃ¡ sÃ­Å¥** (RNN). V RNN prochÃ¡zÃ­ vÄ›ta sÃ­tÃ­ jeden symbol po druhÃ©m a sÃ­Å¥ produkuje nÄ›jakÃ½ **stav**, kterÃ½ se potÃ© pÅ™edÃ¡vÃ¡ sÃ­ti spolu s dalÅ¡Ã­m symbolem.

![RNN](../../../../../translated_images/cs/rnn.27f5c29c53d727b5.webp)

> ObrÃ¡zek od autora

PÅ™i danÃ© vstupnÃ­ sekvenci tokenÅ¯ X<sub>0</sub>,...,X<sub>n</sub> vytvoÅ™Ã­ RNN sekvenci blokÅ¯ neuronovÃ© sÃ­tÄ› a trÃ©nuje tuto sekvenci end-to-end pomocÃ­ zpÄ›tnÃ©ho Å¡Ã­Å™enÃ­. KaÅ¾dÃ½ blok sÃ­tÄ› pÅ™ijÃ­mÃ¡ dvojici (X<sub>i</sub>,S<sub>i</sub>) jako vstup a produkuje S<sub>i+1</sub> jako vÃ½sledek. KoneÄnÃ½ stav S<sub>n</sub> (nebo vÃ½stup Y<sub>n</sub>) se pÅ™edÃ¡vÃ¡ lineÃ¡rnÃ­mu klasifikÃ¡toru, kterÃ½ produkuje vÃ½sledek. VÅ¡echny bloky sÃ­tÄ› sdÃ­lejÃ­ stejnÃ© vÃ¡hy a jsou trÃ©novÃ¡ny end-to-end pomocÃ­ jednoho prÅ¯chodu zpÄ›tnÃ©ho Å¡Ã­Å™enÃ­.

ProtoÅ¾e vektorovÃ© stavy S<sub>0</sub>,...,S<sub>n</sub> prochÃ¡zejÃ­ sÃ­tÃ­, je schopna se nauÄit sekvenÄnÃ­ zÃ¡vislosti mezi slovy. NapÅ™Ã­klad kdyÅ¾ se nÄ›kde v sekvenci objevÃ­ slovo *not*, sÃ­Å¥ se mÅ¯Å¾e nauÄit negovat urÄitÃ© prvky ve stavovÃ©m vektoru, coÅ¾ vede k negaci.

> âœ… ProtoÅ¾e vÃ¡hy vÅ¡ech blokÅ¯ RNN na obrÃ¡zku vÃ½Å¡e jsou sdÃ­lenÃ©, stejnÃ½ obrÃ¡zek lze reprezentovat jako jeden blok (vpravo) s rekurentnÃ­ zpÄ›tnou smyÄkou, kterÃ¡ pÅ™edÃ¡vÃ¡ vÃ½stupnÃ­ stav sÃ­tÄ› zpÄ›t na vstup.

## Anatomie RNN buÅˆky

PodÃ­vejme se, jak je organizovÃ¡na jednoduchÃ¡ RNN buÅˆka. PÅ™ijÃ­mÃ¡ pÅ™edchozÃ­ stav S<sub>i-1</sub> a aktuÃ¡lnÃ­ symbol X<sub>i</sub> jako vstupy a musÃ­ produkovat vÃ½stupnÃ­ stav S<sub>i</sub> (a nÄ›kdy nÃ¡s takÃ© zajÃ­mÃ¡ jinÃ½ vÃ½stup Y<sub>i</sub>, napÅ™Ã­klad u generativnÃ­ch sÃ­tÃ­).

JednoduchÃ¡ RNN buÅˆka mÃ¡ uvnitÅ™ dvÄ› vÃ¡hovÃ© matice: jedna transformuje vstupnÃ­ symbol (nazvÄ›me ji W) a druhÃ¡ transformuje vstupnÃ­ stav (H). V tomto pÅ™Ã­padÄ› se vÃ½stup sÃ­tÄ› vypoÄÃ­tÃ¡ jako &sigma;(W&times;X<sub>i</sub>+H&times;S<sub>i-1</sub>+b), kde &sigma; je aktivaÄnÃ­ funkce a b je dodateÄnÃ¡ bias.

<img alt="Anatomie RNN buÅˆky" src="../../../../../translated_images/cs/rnn-anatomy.79ee3f3920b3294b.webp" width="50%"/>

> ObrÃ¡zek od autora

V mnoha pÅ™Ã­padech jsou vstupnÃ­ tokeny pÅ™ed vstupem do RNN pÅ™edÃ¡ny pÅ™es embeddingovou vrstvu, aby se snÃ­Å¾ila dimenzionalita. V tomto pÅ™Ã­padÄ›, pokud je dimenze vstupnÃ­ch vektorÅ¯ *emb_size* a stavovÃ©ho vektoru *hid_size*, velikost W je *emb_size*&times;*hid_size* a velikost H je *hid_size*&times;*hid_size*.

## Long Short Term Memory (LSTM)

JednÃ­m z hlavnÃ­ch problÃ©mÅ¯ klasickÃ½ch RNN je tzv. **problÃ©m mizejÃ­cÃ­ch gradientÅ¯**. ProtoÅ¾e RNN jsou trÃ©novÃ¡ny end-to-end v jednom prÅ¯chodu zpÄ›tnÃ©ho Å¡Ã­Å™enÃ­, majÃ­ potÃ­Å¾e s propagacÃ­ chyby do prvnÃ­ch vrstev sÃ­tÄ›, a tÃ­m pÃ¡dem se sÃ­Å¥ nemÅ¯Å¾e nauÄit vztahy mezi vzdÃ¡lenÃ½mi tokeny. JednÃ­m ze zpÅ¯sobÅ¯, jak tento problÃ©m obejÃ­t, je zavÃ©st **explicitnÃ­ sprÃ¡vu stavu** pomocÃ­ tzv. **bran**. ExistujÃ­ dvÄ› znÃ¡mÃ© architektury tohoto typu: **Long Short Term Memory** (LSTM) a **Gated Relay Unit** (GRU).

![ObrÃ¡zek ukazujÃ­cÃ­ pÅ™Ã­klad buÅˆky LSTM](../../../../../lessons/5-NLP/16-RNN/images/long-short-term-memory-cell.svg)

> Zdroj obrÃ¡zku TBD

LSTM sÃ­Å¥ je organizovÃ¡na podobnÄ› jako RNN, ale existujÃ­ dva stavy, kterÃ© se pÅ™edÃ¡vajÃ­ z vrstvy do vrstvy: skuteÄnÃ½ stav C a skrytÃ½ vektor H. V kaÅ¾dÃ© jednotce je skrytÃ½ vektor H<sub>i</sub> spojen s vstupem X<sub>i</sub>, a tyto kontrolujÃ­, co se stane se stavem C prostÅ™ednictvÃ­m **bran**. KaÅ¾dÃ¡ brÃ¡na je neuronovÃ¡ sÃ­Å¥ s sigmoidnÃ­ aktivacÃ­ (vÃ½stup v rozmezÃ­ [0,1]), kterou lze povaÅ¾ovat za bitovou masku pÅ™i nÃ¡sobenÃ­ stavovÃ½m vektorem. Na obrÃ¡zku vÃ½Å¡e jsou nÃ¡sledujÃ­cÃ­ brÃ¡ny (zleva doprava):

* **BrÃ¡na zapomÃ­nÃ¡nÃ­** bere skrytÃ½ vektor a urÄuje, kterÃ© komponenty vektoru C je tÅ™eba zapomenout a kterÃ© pÅ™edat dÃ¡l.
* **VstupnÃ­ brÃ¡na** bere urÄitÃ© informace ze vstupnÃ­ho a skrytÃ©ho vektoru a vklÃ¡dÃ¡ je do stavu.
* **VÃ½stupnÃ­ brÃ¡na** transformuje stav pomocÃ­ lineÃ¡rnÃ­ vrstvy s aktivacÃ­ *tanh* a potÃ© vybere nÄ›kterÃ© jeho komponenty pomocÃ­ skrytÃ©ho vektoru H<sub>i</sub>, aby vytvoÅ™ila novÃ½ stav C<sub>i+1</sub>.

Komponenty stavu C lze povaÅ¾ovat za urÄitÃ© pÅ™Ã­znaky, kterÃ© lze zapÃ­nat a vypÃ­nat. NapÅ™Ã­klad kdyÅ¾ v sekvenci narazÃ­me na jmÃ©no *Alice*, mÅ¯Å¾eme pÅ™edpoklÃ¡dat, Å¾e se jednÃ¡ o Å¾enskou postavu, a aktivovat pÅ™Ã­znak ve stavu, Å¾e mÃ¡me ve vÄ›tÄ› Å¾enskÃ© podstatnÃ© jmÃ©no. KdyÅ¾ dÃ¡le narazÃ­me na frÃ¡zi *and Tom*, aktivujeme pÅ™Ã­znak, Å¾e mÃ¡me mnoÅ¾nÃ© ÄÃ­slo. ManipulacÃ­ se stavem tedy mÅ¯Å¾eme ÃºdajnÄ› sledovat gramatickÃ© vlastnosti ÄÃ¡stÃ­ vÄ›ty.

> âœ… SkvÄ›lÃ½m zdrojem pro pochopenÃ­ internÃ­ch mechanismÅ¯ LSTM je tento vÃ½bornÃ½ ÄlÃ¡nek [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) od Christophera Olaha.

## BidirekcionÃ¡lnÃ­ a vÃ­cevrstvÃ© RNN

Diskutovali jsme o rekurentnÃ­ch sÃ­tÃ­ch, kterÃ© fungujÃ­ jednÃ­m smÄ›rem, od zaÄÃ¡tku sekvence do jejÃ­ho konce. To se zdÃ¡ pÅ™irozenÃ©, protoÅ¾e to pÅ™ipomÃ­nÃ¡ zpÅ¯sob, jakÃ½m Äteme a poslouchÃ¡me Å™eÄ. NicmÃ©nÄ›, protoÅ¾e v mnoha praktickÃ½ch pÅ™Ã­padech mÃ¡me nÃ¡hodnÃ½ pÅ™Ã­stup k vstupnÃ­ sekvenci, mÅ¯Å¾e bÃ½t smysluplnÃ© provÃ¡dÄ›t rekurentnÃ­ vÃ½poÄty v obou smÄ›rech. TakovÃ© sÃ­tÄ› se nazÃ½vajÃ­ **bidirekcionÃ¡lnÃ­** RNN. PÅ™i prÃ¡ci s bidirekcionÃ¡lnÃ­ sÃ­tÃ­ bychom potÅ™ebovali dva skrytÃ© stavovÃ© vektory, jeden pro kaÅ¾dÃ½ smÄ›r.

RekurentnÃ­ sÃ­Å¥, aÅ¥ uÅ¾ jednosmÄ›rnÃ¡ nebo bidirekcionÃ¡lnÃ­, zachycuje urÄitÃ© vzory v sekvenci a mÅ¯Å¾e je uloÅ¾it do stavovÃ©ho vektoru nebo pÅ™edat do vÃ½stupu. StejnÄ› jako u konvoluÄnÃ­ch sÃ­tÃ­ mÅ¯Å¾eme na prvnÃ­ vrstvu postavit dalÅ¡Ã­ rekurentnÃ­ vrstvu, kterÃ¡ zachytÃ­ vzory na vyÅ¡Å¡Ã­ Ãºrovni a vytvoÅ™Ã­ vzory na niÅ¾Å¡Ã­ Ãºrovni extrahovanÃ© prvnÃ­ vrstvou. To nÃ¡s vede k pojmu **vÃ­cevrstvÃ¡ RNN**, kterÃ¡ se sklÃ¡dÃ¡ ze dvou nebo vÃ­ce rekurentnÃ­ch sÃ­tÃ­, kde vÃ½stup pÅ™edchozÃ­ vrstvy je pÅ™edÃ¡n dalÅ¡Ã­ vrstvÄ› jako vstup.

![ObrÃ¡zek ukazujÃ­cÃ­ vÃ­cevrstvou LSTM RNN](../../../../../translated_images/cs/multi-layer-lstm.dd975e29bb2a59fe.webp)

*ObrÃ¡zek z [tohoto skvÄ›lÃ©ho pÅ™Ã­spÄ›vku](https://towardsdatascience.com/from-a-lstm-cell-to-a-multilayer-lstm-network-with-pytorch-2899eb5696f3) od Fernanda LÃ³peze*

## âœï¸ CviÄenÃ­: Embeddingy

PokraÄujte ve svÃ©m uÄenÃ­ v nÃ¡sledujÃ­cÃ­ch noteboocÃ­ch:

* [RNNs s PyTorch](RNNPyTorch.ipynb)
* [RNNs s TensorFlow](RNNTF.ipynb)

## ZÃ¡vÄ›r

V tÃ©to jednotce jsme vidÄ›li, Å¾e RNN lze pouÅ¾Ã­t pro klasifikaci sekvencÃ­, ale ve skuteÄnosti zvlÃ¡dnou mnohem vÃ­ce ÃºkolÅ¯, jako je generovÃ¡nÃ­ textu, strojovÃ½ pÅ™eklad a dalÅ¡Ã­. Tyto Ãºkoly budeme zkoumat v dalÅ¡Ã­ jednotce.

## ğŸš€ VÃ½zva

ProjdÄ›te si literaturu o LSTM a zvaÅ¾te jejich aplikace:

- [Grid Long Short-Term Memory](https://arxiv.org/pdf/1507.01526v1.pdf)
- [Show, Attend and Tell: Neural Image Caption
Generation with Visual Attention](https://arxiv.org/pdf/1502.03044v2.pdf)

## [KvÃ­z po pÅ™ednÃ¡Å¡ce](https://ff-quizzes.netlify.app/en/ai/quiz/32)

## PÅ™ehled & Samostudium

- [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) od Christophera Olaha.

## [Ãškol: Notebooks](assignment.md)

---

