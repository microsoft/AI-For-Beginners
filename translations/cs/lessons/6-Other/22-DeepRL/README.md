<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "dbacf9b1915612981d76059678e563e5",
  "translation_date": "2025-08-25T23:30:21+00:00",
  "source_file": "lessons/6-Other/22-DeepRL/README.md",
  "language_code": "cs"
}
-->
# HlubokÃ© posilovanÃ© uÄenÃ­

PosilovanÃ© uÄenÃ­ (Reinforcement Learning, RL) je povaÅ¾ovÃ¡no za jeden ze zÃ¡kladnÃ­ch paradigmat strojovÃ©ho uÄenÃ­, vedle uÄenÃ­ s uÄitelem a bez uÄitele. ZatÃ­mco uÄenÃ­ s uÄitelem spolÃ©hÃ¡ na datovou sadu se znÃ¡mÃ½mi vÃ½sledky, RL je zaloÅ¾eno na **uÄenÃ­ skrze Äinnost**. NapÅ™Ã­klad, kdyÅ¾ poprvÃ© vidÃ­me poÄÃ­taÄovou hru, zaÄneme ji hrÃ¡t, i kdyÅ¾ neznÃ¡me pravidla, a brzy se zlepÅ¡ujeme jen dÃ­ky samotnÃ©mu procesu hranÃ­ a pÅ™izpÅ¯sobovÃ¡nÃ­ naÅ¡eho chovÃ¡nÃ­.

## [KvÃ­z pÅ™ed pÅ™ednÃ¡Å¡kou](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/122)

Pro provÃ¡dÄ›nÃ­ RL potÅ™ebujeme:

* **ProstÅ™edÃ­** nebo **simulÃ¡tor**, kterÃ½ nastavuje pravidla hry. MÄ›li bychom bÃ½t schopni provÃ¡dÄ›t experimenty v simulÃ¡toru a pozorovat vÃ½sledky.
* NÄ›jakou **funkci odmÄ›ny**, kterÃ¡ ukazuje, jak ÃºspÄ›Å¡nÃ½ byl nÃ¡Å¡ experiment. V pÅ™Ã­padÄ› uÄenÃ­ se hrÃ¡t poÄÃ­taÄovou hru by odmÄ›nou bylo naÅ¡e koneÄnÃ© skÃ³re.

Na zÃ¡kladÄ› funkce odmÄ›ny bychom mÄ›li bÃ½t schopni pÅ™izpÅ¯sobit svÃ© chovÃ¡nÃ­ a zlepÅ¡it svÃ© dovednosti, abychom pÅ™Ã­Å¡tÄ› hrÃ¡li lÃ©pe. HlavnÃ­ rozdÃ­l mezi jinÃ½mi typy strojovÃ©ho uÄenÃ­ a RL je ten, Å¾e v RL obvykle nevÃ­me, zda vyhrajeme nebo prohrajeme, dokud hru nedokonÄÃ­me. NemÅ¯Å¾eme tedy Å™Ã­ci, zda je urÄitÃ½ tah sÃ¡m o sobÄ› dobrÃ½ nebo ne - odmÄ›nu dostaneme aÅ¾ na konci hry.

BÄ›hem RL obvykle provÃ¡dÃ­me mnoho experimentÅ¯. BÄ›hem kaÅ¾dÃ©ho experimentu musÃ­me vyvÃ¡Å¾it mezi sledovÃ¡nÃ­m optimÃ¡lnÃ­ strategie, kterou jsme se dosud nauÄili (**exploatace**), a zkoumÃ¡nÃ­m novÃ½ch moÅ¾nÃ½ch stavÅ¯ (**explorace**).

## OpenAI Gym

SkvÄ›lÃ½m nÃ¡strojem pro RL je [OpenAI Gym](https://gym.openai.com/) - **simulaÄnÃ­ prostÅ™edÃ­**, kterÃ© dokÃ¡Å¾e simulovat mnoho rÅ¯znÃ½ch prostÅ™edÃ­, od her Atari aÅ¾ po fyziku za balancovÃ¡nÃ­m tyÄe. Je to jedno z nejpopulÃ¡rnÄ›jÅ¡Ã­ch simulaÄnÃ­ch prostÅ™edÃ­ pro trÃ©novÃ¡nÃ­ algoritmÅ¯ posilovanÃ©ho uÄenÃ­ a je spravovÃ¡no organizacÃ­ [OpenAI](https://openai.com/).

> **Note**: VÅ¡echna prostÅ™edÃ­ dostupnÃ¡ v OpenAI Gym si mÅ¯Å¾ete prohlÃ©dnout [zde](https://gym.openai.com/envs/#classic_control).

## BalancovÃ¡nÃ­ CartPole

PravdÄ›podobnÄ› jste vÅ¡ichni vidÄ›li modernÃ­ balancovacÃ­ zaÅ™Ã­zenÃ­, jako je *Segway* nebo *GyroskÃºtry*. Tato zaÅ™Ã­zenÃ­ dokÃ¡Å¾ou automaticky balancovat tÃ­m, Å¾e pÅ™izpÅ¯sobujÃ­ pohyb svÃ½ch kol na zÃ¡kladÄ› signÃ¡lu z akcelerometru nebo gyroskopu. V tÃ©to sekci se nauÄÃ­me, jak vyÅ™eÅ¡it podobnÃ½ problÃ©m - balancovÃ¡nÃ­ tyÄe. Je to podobnÃ© situaci, kdy cirkusovÃ½ umÄ›lec potÅ™ebuje balancovat tyÄ na ruce - ale toto balancovÃ¡nÃ­ probÃ­hÃ¡ pouze v 1D.

ZjednoduÅ¡enÃ¡ verze balancovÃ¡nÃ­ je znÃ¡mÃ¡ jako problÃ©m **CartPole**. Ve svÄ›tÄ› CartPole mÃ¡me horizontÃ¡lnÃ­ posuvnÃ­k, kterÃ½ se mÅ¯Å¾e pohybovat doleva nebo doprava, a cÃ­lem je balancovat vertikÃ¡lnÃ­ tyÄ na vrcholu posuvnÃ­ku, zatÃ­mco se pohybuje.

<img alt="a cartpole" src="images/cartpole.png" width="200"/>

Pro vytvoÅ™enÃ­ a pouÅ¾itÃ­ tohoto prostÅ™edÃ­ potÅ™ebujeme nÄ›kolik Å™Ã¡dkÅ¯ kÃ³du v Pythonu:

```python
import gym
env = gym.make("CartPole-v1")

env.reset()
done = False
total_reward = 0
while not done:
   env.render()
   action = env.action_space.sample()
   observaton, reward, done, info = env.step(action)
   total_reward += reward

print(f"Total reward: {total_reward}")
```

KaÅ¾dÃ© prostÅ™edÃ­ lze pÅ™istupovat stejnÃ½m zpÅ¯sobem:
* `env.reset` spustÃ­ novÃ½ experiment
* `env.step` provede simulaÄnÃ­ krok. PÅ™ijÃ­mÃ¡ **akci** z **akÄnÃ­ho prostoru** a vracÃ­ **pozorovÃ¡nÃ­** (z pozorovacÃ­ho prostoru), stejnÄ› jako odmÄ›nu a pÅ™Ã­znak ukonÄenÃ­.

V pÅ™Ã­kladu vÃ½Å¡e provÃ¡dÃ­me v kaÅ¾dÃ©m kroku nÃ¡hodnou akci, coÅ¾ je dÅ¯vod, proÄ je Å¾ivotnost experimentu velmi krÃ¡tkÃ¡:

![non-balancing cartpole](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-nobalance.gif)

CÃ­lem algoritmu RL je natrÃ©novat model - tzv. **politiku** Ï€ - kterÃ¡ vrÃ¡tÃ­ akci v reakci na danÃ½ stav. Politiku mÅ¯Å¾eme takÃ© povaÅ¾ovat za pravdÄ›podobnostnÃ­, tj. pro jakÃ½koli stav *s* a akci *a* vrÃ¡tÃ­ pravdÄ›podobnost Ï€(*a*|*s*), Å¾e bychom mÄ›li provÃ©st akci *a* ve stavu *s*.

## Algoritmus Policy Gradients

NejzÅ™ejmÄ›jÅ¡Ã­ zpÅ¯sob, jak modelovat politiku, je vytvoÅ™it neuronovou sÃ­Å¥, kterÃ¡ bude pÅ™ijÃ­mat stavy jako vstup a vracet odpovÃ­dajÃ­cÃ­ akce (nebo spÃ­Å¡e pravdÄ›podobnosti vÅ¡ech akcÃ­). V jistÃ©m smyslu by to bylo podobnÃ© bÄ›Å¾nÃ©mu klasifikaÄnÃ­mu Ãºkolu, s hlavnÃ­m rozdÃ­lem - pÅ™edem nevÃ­me, jakÃ© akce bychom mÄ›li provÃ©st v kaÅ¾dÃ©m kroku.

MyÅ¡lenkou je odhadnout tyto pravdÄ›podobnosti. VytvoÅ™Ã­me vektor **kumulativnÃ­ch odmÄ›n**, kterÃ½ ukazuje naÅ¡i celkovou odmÄ›nu v kaÅ¾dÃ©m kroku experimentu. TakÃ© aplikujeme **diskontovÃ¡nÃ­ odmÄ›n** tÃ­m, Å¾e nÃ¡sobÃ­me dÅ™Ã­vÄ›jÅ¡Ã­ odmÄ›ny nÄ›jakÃ½m koeficientem Î³=0.99, abychom snÃ­Å¾ili vÃ½znam dÅ™Ã­vÄ›jÅ¡Ã­ch odmÄ›n. PotÃ© posilujeme ty kroky podÃ©l experimentÃ¡lnÃ­ cesty, kterÃ© pÅ™inÃ¡Å¡ejÃ­ vÄ›tÅ¡Ã­ odmÄ›ny.

> VÃ­ce o algoritmu Policy Gradient a jeho praktickÃ© ukÃ¡zce najdete v [pÅ™Ã­kladovÃ©m notebooku](../../../../../lessons/6-Other/22-DeepRL/CartPole-RL-TF.ipynb).

## Algoritmus Actor-Critic

VylepÅ¡enÃ¡ verze pÅ™Ã­stupu Policy Gradients se nazÃ½vÃ¡ **Actor-Critic**. HlavnÃ­ myÅ¡lenkou je, Å¾e neuronovÃ¡ sÃ­Å¥ bude trÃ©novÃ¡na tak, aby vracela dvÄ› vÄ›ci:

* Politiku, kterÃ¡ urÄuje, jakou akci provÃ©st. Tato ÄÃ¡st se nazÃ½vÃ¡ **actor**.
* Odhad celkovÃ© odmÄ›ny, kterou mÅ¯Å¾eme oÄekÃ¡vat v danÃ©m stavu - tato ÄÃ¡st se nazÃ½vÃ¡ **critic**.

Tato architektura v jistÃ©m smyslu pÅ™ipomÃ­nÃ¡ [GAN](../../4-ComputerVision/10-GANs/README.md), kde mÃ¡me dvÄ› sÃ­tÄ›, kterÃ© se trÃ©nujÃ­ proti sobÄ›. V modelu actor-critic navrhuje actor akci, kterou bychom mÄ›li provÃ©st, a critic se snaÅ¾Ã­ bÃ½t kritickÃ½ a odhadnout vÃ½sledek. NaÅ¡Ã­m cÃ­lem je vÅ¡ak trÃ©novat tyto sÃ­tÄ› v souladu.

ProtoÅ¾e znÃ¡me jak skuteÄnÃ© kumulativnÃ­ odmÄ›ny, tak vÃ½sledky vrÃ¡cenÃ© criticem bÄ›hem experimentu, je relativnÄ› snadnÃ© vytvoÅ™it ztrÃ¡tovou funkci, kterÃ¡ minimalizuje rozdÃ­l mezi nimi. To nÃ¡m dÃ¡ **ztrÃ¡tu criticu**. **ZtrÃ¡tu actora** mÅ¯Å¾eme vypoÄÃ­tat stejnÃ½m pÅ™Ã­stupem jako v algoritmu Policy Gradient.

Po spuÅ¡tÄ›nÃ­ jednoho z tÄ›chto algoritmÅ¯ mÅ¯Å¾eme oÄekÃ¡vat, Å¾e se nÃ¡Å¡ CartPole bude chovat takto:

![a balancing cartpole](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-balance.gif)

## âœï¸ CviÄenÃ­: Policy Gradients a Actor-Critic RL

PokraÄujte ve svÃ©m uÄenÃ­ v nÃ¡sledujÃ­cÃ­ch noteboocÃ­ch:

* [RL v TensorFlow](../../../../../lessons/6-Other/22-DeepRL/CartPole-RL-TF.ipynb)
* [RL v PyTorch](../../../../../lessons/6-Other/22-DeepRL/CartPole-RL-PyTorch.ipynb)

## DalÅ¡Ã­ Ãºkoly RL

PosilovanÃ© uÄenÃ­ je dnes rychle rostoucÃ­ oblastÃ­ vÃ½zkumu. NÄ›kterÃ© zajÃ­mavÃ© pÅ™Ã­klady posilovanÃ©ho uÄenÃ­ jsou:

* UÄenÃ­ poÄÃ­taÄe hrÃ¡t **hry Atari**. VÃ½zvou v tomto problÃ©mu je, Å¾e nemÃ¡me jednoduchÃ½ stav reprezentovanÃ½ jako vektor, ale spÃ­Å¡e snÃ­mek obrazovky - a musÃ­me pouÅ¾Ã­t CNN k pÅ™evodu tohoto obrazovÃ©ho snÃ­mku na vektor vlastnostÃ­ nebo k extrakci informacÃ­ o odmÄ›nÄ›. Hry Atari jsou dostupnÃ© v Gym.
* UÄenÃ­ poÄÃ­taÄe hrÃ¡t deskovÃ© hry, jako je Å¡achy a Go. NedÃ¡vno byly Å¡piÄkovÃ© programy jako **Alpha Zero** trÃ©novÃ¡ny od nuly dvÄ›ma agenty, kteÅ™Ã­ hrÃ¡li proti sobÄ› a zlepÅ¡ovali se v kaÅ¾dÃ©m kroku.
* V prÅ¯myslu se RL pouÅ¾Ã­vÃ¡ k vytvÃ¡Å™enÃ­ Å™Ã­dicÃ­ch systÃ©mÅ¯ ze simulace. SluÅ¾ba nazvanÃ¡ [Bonsai](https://azure.microsoft.com/services/project-bonsai/?WT.mc_id=academic-77998-cacaste) je speciÃ¡lnÄ› navrÅ¾ena pro tento ÃºÄel.

## ZÃ¡vÄ›r

NynÃ­ jsme se nauÄili, jak trÃ©novat agenty, aby dosÃ¡hli dobrÃ½ch vÃ½sledkÅ¯ pouze tÃ­m, Å¾e jim poskytneme funkci odmÄ›ny, kterÃ¡ definuje poÅ¾adovanÃ½ stav hry, a dÃ¡me jim pÅ™Ã­leÅ¾itost inteligentnÄ› prozkoumat prostor moÅ¾nostÃ­. ÃšspÄ›Å¡nÄ› jsme vyzkouÅ¡eli dva algoritmy a dosÃ¡hli dobrÃ©ho vÃ½sledku v relativnÄ› krÃ¡tkÃ©m Äase. Toto je vÅ¡ak pouze zaÄÃ¡tek vaÅ¡Ã­ cesty do RL, a rozhodnÄ› byste mÄ›li zvÃ¡Å¾it absolvovÃ¡nÃ­ samostatnÃ©ho kurzu, pokud se chcete ponoÅ™it hloubÄ›ji.

## ğŸš€ VÃ½zva

Prozkoumejte aplikace uvedenÃ© v sekci 'DalÅ¡Ã­ Ãºkoly RL' a zkuste nÄ›kterou implementovat!

## [KvÃ­z po pÅ™ednÃ¡Å¡ce](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/222)

## PÅ™ehled a samostudium

ZjistÄ›te vÃ­ce o klasickÃ©m posilovanÃ©m uÄenÃ­ v naÅ¡em [kurikulu StrojovÃ©ho uÄenÃ­ pro zaÄÃ¡teÄnÃ­ky](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/README.md).

PodÃ­vejte se na [toto skvÄ›lÃ© video](https://www.youtube.com/watch?v=qv6UVOQ0F44), kterÃ© ukazuje, jak se poÄÃ­taÄ mÅ¯Å¾e nauÄit hrÃ¡t Super Mario.

## ZadÃ¡nÃ­: [TrÃ©nujte Mountain Car](lab/README.md)

VaÅ¡Ã­m cÃ­lem v tomto zadÃ¡nÃ­ bude natrÃ©novat jinÃ© prostÅ™edÃ­ Gym - [Mountain Car](https://www.gymlibrary.ml/environments/classic_control/mountain_car/).

**ProhlÃ¡Å¡enÃ­:**  
Tento dokument byl pÅ™eloÅ¾en pomocÃ­ sluÅ¾by pro automatickÃ½ pÅ™eklad [Co-op Translator](https://github.com/Azure/co-op-translator). PÅ™estoÅ¾e se snaÅ¾Ã­me o pÅ™esnost, mÄ›jte na pamÄ›ti, Å¾e automatickÃ© pÅ™eklady mohou obsahovat chyby nebo nepÅ™esnosti. PÅ¯vodnÃ­ dokument v jeho pÅ¯vodnÃ­m jazyce by mÄ›l bÃ½t povaÅ¾ovÃ¡n za autoritativnÃ­ zdroj. Pro dÅ¯leÅ¾itÃ© informace doporuÄujeme profesionÃ¡lnÃ­ lidskÃ½ pÅ™eklad. NeodpovÃ­dÃ¡me za Å¾Ã¡dnÃ¡ nedorozumÄ›nÃ­ nebo nesprÃ¡vnÃ© interpretace vyplÃ½vajÃ­cÃ­ z pouÅ¾itÃ­ tohoto pÅ™ekladu.