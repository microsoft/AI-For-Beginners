# HlubokÃ© posilovanÃ© uÄenÃ­

PosilovanÃ© uÄenÃ­ (RL) je povaÅ¾ovÃ¡no za jeden ze zÃ¡kladnÃ­ch paradigmat strojovÃ©ho uÄenÃ­, vedle uÄenÃ­ s uÄitelem a uÄenÃ­ bez uÄitele. ZatÃ­mco u uÄenÃ­ s uÄitelem se spolÃ©hÃ¡me na dataset s pÅ™edem znÃ¡mÃ½mi vÃ½sledky, RL je zaloÅ¾eno na **uÄenÃ­ skrze zkuÅ¡enost**. NapÅ™Ã­klad, kdyÅ¾ poprvÃ© vidÃ­me poÄÃ­taÄovou hru, zaÄneme ji hrÃ¡t, i kdyÅ¾ neznÃ¡me pravidla, a brzy dokÃ¡Å¾eme zlepÅ¡it svÃ© dovednosti jen dÃ­ky procesu hranÃ­ a pÅ™izpÅ¯sobovÃ¡nÃ­ svÃ©ho chovÃ¡nÃ­.

## [KvÃ­z pÅ™ed pÅ™ednÃ¡Å¡kou](https://ff-quizzes.netlify.app/en/ai/quiz/43)

Pro provÃ¡dÄ›nÃ­ RL potÅ™ebujeme:

* **ProstÅ™edÃ­** nebo **simulÃ¡tor**, kterÃ½ nastavÃ­ pravidla hry. MÄ›li bychom bÃ½t schopni provÃ¡dÄ›t experimenty v simulÃ¡toru a pozorovat vÃ½sledky.
* NÄ›jakou **funkci odmÄ›ny**, kterÃ¡ ukazuje, jak ÃºspÄ›Å¡nÃ½ byl nÃ¡Å¡ experiment. V pÅ™Ã­padÄ› uÄenÃ­ se hrÃ¡t poÄÃ­taÄovou hru by odmÄ›nou bylo naÅ¡e koneÄnÃ© skÃ³re.

Na zÃ¡kladÄ› funkce odmÄ›ny bychom mÄ›li bÃ½t schopni pÅ™izpÅ¯sobit svÃ© chovÃ¡nÃ­ a zlepÅ¡it svÃ© dovednosti, aby pÅ™Ã­Å¡tÄ› naÅ¡e hra byla lepÅ¡Ã­. HlavnÃ­ rozdÃ­l mezi jinÃ½mi typy strojovÃ©ho uÄenÃ­ a RL je ten, Å¾e v RL obvykle nevÃ­me, zda vyhrajeme nebo prohrajeme, dokud nedokonÄÃ­me hru. NemÅ¯Å¾eme tedy Å™Ã­ci, zda je urÄitÃ½ tah sÃ¡m o sobÄ› dobrÃ½ nebo ne - odmÄ›nu dostaneme aÅ¾ na konci hry.

BÄ›hem RL obvykle provÃ¡dÃ­me mnoho experimentÅ¯. BÄ›hem kaÅ¾dÃ©ho experimentu musÃ­me vyvÃ¡Å¾it mezi sledovÃ¡nÃ­m optimÃ¡lnÃ­ strategie, kterou jsme se dosud nauÄili (**exploatace**), a zkoumÃ¡nÃ­m novÃ½ch moÅ¾nÃ½ch stavÅ¯ (**explorace**).

## OpenAI Gym

SkvÄ›lÃ½m nÃ¡strojem pro RL je [OpenAI Gym](https://gym.openai.com/) - **simulaÄnÃ­ prostÅ™edÃ­**, kterÃ© dokÃ¡Å¾e simulovat mnoho rÅ¯znÃ½ch prostÅ™edÃ­, od her Atari aÅ¾ po fyziku za balancovÃ¡nÃ­m tyÄe. Je to jedno z nejpopulÃ¡rnÄ›jÅ¡Ã­ch simulaÄnÃ­ch prostÅ™edÃ­ pro trÃ©novÃ¡nÃ­ algoritmÅ¯ posilovanÃ©ho uÄenÃ­ a je spravovÃ¡no spoleÄnostÃ­ [OpenAI](https://openai.com/).

> **Note**: VÅ¡echna dostupnÃ¡ prostÅ™edÃ­ z OpenAI Gym si mÅ¯Å¾ete prohlÃ©dnout [zde](https://gym.openai.com/envs/#classic_control).

## BalancovÃ¡nÃ­ CartPole

PravdÄ›podobnÄ› jste vÅ¡ichni vidÄ›li modernÃ­ balancovacÃ­ zaÅ™Ã­zenÃ­, jako je *Segway* nebo *GyroskÃºtry*. Jsou schopny automaticky balancovat tÃ­m, Å¾e upravujÃ­ svÃ© koleÄka na zÃ¡kladÄ› signÃ¡lu z akcelerometru nebo gyroskopu. V tÃ©to sekci se nauÄÃ­me, jak vyÅ™eÅ¡it podobnÃ½ problÃ©m - balancovÃ¡nÃ­ tyÄe. Je to podobnÃ© situaci, kdy cirkusovÃ½ umÄ›lec potÅ™ebuje balancovat tyÄ na svÃ© ruce - ale toto balancovÃ¡nÃ­ tyÄe probÃ­hÃ¡ pouze v 1D.

ZjednoduÅ¡enÃ¡ verze balancovÃ¡nÃ­ je znÃ¡mÃ¡ jako problÃ©m **CartPole**. Ve svÄ›tÄ› CartPole mÃ¡me horizontÃ¡lnÃ­ posuvnÃ­k, kterÃ½ se mÅ¯Å¾e pohybovat doleva nebo doprava, a cÃ­lem je balancovat vertikÃ¡lnÃ­ tyÄ na vrcholu posuvnÃ­ku, zatÃ­mco se pohybuje.

<img alt="cartpole" src="../../../../../translated_images/cs/cartpole.f52a67f27e058170.webp" width="200"/>

Pro vytvoÅ™enÃ­ a pouÅ¾itÃ­ tohoto prostÅ™edÃ­ potÅ™ebujeme nÄ›kolik Å™Ã¡dkÅ¯ kÃ³du v Pythonu:

```python
import gym
env = gym.make("CartPole-v1")

env.reset()
done = False
total_reward = 0
while not done:
   env.render()
   action = env.action_space.sample()
   observaton, reward, done, info = env.step(action)
   total_reward += reward

print(f"Total reward: {total_reward}")
```

KaÅ¾dÃ© prostÅ™edÃ­ lze pÅ™istupovat stejnÃ½m zpÅ¯sobem:
* `env.reset` zahÃ¡jÃ­ novÃ½ experiment
* `env.step` provede simulaÄnÃ­ krok. PÅ™ijÃ­mÃ¡ **akci** z **akÄnÃ­ho prostoru** a vracÃ­ **pozorovÃ¡nÃ­** (z pozorovacÃ­ho prostoru), stejnÄ› jako odmÄ›nu a pÅ™Ã­znak ukonÄenÃ­.

V pÅ™Ã­kladu vÃ½Å¡e provÃ¡dÃ­me nÃ¡hodnou akci pÅ™i kaÅ¾dÃ©m kroku, coÅ¾ je dÅ¯vod, proÄ je Å¾ivotnost experimentu velmi krÃ¡tkÃ¡:

![nebalancujÃ­cÃ­ cartpole](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-nobalance.gif)

CÃ­lem algoritmu RL je vytrÃ©novat model - tzv. **politiku** &pi; - kterÃ¡ vrÃ¡tÃ­ akci v reakci na danÃ½ stav. Politiku mÅ¯Å¾eme takÃ© povaÅ¾ovat za pravdÄ›podobnostnÃ­, tj. pro jakÃ½koli stav *s* a akci *a* vrÃ¡tÃ­ pravdÄ›podobnost &pi;(*a*|*s*), Å¾e bychom mÄ›li provÃ©st *a* ve stavu *s*.

## Algoritmus Policy Gradients

NejzÅ™ejmÄ›jÅ¡Ã­ zpÅ¯sob, jak modelovat politiku, je vytvoÅ™it neuronovou sÃ­Å¥, kterÃ¡ bude pÅ™ijÃ­mat stavy jako vstup a vracet odpovÃ­dajÃ­cÃ­ akce (nebo spÃ­Å¡e pravdÄ›podobnosti vÅ¡ech akcÃ­). V jistÃ©m smyslu by to bylo podobnÃ© bÄ›Å¾nÃ©mu klasifikaÄnÃ­mu Ãºkolu, s hlavnÃ­m rozdÃ­lem - pÅ™edem nevÃ­me, kterÃ© akce bychom mÄ›li provÃ©st v kaÅ¾dÃ©m kroku.

MyÅ¡lenkou zde je odhadnout tyto pravdÄ›podobnosti. VytvoÅ™Ã­me vektor **kumulativnÃ­ch odmÄ›n**, kterÃ½ ukazuje naÅ¡i celkovou odmÄ›nu v kaÅ¾dÃ©m kroku experimentu. TakÃ© aplikujeme **diskontovÃ¡nÃ­ odmÄ›n** tÃ­m, Å¾e nÃ¡sobÃ­me dÅ™Ã­vÄ›jÅ¡Ã­ odmÄ›ny nÄ›jakÃ½m koeficientem &gamma;=0.99, abychom snÃ­Å¾ili vÃ½znam dÅ™Ã­vÄ›jÅ¡Ã­ch odmÄ›n. PotÃ© posilujeme ty kroky podÃ©l experimentÃ¡lnÃ­ cesty, kterÃ© pÅ™inÃ¡Å¡ejÃ­ vÄ›tÅ¡Ã­ odmÄ›ny.

> VÃ­ce o algoritmu Policy Gradient a jeho pouÅ¾itÃ­ najdete v [pÅ™Ã­kladovÃ©m notebooku](CartPole-RL-TF.ipynb).

## Algoritmus Actor-Critic

VylepÅ¡enÃ¡ verze pÅ™Ã­stupu Policy Gradients se nazÃ½vÃ¡ **Actor-Critic**. HlavnÃ­ myÅ¡lenkou je, Å¾e neuronovÃ¡ sÃ­Å¥ bude trÃ©novÃ¡na tak, aby vracela dvÄ› vÄ›ci:

* Politiku, kterÃ¡ urÄuje, jakou akci provÃ©st. Tato ÄÃ¡st se nazÃ½vÃ¡ **actor**.
* Odhad celkovÃ© odmÄ›ny, kterou mÅ¯Å¾eme oÄekÃ¡vat v tomto stavu - tato ÄÃ¡st se nazÃ½vÃ¡ **critic**.

V jistÃ©m smyslu tato architektura pÅ™ipomÃ­nÃ¡ [GAN](../../4-ComputerVision/10-GANs/README.md), kde mÃ¡me dvÄ› sÃ­tÄ›, kterÃ© se trÃ©nujÃ­ proti sobÄ›. V modelu actor-critic navrhuje actor akci, kterou bychom mÄ›li provÃ©st, a critic se snaÅ¾Ã­ bÃ½t kritickÃ½ a odhadnout vÃ½sledek. NaÅ¡Ã­m cÃ­lem je vÅ¡ak trÃ©novat tyto sÃ­tÄ› spoleÄnÄ›.

ProtoÅ¾e znÃ¡me jak skuteÄnÃ© kumulativnÃ­ odmÄ›ny, tak vÃ½sledky vrÃ¡cenÃ© criticem bÄ›hem experimentu, je relativnÄ› snadnÃ© vytvoÅ™it ztrÃ¡tovou funkci, kterÃ¡ minimalizuje rozdÃ­l mezi nimi. To nÃ¡m poskytne **critic loss**. **Actor loss** mÅ¯Å¾eme vypoÄÃ­tat stejnÃ½m pÅ™Ã­stupem jako v algoritmu Policy Gradient.

Po spuÅ¡tÄ›nÃ­ jednoho z tÄ›chto algoritmÅ¯ mÅ¯Å¾eme oÄekÃ¡vat, Å¾e naÅ¡e CartPole bude vypadat takto:

![balancujÃ­cÃ­ cartpole](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-balance.gif)

## âœï¸ CviÄenÃ­: Policy Gradients a Actor-Critic RL

PokraÄujte ve svÃ©m uÄenÃ­ v nÃ¡sledujÃ­cÃ­ch noteboocÃ­ch:

* [RL v TensorFlow](CartPole-RL-TF.ipynb)
* [RL v PyTorch](CartPole-RL-PyTorch.ipynb)

## DalÅ¡Ã­ Ãºkoly RL

PosilovanÃ© uÄenÃ­ je dnes rychle rostoucÃ­ oblastÃ­ vÃ½zkumu. NÄ›kterÃ© zajÃ­mavÃ© pÅ™Ã­klady posilovanÃ©ho uÄenÃ­ jsou:

* UÄenÃ­ poÄÃ­taÄe hrÃ¡t **hry Atari**. VÃ½zvou v tomto problÃ©mu je, Å¾e nemÃ¡me jednoduchÃ½ stav reprezentovanÃ½ jako vektor, ale spÃ­Å¡e snÃ­mek obrazovky - a musÃ­me pouÅ¾Ã­t CNN k pÅ™evodu tohoto obrazovÃ©ho snÃ­mku na vektor vlastnostÃ­ nebo k extrakci informacÃ­ o odmÄ›nÄ›. Hry Atari jsou dostupnÃ© v Gym.
* UÄenÃ­ poÄÃ­taÄe hrÃ¡t deskovÃ© hry, jako je Å¡achy a Go. NedÃ¡vno byly Å¡piÄkovÃ© programy jako **Alpha Zero** trÃ©novÃ¡ny od nuly dvÄ›ma agenty, kteÅ™Ã­ hrÃ¡li proti sobÄ› a zlepÅ¡ovali se pÅ™i kaÅ¾dÃ©m kroku.
* V prÅ¯myslu se RL pouÅ¾Ã­vÃ¡ k vytvÃ¡Å™enÃ­ Å™Ã­dicÃ­ch systÃ©mÅ¯ ze simulace. SluÅ¾ba [Bonsai](https://azure.microsoft.com/services/project-bonsai/?WT.mc_id=academic-77998-cacaste) je speciÃ¡lnÄ› navrÅ¾ena pro tento ÃºÄel.

## ZÃ¡vÄ›r

NynÃ­ jsme se nauÄili, jak trÃ©novat agenty, aby dosÃ¡hli dobrÃ½ch vÃ½sledkÅ¯ pouze tÃ­m, Å¾e jim poskytneme funkci odmÄ›ny, kterÃ¡ definuje poÅ¾adovanÃ½ stav hry, a dÃ¡me jim pÅ™Ã­leÅ¾itost inteligentnÄ› prozkoumat prostor hledÃ¡nÃ­. ÃšspÄ›Å¡nÄ› jsme vyzkouÅ¡eli dva algoritmy a dosÃ¡hli dobrÃ©ho vÃ½sledku v relativnÄ› krÃ¡tkÃ©m Äase. Toto je vÅ¡ak pouze zaÄÃ¡tek vaÅ¡Ã­ cesty do RL, a mÄ›li byste urÄitÄ› zvÃ¡Å¾it absolvovÃ¡nÃ­ samostatnÃ©ho kurzu, pokud chcete jÃ­t hloubÄ›ji.

## ğŸš€ VÃ½zva

Prozkoumejte aplikace uvedenÃ© v sekci 'DalÅ¡Ã­ Ãºkoly RL' a zkuste jednu implementovat!

## [KvÃ­z po pÅ™ednÃ¡Å¡ce](https://ff-quizzes.netlify.app/en/ai/quiz/44)

## PÅ™ehled & Samostudium

VÃ­ce o klasickÃ©m posilovanÃ©m uÄenÃ­ se dozvÃ­te v naÅ¡em [kurikulu StrojovÃ©ho uÄenÃ­ pro zaÄÃ¡teÄnÃ­ky](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/README.md).

PodÃ­vejte se na [toto skvÄ›lÃ© video](https://www.youtube.com/watch?v=qv6UVOQ0F44), kterÃ© ukazuje, jak se poÄÃ­taÄ mÅ¯Å¾e nauÄit hrÃ¡t Super Mario.

## Ãškol: [VytrÃ©nujte Mountain Car](lab/README.md)

VaÅ¡Ã­m cÃ­lem bÄ›hem tohoto Ãºkolu bude vytrÃ©novat jinÃ© prostÅ™edÃ­ Gym - [Mountain Car](https://www.gymlibrary.ml/environments/classic_control/mountain_car/).

---

