<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "186bf7eeab776b36f557357ea56d4751",
  "translation_date": "2025-08-25T23:44:56+00:00",
  "source_file": "lessons/3-NeuralNetworks/04-OwnFramework/README.md",
  "language_code": "cs"
}
-->
# Ãšvod do neuronovÃ½ch sÃ­tÃ­. VÃ­cevrstvÃ½ perceptron

V pÅ™edchozÃ­ ÄÃ¡sti jste se seznÃ¡mili s nejjednoduÅ¡Å¡Ã­m modelem neuronovÃ© sÃ­tÄ› â€“ jednovrstvÃ½m perceptronem, coÅ¾ je lineÃ¡rnÃ­ model pro dvoutÅ™Ã­dovou klasifikaci.

V tÃ©to ÄÃ¡sti rozÅ¡Ã­Å™Ã­me tento model na flexibilnÄ›jÅ¡Ã­ rÃ¡mec, kterÃ½ nÃ¡m umoÅ¾nÃ­:

* provÃ¡dÄ›t **vÃ­cetÅ™Ã­dovou klasifikaci** kromÄ› dvoutÅ™Ã­dovÃ©
* Å™eÅ¡it **regresnÃ­ Ãºlohy** kromÄ› klasifikace
* oddÄ›lovat tÅ™Ã­dy, kterÃ© nejsou lineÃ¡rnÄ› separovatelnÃ©

TakÃ© vyvineme vlastnÃ­ modulÃ¡rnÃ­ rÃ¡mec v Pythonu, kterÃ½ nÃ¡m umoÅ¾nÃ­ vytvÃ¡Å™et rÅ¯znÃ© architektury neuronovÃ½ch sÃ­tÃ­.

## [KvÃ­z pÅ™ed pÅ™ednÃ¡Å¡kou](https://ff-quizzes.netlify.app/en/ai/quiz/7)

## Formalizace strojovÃ©ho uÄenÃ­

ZaÄnÄ›me formalizacÃ­ problÃ©mu strojovÃ©ho uÄenÃ­. PÅ™edpoklÃ¡dejme, Å¾e mÃ¡me trÃ©novacÃ­ dataset **X** s pÅ™Ã­sluÅ¡nÃ½mi Å¡tÃ­tky **Y**, a potÅ™ebujeme vytvoÅ™it model *f*, kterÃ½ bude poskytovat co nejpÅ™esnÄ›jÅ¡Ã­ predikce. Kvalita predikcÃ­ se mÄ›Å™Ã­ pomocÃ­ **ztrÃ¡tovÃ© funkce** â„’. ÄŒasto se pouÅ¾Ã­vajÃ­ nÃ¡sledujÃ­cÃ­ ztrÃ¡tovÃ© funkce:

* Pro regresnÃ­ Ãºlohy, kdy potÅ™ebujeme pÅ™edpovÄ›dÄ›t ÄÃ­slo, mÅ¯Å¾eme pouÅ¾Ã­t **absolutnÃ­ chybu** âˆ‘<sub>i</sub>|f(x<sup>(i)</sup>)-y<sup>(i)</sup>| nebo **kvadratickou chybu** âˆ‘<sub>i</sub>(f(x<sup>(i)</sup>)-y<sup>(i)</sup>)<sup>2</sup>
* Pro klasifikaci pouÅ¾Ã­vÃ¡me **0-1 ztrÃ¡tu** (coÅ¾ je v podstatÄ› totÃ©Å¾ jako **pÅ™esnost** modelu) nebo **logistickou ztrÃ¡tu**.

Pro jednovrstvÃ½ perceptron byla funkce *f* definovÃ¡na jako lineÃ¡rnÃ­ funkce *f(x)=wx+b* (kde *w* je matice vah, *x* je vektor vstupnÃ­ch vlastnostÃ­ a *b* je vektor biasu). U rÅ¯znÃ½ch architektur neuronovÃ½ch sÃ­tÃ­ mÅ¯Å¾e mÃ­t tato funkce sloÅ¾itÄ›jÅ¡Ã­ formu.

> V pÅ™Ã­padÄ› klasifikace je Äasto Å¾Ã¡doucÃ­ zÃ­skat pravdÄ›podobnosti odpovÃ­dajÃ­cÃ­ch tÅ™Ã­d jako vÃ½stup sÃ­tÄ›. K pÅ™evodu libovolnÃ½ch ÄÃ­sel na pravdÄ›podobnosti (napÅ™. k normalizaci vÃ½stupu) Äasto pouÅ¾Ã­vÃ¡me funkci **softmax** Ïƒ, a funkce *f* se stÃ¡vÃ¡ *f(x)=Ïƒ(wx+b)*.

V definici *f* vÃ½Å¡e se *w* a *b* nazÃ½vajÃ­ **parametry** Î¸=âŸ¨*w,b*âŸ©. Na zÃ¡kladÄ› datasetu âŸ¨**X**,**Y**âŸ© mÅ¯Å¾eme vypoÄÃ­tat celkovou chybu na celÃ©m datasetu jako funkci parametrÅ¯ Î¸.

> âœ… **CÃ­lem trÃ©novÃ¡nÃ­ neuronovÃ© sÃ­tÄ› je minimalizovat chybu zmÄ›nou parametrÅ¯ Î¸**

## Optimalizace pomocÃ­ gradientnÃ­ho sestupu

Existuje znÃ¡mÃ¡ metoda optimalizace funkcÃ­ nazvanÃ¡ **gradientnÃ­ sestup**. MyÅ¡lenka spoÄÃ­vÃ¡ v tom, Å¾e mÅ¯Å¾eme vypoÄÃ­tat derivaci (v multidimenzionÃ¡lnÃ­m pÅ™Ã­padÄ› nazÃ½vanou **gradient**) ztrÃ¡tovÃ© funkce vzhledem k parametrÅ¯m a mÄ›nit parametry tak, aby se chyba sniÅ¾ovala. To lze formalizovat nÃ¡sledovnÄ›:

* Inicializujte parametry nÃ¡hodnÃ½mi hodnotami w<sup>(0)</sup>, b<sup>(0)</sup>
* Opakujte nÃ¡sledujÃ­cÃ­ krok mnohokrÃ¡t:
    - w<sup>(i+1)</sup> = w<sup>(i)</sup>-Î·âˆ‚â„’/âˆ‚w
    - b<sup>(i+1)</sup> = b<sup>(i)</sup>-Î·âˆ‚â„’/âˆ‚b

BÄ›hem trÃ©novÃ¡nÃ­ by mÄ›ly bÃ½t optimalizaÄnÃ­ kroky vypoÄÃ­tÃ¡vÃ¡ny s ohledem na celÃ½ dataset (pamatujte, Å¾e ztrÃ¡ta se poÄÃ­tÃ¡ jako souÄet pÅ™es vÅ¡echny trÃ©novacÃ­ vzorky). V praxi vÅ¡ak bereme malÃ© ÄÃ¡sti datasetu nazÃ½vanÃ© **minibatch** a poÄÃ­tÃ¡me gradienty na zÃ¡kladÄ› podmnoÅ¾iny dat. ProtoÅ¾e podmnoÅ¾ina je pokaÅ¾dÃ© vybÃ­rÃ¡na nÃ¡hodnÄ›, tato metoda se nazÃ½vÃ¡ **stochastickÃ½ gradientnÃ­ sestup** (SGD).

## VÃ­cevrstvÃ© perceptrony a zpÄ›tnÃ© Å¡Ã­Å™enÃ­

JednovrstvÃ¡ sÃ­Å¥, jak jsme vidÄ›li vÃ½Å¡e, je schopna klasifikovat lineÃ¡rnÄ› separovatelnÃ© tÅ™Ã­dy. Pro vytvoÅ™enÃ­ bohatÅ¡Ã­ho modelu mÅ¯Å¾eme kombinovat nÄ›kolik vrstev sÃ­tÄ›. Matematicky by to znamenalo, Å¾e funkce *f* bude mÃ­t sloÅ¾itÄ›jÅ¡Ã­ formu a bude vypoÄÃ­tÃ¡vÃ¡na v nÄ›kolika krocÃ­ch:
* z<sub>1</sub>=w<sub>1</sub>x+b<sub>1</sub>
* z<sub>2</sub>=w<sub>2</sub>Î±(z<sub>1</sub>)+b<sub>2</sub>
* f = Ïƒ(z<sub>2</sub>)

Zde je Î± **nelineÃ¡rnÃ­ aktivaÄnÃ­ funkce**, Ïƒ je funkce softmax a parametry jsou Î¸=<*w<sub>1</sub>,b<sub>1</sub>,w<sub>2</sub>,b<sub>2</sub>*>.

Algoritmus gradientnÃ­ho sestupu zÅ¯stane stejnÃ½, ale vÃ½poÄet gradientÅ¯ bude sloÅ¾itÄ›jÅ¡Ã­. Na zÃ¡kladÄ› pravidla Å™etÄ›zovÃ© derivace mÅ¯Å¾eme vypoÄÃ­tat derivace jako:

* âˆ‚â„’/âˆ‚w<sub>2</sub> = (âˆ‚â„’/âˆ‚Ïƒ)(âˆ‚Ïƒ/âˆ‚z<sub>2</sub>)(âˆ‚z<sub>2</sub>/âˆ‚w<sub>2</sub>)
* âˆ‚â„’/âˆ‚w<sub>1</sub> = (âˆ‚â„’/âˆ‚Ïƒ)(âˆ‚Ïƒ/âˆ‚z<sub>2</sub>)(âˆ‚z<sub>2</sub>/âˆ‚Î±)(âˆ‚Î±/âˆ‚z<sub>1</sub>)(âˆ‚z<sub>1</sub>/âˆ‚w<sub>1</sub>)

> âœ… Pravidlo Å™etÄ›zovÃ© derivace se pouÅ¾Ã­vÃ¡ k vÃ½poÄtu derivacÃ­ ztrÃ¡tovÃ© funkce vzhledem k parametrÅ¯m.

VÅ¡imnÄ›te si, Å¾e levÃ¡ ÄÃ¡st vÅ¡ech tÄ›chto vÃ½razÅ¯ je stejnÃ¡, a proto mÅ¯Å¾eme efektivnÄ› vypoÄÃ­tat derivace poÄÃ­naje ztrÃ¡tovou funkcÃ­ a postupovat "zpÄ›tnÄ›" skrz vÃ½poÄetnÃ­ graf. Proto se metoda trÃ©novÃ¡nÃ­ vÃ­cevrstvÃ©ho perceptronu nazÃ½vÃ¡ **zpÄ›tnÃ© Å¡Ã­Å™enÃ­** nebo 'backprop'.

<img alt="vÃ½poÄetnÃ­ graf" src="images/ComputeGraphGrad.png"/>

> TODO: citace obrÃ¡zku

> âœ… ZpÄ›tnÃ© Å¡Ã­Å™enÃ­ probereme mnohem podrobnÄ›ji v naÅ¡em pÅ™Ã­kladovÃ©m notebooku.  

## ZÃ¡vÄ›r

V tÃ©to lekci jsme vytvoÅ™ili vlastnÃ­ knihovnu neuronovÃ½ch sÃ­tÃ­ a pouÅ¾ili ji pro jednoduchÃ½ dvourozmÄ›rnÃ½ klasifikaÄnÃ­ Ãºkol.

## ğŸš€ VÃ½zva

V pÅ™iloÅ¾enÃ©m notebooku implementujete vlastnÃ­ rÃ¡mec pro vytvÃ¡Å™enÃ­ a trÃ©novÃ¡nÃ­ vÃ­cevrstvÃ½ch perceptronÅ¯. Budete moci podrobnÄ› vidÄ›t, jak modernÃ­ neuronovÃ© sÃ­tÄ› fungujÃ­.

PokraÄujte do notebooku [OwnFramework](../../../../../lessons/3-NeuralNetworks/04-OwnFramework/OwnFramework.ipynb) a projdÄ›te si ho.

## [KvÃ­z po pÅ™ednÃ¡Å¡ce](https://ff-quizzes.netlify.app/en/ai/quiz/8)

## PÅ™ehled & Samostudium

ZpÄ›tnÃ© Å¡Ã­Å™enÃ­ je bÄ›Å¾nÃ½ algoritmus pouÅ¾Ã­vanÃ½ v AI a ML, stojÃ­ za to ho [studovat podrobnÄ›ji](https://wikipedia.org/wiki/Backpropagation).

## [Ãškol](lab/README.md)

V tomto laboratornÃ­m cviÄenÃ­ mÃ¡te za Ãºkol pouÅ¾Ã­t rÃ¡mec, kterÃ½ jste vytvoÅ™ili v tÃ©to lekci, k Å™eÅ¡enÃ­ klasifikace ruÄnÄ› psanÃ½ch ÄÃ­slic MNIST.

* [Instrukce](lab/README.md)
* [Notebook](../../../../../lessons/3-NeuralNetworks/04-OwnFramework/lab/MyFW_MNIST.ipynb)

**UpozornÄ›nÃ­**:  
Tento dokument byl pÅ™eloÅ¾en pomocÃ­ sluÅ¾by pro automatickÃ½ pÅ™eklad [Co-op Translator](https://github.com/Azure/co-op-translator). I kdyÅ¾ se snaÅ¾Ã­me o pÅ™esnost, mÄ›jte prosÃ­m na pamÄ›ti, Å¾e automatickÃ© pÅ™eklady mohou obsahovat chyby nebo nepÅ™esnosti. PÅ¯vodnÃ­ dokument v jeho pÅ¯vodnÃ­m jazyce by mÄ›l bÃ½t povaÅ¾ovÃ¡n za zÃ¡vaznÃ½ zdroj. Pro dÅ¯leÅ¾itÃ© informace se doporuÄuje profesionÃ¡lnÃ­ lidskÃ½ pÅ™eklad. NezodpovÃ­dÃ¡me za jakÃ©koli nedorozumÄ›nÃ­ nebo nesprÃ¡vnÃ© interpretace vyplÃ½vajÃ­cÃ­ z pouÅ¾itÃ­ tohoto pÅ™ekladu.