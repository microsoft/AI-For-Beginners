<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "2b544f20b796402507fb05a0df893323",
  "translation_date": "2025-08-25T23:50:18+00:00",
  "source_file": "lessons/3-NeuralNetworks/05-Frameworks/README.md",
  "language_code": "cs"
}
-->
# Frameworky pro neuronovÃ© sÃ­tÄ›

Jak jsme se jiÅ¾ nauÄili, abychom mohli efektivnÄ› trÃ©novat neuronovÃ© sÃ­tÄ›, musÃ­me udÄ›lat dvÄ› vÄ›ci:

* Pracovat s tensory, napÅ™. nÃ¡sobit, sÄÃ­tat a poÄÃ­tat nÄ›kterÃ© funkce, jako je sigmoid nebo softmax
* PoÄÃ­tat gradienty vÅ¡ech vÃ½razÅ¯, abychom mohli provÃ¡dÄ›t optimalizaci pomocÃ­ gradientnÃ­ho sestupu

## [KvÃ­z pÅ™ed pÅ™ednÃ¡Å¡kou](https://ff-quizzes.netlify.app/en/ai/quiz/9)

ZatÃ­mco knihovna `numpy` zvlÃ¡dne prvnÃ­ ÄÃ¡st, potÅ™ebujeme nÄ›jakÃ½ mechanismus pro vÃ½poÄet gradientÅ¯. V [naÅ¡em frameworku](../../../../../lessons/3-NeuralNetworks/04-OwnFramework/OwnFramework.ipynb), kterÃ½ jsme vytvoÅ™ili v pÅ™edchozÃ­ sekci, jsme museli ruÄnÄ› programovat vÅ¡echny derivace funkcÃ­ uvnitÅ™ metody `backward`, kterÃ¡ provÃ¡dÃ­ zpÄ›tnou propagaci. IdeÃ¡lnÄ› by nÃ¡m framework mÄ›l umoÅ¾nit poÄÃ­tat gradienty *jakÃ©hokoliv vÃ½razu*, kterÃ½ mÅ¯Å¾eme definovat.

DalÅ¡Ã­ dÅ¯leÅ¾itou vÄ›cÃ­ je schopnost provÃ¡dÄ›t vÃ½poÄty na GPU nebo jinÃ½ch specializovanÃ½ch vÃ½poÄetnÃ­ch jednotkÃ¡ch, jako je [TPU](https://en.wikipedia.org/wiki/Tensor_Processing_Unit). TrÃ©novÃ¡nÃ­ hlubokÃ½ch neuronovÃ½ch sÃ­tÃ­ vyÅ¾aduje *obrovskÃ© mnoÅ¾stvÃ­* vÃ½poÄtÅ¯, a moÅ¾nost paralelizovat tyto vÃ½poÄty na GPU je velmi dÅ¯leÅ¾itÃ¡.

> âœ… TermÃ­n 'paralelizovat' znamenÃ¡ rozdÄ›lit vÃ½poÄty mezi vÃ­ce zaÅ™Ã­zenÃ­.

V souÄasnosti jsou nejpopulÃ¡rnÄ›jÅ¡Ã­mi frameworky pro neuronovÃ© sÃ­tÄ›: [TensorFlow](http://TensorFlow.org) a [PyTorch](https://pytorch.org/). Oba poskytujÃ­ nÃ­zkoÃºrovÅˆovÃ© API pro prÃ¡ci s tensory na CPU i GPU. Nad tÄ›mito nÃ­zkoÃºrovÅˆovÃ½mi API existujÃ­ takÃ© vysokoÃºrovÅˆovÃ¡ API, nazÃ½vanÃ¡ [Keras](https://keras.io/) a [PyTorch Lightning](https://pytorchlightning.ai/).

NÃ­zkoÃºrovÅˆovÃ© API | [TensorFlow](http://TensorFlow.org) | [PyTorch](https://pytorch.org/)
------------------|-------------------------------------|--------------------------------
VysokoÃºrovÅˆovÃ© API| [Keras](https://keras.io/) | [PyTorch Lightning](https://pytorchlightning.ai/)

**NÃ­zkoÃºrovÅˆovÃ¡ API** v obou frameworcÃ­ch umoÅ¾ÅˆujÃ­ vytvÃ¡Å™et tzv. **vÃ½poÄetnÃ­ grafy**. Tento graf definuje, jak vypoÄÃ­tat vÃ½stup (obvykle ztrÃ¡tovou funkci) s danÃ½mi vstupnÃ­mi parametry a mÅ¯Å¾e bÃ½t odeslÃ¡n k vÃ½poÄtu na GPU, pokud je k dispozici. ExistujÃ­ funkce pro diferenciaci tohoto vÃ½poÄetnÃ­ho grafu a vÃ½poÄet gradientÅ¯, kterÃ© lze nÃ¡slednÄ› pouÅ¾Ã­t k optimalizaci parametrÅ¯ modelu.

**VysokoÃºrovÅˆovÃ¡ API** povaÅ¾ujÃ­ neuronovÃ© sÃ­tÄ› za **sekvenci vrstev** a usnadÅˆujÃ­ konstrukci vÄ›tÅ¡iny neuronovÃ½ch sÃ­tÃ­. TrÃ©novÃ¡nÃ­ modelu obvykle vyÅ¾aduje pÅ™Ã­pravu dat a nÃ¡slednÃ© zavolÃ¡nÃ­ funkce `fit`, kterÃ¡ provede potÅ™ebnÃ© operace.

VysokoÃºrovÅˆovÃ¡ API umoÅ¾ÅˆujÃ­ velmi rychle sestavit typickÃ© neuronovÃ© sÃ­tÄ›, aniÅ¾ byste se museli starat o mnoho detailÅ¯. Na druhou stranu nÃ­zkoÃºrovÅˆovÃ¡ API poskytujÃ­ mnohem vÄ›tÅ¡Ã­ kontrolu nad procesem trÃ©novÃ¡nÃ­, a proto se Äasto pouÅ¾Ã­vajÃ­ ve vÃ½zkumu, kdyÅ¾ pracujete s novÃ½mi architekturami neuronovÃ½ch sÃ­tÃ­.

Je takÃ© dÅ¯leÅ¾itÃ© pochopit, Å¾e mÅ¯Å¾ete pouÅ¾Ã­vat obÄ› API spoleÄnÄ›, napÅ™. mÅ¯Å¾ete vyvinout vlastnÃ­ architekturu vrstvy s pouÅ¾itÃ­m nÃ­zkoÃºrovÅˆovÃ©ho API a potÃ© ji pouÅ¾Ã­t uvnitÅ™ vÄ›tÅ¡Ã­ sÃ­tÄ› vytvoÅ™enÃ© a trÃ©novanÃ© pomocÃ­ vysokoÃºrovÅˆovÃ©ho API. Nebo mÅ¯Å¾ete definovat sÃ­Å¥ pomocÃ­ vysokoÃºrovÅˆovÃ©ho API jako sekvenci vrstev a potÃ© pouÅ¾Ã­t vlastnÃ­ nÃ­zkoÃºrovÅˆovou trÃ©novacÃ­ smyÄku pro optimalizaci. ObÄ› API pouÅ¾Ã­vajÃ­ stejnÃ© zÃ¡kladnÃ­ koncepty a jsou navrÅ¾ena tak, aby spolu dobÅ™e fungovala.

## UÄenÃ­

V tomto kurzu nabÃ­zÃ­me vÄ›tÅ¡inu obsahu jak pro PyTorch, tak pro TensorFlow. MÅ¯Å¾ete si vybrat preferovanÃ½ framework a projÃ­t pouze odpovÃ­dajÃ­cÃ­ notebooky. Pokud si nejste jisti, kterÃ½ framework zvolit, pÅ™eÄtÄ›te si nÄ›kterÃ© diskuse na internetu na tÃ©ma **PyTorch vs. TensorFlow**. MÅ¯Å¾ete se takÃ© podÃ­vat na oba frameworky, abyste zÃ­skali lepÅ¡Ã­ pÅ™edstavu.

Kde je to moÅ¾nÃ©, pouÅ¾ijeme pro jednoduchost vysokoÃºrovÅˆovÃ¡ API. NicmÃ©nÄ› vÄ›Å™Ã­me, Å¾e je dÅ¯leÅ¾itÃ© pochopit, jak neuronovÃ© sÃ­tÄ› fungujÃ­ od zÃ¡kladÅ¯, a proto zaÄÃ­nÃ¡me pracÃ­ s nÃ­zkoÃºrovÅˆovÃ½m API a tensory. Pokud vÅ¡ak chcete zaÄÃ­t rychle a nechcete trÃ¡vit pÅ™Ã­liÅ¡ mnoho Äasu uÄenÃ­m tÄ›chto detailÅ¯, mÅ¯Å¾ete tyto ÄÃ¡sti pÅ™eskoÄit a pÅ™ejÃ­t rovnou k notebookÅ¯m s vysokoÃºrovÅˆovÃ½m API.

## âœï¸ CviÄenÃ­: Frameworky

PokraÄujte ve svÃ©m uÄenÃ­ v nÃ¡sledujÃ­cÃ­ch noteboocÃ­ch:

NÃ­zkoÃºrovÅˆovÃ© API | [TensorFlow+Keras Notebook](../../../../../lessons/3-NeuralNetworks/05-Frameworks/IntroKerasTF.ipynb) | [PyTorch](../../../../../lessons/3-NeuralNetworks/05-Frameworks/IntroPyTorch.ipynb)
------------------|-------------------------------------|--------------------------------
VysokoÃºrovÅˆovÃ© API| [Keras](../../../../../lessons/3-NeuralNetworks/05-Frameworks/IntroKeras.ipynb) | *PyTorch Lightning*

Po zvlÃ¡dnutÃ­ frameworkÅ¯ si zopakujme pojem pÅ™euÄenÃ­.

# PÅ™euÄenÃ­

PÅ™euÄenÃ­ je extrÃ©mnÄ› dÅ¯leÅ¾itÃ½ koncept ve strojovÃ©m uÄenÃ­ a je velmi dÅ¯leÅ¾itÃ© jej sprÃ¡vnÄ› pochopit!

ZvaÅ¾te nÃ¡sledujÃ­cÃ­ problÃ©m aproximace 5 bodÅ¯ (reprezentovanÃ½ch `x` na grafech nÃ­Å¾e):

![linear](../../../../../translated_images/overfit1.f24b71c6f652e59e6bed7245ffbeaecc3ba320e16e2221f6832b432052c4da43.cs.jpg) | ![overfit](../../../../../translated_images/overfit2.131f5800ae10ca5e41d12a411f5f705d9ee38b1b10916f284b787028dd55cc1c.cs.jpg)
-------------------------|--------------------------
**LineÃ¡rnÃ­ model, 2 parametry** | **NelineÃ¡rnÃ­ model, 7 parametrÅ¯**
Chyba na trÃ©novacÃ­ch datech = 5.3 | Chyba na trÃ©novacÃ­ch datech = 0
Chyba na validaÄnÃ­ch datech = 5.1 | Chyba na validaÄnÃ­ch datech = 20

* Na levÃ© stranÄ› vidÃ­me dobrou aproximaci pÅ™Ã­mkou. ProtoÅ¾e poÄet parametrÅ¯ je pÅ™imÄ›Å™enÃ½, model sprÃ¡vnÄ› zachycuje rozloÅ¾enÃ­ bodÅ¯.
* Na pravÃ© stranÄ› je model pÅ™Ã­liÅ¡ vÃ½konnÃ½. ProtoÅ¾e mÃ¡me pouze 5 bodÅ¯ a model mÃ¡ 7 parametrÅ¯, mÅ¯Å¾e se pÅ™izpÅ¯sobit tak, aby proÅ¡el vÅ¡emi body, coÅ¾ zpÅ¯sobÃ­, Å¾e chyba na trÃ©novacÃ­ch datech bude 0. To vÅ¡ak brÃ¡nÃ­ modelu pochopit sprÃ¡vnÃ½ vzor v datech, a proto je chyba na validaÄnÃ­ch datech velmi vysokÃ¡.

Je velmi dÅ¯leÅ¾itÃ© najÃ­t sprÃ¡vnou rovnovÃ¡hu mezi sloÅ¾itostÃ­ modelu (poÄtem parametrÅ¯) a poÄtem trÃ©novacÃ­ch vzorkÅ¯.

## ProÄ dochÃ¡zÃ­ k pÅ™euÄenÃ­

  * Nedostatek trÃ©novacÃ­ch dat
  * PÅ™Ã­liÅ¡ vÃ½konnÃ½ model
  * PÅ™Ã­liÅ¡ mnoho Å¡umu ve vstupnÃ­ch datech

## Jak detekovat pÅ™euÄenÃ­

Jak mÅ¯Å¾ete vidÄ›t z grafu vÃ½Å¡e, pÅ™euÄenÃ­ lze detekovat velmi nÃ­zkou chybou na trÃ©novacÃ­ch datech a vysokou chybou na validaÄnÃ­ch datech. BÄ›hem trÃ©novÃ¡nÃ­ obvykle vidÃ­me, Å¾e chyby na trÃ©novacÃ­ch i validaÄnÃ­ch datech zaÄÃ­najÃ­ klesat, ale v urÄitÃ©m bodÄ› mÅ¯Å¾e chyba na validaÄnÃ­ch datech pÅ™estat klesat a zaÄÃ­t rÅ¯st. To bude znÃ¡mka pÅ™euÄenÃ­ a indikÃ¡tor, Å¾e bychom mÄ›li pravdÄ›podobnÄ› trÃ©novÃ¡nÃ­ v tomto bodÄ› zastavit (nebo alespoÅˆ uloÅ¾it aktuÃ¡lnÃ­ stav modelu).

![overfitting](../../../../../translated_images/Overfitting.408ad91cd90b4371d0a81f4287e1409c359751adeb1ae450332af50e84f08c3e.cs.png)

## Jak zabrÃ¡nit pÅ™euÄenÃ­

Pokud zjistÃ­te, Å¾e dochÃ¡zÃ­ k pÅ™euÄenÃ­, mÅ¯Å¾ete udÄ›lat nÃ¡sledujÃ­cÃ­:

 * ZvÃ½Å¡it mnoÅ¾stvÃ­ trÃ©novacÃ­ch dat
 * SnÃ­Å¾it sloÅ¾itost modelu
 * PouÅ¾Ã­t nÄ›jakou [regularizaÄnÃ­ techniku](../../4-ComputerVision/08-TransferLearning/TrainingTricks.md), jako je [Dropout](../../4-ComputerVision/08-TransferLearning/TrainingTricks.md#Dropout), kterou si pozdÄ›ji probereme.

## PÅ™euÄenÃ­ a kompromis mezi zkreslenÃ­m a rozptylem

PÅ™euÄenÃ­ je ve skuteÄnosti pÅ™Ã­pad obecnÄ›jÅ¡Ã­ho problÃ©mu ve statistice nazÃ½vanÃ©ho [kompromis mezi zkreslenÃ­m a rozptylem](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff). Pokud vezmeme v Ãºvahu moÅ¾nÃ© zdroje chyb v naÅ¡em modelu, mÅ¯Å¾eme rozliÅ¡it dva typy chyb:

* **Chyby zpÅ¯sobenÃ© zkreslenÃ­m** jsou zpÅ¯sobeny tÃ­m, Å¾e nÃ¡Å¡ algoritmus nedokÃ¡Å¾e sprÃ¡vnÄ› zachytit vztah mezi trÃ©novacÃ­mi daty. MÅ¯Å¾e to bÃ½t dÅ¯sledek toho, Å¾e nÃ¡Å¡ model nenÃ­ dostateÄnÄ› vÃ½konnÃ½ (**podtrÃ©novÃ¡nÃ­**).
* **Chyby zpÅ¯sobenÃ© rozptylem**, kterÃ© jsou zpÅ¯sobeny tÃ­m, Å¾e model aproximuje Å¡um ve vstupnÃ­ch datech mÃ­sto smysluplnÃ©ho vztahu (**pÅ™euÄenÃ­**).

BÄ›hem trÃ©novÃ¡nÃ­ se chyba zpÅ¯sobenÃ¡ zkreslenÃ­m sniÅ¾uje (jak se model uÄÃ­ aproximovat data) a chyba zpÅ¯sobenÃ¡ rozptylem roste. Je dÅ¯leÅ¾itÃ© zastavit trÃ©novÃ¡nÃ­ - buÄ manuÃ¡lnÄ› (kdyÅ¾ detekujeme pÅ™euÄenÃ­), nebo automaticky (zavedenÃ­m regularizace) - aby se zabrÃ¡nilo pÅ™euÄenÃ­.

## ZÃ¡vÄ›r

V tÃ©to lekci jste se dozvÄ›dÄ›li o rozdÃ­lech mezi rÅ¯znÃ½mi API pro dva nejpopulÃ¡rnÄ›jÅ¡Ã­ AI frameworky, TensorFlow a PyTorch. KromÄ› toho jste se nauÄili o velmi dÅ¯leÅ¾itÃ©m tÃ©matu, pÅ™euÄenÃ­.

## ğŸš€ VÃ½zva

V pÅ™iloÅ¾enÃ½ch noteboocÃ­ch najdete na konci 'Ãºkoly'; projdÄ›te si notebooky a splÅˆte Ãºkoly.

## [KvÃ­z po pÅ™ednÃ¡Å¡ce](https://ff-quizzes.netlify.app/en/ai/quiz/10)

## PÅ™ehled a samostudium

ProveÄte vÃ½zkum na nÃ¡sledujÃ­cÃ­ tÃ©mata:

- TensorFlow
- PyTorch
- PÅ™euÄenÃ­

Zeptejte se sami sebe na nÃ¡sledujÃ­cÃ­ otÃ¡zky:

- JakÃ½ je rozdÃ­l mezi TensorFlow a PyTorch?
- JakÃ½ je rozdÃ­l mezi pÅ™euÄenÃ­m a podtrÃ©novÃ¡nÃ­m?

## [ZadÃ¡nÃ­](lab/README.md)

V tomto laboratornÃ­m cviÄenÃ­ mÃ¡te za Ãºkol vyÅ™eÅ¡it dva klasifikaÄnÃ­ problÃ©my pomocÃ­ jednovrstvÃ½ch a vÃ­cevrstvÃ½ch plnÄ› propojenÃ½ch sÃ­tÃ­ s vyuÅ¾itÃ­m PyTorch nebo TensorFlow.

* [Instrukce](lab/README.md)
* [Notebook](../../../../../lessons/3-NeuralNetworks/05-Frameworks/lab/LabFrameworks.ipynb)

**ProhlÃ¡Å¡enÃ­:**  
Tento dokument byl pÅ™eloÅ¾en pomocÃ­ sluÅ¾by pro automatickÃ½ pÅ™eklad [Co-op Translator](https://github.com/Azure/co-op-translator). AÄkoli se snaÅ¾Ã­me o pÅ™esnost, mÄ›jte prosÃ­m na pamÄ›ti, Å¾e automatickÃ© pÅ™eklady mohou obsahovat chyby nebo nepÅ™esnosti. PÅ¯vodnÃ­ dokument v jeho pÅ¯vodnÃ­m jazyce by mÄ›l bÃ½t povaÅ¾ovÃ¡n za autoritativnÃ­ zdroj. Pro dÅ¯leÅ¾itÃ© informace se doporuÄuje profesionÃ¡lnÃ­ lidskÃ½ pÅ™eklad. NeodpovÃ­dÃ¡me za Å¾Ã¡dnÃ¡ nedorozumÄ›nÃ­ nebo nesprÃ¡vnÃ© interpretace vyplÃ½vajÃ­cÃ­ z pouÅ¾itÃ­ tohoto pÅ™ekladu.