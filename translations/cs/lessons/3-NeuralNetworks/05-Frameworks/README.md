# Frameworky pro neuronovÃ© sÃ­tÄ›

Jak jsme se jiÅ¾ nauÄili, abychom mohli efektivnÄ› trÃ©novat neuronovÃ© sÃ­tÄ›, musÃ­me udÄ›lat dvÄ› vÄ›ci:

* Pracovat s tensory, napÅ™Ã­klad nÃ¡sobit, sÄÃ­tat a poÄÃ­tat nÄ›kterÃ© funkce, jako sigmoid nebo softmax
* VypoÄÃ­tat gradienty vÅ¡ech vÃ½razÅ¯, abychom mohli provÃ¡dÄ›t optimalizaci pomocÃ­ gradientnÃ­ho sestupu

## [KvÃ­z pÅ™ed lekcÃ­](https://ff-quizzes.netlify.app/en/ai/quiz/9)

ZatÃ­mco knihovna `numpy` dokÃ¡Å¾e prvnÃ­ ÄÃ¡st, potÅ™ebujeme nÄ›jakÃ½ mechanismus pro vÃ½poÄet gradientÅ¯. V [naÅ¡em frameworku](../04-OwnFramework/OwnFramework.ipynb), kterÃ½ jsme vyvinuli v pÅ™edchozÃ­ sekci, jsme museli ruÄnÄ› naprogramovat vÅ¡echny derivace funkcÃ­ uvnitÅ™ metody `backward`, kterÃ¡ provÃ¡dÃ­ zpÄ›tnou propagaci. IdeÃ¡lnÄ› by nÃ¡m framework mÄ›l umoÅ¾nit vypoÄÃ­tat gradienty *jakÃ©hokoliv vÃ½razu*, kterÃ½ mÅ¯Å¾eme definovat.

DalÅ¡Ã­ dÅ¯leÅ¾itou vÄ›cÃ­ je schopnost provÃ¡dÄ›t vÃ½poÄty na GPU nebo jinÃ½ch specializovanÃ½ch vÃ½poÄetnÃ­ch jednotkÃ¡ch, jako je [TPU](https://en.wikipedia.org/wiki/Tensor_Processing_Unit). TrÃ©novÃ¡nÃ­ hlubokÃ½ch neuronovÃ½ch sÃ­tÃ­ vyÅ¾aduje *velkÃ© mnoÅ¾stvÃ­* vÃ½poÄtÅ¯, a moÅ¾nost paralelizovat tyto vÃ½poÄty na GPU je velmi dÅ¯leÅ¾itÃ¡.

> âœ… TermÃ­n 'paralelizovat' znamenÃ¡ rozdÄ›lit vÃ½poÄty mezi vÃ­ce zaÅ™Ã­zenÃ­.

V souÄasnosti jsou nejpopulÃ¡rnÄ›jÅ¡Ã­mi frameworky pro neuronovÃ© sÃ­tÄ›: [TensorFlow](http://TensorFlow.org) a [PyTorch](https://pytorch.org/). Oba poskytujÃ­ nÃ­zkoÃºrovÅˆovÃ© API pro prÃ¡ci s tensory na CPU i GPU. Nad nÃ­zkoÃºrovÅˆovÃ½m API existuje takÃ© vysokoÃºrovÅˆovÃ© API, nazÃ½vanÃ© [Keras](https://keras.io/) a [PyTorch Lightning](https://pytorchlightning.ai/) odpovÃ­dajÃ­cÃ­m zpÅ¯sobem.

NÃ­zkoÃºrovÅˆovÃ© API | [TensorFlow](http://TensorFlow.org) | [PyTorch](https://pytorch.org/)
------------------|-------------------------------------|--------------------------------
VysokoÃºrovÅˆovÃ© API| [Keras](https://keras.io/) | [PyTorch Lightning](https://pytorchlightning.ai/)

**NÃ­zkoÃºrovÅˆovÃ¡ API** v obou frameworcÃ­ch umoÅ¾ÅˆujÃ­ vytvÃ¡Å™et tzv. **vÃ½poÄetnÃ­ grafy**. Tento graf definuje, jak vypoÄÃ­tat vÃ½stup (obvykle ztrÃ¡tovou funkci) s danÃ½mi vstupnÃ­mi parametry, a mÅ¯Å¾e bÃ½t odeslÃ¡n k vÃ½poÄtu na GPU, pokud je dostupnÃ©. ExistujÃ­ funkce pro diferenciaci tohoto vÃ½poÄetnÃ­ho grafu a vÃ½poÄet gradientÅ¯, kterÃ© lze nÃ¡slednÄ› pouÅ¾Ã­t k optimalizaci parametrÅ¯ modelu.

**VysokoÃºrovÅˆovÃ¡ API** povaÅ¾ujÃ­ neuronovÃ© sÃ­tÄ› spÃ­Å¡e za **sekvenci vrstev** a usnadÅˆujÃ­ konstrukci vÄ›tÅ¡iny neuronovÃ½ch sÃ­tÃ­. TrÃ©novÃ¡nÃ­ modelu obvykle vyÅ¾aduje pÅ™Ã­pravu dat a nÃ¡slednÃ© volÃ¡nÃ­ funkce `fit`, kterÃ¡ provede prÃ¡ci.

VysokoÃºrovÅˆovÃ© API umoÅ¾Åˆuje velmi rychle konstruovat typickÃ© neuronovÃ© sÃ­tÄ›, aniÅ¾ byste se museli starat o mnoho detailÅ¯. Na druhou stranu nÃ­zkoÃºrovÅˆovÃ© API nabÃ­zÃ­ mnohem vÄ›tÅ¡Ã­ kontrolu nad procesem trÃ©novÃ¡nÃ­, a proto se Äasto pouÅ¾Ã­vÃ¡ ve vÃ½zkumu, kdyÅ¾ pracujete s novÃ½mi architekturami neuronovÃ½ch sÃ­tÃ­.

Je takÃ© dÅ¯leÅ¾itÃ© pochopit, Å¾e mÅ¯Å¾ete pouÅ¾Ã­vat obÄ› API spoleÄnÄ›, napÅ™Ã­klad mÅ¯Å¾ete vyvinout vlastnÃ­ architekturu vrstvy pomocÃ­ nÃ­zkoÃºrovÅˆovÃ©ho API a potÃ© ji pouÅ¾Ã­t uvnitÅ™ vÄ›tÅ¡Ã­ sÃ­tÄ› konstruovanÃ© a trÃ©novanÃ© pomocÃ­ vysokoÃºrovÅˆovÃ©ho API. Nebo mÅ¯Å¾ete definovat sÃ­Å¥ pomocÃ­ vysokoÃºrovÅˆovÃ©ho API jako sekvenci vrstev a potÃ© pouÅ¾Ã­t vlastnÃ­ nÃ­zkoÃºrovÅˆovou trÃ©novacÃ­ smyÄku k provedenÃ­ optimalizace. ObÄ› API pouÅ¾Ã­vajÃ­ stejnÃ© zÃ¡kladnÃ­ koncepty a jsou navrÅ¾eny tak, aby spolu dobÅ™e fungovaly.

## UÄenÃ­

V tomto kurzu nabÃ­zÃ­me vÄ›tÅ¡inu obsahu jak pro PyTorch, tak pro TensorFlow. MÅ¯Å¾ete si vybrat preferovanÃ½ framework a projÃ­t pouze odpovÃ­dajÃ­cÃ­ notebooky. Pokud si nejste jisti, kterÃ½ framework zvolit, pÅ™eÄtÄ›te si nÄ›kterÃ© diskuse na internetu o **PyTorch vs. TensorFlow**. MÅ¯Å¾ete se takÃ© podÃ­vat na oba frameworky, abyste zÃ­skali lepÅ¡Ã­ pÅ™edstavu.

Kde je to moÅ¾nÃ©, pouÅ¾ijeme vysokoÃºrovÅˆovÃ¡ API pro jednoduchost. NicmÃ©nÄ› vÄ›Å™Ã­me, Å¾e je dÅ¯leÅ¾itÃ© pochopit, jak neuronovÃ© sÃ­tÄ› fungujÃ­ od zÃ¡kladÅ¯, proto na zaÄÃ¡tku zaÄneme pracovat s nÃ­zkoÃºrovÅˆovÃ½m API a tensory. Pokud vÅ¡ak chcete zaÄÃ­t rychle a nechcete trÃ¡vit mnoho Äasu uÄenÃ­m tÄ›chto detailÅ¯, mÅ¯Å¾ete je pÅ™eskoÄit a jÃ­t pÅ™Ã­mo do notebookÅ¯ s vysokoÃºrovÅˆovÃ½m API.

## âœï¸ CviÄenÃ­: Frameworky

PokraÄujte ve svÃ©m uÄenÃ­ v nÃ¡sledujÃ­cÃ­ch noteboocÃ­ch:

NÃ­zkoÃºrovÅˆovÃ© API | [TensorFlow+Keras Notebook](IntroKerasTF.ipynb) | [PyTorch](IntroPyTorch.ipynb)
------------------|-------------------------------------|--------------------------------
VysokoÃºrovÅˆovÃ© API| [Keras](IntroKeras.ipynb) | *PyTorch Lightning*

Po zvlÃ¡dnutÃ­ frameworkÅ¯ si zopakujeme pojem pÅ™euÄenÃ­.

# PÅ™euÄenÃ­

PÅ™euÄenÃ­ je extrÃ©mnÄ› dÅ¯leÅ¾itÃ½ koncept v strojovÃ©m uÄenÃ­ a je velmi dÅ¯leÅ¾itÃ© ho sprÃ¡vnÄ› pochopit!

ZvaÅ¾te nÃ¡sledujÃ­cÃ­ problÃ©m aproximace 5 bodÅ¯ (reprezentovanÃ½ch `x` na grafech nÃ­Å¾e):

![linear](../../../../../translated_images/cs/overfit1.f24b71c6f652e59e.webp) | ![overfit](../../../../../translated_images/cs/overfit2.131f5800ae10ca5e.webp)
-------------------------|--------------------------
**LineÃ¡rnÃ­ model, 2 parametry** | **NelineÃ¡rnÃ­ model, 7 parametrÅ¯**
Chyba trÃ©novÃ¡nÃ­ = 5.3 | Chyba trÃ©novÃ¡nÃ­ = 0
Chyba validace = 5.1 | Chyba validace = 20

* Na levÃ© stranÄ› vidÃ­me dobrou aproximaci pÅ™Ã­mkou. ProtoÅ¾e poÄet parametrÅ¯ je pÅ™imÄ›Å™enÃ½, model sprÃ¡vnÄ› pochopÃ­ rozloÅ¾enÃ­ bodÅ¯.
* Na pravÃ© stranÄ› je model pÅ™Ã­liÅ¡ vÃ½konnÃ½. ProtoÅ¾e mÃ¡me pouze 5 bodÅ¯ a model mÃ¡ 7 parametrÅ¯, mÅ¯Å¾e se pÅ™izpÅ¯sobit tak, aby proÅ¡el vÅ¡emi body, coÅ¾ zpÅ¯sobÃ­, Å¾e chyba trÃ©novÃ¡nÃ­ bude 0. To vÅ¡ak brÃ¡nÃ­ modelu pochopit sprÃ¡vnÃ½ vzor v datech, coÅ¾ vede k velmi vysokÃ© chybÄ› validace.

Je velmi dÅ¯leÅ¾itÃ© najÃ­t sprÃ¡vnou rovnovÃ¡hu mezi sloÅ¾itostÃ­ modelu (poÄtem parametrÅ¯) a poÄtem trÃ©novacÃ­ch vzorkÅ¯.

## ProÄ dochÃ¡zÃ­ k pÅ™euÄenÃ­

  * Nedostatek trÃ©novacÃ­ch dat
  * PÅ™Ã­liÅ¡ vÃ½konnÃ½ model
  * PÅ™Ã­liÅ¡ mnoho Å¡umu ve vstupnÃ­ch datech

## Jak detekovat pÅ™euÄenÃ­

Jak mÅ¯Å¾ete vidÄ›t z grafu vÃ½Å¡e, pÅ™euÄenÃ­ lze detekovat velmi nÃ­zkou chybou trÃ©novÃ¡nÃ­ a vysokou chybou validace. BÄ›hem trÃ©novÃ¡nÃ­ obvykle vidÃ­me, Å¾e chyby trÃ©novÃ¡nÃ­ i validace zaÄÃ­najÃ­ klesat, a potÃ© v urÄitÃ©m bodÄ› mÅ¯Å¾e chyba validace pÅ™estat klesat a zaÄÃ­t stoupat. To bude znakem pÅ™euÄenÃ­ a indikÃ¡torem, Å¾e bychom pravdÄ›podobnÄ› mÄ›li v tomto bodÄ› zastavit trÃ©novÃ¡nÃ­ (nebo alespoÅˆ vytvoÅ™it snÃ­mek modelu).

![overfitting](../../../../../translated_images/cs/Overfitting.408ad91cd90b4371.webp)

## Jak zabrÃ¡nit pÅ™euÄenÃ­

Pokud vidÃ­te, Å¾e dochÃ¡zÃ­ k pÅ™euÄenÃ­, mÅ¯Å¾ete udÄ›lat nÃ¡sledujÃ­cÃ­:

 * ZvÃ½Å¡it mnoÅ¾stvÃ­ trÃ©novacÃ­ch dat
 * SnÃ­Å¾it sloÅ¾itost modelu
 * PouÅ¾Ã­t nÄ›jakou [techniku regularizace](../../4-ComputerVision/08-TransferLearning/TrainingTricks.md), jako napÅ™Ã­klad [Dropout](../../4-ComputerVision/08-TransferLearning/TrainingTricks.md#Dropout), kterou si pozdÄ›ji probereme.

## PÅ™euÄenÃ­ a kompromis mezi zkreslenÃ­m a rozptylem

PÅ™euÄenÃ­ je vlastnÄ› pÅ™Ã­pad obecnÄ›jÅ¡Ã­ho problÃ©mu ve statistice nazÃ½vanÃ©ho [kompromis mezi zkreslenÃ­m a rozptylem](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff). Pokud zvaÅ¾ujeme moÅ¾nÃ© zdroje chyb v naÅ¡em modelu, mÅ¯Å¾eme vidÄ›t dva typy chyb:

* **Chyby zkreslenÃ­** jsou zpÅ¯sobeny tÃ­m, Å¾e nÃ¡Å¡ algoritmus nedokÃ¡Å¾e sprÃ¡vnÄ› zachytit vztah mezi trÃ©novacÃ­mi daty. MÅ¯Å¾e to bÃ½t dÅ¯sledek toho, Å¾e nÃ¡Å¡ model nenÃ­ dostateÄnÄ› vÃ½konnÃ½ (**podtrÃ©novÃ¡nÃ­**).
* **Chyby rozptylu**, kterÃ© jsou zpÅ¯sobeny tÃ­m, Å¾e model aproximuje Å¡um ve vstupnÃ­ch datech mÃ­sto smysluplnÃ©ho vztahu (**pÅ™euÄenÃ­**).

BÄ›hem trÃ©novÃ¡nÃ­ se chyba zkreslenÃ­ sniÅ¾uje (jak se nÃ¡Å¡ model uÄÃ­ aproximovat data) a chyba rozptylu se zvyÅ¡uje. Je dÅ¯leÅ¾itÃ© zastavit trÃ©novÃ¡nÃ­ - buÄ manuÃ¡lnÄ› (kdyÅ¾ detekujeme pÅ™euÄenÃ­), nebo automaticky (zavedenÃ­m regularizace) - aby se zabrÃ¡nilo pÅ™euÄenÃ­.

## ZÃ¡vÄ›r

V tÃ©to lekci jste se nauÄili rozdÃ­ly mezi rÅ¯znÃ½mi API pro dva nejpopulÃ¡rnÄ›jÅ¡Ã­ AI frameworky, TensorFlow a PyTorch. KromÄ› toho jste se nauÄili o velmi dÅ¯leÅ¾itÃ©m tÃ©matu, pÅ™euÄenÃ­.

## ğŸš€ VÃ½zva

V pÅ™iloÅ¾enÃ½ch noteboocÃ­ch najdete 'Ãºkoly' na konci; projdÄ›te notebooky a splÅˆte Ãºkoly.

## [KvÃ­z po lekci](https://ff-quizzes.netlify.app/en/ai/quiz/10)

## PÅ™ehled & Samostudium

ProveÄte vÃ½zkum na nÃ¡sledujÃ­cÃ­ tÃ©mata:

- TensorFlow
- PyTorch
- PÅ™euÄenÃ­

Zeptejte se sami sebe na nÃ¡sledujÃ­cÃ­ otÃ¡zky:

- JakÃ½ je rozdÃ­l mezi TensorFlow a PyTorch?
- JakÃ½ je rozdÃ­l mezi pÅ™euÄenÃ­m a podtrÃ©novÃ¡nÃ­m?

## [Ãškol](lab/README.md)

V tomto laboratornÃ­m cviÄenÃ­ mÃ¡te za Ãºkol vyÅ™eÅ¡it dva klasifikaÄnÃ­ problÃ©my pomocÃ­ jedno- a vÃ­cevrstvÃ½ch plnÄ› propojenÃ½ch sÃ­tÃ­ s vyuÅ¾itÃ­m PyTorch nebo TensorFlow.

* [Instrukce](lab/README.md)
* [Notebook](lab/LabFrameworks.ipynb)

---

