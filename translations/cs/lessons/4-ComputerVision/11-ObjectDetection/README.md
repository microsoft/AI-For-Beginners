<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "d76a7eda28de5210c8b1ba50a6216c69",
  "translation_date": "2025-09-23T11:23:43+00:00",
  "source_file": "lessons/4-ComputerVision/11-ObjectDetection/README.md",
  "language_code": "cs"
}
-->
# Detekce objektÅ¯

Modely pro klasifikaci obrÃ¡zkÅ¯, se kterÃ½mi jsme se dosud zabÃ½vali, pÅ™ijÃ­maly obrÃ¡zek a produkovaly kategorickÃ½ vÃ½sledek, napÅ™Ã­klad tÅ™Ã­du 'ÄÃ­slo' v problÃ©mu MNIST. V mnoha pÅ™Ã­padech vÅ¡ak nechceme jen vÄ›dÄ›t, Å¾e obrÃ¡zek zobrazuje objekty â€“ chceme bÃ½t schopni urÄit jejich pÅ™esnou polohu. PÅ™esnÄ› o tom je **detekce objektÅ¯**.

## [KvÃ­z pÅ™ed lekcÃ­](https://ff-quizzes.netlify.app/en/ai/quiz/21)

![Detekce objektÅ¯](../../../../../translated_images/Screen_Shot_2016-11-17_at_11.14.54_AM.b4bb3769353287be1b905373ed9c858102c054b16e4595c76ec3f7bba0feb549.cs.png)

> ObrÃ¡zek z [webu YOLO v2](https://pjreddie.com/darknet/yolov2/)

## NaivnÃ­ pÅ™Ã­stup k detekci objektÅ¯

PÅ™edpoklÃ¡dejme, Å¾e chceme najÃ­t koÄku na obrÃ¡zku. Velmi naivnÃ­ pÅ™Ã­stup k detekci objektÅ¯ by mohl bÃ½t nÃ¡sledujÃ­cÃ­:

1. RozdÄ›lte obrÃ¡zek na mnoÅ¾stvÃ­ dlaÅ¾dic.
2. ProveÄte klasifikaci obrÃ¡zkÅ¯ na kaÅ¾dÃ© dlaÅ¾dici.
3. DlaÅ¾dice, kterÃ© vykazujÃ­ dostateÄnÄ› vysokou aktivaci, lze povaÅ¾ovat za obsahujÃ­cÃ­ hledanÃ½ objekt.

![NaivnÃ­ detekce objektÅ¯](../../../../../translated_images/naive-detection.e7f1ba220ccd08c68a2ea8e06a7ed75c3fcc738c2372f9e00b7f4299a8659c01.cs.png)

> *ObrÃ¡zek z [cviÄebnÃ­ho notebooku](ObjectDetection-TF.ipynb)*

Tento pÅ™Ã­stup vÅ¡ak nenÃ­ ideÃ¡lnÃ­, protoÅ¾e algoritmu umoÅ¾Åˆuje pouze velmi nepÅ™esnÄ› lokalizovat ohraniÄujÃ­cÃ­ rÃ¡meÄek objektu. Pro pÅ™esnÄ›jÅ¡Ã­ lokalizaci potÅ™ebujeme pouÅ¾Ã­t nÄ›jakÃ½ typ **regrese**, abychom pÅ™edpovÄ›dÄ›li souÅ™adnice ohraniÄujÃ­cÃ­ch rÃ¡meÄkÅ¯ â€“ a k tomu potÅ™ebujeme specifickÃ© datovÃ© sady.

## Regrese pro detekci objektÅ¯

[Tento blogovÃ½ pÅ™Ã­spÄ›vek](https://towardsdatascience.com/object-detection-with-neural-networks-a4e2c46b4491) nabÃ­zÃ­ skvÄ›lÃ½ Ãºvod do detekce tvarÅ¯.

## DatovÃ© sady pro detekci objektÅ¯

MÅ¯Å¾ete narazit na nÃ¡sledujÃ­cÃ­ datovÃ© sady pro tento Ãºkol:

* [PASCAL VOC](http://host.robots.ox.ac.uk/pascal/VOC/) â€“ 20 tÅ™Ã­d
* [COCO](http://cocodataset.org/#home) â€“ Common Objects in Context. 80 tÅ™Ã­d, ohraniÄujÃ­cÃ­ rÃ¡meÄky a segmentaÄnÃ­ masky

![COCO](../../../../../translated_images/coco-examples.71bc60380fa6cceb7caad48bd09e35b6028caabd363aa04fee89c414e0870e86.cs.jpg)

## Metriky pro detekci objektÅ¯

### PrÅ¯nik pÅ™es sjednocenÃ­ (Intersection over Union)

ZatÃ­mco u klasifikace obrÃ¡zkÅ¯ je snadnÃ© mÄ›Å™it, jak dobÅ™e algoritmus funguje, u detekce objektÅ¯ musÃ­me mÄ›Å™it jak sprÃ¡vnost tÅ™Ã­dy, tak pÅ™esnost urÄenÃ© polohy ohraniÄujÃ­cÃ­ho rÃ¡meÄku. Pro druhÃ© zmÃ­nÄ›nÃ© pouÅ¾Ã­vÃ¡me tzv. **PrÅ¯nik pÅ™es sjednocenÃ­** (IoU), kterÃ½ mÄ›Å™Ã­, jak dobÅ™e se dva rÃ¡meÄky (nebo dvÄ› libovolnÃ© oblasti) pÅ™ekrÃ½vajÃ­.

![IoU](../../../../../translated_images/iou_equation.9a4751d40fff4e119ecd0a7bcca4e71ab1dc83e0d4f2a0d66ff0859736f593cf.cs.png)

> *ObrÃ¡zek 2 z [tohoto skvÄ›lÃ©ho blogovÃ©ho pÅ™Ã­spÄ›vku o IoU](https://pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/)*

Princip je jednoduchÃ½ â€“ rozdÄ›lÃ­me plochu prÅ¯niku dvou ÃºtvarÅ¯ plochou jejich sjednocenÃ­. Pro dvÄ› identickÃ© oblasti bude IoU rovno 1, zatÃ­mco pro zcela nesouvisejÃ­cÃ­ oblasti bude rovno 0. Jinak se bude pohybovat mezi 0 a 1. Obvykle bereme v Ãºvahu pouze ty ohraniÄujÃ­cÃ­ rÃ¡meÄky, u kterÃ½ch je IoU nad urÄitou hodnotou.

### PrÅ¯mÄ›rnÃ¡ pÅ™esnost (Average Precision)

PÅ™edpoklÃ¡dejme, Å¾e chceme mÄ›Å™it, jak dobÅ™e je rozpoznÃ¡na danÃ¡ tÅ™Ã­da objektÅ¯ $C$. K mÄ›Å™enÃ­ pouÅ¾Ã­vÃ¡me metriku **PrÅ¯mÄ›rnÃ¡ pÅ™esnost**, kterÃ¡ se vypoÄÃ­tÃ¡vÃ¡ nÃ¡sledujÃ­cÃ­m zpÅ¯sobem:

1. ZvaÅ¾te kÅ™ivku pÅ™esnosti a odvolÃ¡nÃ­ (Precision-Recall), kterÃ¡ ukazuje pÅ™esnost v zÃ¡vislosti na hodnotÄ› prahovÃ© detekce (od 0 do 1).
2. V zÃ¡vislosti na prahu zÃ­skÃ¡me vÃ­ce Äi mÃ©nÄ› detekovanÃ½ch objektÅ¯ na obrÃ¡zku a rÅ¯znÃ© hodnoty pÅ™esnosti a odvolÃ¡nÃ­.
3. KÅ™ivka bude vypadat takto:

<img src="https://github.com/shwars/NeuroWorkshop/raw/master/images/ObjDetectionPrecisionRecall.png"/>

> *ObrÃ¡zek z [NeuroWorkshop](http://github.com/shwars/NeuroWorkshop)*

PrÅ¯mÄ›rnÃ¡ pÅ™esnost pro danou tÅ™Ã­du $C$ je plocha pod touto kÅ™ivkou. PÅ™esnÄ›ji Å™eÄeno, osa odvolÃ¡nÃ­ je obvykle rozdÄ›lena na 10 ÄÃ¡stÃ­ a pÅ™esnost je prÅ¯mÄ›rovÃ¡na pÅ™es vÅ¡echny tyto body:

$$
AP = {1\over11}\sum_{i=0}^{10}\mbox{Precision}(\mbox{Recall}={i\over10})
$$

### AP a IoU

Budeme brÃ¡t v Ãºvahu pouze ty detekce, u kterÃ½ch je IoU nad urÄitou hodnotou. NapÅ™Ã­klad v datovÃ© sadÄ› PASCAL VOC se obvykle pÅ™edpoklÃ¡dÃ¡ $\mbox{IoU Threshold} = 0.5$, zatÃ­mco v COCO se AP mÄ›Å™Ã­ pro rÅ¯znÃ© hodnoty $\mbox{IoU Threshold}$.

<img src="https://github.com/shwars/NeuroWorkshop/raw/master/images/ObjDetectionPrecisionRecallIoU.png"/>

> *ObrÃ¡zek z [NeuroWorkshop](http://github.com/shwars/NeuroWorkshop)*

### PrÅ¯mÄ›rnÃ¡ prÅ¯mÄ›rnÃ¡ pÅ™esnost â€“ mAP

HlavnÃ­ metrika pro detekci objektÅ¯ se nazÃ½vÃ¡ **PrÅ¯mÄ›rnÃ¡ prÅ¯mÄ›rnÃ¡ pÅ™esnost**, nebo **mAP**. JednÃ¡ se o hodnotu prÅ¯mÄ›rnÃ© pÅ™esnosti, prÅ¯mÄ›rovanou pÅ™es vÅ¡echny tÅ™Ã­dy objektÅ¯, a nÄ›kdy takÃ© pÅ™es $\mbox{IoU Threshold}$. PodrobnÄ›jÅ¡Ã­ popis procesu vÃ½poÄtu **mAP** najdete
[v tomto blogovÃ©m pÅ™Ã­spÄ›vku](https://medium.com/@timothycarlen/understanding-the-map-evaluation-metric-for-object-detection-a07fe6962cf3)), a takÃ© [zde s ukÃ¡zkami kÃ³du](https://gist.github.com/tarlen5/008809c3decf19313de216b9208f3734).

## RÅ¯znÃ© pÅ™Ã­stupy k detekci objektÅ¯

ExistujÃ­ dvÄ› Å¡irokÃ© kategorie algoritmÅ¯ pro detekci objektÅ¯:

* **SÃ­tÄ› pro nÃ¡vrh oblastÃ­** (R-CNN, Fast R-CNN, Faster R-CNN). HlavnÃ­ myÅ¡lenkou je generovat **oblasti zÃ¡jmu** (ROI) a spustit na nich CNN, hledajÃ­cÃ­ maximÃ¡lnÃ­ aktivaci. Je to trochu podobnÃ© naivnÃ­mu pÅ™Ã­stupu, s vÃ½jimkou toho, Å¾e ROI jsou generovÃ¡ny chytÅ™ejÅ¡Ã­m zpÅ¯sobem. Jednou z hlavnÃ­ch nevÃ½hod tÄ›chto metod je, Å¾e jsou pomalÃ©, protoÅ¾e potÅ™ebujeme mnoho prÅ¯chodÅ¯ klasifikÃ¡toru CNN pÅ™es obrÃ¡zek.
* **JednoprÅ¯chodovÃ©** (YOLO, SSD, RetinaNet) metody. V tÄ›chto architekturÃ¡ch navrhujeme sÃ­Å¥ tak, aby pÅ™edpovÃ­dala jak tÅ™Ã­dy, tak ROI v jednom prÅ¯chodu.

### R-CNN: Region-Based CNN

[R-CNN](http://islab.ulsan.ac.kr/files/announcement/513/rcnn_pami.pdf) pouÅ¾Ã­vÃ¡ [SelektivnÃ­ vyhledÃ¡vÃ¡nÃ­](http://www.huppelen.nl/publications/selectiveSearchDraft.pdf) k vytvoÅ™enÃ­ hierarchickÃ© struktury oblastÃ­ ROI, kterÃ© jsou nÃ¡slednÄ› zpracovÃ¡ny extraktory funkcÃ­ CNN a klasifikÃ¡tory SVM k urÄenÃ­ tÅ™Ã­dy objektu, a lineÃ¡rnÃ­ regresÃ­ k urÄenÃ­ souÅ™adnic *ohraniÄujÃ­cÃ­ho rÃ¡meÄku*. [OficiÃ¡lnÃ­ ÄlÃ¡nek](https://arxiv.org/pdf/1506.01497v1.pdf)

![RCNN](../../../../../translated_images/rcnn1.cae407020dfb1d1fb572656e44f75cd6c512cc220591c116c506652c10e47f26.cs.png)

> *ObrÃ¡zek od van de Sande et al. ICCVâ€™11*

![RCNN-1](../../../../../translated_images/rcnn2.2d9530bb83516484ec65b250c22dbf37d3d23244f32864ebcb91d98fe7c3112c.cs.png)

> *ObrÃ¡zky z [tohoto blogu](https://towardsdatascience.com/r-cnn-fast-r-cnn-faster-r-cnn-yolo-object-detection-algorithms-36d53571365e)*

### F-RCNN - Fast R-CNN

Tento pÅ™Ã­stup je podobnÃ½ R-CNN, ale oblasti jsou definovÃ¡ny po aplikaci konvoluÄnÃ­ch vrstev.

![FRCNN](../../../../../translated_images/f-rcnn.3cda6d9bb41888754037d2d9763e2298a96de5d9bc2a21db3147357aa5da9b1a.cs.png)

> ObrÃ¡zek z [oficiÃ¡lnÃ­ho ÄlÃ¡nku](https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Girshick_Fast_R-CNN_ICCV_2015_paper.pdf), [arXiv](https://arxiv.org/pdf/1504.08083.pdf), 2015

### Faster R-CNN

HlavnÃ­ myÅ¡lenkou tohoto pÅ™Ã­stupu je pouÅ¾itÃ­ neuronovÃ© sÃ­tÄ› k pÅ™edpovÄ›di ROI â€“ tzv. *Region Proposal Network*. [ÄŒlÃ¡nek](https://arxiv.org/pdf/1506.01497.pdf), 2016

![FasterRCNN](../../../../../translated_images/faster-rcnn.8d46c099b87ef30ab2ea26dbc4bdd85b974a57ba8eb526f65dc4cd0a4711de30.cs.png)

> ObrÃ¡zek z [oficiÃ¡lnÃ­ho ÄlÃ¡nku](https://arxiv.org/pdf/1506.01497.pdf)

### R-FCN: Region-Based Fully Convolutional Network

Tento algoritmus je jeÅ¡tÄ› rychlejÅ¡Ã­ neÅ¾ Faster R-CNN. HlavnÃ­ myÅ¡lenka je nÃ¡sledujÃ­cÃ­:

1. Extrahujeme funkce pomocÃ­ ResNet-101.
2. Funkce jsou zpracovÃ¡ny pomocÃ­ **Position-Sensitive Score Map**. KaÅ¾dÃ½ objekt z $C$ tÅ™Ã­d je rozdÄ›len na $k\times k$ oblasti a trÃ©nujeme na pÅ™edpovÄ›Ä ÄÃ¡stÃ­ objektÅ¯.
3. Pro kaÅ¾dou ÄÃ¡st z $k\times k$ oblastÃ­ vÅ¡echny sÃ­tÄ› hlasujÃ­ pro tÅ™Ã­dy objektÅ¯ a tÅ™Ã­da objektu s maximÃ¡lnÃ­m poÄtem hlasÅ¯ je vybrÃ¡na.

![r-fcn image](../../../../../translated_images/r-fcn.13eb88158b99a3da50fa2787a6be5cb310d47f0e9655cc93a1090dc7aab338d1.cs.png)

> ObrÃ¡zek z [oficiÃ¡lnÃ­ho ÄlÃ¡nku](https://arxiv.org/abs/1605.06409)

### YOLO - You Only Look Once

YOLO je algoritmus pro detekci v reÃ¡lnÃ©m Äase s jednÃ­m prÅ¯chodem. HlavnÃ­ myÅ¡lenka je nÃ¡sledujÃ­cÃ­:

 * ObrÃ¡zek je rozdÄ›len na $S\times S$ oblasti.
 * Pro kaÅ¾dou oblast **CNN** pÅ™edpovÃ­dÃ¡ $n$ moÅ¾nÃ½ch objektÅ¯, souÅ™adnice *ohraniÄujÃ­cÃ­ho rÃ¡meÄku* a *dÅ¯vÄ›ru*=*pravdÄ›podobnost* * IoU.

 ![YOLO](../../../../../translated_images/yolo.a2648ec82ee8bb4ea27537677adb482fd4b733ca1705c561b6a24a85102dced5.cs.png)

> ObrÃ¡zek z [oficiÃ¡lnÃ­ho ÄlÃ¡nku](https://arxiv.org/abs/1506.02640)

### DalÅ¡Ã­ algoritmy

* RetinaNet: [oficiÃ¡lnÃ­ ÄlÃ¡nek](https://arxiv.org/abs/1708.02002)
   - [Implementace v PyTorch v Torchvision](https://pytorch.org/vision/stable/_modules/torchvision/models/detection/retinanet.html)
   - [Implementace v Keras](https://github.com/fizyr/keras-retinanet)
   - [Detekce objektÅ¯ pomocÃ­ RetinaNet](https://keras.io/examples/vision/retinanet/) v ukÃ¡zkÃ¡ch Keras
* SSD (Single Shot Detector): [oficiÃ¡lnÃ­ ÄlÃ¡nek](https://arxiv.org/abs/1512.02325)

## âœï¸ CviÄenÃ­: Detekce objektÅ¯

PokraÄujte ve svÃ©m uÄenÃ­ v nÃ¡sledujÃ­cÃ­m notebooku:

[ObjectDetection.ipynb](ObjectDetection.ipynb)

## ZÃ¡vÄ›r

V tÃ©to lekci jste se rychle seznÃ¡mili s rÅ¯znÃ½mi zpÅ¯soby, jak lze detekci objektÅ¯ provÃ¡dÄ›t!

## ğŸš€ VÃ½zva

ProjdÄ›te si tyto ÄlÃ¡nky a notebooky o YOLO a vyzkouÅ¡ejte je sami:

* [SkvÄ›lÃ½ blogovÃ½ pÅ™Ã­spÄ›vek](https://www.analyticsvidhya.com/blog/2018/12/practical-guide-object-detection-yolo-framewor-python/) popisujÃ­cÃ­ YOLO
 * [OficiÃ¡lnÃ­ web](https://pjreddie.com/darknet/yolo/)
 * Yolo: [Implementace v Keras](https://github.com/experiencor/keras-yolo2), [podrobnÃ½ notebook](https://github.com/experiencor/basic-yolo-keras/blob/master/Yolo%20Step-by-Step.ipynb)
 * Yolo v2: [Implementace v Keras](https://github.com/experiencor/keras-yolo2), [podrobnÃ½ notebook](https://github.com/experiencor/keras-yolo2/blob/master/Yolo%20Step-by-Step.ipynb)

## [KvÃ­z po lekci](https://ff-quizzes.netlify.app/en/ai/quiz/22)

## PÅ™ehled & Samostudium

* [Detekce objektÅ¯](https://tjmachinelearning.com/lectures/1718/obj/) od Nikhila Sardany
* [DobrÃ½ pÅ™ehled algoritmÅ¯ pro detekci objektÅ¯](https://lilianweng.github.io/lil-log/2018/12/27/object-detection-part-4.html)
* [PÅ™ehled algoritmÅ¯ hlubokÃ©ho uÄenÃ­ pro detekci objektÅ¯](https://medium.com/comet-app/review-of-deep-learning-algorithms-for-object-detection-c1f3d437b852)
* [Ãšvod do zÃ¡kladnÃ­ch algoritmÅ¯ pro detekci objektÅ¯ krok za krokem](https://www.analyticsvidhya.com/blog/2018/10/a-step-by-step-introduction-to-the-basic-object-detection-algorithms-part-1/)
* [Implementace Faster R-CNN v Pythonu pro detekci objektÅ¯](https://www.analyticsvidhya.com/blog/2018/11/implementation-faster-r-cnn-python-object-detection/)

## [Ãškol: Detekce objektÅ¯](lab/README.md)

---

