# GenerativnÃ­ adversariÃ¡lnÃ­ sÃ­tÄ›

V pÅ™edchozÃ­ ÄÃ¡sti jsme se nauÄili o **generativnÃ­ch modelech**: modelech, kterÃ© dokÃ¡Å¾ou generovat novÃ© obrÃ¡zky podobnÃ© tÄ›m v trÃ©novacÃ­ sadÄ›. VAE byl dobrÃ½m pÅ™Ã­kladem generativnÃ­ho modelu.

## [KvÃ­z pÅ™ed lekcÃ­](https://ff-quizzes.netlify.app/en/ai/quiz/19)

Pokud se vÅ¡ak pokusÃ­me generovat nÄ›co opravdu smysluplnÃ©ho, napÅ™Ã­klad malbu v rozumnÃ©m rozliÅ¡enÃ­, pomocÃ­ VAE zjistÃ­me, Å¾e trÃ©nink neprobÃ­hÃ¡ dobÅ™e. Pro tento pÅ™Ã­pad bychom se mÄ›li seznÃ¡mit s jinou architekturou, kterÃ¡ je specificky zamÄ›Å™enÃ¡ na generativnÃ­ modely - **GenerativnÃ­ adversariÃ¡lnÃ­ sÃ­tÄ›**, neboli GANs.

HlavnÃ­ myÅ¡lenkou GAN je mÃ­t dvÄ› neuronovÃ© sÃ­tÄ›, kterÃ© se budou trÃ©novat proti sobÄ›:

<img src="../../../../../translated_images/cs/gan_architecture.8f3a5ab62b8d5d69.webp" width="70%"/>

> ObrÃ¡zek od [Dmitry Soshnikov](http://soshnikov.com)

> âœ… MalÃ½ slovnÃ­Äek:
> * **GenerÃ¡tor** je sÃ­Å¥, kterÃ¡ vezme nÄ›jakÃ½ nÃ¡hodnÃ½ vektor a vytvoÅ™Ã­ obrÃ¡zek jako vÃ½sledek.
> * **DiscriminÃ¡tor** je sÃ­Å¥, kterÃ¡ vezme obrÃ¡zek a mÃ¡ urÄit, zda se jednÃ¡ o skuteÄnÃ½ obrÃ¡zek (z trÃ©novacÃ­ sady), nebo zda byl vytvoÅ™en generÃ¡torem. V podstatÄ› jde o klasifikÃ¡tor obrÃ¡zkÅ¯.

### DiscriminÃ¡tor

Architektura discriminÃ¡toru se neliÅ¡Ã­ od bÄ›Å¾nÃ© sÃ­tÄ› pro klasifikaci obrÃ¡zkÅ¯. V nejjednoduÅ¡Å¡Ã­m pÅ™Ã­padÄ› mÅ¯Å¾e jÃ­t o plnÄ› propojenÃ½ klasifikÃ¡tor, ale pravdÄ›podobnÄ›ji to bude [konvoluÄnÃ­ sÃ­Å¥](../07-ConvNets/README.md).

> âœ… GAN zaloÅ¾enÃ½ na konvoluÄnÃ­ch sÃ­tÃ­ch se nazÃ½vÃ¡ [DCGAN](https://arxiv.org/pdf/1511.06434.pdf)

DiscriminÃ¡tor CNN se sklÃ¡dÃ¡ z nÃ¡sledujÃ­cÃ­ch vrstev: nÄ›kolik konvolucÃ­ + poolingÅ¯ (s klesajÃ­cÃ­ prostorovou velikostÃ­) a jednÃ© nebo vÃ­ce plnÄ› propojenÃ½ch vrstev pro zÃ­skÃ¡nÃ­ "vektorÅ¯ vlastnostÃ­", a nakonec binÃ¡rnÃ­ho klasifikÃ¡toru.

> âœ… 'Pooling' v tomto kontextu je technika, kterÃ¡ zmenÅ¡uje velikost obrÃ¡zku. "Pooling vrstvy sniÅ¾ujÃ­ rozmÄ›ry dat kombinacÃ­ vÃ½stupÅ¯ neuronovÃ½ch klastrÅ¯ v jednÃ© vrstvÄ› do jednoho neuronu v dalÅ¡Ã­ vrstvÄ›." - [zdroj](https://wikipedia.org/wiki/Convolutional_neural_network#Pooling_layers)

### GenerÃ¡tor

GenerÃ¡tor je o nÄ›co sloÅ¾itÄ›jÅ¡Ã­. MÅ¯Å¾ete si ho pÅ™edstavit jako obrÃ¡cenÃ½ discriminÃ¡tor. ZaÄÃ­nÃ¡ latentnÃ­m vektorem (namÃ­sto vektoru vlastnostÃ­), mÃ¡ plnÄ› propojenou vrstvu, kterÃ¡ ho pÅ™evede na poÅ¾adovanou velikost/tvar, nÃ¡sledovanou dekonvolucemi + zvÄ›tÅ¡ovÃ¡nÃ­m. To je podobnÃ© *dekodÃ©ru* v [autoenkodÃ©ru](../09-Autoencoders/README.md).

> âœ… ProtoÅ¾e je konvoluÄnÃ­ vrstva implementovÃ¡na jako lineÃ¡rnÃ­ filtr prochÃ¡zejÃ­cÃ­ obrÃ¡zkem, dekonvoluce je v podstatÄ› podobnÃ¡ konvoluci a mÅ¯Å¾e bÃ½t implementovÃ¡na pomocÃ­ stejnÃ© logiky vrstvy.

<img src="../../../../../translated_images/cs/gan_arch_detail.46b95fd366f8e543.webp" width="70%"/>

> ObrÃ¡zek od [Dmitry Soshnikov](http://soshnikov.com)

### TrÃ©nink GAN

GANy se nazÃ½vajÃ­ **adversariÃ¡lnÃ­**, protoÅ¾e mezi generÃ¡torem a discriminÃ¡torem probÃ­hÃ¡ neustÃ¡lÃ¡ soutÄ›Å¾. BÄ›hem tÃ©to soutÄ›Å¾e se oba generÃ¡tor i discriminÃ¡tor zlepÅ¡ujÃ­, a sÃ­Å¥ se tak uÄÃ­ vytvÃ¡Å™et stÃ¡le lepÅ¡Ã­ obrÃ¡zky.

TrÃ©nink probÃ­hÃ¡ ve dvou fÃ¡zÃ­ch:

* **TrÃ©nink discriminÃ¡toru**. Tento Ãºkol je pomÄ›rnÄ› pÅ™Ã­moÄarÃ½: vygenerujeme dÃ¡vku obrÃ¡zkÅ¯ pomocÃ­ generÃ¡toru, oznaÄÃ­me je 0, coÅ¾ znamenÃ¡ faleÅ¡nÃ½ obrÃ¡zek, a vezmeme dÃ¡vku obrÃ¡zkÅ¯ z trÃ©novacÃ­ sady (s oznaÄenÃ­m 1, skuteÄnÃ½ obrÃ¡zek). ZÃ­skÃ¡me nÄ›jakou *ztrÃ¡tu discriminÃ¡toru* a provedeme zpÄ›tnÃ© Å¡Ã­Å™enÃ­.
* **TrÃ©nink generÃ¡toru**. To je o nÄ›co sloÅ¾itÄ›jÅ¡Ã­, protoÅ¾e neznÃ¡me oÄekÃ¡vanÃ½ vÃ½stup generÃ¡toru pÅ™Ã­mo. Vezmeme celou GAN sÃ­Å¥ sestÃ¡vajÃ­cÃ­ z generÃ¡toru nÃ¡sledovanÃ©ho discriminÃ¡torem, nakrmÃ­me ji nÄ›jakÃ½mi nÃ¡hodnÃ½mi vektory a oÄekÃ¡vÃ¡me, Å¾e vÃ½sledek bude 1 (odpovÃ­dajÃ­cÃ­ skuteÄnÃ½m obrÃ¡zkÅ¯m). PotÃ© zmrazÃ­me parametry discriminÃ¡toru (nechceme, aby se v tomto kroku trÃ©noval) a provedeme zpÄ›tnÃ© Å¡Ã­Å™enÃ­.

BÄ›hem tohoto procesu ztrÃ¡ty generÃ¡toru i discriminÃ¡toru neklesajÃ­ vÃ½raznÄ›. V ideÃ¡lnÃ­m pÅ™Ã­padÄ› by mÄ›ly oscilovat, coÅ¾ odpovÃ­dÃ¡ zlepÅ¡ovÃ¡nÃ­ vÃ½konu obou sÃ­tÃ­.

## âœï¸ CviÄenÃ­: GANs

* [GAN Notebook v TensorFlow/Keras](GANTF.ipynb)
* [GAN Notebook v PyTorch](GANPyTorch.ipynb)

### ProblÃ©my s trÃ©ninkem GAN

GANy jsou znÃ¡mÃ© tÃ­m, Å¾e je obzvlÃ¡Å¡tÄ› obtÃ­Å¾nÃ© je trÃ©novat. Zde je nÄ›kolik problÃ©mÅ¯:

* **Kolaps mÃ³du**. Tento termÃ­n znamenÃ¡, Å¾e se generÃ¡tor nauÄÃ­ vytvÃ¡Å™et jeden ÃºspÄ›Å¡nÃ½ obrÃ¡zek, kterÃ½ oklame discriminÃ¡tor, a ne rÅ¯znÃ© obrÃ¡zky.
* **Citlivost na hyperparametry**. ÄŒasto se stÃ¡vÃ¡, Å¾e GAN vÅ¯bec nekonverguje, a pak nÃ¡hle snÃ­Å¾enÃ­ rychlosti uÄenÃ­ vede ke konvergenci.
* UdrÅ¾enÃ­ **rovnovÃ¡hy** mezi generÃ¡torem a discriminÃ¡torem. V mnoha pÅ™Ã­padech mÅ¯Å¾e ztrÃ¡ta discriminÃ¡toru relativnÄ› rychle klesnout na nulu, coÅ¾ zpÅ¯sobÃ­, Å¾e generÃ¡tor nebude schopen dÃ¡le trÃ©novat. Abychom tomu pÅ™edeÅ¡li, mÅ¯Å¾eme zkusit nastavit rÅ¯znÃ© rychlosti uÄenÃ­ pro generÃ¡tor a discriminÃ¡tor, nebo pÅ™eskoÄit trÃ©nink discriminÃ¡toru, pokud je ztrÃ¡ta jiÅ¾ pÅ™Ã­liÅ¡ nÃ­zkÃ¡.
* TrÃ©nink pro **vysokÃ© rozliÅ¡enÃ­**. Tento problÃ©m, podobnÄ› jako u autoenkodÃ©rÅ¯, nastÃ¡vÃ¡, protoÅ¾e rekonstrukce pÅ™Ã­liÅ¡ mnoha vrstev konvoluÄnÃ­ sÃ­tÄ› vede k artefaktÅ¯m. Tento problÃ©m se obvykle Å™eÅ¡Ã­ tzv. **progresivnÃ­m rÅ¯stem**, kdy se nejprve nÄ›kolik vrstev trÃ©nuje na obrÃ¡zcÃ­ch s nÃ­zkÃ½m rozliÅ¡enÃ­m, a potÃ© se vrstvy "odblokujÃ­" nebo pÅ™idajÃ­. DalÅ¡Ã­m Å™eÅ¡enÃ­m by bylo pÅ™idÃ¡nÃ­ dalÅ¡Ã­ch spojenÃ­ mezi vrstvami a trÃ©nink nÄ›kolika rozliÅ¡enÃ­ najednou - podrobnosti naleznete v tomto [Multi-Scale Gradient GANs ÄlÃ¡nku](https://arxiv.org/abs/1903.06048).

## PÅ™enos stylu

GANy jsou skvÄ›lÃ½m zpÅ¯sobem, jak generovat umÄ›leckÃ© obrÃ¡zky. DalÅ¡Ã­ zajÃ­mavou technikou je tzv. **pÅ™enos stylu**, kterÃ½ vezme jeden **obsahovÃ½ obrÃ¡zek** a pÅ™ekreslÃ­ ho v jinÃ©m stylu, aplikovÃ¡nÃ­m filtrÅ¯ z **stylovÃ©ho obrÃ¡zku**.

Jak to funguje:
* ZaÄneme s nÃ¡hodnÃ½m Å¡umovÃ½m obrÃ¡zkem (nebo s obsahovÃ½m obrÃ¡zkem, ale pro pochopenÃ­ je jednoduÅ¡Å¡Ã­ zaÄÃ­t s nÃ¡hodnÃ½m Å¡umem).
* NaÅ¡Ã­m cÃ­lem bude vytvoÅ™it takovÃ½ obrÃ¡zek, kterÃ½ bude blÃ­zkÃ½ jak obsahovÃ©mu obrÃ¡zku, tak stylovÃ©mu obrÃ¡zku. To bude urÄeno dvÄ›ma funkcemi ztrÃ¡ty:
   - **ZtrÃ¡ta obsahu** se vypoÄÃ­tÃ¡ na zÃ¡kladÄ› vlastnostÃ­ extrahovanÃ½ch CNN na nÄ›kterÃ½ch vrstvÃ¡ch z aktuÃ¡lnÃ­ho obrÃ¡zku a obsahovÃ©ho obrÃ¡zku.
   - **ZtrÃ¡ta stylu** se vypoÄÃ­tÃ¡ mezi aktuÃ¡lnÃ­m obrÃ¡zkem a stylovÃ½m obrÃ¡zkem chytrÃ½m zpÅ¯sobem pomocÃ­ GramovÃ½ch matic (vÃ­ce podrobnostÃ­ v [pÅ™Ã­kladovÃ©m notebooku](StyleTransfer.ipynb)).
* Aby byl obrÃ¡zek hladÅ¡Ã­ a odstranil Å¡um, zavedeme takÃ© **ztrÃ¡tu variace**, kterÃ¡ vypoÄÃ­tÃ¡ prÅ¯mÄ›rnou vzdÃ¡lenost mezi sousednÃ­mi pixely.
* HlavnÃ­ optimalizaÄnÃ­ smyÄka upravuje aktuÃ¡lnÃ­ obrÃ¡zek pomocÃ­ gradientnÃ­ho sestupu (nebo jinÃ©ho optimalizaÄnÃ­ho algoritmu) tak, aby minimalizovala celkovou ztrÃ¡tu, kterÃ¡ je vÃ¡Å¾enÃ½m souÄtem vÅ¡ech tÅ™Ã­ ztrÃ¡t.

## âœï¸ PÅ™Ã­klad: [PÅ™enos stylu](StyleTransfer.ipynb)

## [KvÃ­z po lekci](https://ff-quizzes.netlify.app/en/ai/quiz/20)

## ZÃ¡vÄ›r

V tÃ©to lekci jste se nauÄili o GANech a jak je trÃ©novat. TakÃ© jste se dozvÄ›dÄ›li o speciÃ¡lnÃ­ch vÃ½zvÃ¡ch, kterÃ½m tento typ neuronovÃ© sÃ­tÄ› mÅ¯Å¾e Äelit, a o nÄ›kterÃ½ch strategiÃ­ch, jak je pÅ™ekonat.

## ğŸš€ VÃ½zva

ProjdÄ›te si [notebook PÅ™enos stylu](StyleTransfer.ipynb) s pouÅ¾itÃ­m vlastnÃ­ch obrÃ¡zkÅ¯.

## PÅ™ehled a samostudium

Pro dalÅ¡Ã­ informace si pÅ™eÄtÄ›te vÃ­ce o GANech v tÄ›chto zdrojÃ­ch:

* Marco Pasini, [10 Lessons I Learned Training GANs for one Year](https://towardsdatascience.com/10-lessons-i-learned-training-generative-adversarial-networks-gans-for-a-year-c9071159628)
* [StyleGAN](https://en.wikipedia.org/wiki/StyleGAN), *de facto* architektura GAN, kterou stojÃ­ za to zvÃ¡Å¾it
* [VytvÃ¡Å™enÃ­ generativnÃ­ho umÄ›nÃ­ pomocÃ­ GAN na Azure ML](https://soshnikov.com/scienceart/creating-generative-art-using-gan-on-azureml/)

## ZadÃ¡nÃ­

Znovu si projdÄ›te jeden ze dvou notebookÅ¯ spojenÃ½ch s touto lekcÃ­ a znovu natrÃ©nujte GAN na vlastnÃ­ch obrÃ¡zcÃ­ch. Co dokÃ¡Å¾ete vytvoÅ™it?

---

