<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "f07c85bbf05a1f67505da98f4ecc124c",
  "translation_date": "2025-08-25T22:38:46+00:00",
  "source_file": "lessons/4-ComputerVision/10-GANs/README.md",
  "language_code": "cs"
}
-->
# GenerativnÃ­ adversariÃ¡lnÃ­ sÃ­tÄ›

V pÅ™edchozÃ­ ÄÃ¡sti jsme se nauÄili o **generativnÃ­ch modelech**: modelech, kterÃ© dokÃ¡Å¾ou generovat novÃ© obrÃ¡zky podobnÃ© tÄ›m v trÃ©novacÃ­m datasetu. VAE byl dobrÃ½m pÅ™Ã­kladem generativnÃ­ho modelu.

## [Pre-lecture quiz](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/110)

NicmÃ©nÄ›, pokud se pokusÃ­me generovat nÄ›co opravdu smysluplnÃ©ho, napÅ™Ã­klad malbu v rozumnÃ©m rozliÅ¡enÃ­, pomocÃ­ VAE, zjistÃ­me, Å¾e trÃ©novÃ¡nÃ­ neprobÃ­hÃ¡ dobÅ™e. Pro tento pÅ™Ã­pad bychom se mÄ›li nauÄit o jinÃ© architektuÅ™e, kterÃ¡ je specificky zamÄ›Å™enÃ¡ na generativnÃ­ modely - **GenerativnÃ­ adversariÃ¡lnÃ­ sÃ­tÄ›**, neboli GANs.

HlavnÃ­ myÅ¡lenkou GAN je mÃ­t dvÄ› neuronovÃ© sÃ­tÄ›, kterÃ© se budou trÃ©novat proti sobÄ›:

<img src="images/gan_architecture.png" width="70%"/>

> ObrÃ¡zek od [Dmitry Soshnikov](http://soshnikov.com)

> âœ… MalÃ½ slovnÃ­Äek:
> * **GenerÃ¡tor** je sÃ­Å¥, kterÃ¡ vezme nÄ›jakÃ½ nÃ¡hodnÃ½ vektor a jako vÃ½sledek vytvoÅ™Ã­ obrÃ¡zek.
> * **DiskriminÃ¡tor** je sÃ­Å¥, kterÃ¡ vezme obrÃ¡zek a mÄ›la by urÄit, zda se jednÃ¡ o skuteÄnÃ½ obrÃ¡zek (z trÃ©novacÃ­ho datasetu), nebo zda byl vytvoÅ™en generÃ¡torem. V podstatÄ› jde o klasifikÃ¡tor obrÃ¡zkÅ¯.

### DiskriminÃ¡tor

Architektura diskriminÃ¡toru se neliÅ¡Ã­ od bÄ›Å¾nÃ© klasifikaÄnÃ­ sÃ­tÄ› pro obrÃ¡zky. V nejjednoduÅ¡Å¡Ã­m pÅ™Ã­padÄ› mÅ¯Å¾e bÃ½t plnÄ› propojenÃ½m klasifikÃ¡torem, ale pravdÄ›podobnÄ›ji pÅ¯jde o [konvoluÄnÃ­ sÃ­Å¥](../07-ConvNets/README.md).

> âœ… GAN zaloÅ¾enÃ½ na konvoluÄnÃ­ch sÃ­tÃ­ch se nazÃ½vÃ¡ [DCGAN](https://arxiv.org/pdf/1511.06434.pdf)

DiskriminÃ¡tor CNN se sklÃ¡dÃ¡ z nÃ¡sledujÃ­cÃ­ch vrstev: nÄ›kolik konvolucÃ­+poolingÅ¯ (s klesajÃ­cÃ­ prostorovou velikostÃ­) a jednÃ© nebo vÃ­ce plnÄ› propojenÃ½ch vrstev pro zÃ­skÃ¡nÃ­ "vektorÅ¯ vlastnostÃ­", koneÄnÃ½ binÃ¡rnÃ­ klasifikÃ¡tor.

> âœ… 'Pooling' v tomto kontextu je technika, kterÃ¡ zmenÅ¡uje velikost obrÃ¡zku. "PoolingovÃ© vrstvy sniÅ¾ujÃ­ rozmÄ›ry dat kombinovÃ¡nÃ­m vÃ½stupÅ¯ neuronovÃ½ch clusterÅ¯ na jednÃ© vrstvÄ› do jednoho neuronu na dalÅ¡Ã­ vrstvÄ›." - [zdroj](https://wikipedia.org/wiki/Convolutional_neural_network#Pooling_layers)

### GenerÃ¡tor

GenerÃ¡tor je o nÄ›co sloÅ¾itÄ›jÅ¡Ã­. MÅ¯Å¾ete si ho pÅ™edstavit jako obrÃ¡cenÃ½ diskriminÃ¡tor. ZaÄÃ­nÃ¡ latentnÃ­m vektorem (namÃ­sto vektoru vlastnostÃ­), mÃ¡ plnÄ› propojenou vrstvu, kterÃ¡ ho pÅ™evede na poÅ¾adovanou velikost/tvar, nÃ¡sledovanou dekonvolucemi+zvÄ›tÅ¡ovÃ¡nÃ­m. To je podobnÃ© *dekodÃ©ru* ÄÃ¡sti [autoenkodÃ©ru](../09-Autoencoders/README.md).

> âœ… ProtoÅ¾e konvoluÄnÃ­ vrstva je implementovÃ¡na jako lineÃ¡rnÃ­ filtr prochÃ¡zejÃ­cÃ­ obrÃ¡zkem, dekonvoluce je v podstatÄ› podobnÃ¡ konvoluci a mÅ¯Å¾e bÃ½t implementovÃ¡na pomocÃ­ stejnÃ© logiky vrstvy.

<img src="images/gan_arch_detail.png" width="70%"/>

> ObrÃ¡zek od [Dmitry Soshnikov](http://soshnikov.com)

### TrÃ©novÃ¡nÃ­ GAN

GANs se nazÃ½vajÃ­ **adversariÃ¡lnÃ­**, protoÅ¾e mezi generÃ¡torem a diskriminÃ¡torem probÃ­hÃ¡ neustÃ¡lÃ¡ soutÄ›Å¾. BÄ›hem tÃ©to soutÄ›Å¾e se oba generÃ¡tor i diskriminÃ¡tor zlepÅ¡ujÃ­, takÅ¾e sÃ­Å¥ se uÄÃ­ vytvÃ¡Å™et stÃ¡le lepÅ¡Ã­ obrÃ¡zky.

TrÃ©novÃ¡nÃ­ probÃ­hÃ¡ ve dvou fÃ¡zÃ­ch:

* **TrÃ©novÃ¡nÃ­ diskriminÃ¡toru**. Tento Ãºkol je pomÄ›rnÄ› pÅ™Ã­moÄarÃ½: generujeme dÃ¡vku obrÃ¡zkÅ¯ pomocÃ­ generÃ¡toru, oznaÄÃ­me je 0, coÅ¾ znamenÃ¡ faleÅ¡nÃ½ obrÃ¡zek, a vezmeme dÃ¡vku obrÃ¡zkÅ¯ z vstupnÃ­ho datasetu (s oznaÄenÃ­m 1, skuteÄnÃ½ obrÃ¡zek). ZÃ­skÃ¡me nÄ›jakou *ztrÃ¡tu diskriminÃ¡toru* a provedeme zpÄ›tnÃ© Å¡Ã­Å™enÃ­.
* **TrÃ©novÃ¡nÃ­ generÃ¡toru**. To je o nÄ›co sloÅ¾itÄ›jÅ¡Ã­, protoÅ¾e neznÃ¡me oÄekÃ¡vanÃ½ vÃ½stup generÃ¡toru pÅ™Ã­mo. Vezmeme celou GAN sÃ­Å¥ sklÃ¡dajÃ­cÃ­ se z generÃ¡toru nÃ¡sledovanÃ©ho diskriminÃ¡torem, nakrmÃ­me ji nÄ›jakÃ½mi nÃ¡hodnÃ½mi vektory a oÄekÃ¡vÃ¡me vÃ½sledek 1 (odpovÃ­dajÃ­cÃ­ skuteÄnÃ½m obrÃ¡zkÅ¯m). PotÃ© zmrazÃ­me parametry diskriminÃ¡toru (nechceme, aby se v tomto kroku trÃ©noval) a provedeme zpÄ›tnÃ© Å¡Ã­Å™enÃ­.

BÄ›hem tohoto procesu ztrÃ¡ty generÃ¡toru i diskriminÃ¡toru neklesajÃ­ vÃ½raznÄ›. V ideÃ¡lnÃ­ situaci by mÄ›ly oscilovat, coÅ¾ odpovÃ­dÃ¡ zlepÅ¡ovÃ¡nÃ­ vÃ½konu obou sÃ­tÃ­.

## âœï¸ CviÄenÃ­: GANs

* [GAN Notebook v TensorFlow/Keras](../../../../../lessons/4-ComputerVision/10-GANs/GANTF.ipynb)
* [GAN Notebook v PyTorch](../../../../../lessons/4-ComputerVision/10-GANs/GANPyTorch.ipynb)

### ProblÃ©my s trÃ©novÃ¡nÃ­m GAN

GANs jsou znÃ¡mÃ© tÃ­m, Å¾e jsou obzvlÃ¡Å¡tÄ› obtÃ­Å¾nÃ© na trÃ©novÃ¡nÃ­. Zde je nÄ›kolik problÃ©mÅ¯:

* **Kolaps mÃ³du**. Tento termÃ­n znamenÃ¡, Å¾e generÃ¡tor se nauÄÃ­ vytvÃ¡Å™et jeden ÃºspÄ›Å¡nÃ½ obrÃ¡zek, kterÃ½ oklame diskriminÃ¡tor, a ne rÅ¯znÃ© obrÃ¡zky.
* **Citlivost na hyperparametry**. ÄŒasto mÅ¯Å¾ete vidÄ›t, Å¾e GAN vÅ¯bec nekonverguje, a pak nÃ¡hle snÃ­Å¾enÃ­ rychlosti uÄenÃ­ vede ke konvergenci.
* UdrÅ¾enÃ­ **rovnovÃ¡hy** mezi generÃ¡torem a diskriminÃ¡torem. V mnoha pÅ™Ã­padech mÅ¯Å¾e ztrÃ¡ta diskriminÃ¡toru relativnÄ› rychle klesnout na nulu, coÅ¾ vede k tomu, Å¾e generÃ¡tor uÅ¾ nemÅ¯Å¾e dÃ¡le trÃ©novat. Abychom tomu pÅ™edeÅ¡li, mÅ¯Å¾eme zkusit nastavit rÅ¯znÃ© rychlosti uÄenÃ­ pro generÃ¡tor a diskriminÃ¡tor nebo pÅ™eskoÄit trÃ©novÃ¡nÃ­ diskriminÃ¡toru, pokud je ztrÃ¡ta jiÅ¾ pÅ™Ã­liÅ¡ nÃ­zkÃ¡.
* TrÃ©novÃ¡nÃ­ na **vysokÃ© rozliÅ¡enÃ­**. Tento problÃ©m odrÃ¡Å¾Ã­ stejnÃ½ problÃ©m jako u autoenkodÃ©rÅ¯, kterÃ½ je vyvolÃ¡n tÃ­m, Å¾e rekonstrukce pÅ™Ã­liÅ¡ mnoha vrstev konvoluÄnÃ­ sÃ­tÄ› vede k artefaktÅ¯m. Tento problÃ©m se obvykle Å™eÅ¡Ã­ tzv. **progresivnÃ­m rÅ¯stem**, kdy se nejprve nÄ›kolik vrstev trÃ©nuje na obrÃ¡zcÃ­ch s nÃ­zkÃ½m rozliÅ¡enÃ­m, a potÃ© se vrstvy "odblokujÃ­" nebo pÅ™idajÃ­. DalÅ¡Ã­m Å™eÅ¡enÃ­m by bylo pÅ™idÃ¡nÃ­ dalÅ¡Ã­ch spojenÃ­ mezi vrstvami a trÃ©novÃ¡nÃ­ nÄ›kolika rozliÅ¡enÃ­ najednou - podrobnosti viz tento [Multi-Scale Gradient GANs paper](https://arxiv.org/abs/1903.06048).

## PÅ™enos stylu

GANs jsou skvÄ›lÃ½m zpÅ¯sobem, jak generovat umÄ›leckÃ© obrÃ¡zky. DalÅ¡Ã­ zajÃ­mavou technikou je tzv. **pÅ™enos stylu**, kterÃ½ vezme jeden **obrÃ¡zek obsahu** a pÅ™ekreslÃ­ ho v jinÃ©m stylu, aplikovÃ¡nÃ­m filtrÅ¯ z **obrÃ¡zku stylu**.

Jak to funguje:
* ZaÄneme s nÃ¡hodnÃ½m Å¡umovÃ½m obrÃ¡zkem (nebo s obrÃ¡zkem obsahu, ale pro lepÅ¡Ã­ pochopenÃ­ je jednoduÅ¡Å¡Ã­ zaÄÃ­t s nÃ¡hodnÃ½m Å¡umem).
* NaÅ¡Ã­m cÃ­lem bude vytvoÅ™it takovÃ½ obrÃ¡zek, kterÃ½ bude blÃ­zkÃ½ jak obrÃ¡zku obsahu, tak obrÃ¡zku stylu. To bude urÄeno dvÄ›ma funkcemi ztrÃ¡ty:
   - **ZtrÃ¡ta obsahu** se vypoÄÃ­tÃ¡vÃ¡ na zÃ¡kladÄ› vlastnostÃ­ extrahovanÃ½ch CNN na nÄ›kterÃ½ch vrstvÃ¡ch z aktuÃ¡lnÃ­ho obrÃ¡zku a obrÃ¡zku obsahu.
   - **ZtrÃ¡ta stylu** se vypoÄÃ­tÃ¡vÃ¡ mezi aktuÃ¡lnÃ­m obrÃ¡zkem a obrÃ¡zkem stylu chytrÃ½m zpÅ¯sobem pomocÃ­ GramovÃ½ch matic (vÃ­ce podrobnostÃ­ v [pÅ™Ã­kladovÃ©m notebooku](../../../../../lessons/4-ComputerVision/10-GANs/StyleTransfer.ipynb)).
* Aby byl obrÃ¡zek hladÅ¡Ã­ a odstranil Å¡um, zavÃ¡dÃ­me takÃ© **ztrÃ¡tu variace**, kterÃ¡ vypoÄÃ­tÃ¡vÃ¡ prÅ¯mÄ›rnou vzdÃ¡lenost mezi sousednÃ­mi pixely.
* HlavnÃ­ optimalizaÄnÃ­ smyÄka upravuje aktuÃ¡lnÃ­ obrÃ¡zek pomocÃ­ gradientnÃ­ho sestupu (nebo jinÃ©ho optimalizaÄnÃ­ho algoritmu) tak, aby minimalizovala celkovou ztrÃ¡tu, kterÃ¡ je vÃ¡Å¾enÃ½m souÄtem vÅ¡ech tÅ™Ã­ ztrÃ¡t.

## âœï¸ PÅ™Ã­klad: [PÅ™enos stylu](../../../../../lessons/4-ComputerVision/10-GANs/StyleTransfer.ipynb)

## [Post-lecture quiz](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/210)

## ZÃ¡vÄ›r

V tÃ©to lekci jste se nauÄili o GANs a jak je trÃ©novat. TakÃ© jste se dozvÄ›dÄ›li o speciÃ¡lnÃ­ch vÃ½zvÃ¡ch, kterÃ½m tento typ neuronovÃ© sÃ­tÄ› mÅ¯Å¾e Äelit, a o nÄ›kterÃ½ch strategiÃ­ch, jak je pÅ™ekonat.

## ğŸš€ VÃ½zva

ProjdÄ›te si [notebook PÅ™enos stylu](../../../../../lessons/4-ComputerVision/10-GANs/StyleTransfer.ipynb) s pouÅ¾itÃ­m vlastnÃ­ch obrÃ¡zkÅ¯.

## PÅ™ehled & Samostudium

Pro referenci si pÅ™eÄtÄ›te vÃ­ce o GANs v tÄ›chto zdrojÃ­ch:

* Marco Pasini, [10 Lessons I Learned Training GANs for one Year](https://towardsdatascience.com/10-lessons-i-learned-training-generative-adversarial-networks-gans-for-a-year-c9071159628)
* [StyleGAN](https://en.wikipedia.org/wiki/StyleGAN), *de facto* architektura GAN, kterou stojÃ­ za to zvÃ¡Å¾it
* [Creating Generative Art using GANs on Azure ML](https://soshnikov.com/scienceart/creating-generative-art-using-gan-on-azureml/)

## ZadÃ¡nÃ­

Znovu si projdÄ›te jeden ze dvou notebookÅ¯ spojenÃ½ch s touto lekcÃ­ a znovu natrÃ©nujte GAN na vlastnÃ­ch obrÃ¡zcÃ­ch. Co dokÃ¡Å¾ete vytvoÅ™it?

**ProhlÃ¡Å¡enÃ­:**  
Tento dokument byl pÅ™eloÅ¾en pomocÃ­ sluÅ¾by pro automatickÃ½ pÅ™eklad [Co-op Translator](https://github.com/Azure/co-op-translator). AÄkoli se snaÅ¾Ã­me o pÅ™esnost, mÄ›jte prosÃ­m na pamÄ›ti, Å¾e automatickÃ© pÅ™eklady mohou obsahovat chyby nebo nepÅ™esnosti. PÅ¯vodnÃ­ dokument v jeho pÅ¯vodnÃ­m jazyce by mÄ›l bÃ½t povaÅ¾ovÃ¡n za autoritativnÃ­ zdroj. Pro dÅ¯leÅ¾itÃ© informace se doporuÄuje profesionÃ¡lnÃ­ lidskÃ½ pÅ™eklad. NeodpovÃ­dÃ¡me za Å¾Ã¡dnÃ¡ nedorozumÄ›nÃ­ nebo nesprÃ¡vnÃ© interpretace vyplÃ½vajÃ­cÃ­ z pouÅ¾itÃ­ tohoto pÅ™ekladu.