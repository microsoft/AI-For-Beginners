# PÅ™edtrÃ©novanÃ© sÃ­tÄ› a transfer learning

TrÃ©novÃ¡nÃ­ CNN mÅ¯Å¾e bÃ½t ÄasovÄ› nÃ¡roÄnÃ© a vyÅ¾aduje velkÃ© mnoÅ¾stvÃ­ dat. VÄ›tÅ¡ina Äasu je vÅ¡ak vÄ›novÃ¡na uÄenÃ­ nejlepÅ¡Ã­ch nÃ­zkoÃºrovÅˆovÃ½ch filtrÅ¯, kterÃ© sÃ­Å¥ mÅ¯Å¾e pouÅ¾Ã­t k extrakci vzorÅ¯ z obrÃ¡zkÅ¯. NabÃ­zÃ­ se pÅ™irozenÃ¡ otÃ¡zka â€“ mÅ¯Å¾eme pouÅ¾Ã­t neuronovou sÃ­Å¥ natrÃ©novanou na jednom datasetu a pÅ™izpÅ¯sobit ji k klasifikaci jinÃ½ch obrÃ¡zkÅ¯ bez nutnosti kompletnÃ­ho trÃ©novacÃ­ho procesu?

## [KvÃ­z pÅ™ed pÅ™ednÃ¡Å¡kou](https://ff-quizzes.netlify.app/en/ai/quiz/15)

Tento pÅ™Ã­stup se nazÃ½vÃ¡ **transfer learning**, protoÅ¾e pÅ™enÃ¡Å¡Ã­me urÄitÃ© znalosti z jednoho modelu neuronovÃ© sÃ­tÄ› na jinÃ½. PÅ™i transfer learningu obvykle zaÄÃ­nÃ¡me s pÅ™edtrÃ©novanÃ½m modelem, kterÃ½ byl natrÃ©novÃ¡n na nÄ›jakÃ©m velkÃ©m datasetu obrÃ¡zkÅ¯, napÅ™Ã­klad na **ImageNet**. Tyto modely jiÅ¾ dokÃ¡Å¾ou dobÅ™e extrahovat rÅ¯znÃ© rysy z obecnÃ½ch obrÃ¡zkÅ¯, a v mnoha pÅ™Ã­padech staÄÃ­ postavit klasifikÃ¡tor na vrcholu tÄ›chto extrahovanÃ½ch rysÅ¯, aby bylo dosaÅ¾eno dobrÃ©ho vÃ½sledku.

> âœ… Transfer learning je termÃ­n, kterÃ½ se objevuje i v jinÃ½ch akademickÃ½ch oborech, napÅ™Ã­klad ve vzdÄ›lÃ¡vÃ¡nÃ­. OznaÄuje proces pÅ™enosu znalostÃ­ z jednÃ© oblasti do jinÃ©.

## PÅ™edtrÃ©novanÃ© modely jako extraktory rysÅ¯

KonvoluÄnÃ­ sÃ­tÄ›, o kterÃ½ch jsme mluvili v pÅ™edchozÃ­ ÄÃ¡sti, obsahujÃ­ Å™adu vrstev, z nichÅ¾ kaÅ¾dÃ¡ mÃ¡ za Ãºkol extrahovat urÄitÃ© rysy z obrÃ¡zku, poÄÃ­naje nÃ­zkoÃºrovÅˆovÃ½mi kombinacemi pixelÅ¯ (napÅ™Ã­klad horizontÃ¡lnÃ­/vertikÃ¡lnÃ­ ÄÃ¡ry nebo tahy), aÅ¾ po vyÅ¡Å¡Ã­ ÃºrovnÄ› kombinacÃ­ rysÅ¯, odpovÃ­dajÃ­cÃ­ napÅ™Ã­klad oku nebo plameni. Pokud natrÃ©nujeme CNN na dostateÄnÄ› velkÃ©m datasetu obecnÃ½ch a rÅ¯znorodÃ½ch obrÃ¡zkÅ¯, sÃ­Å¥ by se mÄ›la nauÄit extrahovat tyto bÄ›Å¾nÃ© rysy.

Keras i PyTorch obsahujÃ­ funkce pro snadnÃ© naÄtenÃ­ pÅ™edtrÃ©novanÃ½ch vah neuronovÃ½ch sÃ­tÃ­ pro nÄ›kterÃ© bÄ›Å¾nÃ© architektury, z nichÅ¾ vÄ›tÅ¡ina byla natrÃ©novÃ¡na na obrÃ¡zcÃ­ch z ImageNet. NejÄastÄ›ji pouÅ¾Ã­vanÃ© jsou popsÃ¡ny na strÃ¡nce [Architektury CNN](../07-ConvNets/CNN_Architectures.md) z pÅ™edchozÃ­ lekce. ZejmÃ©na mÅ¯Å¾ete zvÃ¡Å¾it pouÅ¾itÃ­ nÄ›kterÃ© z nÃ¡sledujÃ­cÃ­ch:

* **VGG-16/VGG-19**, coÅ¾ jsou relativnÄ› jednoduchÃ© modely, kterÃ© stÃ¡le poskytujÃ­ dobrou pÅ™esnost. PouÅ¾itÃ­ VGG jako prvnÃ­ pokus je Äasto dobrÃ¡ volba, jak zjistit, jak transfer learning funguje.
* **ResNet** je rodina modelÅ¯ navrÅ¾enÃ¡ Microsoft Research v roce 2015. MajÃ­ vÃ­ce vrstev, a proto vyÅ¾adujÃ­ vÃ­ce zdrojÅ¯.
* **MobileNet** je rodina modelÅ¯ s menÅ¡Ã­ velikostÃ­, vhodnÃ¡ pro mobilnÃ­ zaÅ™Ã­zenÃ­. PouÅ¾ijte je, pokud mÃ¡te omezenÃ© zdroje a mÅ¯Å¾ete obÄ›tovat trochu pÅ™esnosti.

Zde jsou ukÃ¡zkovÃ© rysy extrahovanÃ© z obrÃ¡zku koÄky pomocÃ­ sÃ­tÄ› VGG-16:

![Rysy extrahovanÃ© sÃ­tÃ­ VGG-16](../../../../../translated_images/cs/features.6291f9c7ba3a0b95.webp)

## Dataset KoÄky vs. Psi

V tomto pÅ™Ã­kladu pouÅ¾ijeme dataset [KoÄky a Psi](https://www.microsoft.com/download/details.aspx?id=54765&WT.mc_id=academic-77998-cacaste), kterÃ½ je velmi blÃ­zkÃ½ reÃ¡lnÃ©mu scÃ©nÃ¡Å™i klasifikace obrÃ¡zkÅ¯.

## âœï¸ CviÄenÃ­: Transfer learning

PodÃ­vejme se na transfer learning v praxi v odpovÃ­dajÃ­cÃ­ch noteboocÃ­ch:

* [Transfer Learning - PyTorch](TransferLearningPyTorch.ipynb)
* [Transfer Learning - TensorFlow](TransferLearningTF.ipynb)

## Vizualizace Adversarial Cat

PÅ™edtrÃ©novanÃ¡ neuronovÃ¡ sÃ­Å¥ obsahuje rÅ¯znÃ© vzory uvnitÅ™ svÃ©ho *mozku*, vÄetnÄ› pÅ™edstav o **ideÃ¡lnÃ­ koÄce** (stejnÄ› jako ideÃ¡lnÃ­m psovi, ideÃ¡lnÃ­ zebÅ™e atd.). Bylo by zajÃ­mavÃ© nÄ›jakÃ½m zpÅ¯sobem **vizualizovat tento obrÃ¡zek**. NenÃ­ to vÅ¡ak jednoduchÃ©, protoÅ¾e vzory jsou rozprostÅ™eny po celÃ½ch vahÃ¡ch sÃ­tÄ› a takÃ© organizovÃ¡ny v hierarchickÃ© struktuÅ™e.

JednÃ­m z pÅ™Ã­stupÅ¯, kterÃ© mÅ¯Å¾eme pouÅ¾Ã­t, je zaÄÃ­t s nÃ¡hodnÃ½m obrÃ¡zkem a potÃ© se pokusit pomocÃ­ techniky **optimalizace gradientnÃ­ho sestupu** upravit tento obrÃ¡zek tak, aby si sÃ­Å¥ zaÄala myslet, Å¾e je to koÄka.

![OptimalizaÄnÃ­ smyÄka obrÃ¡zku](../../../../../translated_images/cs/ideal-cat-loop.999fbb8ff306e044.webp)

Pokud to vÅ¡ak udÄ›lÃ¡me, obdrÅ¾Ã­me nÄ›co velmi podobnÃ©ho nÃ¡hodnÃ©mu Å¡umu. To je proto, Å¾e *existuje mnoho zpÅ¯sobÅ¯, jak sÃ­Å¥ pÅ™imÄ›t myslet si, Å¾e vstupnÃ­ obrÃ¡zek je koÄka*, vÄetnÄ› nÄ›kterÃ½ch, kterÃ© vizuÃ¡lnÄ› nedÃ¡vajÃ­ smysl. ZatÃ­mco tyto obrÃ¡zky obsahujÃ­ mnoho vzorÅ¯ typickÃ½ch pro koÄku, nic je neomezuje, aby byly vizuÃ¡lnÄ› rozliÅ¡itelnÃ©.

Pro zlepÅ¡enÃ­ vÃ½sledku mÅ¯Å¾eme do ztrÃ¡tovÃ© funkce pÅ™idat dalÅ¡Ã­ Älen, kterÃ½ se nazÃ½vÃ¡ **variation loss**. Je to metrika, kterÃ¡ ukazuje, jak podobnÃ© jsou sousednÃ­ pixely obrÃ¡zku. Minimalizace variation loss ÄinÃ­ obrÃ¡zek hladÅ¡Ã­m a zbavuje se Å¡umu â€“ tÃ­m odhaluje vizuÃ¡lnÄ› pÅ™itaÅ¾livÄ›jÅ¡Ã­ vzory. Zde je pÅ™Ã­klad takovÃ½ch "ideÃ¡lnÃ­ch" obrÃ¡zkÅ¯, kterÃ© jsou klasifikovÃ¡ny jako koÄka a jako zebra s vysokou pravdÄ›podobnostÃ­:

![IdeÃ¡lnÃ­ koÄka](../../../../../translated_images/cs/ideal-cat.203dd4597643d6b0.webp) | ![IdeÃ¡lnÃ­ zebra](../../../../../translated_images/cs/ideal-zebra.7f70e8b54ee15a7a.webp)
-----|-----
*IdeÃ¡lnÃ­ koÄka* | *IdeÃ¡lnÃ­ zebra*

PodobnÃ½ pÅ™Ã­stup lze pouÅ¾Ã­t k provÃ¡dÄ›nÃ­ tzv. **adversarial ÃºtokÅ¯** na neuronovou sÃ­Å¥. PÅ™edstavme si, Å¾e chceme oklamat neuronovou sÃ­Å¥ a pÅ™imÄ›t ji, aby psa povaÅ¾ovala za koÄku. Pokud vezmeme obrÃ¡zek psa, kterÃ½ je sÃ­tÃ­ rozpoznÃ¡n jako pes, mÅ¯Å¾eme jej trochu upravit pomocÃ­ optimalizace gradientnÃ­ho sestupu, dokud sÃ­Å¥ nezaÄne klasifikovat obrÃ¡zek jako koÄku:

![ObrÃ¡zek psa](../../../../../translated_images/cs/original-dog.8f68a67d2fe0911f.webp) | ![ObrÃ¡zek psa klasifikovanÃ½ jako koÄka](../../../../../translated_images/cs/adversarial-dog.d9fc7773b0142b89.webp)
-----|-----
*PÅ¯vodnÃ­ obrÃ¡zek psa* | *ObrÃ¡zek psa klasifikovanÃ½ jako koÄka*

PodÃ­vejte se na kÃ³d pro reprodukci vÃ½Å¡e uvedenÃ½ch vÃ½sledkÅ¯ v nÃ¡sledujÃ­cÃ­m notebooku:

* [IdeÃ¡lnÃ­ a Adversarial Cat - TensorFlow](AdversarialCat_TF.ipynb)

## ZÃ¡vÄ›r

PomocÃ­ transfer learningu mÅ¯Å¾ete rychle sestavit klasifikÃ¡tor pro Ãºlohu klasifikace vlastnÃ­ch objektÅ¯ a dosÃ¡hnout vysokÃ© pÅ™esnosti. VidÃ­te, Å¾e sloÅ¾itÄ›jÅ¡Ã­ Ãºlohy, kterÃ© nynÃ­ Å™eÅ¡Ã­me, vyÅ¾adujÃ­ vyÅ¡Å¡Ã­ vÃ½poÄetnÃ­ vÃ½kon a nelze je snadno Å™eÅ¡it na CPU. V dalÅ¡Ã­ ÄÃ¡sti se pokusÃ­me pouÅ¾Ã­t lehÄÃ­ implementaci k natrÃ©novÃ¡nÃ­ stejnÃ©ho modelu s niÅ¾Å¡Ã­mi vÃ½poÄetnÃ­mi zdroji, coÅ¾ povede jen k mÃ­rnÄ› niÅ¾Å¡Ã­ pÅ™esnosti.

## ğŸš€ VÃ½zva

V doprovodnÃ½ch noteboocÃ­ch jsou poznÃ¡mky na konci o tom, jak transfer knowledge nejlÃ©pe funguje s podobnÃ½mi trÃ©novacÃ­mi daty (napÅ™Ã­klad novÃ½ typ zvÃ­Å™ete). ProveÄte experimenty s ÃºplnÄ› novÃ½mi typy obrÃ¡zkÅ¯ a zjistÄ›te, jak dobÅ™e nebo Å¡patnÄ› vaÅ¡e modely transfer knowledge fungujÃ­.

## [KvÃ­z po pÅ™ednÃ¡Å¡ce](https://ff-quizzes.netlify.app/en/ai/quiz/16)

## PÅ™ehled & Samostudium

ProjdÄ›te si [TrainingTricks.md](TrainingTricks.md), abyste si prohloubili znalosti o dalÅ¡Ã­ch zpÅ¯sobech trÃ©novÃ¡nÃ­ modelÅ¯.

## [Ãškol](lab/README.md)

V tomto laboratornÃ­m cviÄenÃ­ pouÅ¾ijeme dataset skuteÄnÃ½ch [Oxford-IIIT](https://www.robots.ox.ac.uk/~vgg/data/pets/) domÃ¡cÃ­ch mazlÃ­ÄkÅ¯ s 35 plemeny koÄek a psÅ¯ a vytvoÅ™Ã­me klasifikÃ¡tor pomocÃ­ transfer learningu.

---

