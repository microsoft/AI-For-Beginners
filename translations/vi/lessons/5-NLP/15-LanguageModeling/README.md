# M√¥ h√¨nh Ng√¥n ng·ªØ

C√°c bi·ªÉu di·ªÖn ng·ªØ nghƒ©a, nh∆∞ Word2Vec v√† GloVe, th·ª±c ch·∫•t l√† b∆∞·ªõc ƒë·∫ßu ti√™n h∆∞·ªõng t·ªõi **m√¥ h√¨nh ng√¥n ng·ªØ** - t·∫°o ra c√°c m√¥ h√¨nh c√≥ th·ªÉ *hi·ªÉu* (ho·∫∑c *bi·ªÉu di·ªÖn*) b·∫£n ch·∫•t c·ªßa ng√¥n ng·ªØ.

## [C√¢u h·ªèi tr∆∞·ªõc b√†i gi·∫£ng](https://ff-quizzes.netlify.app/en/ai/quiz/29)

√ù t∆∞·ªüng ch√≠nh ƒë·∫±ng sau m√¥ h√¨nh ng√¥n ng·ªØ l√† hu·∫•n luy·ªán ch√∫ng tr√™n c√°c t·∫≠p d·ªØ li·ªáu kh√¥ng g√°n nh√£n theo c√°ch kh√¥ng gi√°m s√°t. ƒêi·ªÅu n√†y quan tr·ªçng v√¨ ch√∫ng ta c√≥ m·ªôt l∆∞·ª£ng l·ªõn vƒÉn b·∫£n kh√¥ng g√°n nh√£n, trong khi l∆∞·ª£ng vƒÉn b·∫£n c√≥ g√°n nh√£n lu√¥n b·ªã gi·ªõi h·∫°n b·ªüi c√¥ng s·ª©c m√† ch√∫ng ta c√≥ th·ªÉ b·ªè ra ƒë·ªÉ g√°n nh√£n. Th√¥ng th∆∞·ªùng, ch√∫ng ta c√≥ th·ªÉ x√¢y d·ª±ng c√°c m√¥ h√¨nh ng√¥n ng·ªØ c√≥ kh·∫£ nƒÉng **d·ª± ƒëo√°n t·ª´ b·ªã thi·∫øu** trong vƒÉn b·∫£n, b·ªüi v√¨ vi·ªác che gi·∫•u m·ªôt t·ª´ ng·∫´u nhi√™n trong vƒÉn b·∫£n v√† s·ª≠ d·ª•ng n√≥ l√†m m·∫´u hu·∫•n luy·ªán l√† r·∫•t d·ªÖ d√†ng.

## Hu·∫•n luy·ªán Bi·ªÉu di·ªÖn

Trong c√°c v√≠ d·ª• tr∆∞·ªõc, ch√∫ng ta ƒë√£ s·ª≠ d·ª•ng c√°c bi·ªÉu di·ªÖn ng·ªØ nghƒ©a ƒë∆∞·ª£c hu·∫•n luy·ªán s·∫µn, nh∆∞ng s·∫Ω r·∫•t th√∫ v·ªã khi xem c√°ch c√°c bi·ªÉu di·ªÖn n√†y ƒë∆∞·ª£c hu·∫•n luy·ªán. C√≥ m·ªôt s·ªë √Ω t∆∞·ªüng c√≥ th·ªÉ ƒë∆∞·ª£c s·ª≠ d·ª•ng:

* **M√¥ h√¨nh ng√¥n ng·ªØ N-Gram**, khi ch√∫ng ta d·ª± ƒëo√°n m·ªôt token b·∫±ng c√°ch nh√¨n v√†o N token tr∆∞·ªõc ƒë√≥ (N-gram).
* **Continuous Bag-of-Words** (CBoW), khi ch√∫ng ta d·ª± ƒëo√°n token ·ªü gi·ªØa $W_0$ trong m·ªôt chu·ªói token $W_{-N}$, ..., $W_N$.
* **Skip-gram**, n∆°i ch√∫ng ta d·ª± ƒëo√°n m·ªôt t·∫≠p h·ª£p c√°c token l√¢n c·∫≠n {$W_{-N},\dots, W_{-1}, W_1,\dots, W_N$} t·ª´ token ·ªü gi·ªØa $W_0$.

![h√¨nh ·∫£nh t·ª´ b√†i b√°o v·ªÅ chuy·ªÉn ƒë·ªïi t·ª´ th√†nh vector](../../../../../translated_images/vi/example-algorithms-for-converting-words-to-vectors.fbe9207a726922f6.webp)

> H√¨nh ·∫£nh t·ª´ [b√†i b√°o n√†y](https://arxiv.org/pdf/1301.3781.pdf)

## ‚úçÔ∏è V√≠ d·ª• Notebook: Hu·∫•n luy·ªán m√¥ h√¨nh CBoW

Ti·∫øp t·ª•c h·ªçc t·∫≠p qua c√°c notebook sau:

* [Hu·∫•n luy·ªán CBoW Word2Vec v·ªõi TensorFlow](CBoW-TF.ipynb)
* [Hu·∫•n luy·ªán CBoW Word2Vec v·ªõi PyTorch](CBoW-PyTorch.ipynb)

## K·∫øt lu·∫≠n

Trong b√†i h·ªçc tr∆∞·ªõc, ch√∫ng ta ƒë√£ th·∫•y r·∫±ng c√°c bi·ªÉu di·ªÖn t·ª´ ho·∫°t ƒë·ªông nh∆∞ m·ªôt ph√©p m√†u! Gi·ªù ƒë√¢y, ch√∫ng ta bi·∫øt r·∫±ng vi·ªác hu·∫•n luy·ªán c√°c bi·ªÉu di·ªÖn t·ª´ kh√¥ng ph·∫£i l√† m·ªôt nhi·ªám v·ª• qu√° ph·ª©c t·∫°p, v√† ch√∫ng ta c√≥ th·ªÉ t·ª± hu·∫•n luy·ªán c√°c bi·ªÉu di·ªÖn t·ª´ cho vƒÉn b·∫£n chuy√™n ng√†nh n·∫øu c·∫ßn.

## [C√¢u h·ªèi sau b√†i gi·∫£ng](https://ff-quizzes.netlify.app/en/ai/quiz/30)

## √în t·∫≠p & T·ª± h·ªçc

* [H∆∞·ªõng d·∫´n ch√≠nh th·ª©c c·ªßa PyTorch v·ªÅ M√¥ h√¨nh Ng√¥n ng·ªØ](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html).
* [H∆∞·ªõng d·∫´n ch√≠nh th·ª©c c·ªßa TensorFlow v·ªÅ hu·∫•n luy·ªán m√¥ h√¨nh Word2Vec](https://www.TensorFlow.org/tutorials/text/word2vec).
* S·ª≠ d·ª•ng framework **gensim** ƒë·ªÉ hu·∫•n luy·ªán c√°c bi·ªÉu di·ªÖn ph·ªï bi·∫øn nh·∫•t ch·ªâ trong v√†i d√≤ng m√£ ƒë∆∞·ª£c m√¥ t·∫£ [trong t√†i li·ªáu n√†y](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html).

## üöÄ [B√†i t·∫≠p: Hu·∫•n luy·ªán M√¥ h√¨nh Skip-Gram](lab/README.md)

Trong ph√≤ng th√≠ nghi·ªám, ch√∫ng t√¥i th√°ch th·ª©c b·∫°n ch·ªânh s·ª≠a m√£ t·ª´ b√†i h·ªçc n√†y ƒë·ªÉ hu·∫•n luy·ªán m√¥ h√¨nh skip-gram thay v√¨ CBoW. [ƒê·ªçc chi ti·∫øt](lab/README.md)

---

