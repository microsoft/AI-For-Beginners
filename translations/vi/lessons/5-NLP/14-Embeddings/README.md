<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "e40b47ac3fd48f71304ede1474e66293",
  "translation_date": "2025-08-29T12:48:37+00:00",
  "source_file": "lessons/5-NLP/14-Embeddings/README.md",
  "language_code": "vi"
}
-->
# NhÃºng (Embeddings)

## [CÃ¢u há»i trÆ°á»›c bÃ i giáº£ng](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/114)

Khi huáº¥n luyá»‡n cÃ¡c bá»™ phÃ¢n loáº¡i dá»±a trÃªn BoW hoáº·c TF/IDF, chÃºng ta lÃ m viá»‡c vá»›i cÃ¡c vector tÃºi tá»« (bag-of-words) cÃ³ chiá»u cao vá»›i Ä‘á»™ dÃ i `vocab_size`, vÃ  chÃºng ta Ä‘Ã£ chuyá»ƒn Ä‘á»•i rÃµ rÃ ng tá»« cÃ¡c vector biá»ƒu diá»…n vá»‹ trÃ­ cÃ³ chiá»u tháº¥p sang biá»ƒu diá»…n one-hot thÆ°a thá»›t. Tuy nhiÃªn, biá»ƒu diá»…n one-hot nÃ y khÃ´ng tiáº¿t kiá»‡m bá»™ nhá»›. NgoÃ i ra, má»—i tá»« Ä‘Æ°á»£c xá»­ lÃ½ Ä‘á»™c láº­p vá»›i nhau, tá»©c lÃ  cÃ¡c vector mÃ£ hÃ³a one-hot khÃ´ng thá»ƒ hiá»‡n báº¥t ká»³ sá»± tÆ°Æ¡ng Ä‘á»“ng ngá»¯ nghÄ©a nÃ o giá»¯a cÃ¡c tá»«.

Ã tÆ°á»Ÿng cá»§a **embedding** lÃ  biá»ƒu diá»…n cÃ¡c tá»« báº±ng cÃ¡c vector dÃ y Ä‘áº·c cÃ³ chiá»u tháº¥p hÆ¡n, pháº£n Ã¡nh pháº§n nÃ o Ã½ nghÄ©a ngá»¯ nghÄ©a cá»§a tá»«. ChÃºng ta sáº½ tháº£o luáº­n sau vá» cÃ¡ch xÃ¢y dá»±ng cÃ¡c nhÃºng tá»« cÃ³ Ã½ nghÄ©a, nhÆ°ng hiá»‡n táº¡i hÃ£y chá»‰ nghÄ© vá» embedding nhÆ° má»™t cÃ¡ch Ä‘á»ƒ giáº£m chiá»u cá»§a vector tá»«.

VÃ¬ váº­y, lá»›p embedding sáº½ nháº­n má»™t tá»« lÃ m Ä‘áº§u vÃ o vÃ  táº¡o ra má»™t vector Ä‘áº§u ra vá»›i kÃ­ch thÆ°á»›c `embedding_size` Ä‘Æ°á»£c chá»‰ Ä‘á»‹nh. Theo má»™t cÃ¡ch nÃ o Ä‘Ã³, nÃ³ ráº¥t giá»‘ng vá»›i lá»›p `Linear`, nhÆ°ng thay vÃ¬ nháº­n má»™t vector mÃ£ hÃ³a one-hot, nÃ³ cÃ³ thá»ƒ nháº­n sá»‘ thá»© tá»± cá»§a tá»« lÃ m Ä‘áº§u vÃ o, giÃºp chÃºng ta trÃ¡nh pháº£i táº¡o ra cÃ¡c vector mÃ£ hÃ³a one-hot lá»›n.

Báº±ng cÃ¡ch sá»­ dá»¥ng lá»›p embedding lÃ m lá»›p Ä‘áº§u tiÃªn trong máº¡ng phÃ¢n loáº¡i cá»§a chÃºng ta, chÃºng ta cÃ³ thá»ƒ chuyá»ƒn tá»« mÃ´ hÃ¬nh tÃºi tá»« (bag-of-words) sang mÃ´ hÃ¬nh **embedding bag**, trong Ä‘Ã³ chÃºng ta Ä‘áº§u tiÃªn chuyá»ƒn Ä‘á»•i má»—i tá»« trong vÄƒn báº£n thÃ nh embedding tÆ°Æ¡ng á»©ng, sau Ä‘Ã³ tÃ­nh toÃ¡n má»™t hÃ m tá»•ng há»£p nÃ o Ä‘Ã³ trÃªn táº¥t cáº£ cÃ¡c embedding nÃ y, cháº³ng háº¡n nhÆ° `sum`, `average` hoáº·c `max`.

![HÃ¬nh áº£nh minh há»a má»™t bá»™ phÃ¢n loáº¡i embedding cho nÄƒm tá»« trong chuá»—i.](../../../../../translated_images/embedding-classifier-example.b77f021a7ee67eeec8e68bfe11636c5b97d6eaa067515a129bfb1d0034b1ac5b.vi.png)

> HÃ¬nh áº£nh cá»§a tÃ¡c giáº£

## âœï¸ BÃ i táº­p: NhÃºng (Embeddings)

Tiáº¿p tá»¥c há»c trong cÃ¡c sá»• tay sau:
* [Embeddings vá»›i PyTorch](EmbeddingsPyTorch.ipynb)
* [Embeddings vá»›i TensorFlow](EmbeddingsTF.ipynb)

## NhÃºng ngá»¯ nghÄ©a: Word2Vec

Máº·c dÃ¹ lá»›p embedding há»c cÃ¡ch Ã¡nh xáº¡ cÃ¡c tá»« thÃ nh biá»ƒu diá»…n vector, nhÆ°ng biá»ƒu diá»…n nÃ y khÃ´ng nháº¥t thiáº¿t pháº£i mang nhiá»u Ã½ nghÄ©a ngá»¯ nghÄ©a. Sáº½ ráº¥t há»¯u Ã­ch náº¿u há»c Ä‘Æ°á»£c má»™t biá»ƒu diá»…n vector sao cho cÃ¡c tá»« tÆ°Æ¡ng tá»± hoáº·c Ä‘á»“ng nghÄ©a tÆ°Æ¡ng á»©ng vá»›i cÃ¡c vector gáº§n nhau theo má»™t khoáº£ng cÃ¡ch vector nÃ o Ä‘Ã³ (vÃ­ dá»¥: khoáº£ng cÃ¡ch Euclid).

Äá»ƒ lÃ m Ä‘iá»u Ä‘Ã³, chÃºng ta cáº§n huáº¥n luyá»‡n trÆ°á»›c mÃ´ hÃ¬nh embedding trÃªn má»™t táº­p há»£p vÄƒn báº£n lá»›n theo má»™t cÃ¡ch cá»¥ thá»ƒ. Má»™t cÃ¡ch Ä‘á»ƒ huáº¥n luyá»‡n nhÃºng ngá»¯ nghÄ©a Ä‘Æ°á»£c gá»i lÃ  [Word2Vec](https://en.wikipedia.org/wiki/Word2vec). NÃ³ dá»±a trÃªn hai kiáº¿n trÃºc chÃ­nh Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ táº¡o ra biá»ƒu diá»…n phÃ¢n tÃ¡n cá»§a tá»«:

 - **Continuous bag-of-words** (CBoW) â€” trong kiáº¿n trÃºc nÃ y, chÃºng ta huáº¥n luyá»‡n mÃ´ hÃ¬nh Ä‘á»ƒ dá»± Ä‘oÃ¡n má»™t tá»« tá»« ngá»¯ cáº£nh xung quanh. Vá»›i ngram $(W_{-2},W_{-1},W_0,W_1,W_2)$, má»¥c tiÃªu cá»§a mÃ´ hÃ¬nh lÃ  dá»± Ä‘oÃ¡n $W_0$ tá»« $(W_{-2},W_{-1},W_1,W_2)$.
 - **Continuous skip-gram** thÃ¬ ngÆ°á»£c láº¡i vá»›i CBoW. MÃ´ hÃ¬nh sá»­ dá»¥ng cá»­a sá»• ngá»¯ cáº£nh xung quanh Ä‘á»ƒ dá»± Ä‘oÃ¡n tá»« hiá»‡n táº¡i.

CBoW nhanh hÆ¡n, trong khi skip-gram cháº­m hÆ¡n nhÆ°ng lÃ m tá»‘t hÆ¡n trong viá»‡c biá»ƒu diá»…n cÃ¡c tá»« Ã­t xuáº¥t hiá»‡n.

![HÃ¬nh áº£nh minh há»a cáº£ hai thuáº­t toÃ¡n CBoW vÃ  Skip-Gram Ä‘á»ƒ chuyá»ƒn Ä‘á»•i tá»« thÃ nh vector.](../../../../../translated_images/example-algorithms-for-converting-words-to-vectors.fbe9207a726922f6f0f5de66427e8a6eda63809356114e28fb1fa5f4a83ebda7.vi.png)

> HÃ¬nh áº£nh tá»« [bÃ i bÃ¡o nÃ y](https://arxiv.org/pdf/1301.3781.pdf)

CÃ¡c nhÃºng Word2Vec Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c (cÅ©ng nhÆ° cÃ¡c mÃ´ hÃ¬nh tÆ°Æ¡ng tá»± khÃ¡c nhÆ° GloVe) cÅ©ng cÃ³ thá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng thay cho lá»›p embedding trong máº¡ng nÆ¡-ron. Tuy nhiÃªn, chÃºng ta cáº§n xá»­ lÃ½ cÃ¡c tá»« vá»±ng, vÃ¬ tá»« vá»±ng Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ huáº¥n luyá»‡n trÆ°á»›c Word2Vec/GloVe cÃ³ kháº£ nÄƒng khÃ¡c vá»›i tá»« vá»±ng trong táº­p vÄƒn báº£n cá»§a chÃºng ta. HÃ£y xem cÃ¡c sá»• tay á»Ÿ trÃªn Ä‘á»ƒ tÃ¬m hiá»ƒu cÃ¡ch giáº£i quyáº¿t váº¥n Ä‘á» nÃ y.

## NhÃºng ngá»¯ cáº£nh

Má»™t háº¡n cháº¿ chÃ­nh cá»§a cÃ¡c biá»ƒu diá»…n embedding Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c truyá»n thá»‘ng nhÆ° Word2Vec lÃ  váº¥n Ä‘á» phÃ¢n biá»‡t nghÄ©a cá»§a tá»«. Máº·c dÃ¹ cÃ¡c embedding Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c cÃ³ thá»ƒ náº¯m báº¯t má»™t pháº§n Ã½ nghÄ©a cá»§a tá»« trong ngá»¯ cáº£nh, nhÆ°ng má»i nghÄ©a cÃ³ thá»ƒ cÃ³ cá»§a má»™t tá»« Ä‘á»u Ä‘Æ°á»£c mÃ£ hÃ³a vÃ o cÃ¹ng má»™t embedding. Äiá»u nÃ y cÃ³ thá»ƒ gÃ¢y ra váº¥n Ä‘á» trong cÃ¡c mÃ´ hÃ¬nh háº¡ nguá»“n, vÃ¬ nhiá»u tá»« nhÆ° tá»« 'play' cÃ³ cÃ¡c nghÄ©a khÃ¡c nhau tÃ¹y thuá»™c vÃ o ngá»¯ cáº£nh mÃ  chÃºng Ä‘Æ°á»£c sá»­ dá»¥ng.

VÃ­ dá»¥, tá»« 'play' trong hai cÃ¢u sau cÃ³ Ã½ nghÄ©a khÃ¡ khÃ¡c nhau:

- TÃ´i Ä‘Ã£ Ä‘i xem má»™t **vá»Ÿ ká»‹ch** á»Ÿ nhÃ  hÃ¡t.
- John muá»‘n **chÆ¡i** vá»›i báº¡n bÃ¨ cá»§a mÃ¬nh.

CÃ¡c embedding Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c á»Ÿ trÃªn biá»ƒu diá»…n cáº£ hai nghÄ©a cá»§a tá»« 'play' trong cÃ¹ng má»™t embedding. Äá»ƒ kháº¯c phá»¥c háº¡n cháº¿ nÃ y, chÃºng ta cáº§n xÃ¢y dá»±ng cÃ¡c embedding dá»±a trÃªn **mÃ´ hÃ¬nh ngÃ´n ngá»¯**, Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn má»™t táº­p há»£p vÄƒn báº£n lá»›n vÃ  *hiá»ƒu* cÃ¡ch cÃ¡c tá»« cÃ³ thá»ƒ Ä‘Æ°á»£c sáº¯p xáº¿p trong cÃ¡c ngá»¯ cáº£nh khÃ¡c nhau. Viá»‡c tháº£o luáº­n vá» nhÃºng ngá»¯ cáº£nh náº±m ngoÃ i pháº¡m vi cá»§a bÃ i há»c nÃ y, nhÆ°ng chÃºng ta sáº½ quay láº¡i chÃºng khi nÃ³i vá» cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ sau trong khÃ³a há»c.

## Káº¿t luáº­n

Trong bÃ i há»c nÃ y, báº¡n Ä‘Ã£ khÃ¡m phÃ¡ cÃ¡ch xÃ¢y dá»±ng vÃ  sá»­ dá»¥ng cÃ¡c lá»›p embedding trong TensorFlow vÃ  PyTorch Ä‘á»ƒ pháº£n Ã¡nh tá»‘t hÆ¡n Ã½ nghÄ©a ngá»¯ nghÄ©a cá»§a tá»«.

## ğŸš€ Thá»­ thÃ¡ch

Word2Vec Ä‘Ã£ Ä‘Æ°á»£c sá»­ dá»¥ng cho má»™t sá»‘ á»©ng dá»¥ng thÃº vá»‹, bao gá»“m táº¡o lá»i bÃ i hÃ¡t vÃ  thÆ¡. HÃ£y xem [bÃ i viáº¿t nÃ y](https://www.politetype.com/blog/word2vec-color-poems) Ä‘á»ƒ tÃ¬m hiá»ƒu cÃ¡ch tÃ¡c giáº£ sá»­ dá»¥ng Word2Vec Ä‘á»ƒ táº¡o thÆ¡. Xem [video nÃ y cá»§a Dan Shiffmann](https://www.youtube.com/watch?v=LSS_bos_TPI&ab_channel=TheCodingTrain) Ä‘á»ƒ khÃ¡m phÃ¡ má»™t cÃ¡ch giáº£i thÃ­ch khÃ¡c vá» ká»¹ thuáº­t nÃ y. Sau Ä‘Ã³, hÃ£y thá»­ Ã¡p dá»¥ng cÃ¡c ká»¹ thuáº­t nÃ y vÃ o táº­p vÄƒn báº£n cá»§a riÃªng báº¡n, cÃ³ thá»ƒ láº¥y tá»« Kaggle.

## [CÃ¢u há»i sau bÃ i giáº£ng](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/214)

## Ã”n táº­p & Tá»± há»c

Äá»c bÃ i bÃ¡o nÃ y vá» Word2Vec: [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf)

## [BÃ i táº­p: Sá»• tay](assignment.md)

---

**TuyÃªn bá»‘ miá»…n trá»« trÃ¡ch nhiá»‡m**:  
TÃ i liá»‡u nÃ y Ä‘Ã£ Ä‘Æ°á»£c dá»‹ch báº±ng dá»‹ch vá»¥ dá»‹ch thuáº­t AI [Co-op Translator](https://github.com/Azure/co-op-translator). Máº·c dÃ¹ chÃºng tÃ´i cá»‘ gáº¯ng Ä‘áº£m báº£o Ä‘á»™ chÃ­nh xÃ¡c, xin lÆ°u Ã½ ráº±ng cÃ¡c báº£n dá»‹ch tá»± Ä‘á»™ng cÃ³ thá»ƒ chá»©a lá»—i hoáº·c khÃ´ng chÃ­nh xÃ¡c. TÃ i liá»‡u gá»‘c báº±ng ngÃ´n ngá»¯ báº£n Ä‘á»‹a nÃªn Ä‘Æ°á»£c coi lÃ  nguá»“n thÃ´ng tin chÃ­nh thá»©c. Äá»‘i vá»›i cÃ¡c thÃ´ng tin quan trá»ng, nÃªn sá»­ dá»¥ng dá»‹ch vá»¥ dá»‹ch thuáº­t chuyÃªn nghiá»‡p tá»« con ngÆ°á»i. ChÃºng tÃ´i khÃ´ng chá»‹u trÃ¡ch nhiá»‡m cho báº¥t ká»³ sá»± hiá»ƒu láº§m hoáº·c diá»…n giáº£i sai nÃ o phÃ¡t sinh tá»« viá»‡c sá»­ dá»¥ng báº£n dá»‹ch nÃ y.