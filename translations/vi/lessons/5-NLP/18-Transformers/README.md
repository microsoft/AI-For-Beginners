# CÆ¡ cháº¿ Attention vÃ  Transformers

## [CÃ¢u há»i trÆ°á»›c bÃ i giáº£ng](https://ff-quizzes.netlify.app/en/ai/quiz/35)

Má»™t trong nhá»¯ng váº¥n Ä‘á» quan trá»ng nháº¥t trong lÄ©nh vá»±c NLP lÃ  **dá»‹ch mÃ¡y**, má»™t nhiá»‡m vá»¥ thiáº¿t yáº¿u lÃ m ná»n táº£ng cho cÃ¡c cÃ´ng cá»¥ nhÆ° Google Translate. Trong pháº§n nÃ y, chÃºng ta sáº½ táº­p trung vÃ o dá»‹ch mÃ¡y, hoáº·c nÃ³i chung hÆ¡n, vÃ o báº¥t ká»³ nhiá»‡m vá»¥ *sequence-to-sequence* nÃ o (cÃ²n Ä‘Æ°á»£c gá»i lÃ  **chuyá»ƒn Ä‘á»•i cÃ¢u**).

Vá»›i RNNs, sequence-to-sequence Ä‘Æ°á»£c thá»±c hiá»‡n bá»Ÿi hai máº¡ng há»“i quy, trong Ä‘Ã³ má»™t máº¡ng, **encoder**, nÃ©n má»™t chuá»—i Ä‘áº§u vÃ o thÃ nh tráº¡ng thÃ¡i áº©n, trong khi máº¡ng khÃ¡c, **decoder**, má»Ÿ rá»™ng tráº¡ng thÃ¡i áº©n nÃ y thÃ nh káº¿t quáº£ Ä‘Ã£ dá»‹ch. CÃ³ má»™t sá»‘ váº¥n Ä‘á» vá»›i cÃ¡ch tiáº¿p cáº­n nÃ y:

* Tráº¡ng thÃ¡i cuá»‘i cÃ¹ng cá»§a máº¡ng encoder gáº·p khÃ³ khÄƒn trong viá»‡c nhá»› pháº§n Ä‘áº§u cá»§a cÃ¢u, dáº«n Ä‘áº¿n cháº¥t lÆ°á»£ng mÃ´ hÃ¬nh kÃ©m Ä‘á»‘i vá»›i cÃ¡c cÃ¢u dÃ i.
* Táº¥t cáº£ cÃ¡c tá»« trong má»™t chuá»—i Ä‘á»u cÃ³ tÃ¡c Ä‘á»™ng nhÆ° nhau Ä‘áº¿n káº¿t quáº£. Tuy nhiÃªn, trong thá»±c táº¿, cÃ¡c tá»« cá»¥ thá»ƒ trong chuá»—i Ä‘áº§u vÃ o thÆ°á»ng cÃ³ tÃ¡c Ä‘á»™ng lá»›n hÆ¡n Ä‘áº¿n cÃ¡c Ä‘áº§u ra tuáº§n tá»± so vá»›i cÃ¡c tá»« khÃ¡c.

**CÆ¡ cháº¿ Attention** cung cáº¥p má»™t cÃ¡ch Ä‘á»ƒ cÃ¢n nháº¯c tÃ¡c Ä‘á»™ng ngá»¯ cáº£nh cá»§a tá»«ng vector Ä‘áº§u vÃ o lÃªn tá»«ng dá»± Ä‘oÃ¡n Ä‘áº§u ra cá»§a RNN. CÃ¡ch nÃ³ Ä‘Æ°á»£c thá»±c hiá»‡n lÃ  táº¡o cÃ¡c Ä‘Æ°á»ng táº¯t giá»¯a cÃ¡c tráº¡ng thÃ¡i trung gian cá»§a RNN Ä‘áº§u vÃ o vÃ  RNN Ä‘áº§u ra. Theo cÃ¡ch nÃ y, khi táº¡o ra kÃ½ hiá»‡u Ä‘áº§u ra y<sub>t</sub>, chÃºng ta sáº½ xem xÃ©t táº¥t cáº£ cÃ¡c tráº¡ng thÃ¡i áº©n Ä‘áº§u vÃ o h<sub>i</sub>, vá»›i cÃ¡c há»‡ sá»‘ trá»ng sá»‘ khÃ¡c nhau &alpha;<sub>t,i</sub>.

![HÃ¬nh áº£nh mÃ´ táº£ mÃ´ hÃ¬nh encoder/decoder vá»›i lá»›p attention cá»™ng](../../../../../translated_images/vi/encoder-decoder-attention.7a726296894fb567.webp)

> MÃ´ hÃ¬nh encoder-decoder vá»›i cÆ¡ cháº¿ attention cá»™ng trong [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf), trÃ­ch dáº«n tá»« [bÃ i viáº¿t blog nÃ y](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)

Ma tráº­n attention {&alpha;<sub>i,j</sub>} sáº½ biá»ƒu thá»‹ má»©c Ä‘á»™ mÃ  cÃ¡c tá»« Ä‘áº§u vÃ o nháº¥t Ä‘á»‹nh áº£nh hÆ°á»Ÿng Ä‘áº¿n viá»‡c táº¡o ra má»™t tá»« cá»¥ thá»ƒ trong chuá»—i Ä‘áº§u ra. DÆ°á»›i Ä‘Ã¢y lÃ  má»™t vÃ­ dá»¥ vá» ma tráº­n nhÆ° váº­y:

![HÃ¬nh áº£nh hiá»ƒn thá»‹ má»™t máº«u cÄƒn chá»‰nh Ä‘Æ°á»£c tÃ¬m tháº¥y bá»Ÿi RNNsearch-50, láº¥y tá»« Bahdanau - arviz.org](../../../../../translated_images/vi/bahdanau-fig3.09ba2d37f202a6af.webp)

> HÃ¬nh tá»« [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) (HÃ¬nh 3)

CÆ¡ cháº¿ attention chá»‹u trÃ¡ch nhiá»‡m cho pháº§n lá»›n tráº¡ng thÃ¡i hiá»‡n táº¡i hoáº·c gáº§n tráº¡ng thÃ¡i hiá»‡n táº¡i trong NLP. Tuy nhiÃªn, viá»‡c thÃªm attention lÃ m tÄƒng Ä‘Ã¡ng ká»ƒ sá»‘ lÆ°á»£ng tham sá»‘ mÃ´ hÃ¬nh, dáº«n Ä‘áº¿n cÃ¡c váº¥n Ä‘á» vá» kháº£ nÄƒng má»Ÿ rá»™ng vá»›i RNNs. Má»™t háº¡n cháº¿ chÃ­nh cá»§a viá»‡c má»Ÿ rá»™ng RNNs lÃ  tÃ­nh cháº¥t há»“i quy cá»§a cÃ¡c mÃ´ hÃ¬nh khiáº¿n viá»‡c xá»­ lÃ½ theo lÃ´ vÃ  song song hÃ³a huáº¥n luyá»‡n trá»Ÿ nÃªn khÃ³ khÄƒn. Trong RNN, má»—i pháº§n tá»­ cá»§a má»™t chuá»—i cáº§n Ä‘Æ°á»£c xá»­ lÃ½ theo thá»© tá»± tuáº§n tá»±, Ä‘iá»u nÃ y cÃ³ nghÄ©a lÃ  nÃ³ khÃ´ng thá»ƒ dá»… dÃ ng song song hÃ³a.

![Encoder Decoder vá»›i Attention](../../../../../lessons/5-NLP/18-Transformers/images/EncDecAttention.gif)

> HÃ¬nh tá»« [Blog cá»§a Google](https://research.googleblog.com/2016/09/a-neural-network-for-machine.html)

Viá»‡c Ã¡p dá»¥ng cÆ¡ cháº¿ attention káº¿t há»£p vá»›i háº¡n cháº¿ nÃ y Ä‘Ã£ dáº«n Ä‘áº¿n sá»± ra Ä‘á»i cá»§a cÃ¡c mÃ´ hÃ¬nh Transformer hiá»‡n nay, nhÆ° BERT vÃ  Open-GPT3.

## MÃ´ hÃ¬nh Transformer

Má»™t trong nhá»¯ng Ã½ tÆ°á»Ÿng chÃ­nh Ä‘áº±ng sau transformers lÃ  trÃ¡nh tÃ­nh cháº¥t tuáº§n tá»± cá»§a RNNs vÃ  táº¡o ra má»™t mÃ´ hÃ¬nh cÃ³ thá»ƒ song song hÃ³a trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n. Äiá»u nÃ y Ä‘Æ°á»£c thá»±c hiá»‡n báº±ng cÃ¡ch triá»ƒn khai hai Ã½ tÆ°á»Ÿng:

* mÃ£ hÃ³a vá»‹ trÃ­
* sá»­ dá»¥ng cÆ¡ cháº¿ self-attention Ä‘á»ƒ náº¯m báº¯t cÃ¡c máº«u thay vÃ¬ RNNs (hoáº·c CNNs) (Ä‘Ã³ lÃ  lÃ½ do táº¡i sao bÃ i bÃ¡o giá»›i thiá»‡u transformers Ä‘Æ°á»£c gá»i lÃ  *[Attention is all you need](https://arxiv.org/abs/1706.03762)*)

### MÃ£ hÃ³a/nhÃºng vá»‹ trÃ­

Ã tÆ°á»Ÿng cá»§a mÃ£ hÃ³a vá»‹ trÃ­ nhÆ° sau. 
1. Khi sá»­ dá»¥ng RNNs, vá»‹ trÃ­ tÆ°Æ¡ng Ä‘á»‘i cá»§a cÃ¡c token Ä‘Æ°á»£c biá»ƒu thá»‹ báº±ng sá»‘ bÆ°á»›c, do Ä‘Ã³ khÃ´ng cáº§n pháº£i biá»ƒu thá»‹ rÃµ rÃ ng. 
2. Tuy nhiÃªn, khi chuyá»ƒn sang attention, chÃºng ta cáº§n biáº¿t vá»‹ trÃ­ tÆ°Æ¡ng Ä‘á»‘i cá»§a cÃ¡c token trong má»™t chuá»—i. 
3. Äá»ƒ cÃ³ mÃ£ hÃ³a vá»‹ trÃ­, chÃºng ta bá»• sung chuá»—i token cá»§a mÃ¬nh báº±ng má»™t chuá»—i vá»‹ trÃ­ cá»§a token trong chuá»—i (tá»©c lÃ  má»™t chuá»—i sá»‘ 0,1,...).
4. Sau Ä‘Ã³, chÃºng ta trá»™n vá»‹ trÃ­ cá»§a token vá»›i vector nhÃºng cá»§a token. Äá»ƒ chuyá»ƒn Ä‘á»•i vá»‹ trÃ­ (sá»‘ nguyÃªn) thÃ nh vector, chÃºng ta cÃ³ thá»ƒ sá»­ dá»¥ng cÃ¡c phÆ°Æ¡ng phÃ¡p khÃ¡c nhau:

* NhÃºng cÃ³ thá»ƒ huáº¥n luyá»‡n, tÆ°Æ¡ng tá»± nhÆ° nhÃºng token. ÄÃ¢y lÃ  cÃ¡ch tiáº¿p cáº­n chÃºng ta xem xÃ©t á»Ÿ Ä‘Ã¢y. ChÃºng ta Ã¡p dá»¥ng cÃ¡c lá»›p nhÃºng lÃªn cáº£ token vÃ  vá»‹ trÃ­ cá»§a chÃºng, táº¡o ra cÃ¡c vector nhÃºng cÃ³ cÃ¹ng kÃ­ch thÆ°á»›c, sau Ä‘Ã³ cá»™ng chÃºng láº¡i vá»›i nhau.
* HÃ m mÃ£ hÃ³a vá»‹ trÃ­ cá»‘ Ä‘á»‹nh, nhÆ° Ä‘Æ°á»£c Ä‘á» xuáº¥t trong bÃ i bÃ¡o gá»‘c.

<img src="../../../../../translated_images/vi/pos-embedding.e41ce9b6cf6078af.webp" width="50%"/>

> HÃ¬nh áº£nh cá»§a tÃ¡c giáº£

Káº¿t quáº£ mÃ  chÃºng ta nháº­n Ä‘Æ°á»£c vá»›i nhÃºng vá»‹ trÃ­ lÃ  nhÃºng cáº£ token gá»‘c vÃ  vá»‹ trÃ­ cá»§a nÃ³ trong má»™t chuá»—i.

### Multi-Head Self-Attention

Tiáº¿p theo, chÃºng ta cáº§n náº¯m báº¯t má»™t sá»‘ máº«u trong chuá»—i cá»§a mÃ¬nh. Äá»ƒ lÃ m Ä‘iá»u nÃ y, transformers sá»­ dá»¥ng cÆ¡ cháº¿ **self-attention**, vá» cÆ¡ báº£n lÃ  attention Ä‘Æ°á»£c Ã¡p dá»¥ng cho cÃ¹ng má»™t chuá»—i lÃ m Ä‘áº§u vÃ o vÃ  Ä‘áº§u ra. Viá»‡c Ã¡p dá»¥ng self-attention cho phÃ©p chÃºng ta xem xÃ©t **ngá»¯ cáº£nh** trong cÃ¢u vÃ  xem cÃ¡c tá»« nÃ o cÃ³ liÃªn quan Ä‘áº¿n nhau. VÃ­ dá»¥, nÃ³ cho phÃ©p chÃºng ta tháº¥y cÃ¡c tá»« nÃ o Ä‘Æ°á»£c tham chiáº¿u bá»Ÿi cÃ¡c Ä‘áº¡i tá»« nhÆ° *it*, vÃ  cÅ©ng xem xÃ©t ngá»¯ cáº£nh:

![](../../../../../translated_images/vi/CoreferenceResolution.861924d6d384a7d6.webp)

> HÃ¬nh áº£nh tá»« [Blog cá»§a Google](https://research.googleblog.com/2017/08/transformer-novel-neural-network.html)

Trong transformers, chÃºng ta sá»­ dá»¥ng **Multi-Head Attention** Ä‘á»ƒ cung cáº¥p cho máº¡ng kháº£ nÄƒng náº¯m báº¯t nhiá»u loáº¡i phá»¥ thuá»™c khÃ¡c nhau, vÃ­ dá»¥: má»‘i quan há»‡ tá»« ngá»¯ dÃ i háº¡n so vá»›i ngáº¯n háº¡n, Ä‘á»“ng tham chiáº¿u so vá»›i má»™t thá»© khÃ¡c, v.v.

[Notebook TensorFlow](TransformersTF.ipynb) chá»©a thÃªm chi tiáº¿t vá» viá»‡c triá»ƒn khai cÃ¡c lá»›p transformer.

### Attention Encoder-Decoder

Trong transformers, attention Ä‘Æ°á»£c sá»­ dá»¥ng á»Ÿ hai nÆ¡i:

* Äá»ƒ náº¯m báº¯t cÃ¡c máº«u trong vÄƒn báº£n Ä‘áº§u vÃ o báº±ng self-attention
* Äá»ƒ thá»±c hiá»‡n dá»‹ch chuá»—i - Ä‘Ã³ lÃ  lá»›p attention giá»¯a encoder vÃ  decoder.

Attention encoder-decoder ráº¥t giá»‘ng vá»›i cÆ¡ cháº¿ attention Ä‘Æ°á»£c sá»­ dá»¥ng trong RNNs, nhÆ° Ä‘Ã£ mÃ´ táº£ á»Ÿ pháº§n Ä‘áº§u cá»§a pháº§n nÃ y. Biá»ƒu Ä‘á»“ Ä‘á»™ng nÃ y giáº£i thÃ­ch vai trÃ² cá»§a attention encoder-decoder.

![GIF Ä‘á»™ng hiá»ƒn thá»‹ cÃ¡ch cÃ¡c Ä‘Ã¡nh giÃ¡ Ä‘Æ°á»£c thá»±c hiá»‡n trong mÃ´ hÃ¬nh transformer.](../../../../../lessons/5-NLP/18-Transformers/images/transformer-animated-explanation.gif)

VÃ¬ má»—i vá»‹ trÃ­ Ä‘áº§u vÃ o Ä‘Æ°á»£c Ã¡nh xáº¡ Ä‘á»™c láº­p Ä‘áº¿n má»—i vá»‹ trÃ­ Ä‘áº§u ra, transformers cÃ³ thá»ƒ song song hÃ³a tá»‘t hÆ¡n RNNs, Ä‘iá»u nÃ y cho phÃ©p cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n hÆ¡n vÃ  biá»ƒu Ä‘áº¡t hÆ¡n. Má»—i attention head cÃ³ thá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ há»c cÃ¡c má»‘i quan há»‡ khÃ¡c nhau giá»¯a cÃ¡c tá»«, cáº£i thiá»‡n cÃ¡c nhiá»‡m vá»¥ xá»­ lÃ½ ngÃ´n ngá»¯ tá»± nhiÃªn.

## BERT

**BERT** (Bidirectional Encoder Representations from Transformers) lÃ  má»™t máº¡ng transformer nhiá»u lá»›p ráº¥t lá»›n vá»›i 12 lá»›p cho *BERT-base*, vÃ  24 lá»›p cho *BERT-large*. MÃ´ hÃ¬nh nÃ y Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c trÃªn má»™t táº­p dá»¯ liá»‡u vÄƒn báº£n lá»›n (WikiPedia + sÃ¡ch) báº±ng cÃ¡ch huáº¥n luyá»‡n khÃ´ng giÃ¡m sÃ¡t (dá»± Ä‘oÃ¡n cÃ¡c tá»« bá»‹ che trong cÃ¢u). Trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n trÆ°á»›c, mÃ´ hÃ¬nh háº¥p thá»¥ má»©c Ä‘á»™ hiá»ƒu biáº¿t ngÃ´n ngá»¯ Ä‘Ã¡ng ká»ƒ, sau Ä‘Ã³ cÃ³ thá»ƒ Ä‘Æ°á»£c táº­n dá»¥ng vá»›i cÃ¡c táº­p dá»¯ liá»‡u khÃ¡c báº±ng cÃ¡ch tinh chá»‰nh. QuÃ¡ trÃ¬nh nÃ y Ä‘Æ°á»£c gá»i lÃ  **há»c chuyá»ƒn giao**.

![HÃ¬nh áº£nh tá»« http://jalammar.github.io/illustrated-bert/](../../../../../translated_images/vi/jalammarBERT-language-modeling-masked-lm.34f113ea5fec4362.webp)

> HÃ¬nh áº£nh [nguá»“n](http://jalammar.github.io/illustrated-bert/)

## âœï¸ BÃ i táº­p: Transformers

Tiáº¿p tá»¥c há»c trong cÃ¡c notebook sau:

* [Transformers trong PyTorch](TransformersPyTorch.ipynb)
* [Transformers trong TensorFlow](TransformersTF.ipynb)

## Káº¿t luáº­n

Trong bÃ i há»c nÃ y, báº¡n Ä‘Ã£ tÃ¬m hiá»ƒu vá» Transformers vÃ  cÆ¡ cháº¿ Attention, táº¥t cáº£ Ä‘á»u lÃ  cÃ´ng cá»¥ thiáº¿t yáº¿u trong bá»™ cÃ´ng cá»¥ NLP. CÃ³ nhiá»u biáº¿n thá»ƒ cá»§a kiáº¿n trÃºc Transformer bao gá»“m BERT, DistilBERT, BigBird, OpenGPT3 vÃ  nhiá»u hÆ¡n ná»¯a cÃ³ thá»ƒ Ä‘Æ°á»£c tinh chá»‰nh. GÃ³i [HuggingFace](https://github.com/huggingface/) cung cáº¥p kho lÆ°u trá»¯ Ä‘á»ƒ huáº¥n luyá»‡n nhiá»u kiáº¿n trÃºc nÃ y vá»›i cáº£ PyTorch vÃ  TensorFlow.

## ğŸš€ Thá»­ thÃ¡ch

## [CÃ¢u há»i sau bÃ i giáº£ng](https://ff-quizzes.netlify.app/en/ai/quiz/36)

## Ã”n táº­p & Tá»± há»c

* [BÃ i viáº¿t blog](https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/), giáº£i thÃ­ch bÃ i bÃ¡o cá»• Ä‘iá»ƒn [Attention is all you need](https://arxiv.org/abs/1706.03762) vá» transformers.
* [Má»™t loáº¡t bÃ i viáº¿t blog](https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452) vá» transformers, giáº£i thÃ­ch chi tiáº¿t kiáº¿n trÃºc.

## [BÃ i táº­p](assignment.md)

---

