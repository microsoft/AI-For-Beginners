{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cơ chế Attention và mô hình Transformer\n",
    "\n",
    "Một nhược điểm lớn của mạng hồi quy (recurrent networks) là tất cả các từ trong một chuỗi đều có tác động như nhau đến kết quả. Điều này dẫn đến hiệu suất không tối ưu với các mô hình LSTM encoder-decoder tiêu chuẩn cho các nhiệm vụ chuỗi sang chuỗi, chẳng hạn như Nhận diện Thực thể Được đặt tên (Named Entity Recognition) và Dịch Máy (Machine Translation). Trên thực tế, các từ cụ thể trong chuỗi đầu vào thường có ảnh hưởng lớn hơn đến các đầu ra tuần tự so với các từ khác.\n",
    "\n",
    "Hãy xem xét mô hình chuỗi sang chuỗi, chẳng hạn như dịch máy. Mô hình này được triển khai bằng hai mạng hồi quy, trong đó một mạng (**encoder**) sẽ nén chuỗi đầu vào thành trạng thái ẩn, và mạng còn lại (**decoder**) sẽ mở rộng trạng thái ẩn này thành kết quả đã dịch. Vấn đề với cách tiếp cận này là trạng thái cuối cùng của mạng sẽ gặp khó khăn trong việc ghi nhớ phần đầu của câu, dẫn đến chất lượng mô hình kém đối với các câu dài.\n",
    "\n",
    "**Cơ chế Attention** cung cấp một cách để cân nhắc mức độ ảnh hưởng ngữ cảnh của từng vector đầu vào lên từng dự đoán đầu ra của RNN. Cách triển khai là tạo các đường tắt giữa các trạng thái trung gian của RNN đầu vào và RNN đầu ra. Theo cách này, khi tạo ra ký hiệu đầu ra $y_t$, chúng ta sẽ xem xét tất cả các trạng thái ẩn đầu vào $h_i$, với các hệ số trọng số khác nhau $\\alpha_{t,i}$.\n",
    "\n",
    "![Hình ảnh mô hình encoder/decoder với lớp attention cộng tính](../../../../../translated_images/vi/encoder-decoder-attention.7a726296894fb567.webp)\n",
    "*Mô hình encoder-decoder với cơ chế attention cộng tính trong [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf), được trích dẫn từ [bài viết blog này](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)*\n",
    "\n",
    "Ma trận Attention $\\{\\alpha_{i,j}\\}$ sẽ đại diện cho mức độ mà các từ đầu vào cụ thể ảnh hưởng đến việc tạo ra một từ nhất định trong chuỗi đầu ra. Dưới đây là ví dụ về một ma trận như vậy:\n",
    "\n",
    "![Hình ảnh minh họa sự liên kết mẫu được tìm thấy bởi RNNsearch-50, lấy từ Bahdanau - arviz.org](../../../../../translated_images/vi/bahdanau-fig3.09ba2d37f202a6af.webp)\n",
    "\n",
    "*Hình ảnh được lấy từ [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) (Hình 3)*\n",
    "\n",
    "Cơ chế Attention chịu trách nhiệm cho phần lớn các thành tựu hiện tại hoặc gần đây trong Xử lý Ngôn ngữ Tự nhiên. Tuy nhiên, việc thêm Attention làm tăng đáng kể số lượng tham số của mô hình, dẫn đến các vấn đề về khả năng mở rộng với RNNs. Một hạn chế chính của việc mở rộng RNNs là tính chất hồi quy của các mô hình khiến việc xử lý theo lô và song song hóa huấn luyện trở nên khó khăn. Trong RNN, mỗi phần tử của một chuỗi cần được xử lý theo thứ tự tuần tự, điều này có nghĩa là không thể dễ dàng song song hóa.\n",
    "\n",
    "Việc áp dụng cơ chế Attention kết hợp với hạn chế này đã dẫn đến sự ra đời của các Mô hình Transformer hiện đại mà chúng ta biết và sử dụng ngày nay, từ BERT đến OpenGPT3.\n",
    "\n",
    "## Mô hình Transformer\n",
    "\n",
    "Thay vì chuyển tiếp ngữ cảnh của từng dự đoán trước đó vào bước đánh giá tiếp theo, **mô hình Transformer** sử dụng **mã hóa vị trí** (positional encodings) và Attention để nắm bắt ngữ cảnh của đầu vào trong một cửa sổ văn bản được cung cấp. Hình ảnh dưới đây minh họa cách mã hóa vị trí kết hợp với Attention có thể nắm bắt ngữ cảnh trong một cửa sổ nhất định.\n",
    "\n",
    "![GIF động minh họa cách các đánh giá được thực hiện trong mô hình Transformer.](../../../../../lessons/5-NLP/18-Transformers/images/transformer-animated-explanation.gif)\n",
    "\n",
    "Vì mỗi vị trí đầu vào được ánh xạ độc lập đến mỗi vị trí đầu ra, các mô hình Transformer có thể song song hóa tốt hơn so với RNNs, điều này cho phép tạo ra các mô hình ngôn ngữ lớn hơn và biểu đạt hơn. Mỗi Attention head có thể được sử dụng để học các mối quan hệ khác nhau giữa các từ, cải thiện các nhiệm vụ Xử lý Ngôn ngữ Tự nhiên.\n",
    "\n",
    "**BERT** (Bidirectional Encoder Representations from Transformers) là một mạng Transformer đa lớp rất lớn với 12 lớp cho *BERT-base*, và 24 lớp cho *BERT-large*. Mô hình này được huấn luyện trước trên một tập dữ liệu văn bản lớn (WikiPedia + sách) bằng cách huấn luyện không giám sát (dự đoán các từ bị che trong một câu). Trong quá trình huấn luyện trước, mô hình hấp thụ một mức độ hiểu biết ngôn ngữ đáng kể, sau đó có thể được tận dụng với các tập dữ liệu khác thông qua việc tinh chỉnh. Quá trình này được gọi là **học chuyển giao** (transfer learning).\n",
    "\n",
    "![Hình ảnh từ http://jalammar.github.io/illustrated-bert/](../../../../../translated_images/vi/jalammarBERT-language-modeling-masked-lm.34f113ea5fec4362.webp)\n",
    "\n",
    "Có nhiều biến thể của kiến trúc Transformer bao gồm BERT, DistilBERT, BigBird, OpenGPT3 và nhiều hơn nữa có thể được tinh chỉnh. Gói [HuggingFace](https://github.com/huggingface/) cung cấp kho lưu trữ để huấn luyện nhiều kiến trúc này với PyTorch.\n",
    "\n",
    "## Sử dụng BERT để phân loại văn bản\n",
    "\n",
    "Hãy xem cách chúng ta có thể sử dụng mô hình BERT đã được huấn luyện trước để giải quyết nhiệm vụ truyền thống: phân loại chuỗi. Chúng ta sẽ phân loại tập dữ liệu AG News ban đầu của mình.\n",
    "\n",
    "Đầu tiên, hãy tải thư viện HuggingFace và tập dữ liệu của chúng ta:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Building vocab...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "from torchnlp import *\n",
    "import transformers\n",
    "train_dataset, test_dataset, classes, vocab = load_dataset()\n",
    "vocab_len = len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vì chúng ta sẽ sử dụng mô hình BERT đã được huấn luyện trước, nên cần sử dụng một tokenizer cụ thể. Đầu tiên, chúng ta sẽ tải một tokenizer liên kết với mô hình BERT đã được huấn luyện trước.\n",
    "\n",
    "Thư viện HuggingFace chứa một kho các mô hình đã được huấn luyện trước, bạn có thể sử dụng chỉ bằng cách chỉ định tên của chúng làm tham số cho các hàm `from_pretrained`. Tất cả các tệp nhị phân cần thiết cho mô hình sẽ được tự động tải xuống.\n",
    "\n",
    "Tuy nhiên, trong một số trường hợp, bạn sẽ cần tải mô hình của riêng mình. Khi đó, bạn có thể chỉ định thư mục chứa tất cả các tệp liên quan, bao gồm các tham số cho tokenizer, tệp `config.json` với các tham số của mô hình, trọng số nhị phân, v.v.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load the model from Internet repository using model name. \n",
    "# Use this if you are running from your own copy of the notebooks\n",
    "bert_model = 'bert-base-uncased' \n",
    "\n",
    "# To load the model from the directory on disk. Use this for Microsoft Learn module, because we have\n",
    "# prepared all required files for you.\n",
    "bert_model = './bert'\n",
    "\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(bert_model)\n",
    "\n",
    "MAX_SEQ_LEN = 128\n",
    "PAD_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "UNK_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Đối tượng `tokenizer` chứa hàm `encode` có thể được sử dụng trực tiếp để mã hóa văn bản:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 1052, 22123, 2953, 2818, 2003, 1037, 2307, 7705, 2005, 17953, 2361, 102]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('PyTorch is a great framework for NLP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sau đó, hãy tạo các trình lặp mà chúng ta sẽ sử dụng trong quá trình huấn luyện để truy cập dữ liệu. Vì BERT sử dụng hàm mã hóa riêng của nó, chúng ta sẽ cần định nghĩa một hàm đệm tương tự như `padify` mà chúng ta đã định nghĩa trước đó:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_bert(b):\n",
    "    # b is the list of tuples of length batch_size\n",
    "    #   - first element of a tuple = label, \n",
    "    #   - second = feature (text sequence)\n",
    "    # build vectorized sequence\n",
    "    v = [tokenizer.encode(x[1]) for x in b]\n",
    "    # compute max length of a sequence in this minibatch\n",
    "    l = max(map(len,v))\n",
    "    return ( # tuple of two tensors - labels and features\n",
    "        torch.LongTensor([t[0] for t in b]),\n",
    "        torch.stack([torch.nn.functional.pad(torch.tensor(t),(0,l-len(t)),mode='constant',value=0) for t in v])\n",
    "    )\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=8, collate_fn=pad_bert, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=8, collate_fn=pad_bert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong trường hợp của chúng ta, chúng ta sẽ sử dụng mô hình BERT đã được huấn luyện trước có tên là `bert-base-uncased`. Hãy tải mô hình bằng gói `BertForSequenceClassfication`. Điều này đảm bảo rằng mô hình của chúng ta đã có sẵn kiến trúc cần thiết cho việc phân loại, bao gồm cả bộ phân loại cuối cùng. Bạn sẽ thấy thông báo cảnh báo cho biết rằng các trọng số của bộ phân loại cuối cùng chưa được khởi tạo, và mô hình sẽ cần được huấn luyện trước - điều đó hoàn toàn ổn, vì đó chính xác là những gì chúng ta sắp làm!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./bert were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./bert and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = transformers.BertForSequenceClassification.from_pretrained(bert_model,num_labels=4).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bây giờ chúng ta đã sẵn sàng bắt đầu huấn luyện! Vì BERT đã được huấn luyện trước, chúng ta muốn bắt đầu với một tốc độ học khá nhỏ để không làm hỏng các trọng số ban đầu.\n",
    "\n",
    "Toàn bộ công việc khó khăn được thực hiện bởi mô hình `BertForSequenceClassification`. Khi chúng ta gọi mô hình trên dữ liệu huấn luyện, nó trả về cả giá trị mất mát (loss) và đầu ra của mạng cho minibatch đầu vào. Chúng ta sử dụng giá trị mất mát để tối ưu hóa tham số (`loss.backward()` thực hiện bước lan truyền ngược), và `out` để tính độ chính xác của việc huấn luyện bằng cách so sánh các nhãn thu được `labs` (được tính bằng cách sử dụng `argmax`) với các nhãn mong đợi `labels`.\n",
    "\n",
    "Để kiểm soát quá trình, chúng ta tích lũy giá trị mất mát và độ chính xác qua một số lần lặp, và in chúng ra sau mỗi chu kỳ huấn luyện `report_freq`.\n",
    "\n",
    "Quá trình huấn luyện này có thể sẽ mất khá nhiều thời gian, vì vậy chúng ta giới hạn số lần lặp lại.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 1.1254194641113282, Accuracy = 0.585\n",
      "Loss = 0.6194715118408203, Accuracy = 0.83\n",
      "Loss = 0.46665248870849607, Accuracy = 0.8475\n",
      "Loss = 0.4309701919555664, Accuracy = 0.8575\n",
      "Loss = 0.35427074432373046, Accuracy = 0.8825\n",
      "Loss = 0.3306886291503906, Accuracy = 0.8975\n",
      "Loss = 0.30340143203735354, Accuracy = 0.8975\n",
      "Loss = 0.26139299392700194, Accuracy = 0.915\n",
      "Loss = 0.26708646774291994, Accuracy = 0.9225\n",
      "Loss = 0.3667240524291992, Accuracy = 0.8675\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
    "\n",
    "report_freq = 50\n",
    "iterations = 500 # make this larger to train for longer time!\n",
    "\n",
    "model.train()\n",
    "\n",
    "i,c = 0,0\n",
    "acc_loss = 0\n",
    "acc_acc = 0\n",
    "\n",
    "for labels,texts in train_loader:\n",
    "    labels = labels.to(device)-1 # get labels in the range 0-3         \n",
    "    texts = texts.to(device)\n",
    "    loss, out = model(texts, labels=labels)[:2]\n",
    "    labs = out.argmax(dim=1)\n",
    "    acc = torch.mean((labs==labels).type(torch.float32))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    acc_loss += loss\n",
    "    acc_acc += acc\n",
    "    i+=1\n",
    "    c+=1\n",
    "    if i%report_freq==0:\n",
    "        print(f\"Loss = {acc_loss.item()/c}, Accuracy = {acc_acc.item()/c}\")\n",
    "        c = 0\n",
    "        acc_loss = 0\n",
    "        acc_acc = 0\n",
    "    iterations-=1\n",
    "    if not iterations:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bạn có thể thấy (đặc biệt nếu bạn tăng số lần lặp và chờ đủ lâu) rằng việc phân loại bằng BERT mang lại độ chính xác khá tốt! Điều này là do BERT đã hiểu khá rõ cấu trúc của ngôn ngữ, và chúng ta chỉ cần tinh chỉnh bộ phân loại cuối cùng. Tuy nhiên, vì BERT là một mô hình lớn, toàn bộ quá trình huấn luyện mất rất nhiều thời gian và đòi hỏi sức mạnh tính toán đáng kể! (GPU, và tốt nhất là nhiều hơn một GPU).\n",
    "\n",
    "> **Note:** Trong ví dụ của chúng ta, chúng ta đã sử dụng một trong những mô hình BERT được huấn luyện sẵn nhỏ nhất. Có những mô hình lớn hơn có khả năng mang lại kết quả tốt hơn.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Đánh giá hiệu suất của mô hình\n",
    "\n",
    "Bây giờ chúng ta có thể đánh giá hiệu suất của mô hình trên tập dữ liệu kiểm tra. Vòng lặp đánh giá khá giống với vòng lặp huấn luyện, nhưng đừng quên chuyển mô hình sang chế độ đánh giá bằng cách gọi `model.eval()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy: 0.9047029702970297\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "iterations = 100\n",
    "acc = 0\n",
    "i = 0\n",
    "for labels,texts in test_loader:\n",
    "    labels = labels.to(device)-1      \n",
    "    texts = texts.to(device)\n",
    "    _, out = model(texts, labels=labels)[:2]\n",
    "    labs = out.argmax(dim=1)\n",
    "    acc += torch.mean((labs==labels).type(torch.float32))\n",
    "    i+=1\n",
    "    if i>iterations: break\n",
    "        \n",
    "print(f\"Final accuracy: {acc.item()/i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Điểm chính\n",
    "\n",
    "Trong bài học này, chúng ta đã thấy việc sử dụng mô hình ngôn ngữ đã được huấn luyện sẵn từ thư viện **transformers** và điều chỉnh nó cho nhiệm vụ phân loại văn bản của chúng ta dễ dàng như thế nào. Tương tự, các mô hình BERT có thể được sử dụng cho các nhiệm vụ như trích xuất thực thể, trả lời câu hỏi và các tác vụ NLP khác.\n",
    "\n",
    "Các mô hình Transformer hiện đang đại diện cho công nghệ tiên tiến nhất trong lĩnh vực NLP, và trong hầu hết các trường hợp, đây nên là giải pháp đầu tiên bạn thử nghiệm khi triển khai các giải pháp NLP tùy chỉnh. Tuy nhiên, việc hiểu các nguyên tắc cơ bản của mạng nơ-ron hồi tiếp (recurrent neural networks) được thảo luận trong bài học này là cực kỳ quan trọng nếu bạn muốn xây dựng các mô hình nơ-ron tiên tiến.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Tuyên bố miễn trừ trách nhiệm**:  \nTài liệu này đã được dịch bằng dịch vụ dịch thuật AI [Co-op Translator](https://github.com/Azure/co-op-translator). Mặc dù chúng tôi cố gắng đảm bảo độ chính xác, xin lưu ý rằng các bản dịch tự động có thể chứa lỗi hoặc không chính xác. Tài liệu gốc bằng ngôn ngữ bản địa nên được coi là nguồn thông tin chính thức. Đối với các thông tin quan trọng, nên sử dụng dịch vụ dịch thuật chuyên nghiệp từ con người. Chúng tôi không chịu trách nhiệm cho bất kỳ sự hiểu lầm hoặc diễn giải sai nào phát sinh từ việc sử dụng bản dịch này.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37_pytorch",
   "language": "python",
   "name": "conda-env-py37_pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "coopTranslator": {
   "original_hash": "753865967678a92dbce7d7efbd36d980",
   "translation_date": "2025-08-29T15:55:02+00:00",
   "source_file": "lessons/5-NLP/18-Transformers/TransformersPyTorch.ipynb",
   "language_code": "vi"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}