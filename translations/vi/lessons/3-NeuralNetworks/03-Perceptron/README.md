<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "0c37770bba4fff3c71dc00eb261ee61b",
  "translation_date": "2025-08-29T12:35:32+00:00",
  "source_file": "lessons/3-NeuralNetworks/03-Perceptron/README.md",
  "language_code": "vi"
}
-->
# Giá»›i thiá»‡u vá» Máº¡ng NÆ¡-ron: Perceptron

## [CÃ¢u há»i trÆ°á»›c bÃ i giáº£ng](https://ff-quizzes.netlify.app/en/ai/quiz/5)

Má»™t trong nhá»¯ng ná»— lá»±c Ä‘áº§u tiÃªn Ä‘á»ƒ triá»ƒn khai má»™t thá»© gÃ¬ Ä‘Ã³ tÆ°Æ¡ng tá»± nhÆ° máº¡ng nÆ¡-ron hiá»‡n Ä‘áº¡i Ä‘Æ°á»£c thá»±c hiá»‡n bá»Ÿi Frank Rosenblatt tá»« PhÃ²ng thÃ­ nghiá»‡m HÃ ng khÃ´ng Cornell vÃ o nÄƒm 1957. ÄÃ¢y lÃ  má»™t triá»ƒn khai pháº§n cá»©ng cÃ³ tÃªn "Mark-1", Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ nháº­n diá»‡n cÃ¡c hÃ¬nh há»c cÆ¡ báº£n nhÆ° tam giÃ¡c, hÃ¬nh vuÃ´ng vÃ  hÃ¬nh trÃ²n.

|      |      |
|--------------|-----------|
|<img src='images/Rosenblatt-wikipedia.jpg' alt='Frank Rosenblatt'/> | <img src='images/Mark_I_perceptron_wikipedia.jpg' alt='The Mark 1 Perceptron' />|

> HÃ¬nh áº£nh [tá»« Wikipedia](https://en.wikipedia.org/wiki/Perceptron)

HÃ¬nh áº£nh Ä‘áº§u vÃ o Ä‘Æ°á»£c biá»ƒu diá»…n báº±ng má»™t máº£ng 20x20 táº¿ bÃ o quang, vÃ¬ váº­y máº¡ng nÆ¡-ron cÃ³ 400 Ä‘áº§u vÃ o vÃ  má»™t Ä‘áº§u ra nhá»‹ phÃ¢n. Má»™t máº¡ng Ä‘Æ¡n giáº£n chá»©a má»™t nÆ¡-ron, cÃ²n Ä‘Æ°á»£c gá»i lÃ  **Ä‘Æ¡n vá»‹ logic ngÆ°á»¡ng**. CÃ¡c trá»ng sá»‘ cá»§a máº¡ng nÆ¡-ron hoáº¡t Ä‘á»™ng nhÆ° cÃ¡c biáº¿n trá»Ÿ cáº§n Ä‘Æ°á»£c Ä‘iá»u chá»‰nh thá»§ cÃ´ng trong giai Ä‘oáº¡n huáº¥n luyá»‡n.

> âœ… Biáº¿n trá»Ÿ lÃ  má»™t thiáº¿t bá»‹ cho phÃ©p ngÆ°á»i dÃ¹ng Ä‘iá»u chá»‰nh Ä‘iá»‡n trá»Ÿ cá»§a má»™t máº¡ch.

> Tá» New York Times Ä‘Ã£ viáº¿t vá» perceptron vÃ o thá»i Ä‘iá»ƒm Ä‘Ã³: *phÃ´i thai cá»§a má»™t mÃ¡y tÃ­nh Ä‘iá»‡n tá»­ mÃ  [Háº£i quÃ¢n] ká»³ vá»ng sáº½ cÃ³ thá»ƒ Ä‘i láº¡i, nÃ³i chuyá»‡n, nhÃ¬n, viáº¿t, tá»± tÃ¡i táº¡o vÃ  nháº­n thá»©c Ä‘Æ°á»£c sá»± tá»“n táº¡i cá»§a chÃ­nh nÃ³.*

## MÃ´ hÃ¬nh Perceptron

Giáº£ sá»­ chÃºng ta cÃ³ N Ä‘áº·c trÆ°ng trong mÃ´ hÃ¬nh cá»§a mÃ¬nh, khi Ä‘Ã³ vector Ä‘áº§u vÃ o sáº½ lÃ  má»™t vector kÃ­ch thÆ°á»›c N. Perceptron lÃ  má»™t mÃ´ hÃ¬nh **phÃ¢n loáº¡i nhá»‹ phÃ¢n**, tá»©c lÃ  nÃ³ cÃ³ thá»ƒ phÃ¢n biá»‡t giá»¯a hai lá»›p dá»¯ liá»‡u Ä‘áº§u vÃ o. ChÃºng ta sáº½ giáº£ Ä‘á»‹nh ráº±ng Ä‘á»‘i vá»›i má»—i vector Ä‘áº§u vÃ o x, Ä‘áº§u ra cá»§a perceptron sáº½ lÃ  +1 hoáº·c -1, tÃ¹y thuá»™c vÃ o lá»›p. Äáº§u ra Ä‘Æ°á»£c tÃ­nh báº±ng cÃ´ng thá»©c:

y(x) = f(w<sup>T</sup>x)

trong Ä‘Ã³ f lÃ  má»™t hÃ m kÃ­ch hoáº¡t dáº¡ng báº­c.

<!-- img src="http://www.sciweavers.org/tex2img.php?eq=f%28x%29%20%3D%20%5Cbegin%7Bcases%7D%0A%20%20%20%20%20%20%20%20%20%2B1%20%26%20x%20%5Cgeq%200%20%5C%5C%0A%20%20%20%20%20%20%20%20%20-1%20%26%20x%20%3C%200%0A%20%20%20%20%20%20%20%5Cend%7Bcases%7D%20%5C%5C%0A&bc=White&fc=Black&im=jpg&fs=12&ff=arev&edit=0" align="center" border="0" alt="f(x) = \begin{cases} +1 & x \geq 0 \\ -1 & x < 0 \end{cases} \\" width="154" height="50" / -->
<img src="images/activation-func.png"/>

## Huáº¥n luyá»‡n Perceptron

Äá»ƒ huáº¥n luyá»‡n má»™t perceptron, chÃºng ta cáº§n tÃ¬m má»™t vector trá»ng sá»‘ w sao cho phÃ¢n loáº¡i Ä‘Ãºng háº§u háº¿t cÃ¡c giÃ¡ trá»‹, tá»©c lÃ  dáº«n Ä‘áº¿n **lá»—i** nhá» nháº¥t. Lá»—i E nÃ y Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a bá»Ÿi **tiÃªu chÃ­ perceptron** nhÆ° sau:

E(w) = -âˆ‘w<sup>T</sup>x<sub>i</sub>t<sub>i</sub>

trong Ä‘Ã³:

* tá»•ng Ä‘Æ°á»£c thá»±c hiá»‡n trÃªn cÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u huáº¥n luyá»‡n i dáº«n Ä‘áº¿n phÃ¢n loáº¡i sai
* x<sub>i</sub> lÃ  dá»¯ liá»‡u Ä‘áº§u vÃ o, vÃ  t<sub>i</sub> lÃ  -1 hoáº·c +1 tÆ°Æ¡ng á»©ng vá»›i vÃ­ dá»¥ Ã¢m vÃ  dÆ°Æ¡ng.

TiÃªu chÃ­ nÃ y Ä‘Æ°á»£c coi lÃ  má»™t hÃ m cá»§a trá»ng sá»‘ w, vÃ  chÃºng ta cáº§n tá»‘i thiá»ƒu hÃ³a nÃ³. ThÃ´ng thÆ°á»ng, má»™t phÆ°Æ¡ng phÃ¡p gá»i lÃ  **gradient descent** Ä‘Æ°á»£c sá»­ dá»¥ng, trong Ä‘Ã³ chÃºng ta báº¯t Ä‘áº§u vá»›i má»™t trá»ng sá»‘ ban Ä‘áº§u w<sup>(0)</sup>, vÃ  sau Ä‘Ã³ táº¡i má»—i bÆ°á»›c cáº­p nháº­t trá»ng sá»‘ theo cÃ´ng thá»©c:

w<sup>(t+1)</sup> = w<sup>(t)</sup> - Î·âˆ‡E(w)

á» Ä‘Ã¢y Î· Ä‘Æ°á»£c gá»i lÃ  **tá»‘c Ä‘á»™ há»c**, vÃ  âˆ‡E(w) biá»ƒu thá»‹ **gradient** cá»§a E. Sau khi tÃ­nh gradient, chÃºng ta cÃ³:

w<sup>(t+1)</sup> = w<sup>(t)</sup> + âˆ‘Î·x<sub>i</sub>t<sub>i</sub>

Thuáº­t toÃ¡n trong Python trÃ´ng nhÆ° sau:

```python
def train(positive_examples, negative_examples, num_iterations = 100, eta = 1):

    weights = [0,0,0] # Initialize weights (almost randomly :)
        
    for i in range(num_iterations):
        pos = random.choice(positive_examples)
        neg = random.choice(negative_examples)

        z = np.dot(pos, weights) # compute perceptron output
        if z < 0: # positive example classified as negative
            weights = weights + eta*weights.shape

        z  = np.dot(neg, weights)
        if z >= 0: # negative example classified as positive
            weights = weights - eta*weights.shape

    return weights
```

## Káº¿t luáº­n

Trong bÃ i há»c nÃ y, báº¡n Ä‘Ã£ tÃ¬m hiá»ƒu vá» perceptron, má»™t mÃ´ hÃ¬nh phÃ¢n loáº¡i nhá»‹ phÃ¢n, vÃ  cÃ¡ch huáº¥n luyá»‡n nÃ³ báº±ng cÃ¡ch sá»­ dá»¥ng vector trá»ng sá»‘.

## ğŸš€ Thá»­ thÃ¡ch

Náº¿u báº¡n muá»‘n tá»± xÃ¢y dá»±ng perceptron cá»§a riÃªng mÃ¬nh, hÃ£y thá»­ [bÃ i thá»±c hÃ nh nÃ y trÃªn Microsoft Learn](https://docs.microsoft.com/en-us/azure/machine-learning/component-reference/two-class-averaged-perceptron?WT.mc_id=academic-77998-cacaste) sá»­ dá»¥ng [Azure ML designer](https://docs.microsoft.com/en-us/azure/machine-learning/concept-designer?WT.mc_id=academic-77998-cacaste).

## [CÃ¢u há»i sau bÃ i giáº£ng](https://ff-quizzes.netlify.app/en/ai/quiz/6)

## Ã”n táº­p & Tá»± há»c

Äá»ƒ xem cÃ¡ch chÃºng ta cÃ³ thá»ƒ sá»­ dá»¥ng perceptron Ä‘á»ƒ giáº£i quyáº¿t má»™t bÃ i toÃ¡n Ä‘Æ¡n giáº£n cÅ©ng nhÆ° cÃ¡c bÃ i toÃ¡n thá»±c táº¿, vÃ  Ä‘á»ƒ tiáº¿p tá»¥c há»c - hÃ£y truy cáº­p notebook [Perceptron](Perceptron.ipynb).

ÄÃ¢y lÃ  má»™t [bÃ i viáº¿t thÃº vá»‹ vá» perceptron](https://towardsdatascience.com/what-is-a-perceptron-basics-of-neural-networks-c4cfea20c590) mÃ  báº¡n cÃ³ thá»ƒ tham kháº£o.

## [BÃ i táº­p](lab/README.md)

Trong bÃ i há»c nÃ y, chÃºng ta Ä‘Ã£ triá»ƒn khai má»™t perceptron cho nhiá»‡m vá»¥ phÃ¢n loáº¡i nhá»‹ phÃ¢n, vÃ  chÃºng ta Ä‘Ã£ sá»­ dá»¥ng nÃ³ Ä‘á»ƒ phÃ¢n loáº¡i giá»¯a hai chá»¯ sá»‘ viáº¿t tay. Trong bÃ i thá»±c hÃ nh nÃ y, báº¡n Ä‘Æ°á»£c yÃªu cáº§u giáº£i quyáº¿t hoÃ n toÃ n bÃ i toÃ¡n phÃ¢n loáº¡i chá»¯ sá»‘, tá»©c lÃ  xÃ¡c Ä‘á»‹nh chá»¯ sá»‘ nÃ o cÃ³ kháº£ nÄƒng tÆ°Æ¡ng á»©ng nháº¥t vá»›i má»™t hÃ¬nh áº£nh Ä‘Ã£ cho.

* [HÆ°á»›ng dáº«n](lab/README.md)
* [Notebook](lab/PerceptronMultiClass.ipynb)

---

**TuyÃªn bá»‘ miá»…n trá»« trÃ¡ch nhiá»‡m**:  
TÃ i liá»‡u nÃ y Ä‘Ã£ Ä‘Æ°á»£c dá»‹ch báº±ng dá»‹ch vá»¥ dá»‹ch thuáº­t AI [Co-op Translator](https://github.com/Azure/co-op-translator). Máº·c dÃ¹ chÃºng tÃ´i cá»‘ gáº¯ng Ä‘áº£m báº£o Ä‘á»™ chÃ­nh xÃ¡c, xin lÆ°u Ã½ ráº±ng cÃ¡c báº£n dá»‹ch tá»± Ä‘á»™ng cÃ³ thá»ƒ chá»©a lá»—i hoáº·c khÃ´ng chÃ­nh xÃ¡c. TÃ i liá»‡u gá»‘c báº±ng ngÃ´n ngá»¯ báº£n Ä‘á»‹a nÃªn Ä‘Æ°á»£c coi lÃ  nguá»“n thÃ´ng tin chÃ­nh thá»©c. Äá»‘i vá»›i cÃ¡c thÃ´ng tin quan trá»ng, khuyáº¿n nghá»‹ sá»­ dá»¥ng dá»‹ch vá»¥ dá»‹ch thuáº­t chuyÃªn nghiá»‡p bá»Ÿi con ngÆ°á»i. ChÃºng tÃ´i khÃ´ng chá»‹u trÃ¡ch nhiá»‡m cho báº¥t ká»³ sá»± hiá»ƒu láº§m hoáº·c diá»…n giáº£i sai nÃ o phÃ¡t sinh tá»« viá»‡c sá»­ dá»¥ng báº£n dá»‹ch nÃ y.