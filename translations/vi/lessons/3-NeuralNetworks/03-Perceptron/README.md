# Giá»›i thiá»‡u vá» Máº¡ng NÆ¡-ron: Perceptron

## [CÃ¢u há»i trÆ°á»›c bÃ i giáº£ng](https://ff-quizzes.netlify.app/en/ai/quiz/5)

Má»™t trong nhá»¯ng ná»— lá»±c Ä‘áº§u tiÃªn Ä‘á»ƒ triá»ƒn khai má»™t thá»© gÃ¬ Ä‘Ã³ tÆ°Æ¡ng tá»± nhÆ° máº¡ng nÆ¡-ron hiá»‡n Ä‘áº¡i Ä‘Æ°á»£c thá»±c hiá»‡n bá»Ÿi Frank Rosenblatt tá»« Cornell Aeronautical Laboratory vÃ o nÄƒm 1957. ÄÃ¢y lÃ  má»™t thiáº¿t bá»‹ pháº§n cá»©ng cÃ³ tÃªn "Mark-1", Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ nháº­n diá»‡n cÃ¡c hÃ¬nh há»c cÆ¡ báº£n nhÆ° tam giÃ¡c, hÃ¬nh vuÃ´ng vÃ  hÃ¬nh trÃ²n.

|      |      |
|--------------|-----------|
|<img src='../../../../../translated_images/vi/Rosenblatt-wikipedia.294821b285ac796d.webp' alt='Frank Rosenblatt'/> | <img src='../../../../../translated_images/vi/Mark_I_perceptron_wikipedia.1f84eaa2d4b76ec9.webp' alt='The Mark 1 Perceptron' />|

> HÃ¬nh áº£nh [tá»« Wikipedia](https://en.wikipedia.org/wiki/Perceptron)

Má»™t hÃ¬nh áº£nh Ä‘áº§u vÃ o Ä‘Æ°á»£c biá»ƒu diá»…n báº±ng máº£ng 20x20 táº¿ bÃ o quang, vÃ¬ váº­y máº¡ng nÆ¡-ron cÃ³ 400 Ä‘áº§u vÃ o vÃ  má»™t Ä‘áº§u ra nhá»‹ phÃ¢n. Má»™t máº¡ng Ä‘Æ¡n giáº£n chá»‰ chá»©a má»™t nÆ¡-ron, cÃ²n Ä‘Æ°á»£c gá»i lÃ  **Ä‘Æ¡n vá»‹ logic ngÆ°á»¡ng**. CÃ¡c trá»ng sá»‘ cá»§a máº¡ng nÆ¡-ron hoáº¡t Ä‘á»™ng nhÆ° cÃ¡c chiáº¿t Ã¡p, cáº§n Ä‘Æ°á»£c Ä‘iá»u chá»‰nh thá»§ cÃ´ng trong giai Ä‘oáº¡n huáº¥n luyá»‡n.

> âœ… Chiáº¿t Ã¡p lÃ  má»™t thiáº¿t bá»‹ cho phÃ©p ngÆ°á»i dÃ¹ng Ä‘iá»u chá»‰nh Ä‘iá»‡n trá»Ÿ cá»§a má»™t máº¡ch.

> Tá» New York Times Ä‘Ã£ viáº¿t vá» perceptron vÃ o thá»i Ä‘iá»ƒm Ä‘Ã³: *phÃ´i thai cá»§a má»™t mÃ¡y tÃ­nh Ä‘iá»‡n tá»­ mÃ  [Háº£i quÃ¢n] ká»³ vá»ng sáº½ cÃ³ thá»ƒ Ä‘i, nÃ³i, nhÃ¬n, viáº¿t, tá»± tÃ¡i táº¡o vÃ  nháº­n thá»©c Ä‘Æ°á»£c sá»± tá»“n táº¡i cá»§a chÃ­nh nÃ³.*

## MÃ´ hÃ¬nh Perceptron

Giáº£ sá»­ chÃºng ta cÃ³ N Ä‘áº·c trÆ°ng trong mÃ´ hÃ¬nh, khi Ä‘Ã³ vector Ä‘áº§u vÃ o sáº½ lÃ  má»™t vector kÃ­ch thÆ°á»›c N. Perceptron lÃ  má»™t mÃ´ hÃ¬nh **phÃ¢n loáº¡i nhá»‹ phÃ¢n**, tá»©c lÃ  nÃ³ cÃ³ thá»ƒ phÃ¢n biá»‡t giá»¯a hai lá»›p dá»¯ liá»‡u Ä‘áº§u vÃ o. ChÃºng ta sáº½ giáº£ Ä‘á»‹nh ráº±ng vá»›i má»—i vector Ä‘áº§u vÃ o x, Ä‘áº§u ra cá»§a perceptron sáº½ lÃ  +1 hoáº·c -1, tÃ¹y thuá»™c vÃ o lá»›p. Äáº§u ra Ä‘Æ°á»£c tÃ­nh báº±ng cÃ´ng thá»©c:

y(x) = f(w<sup>T</sup>x)

trong Ä‘Ã³ f lÃ  hÃ m kÃ­ch hoáº¡t dáº¡ng bÆ°á»›c.

<!-- img src="http://www.sciweavers.org/tex2img.php?eq=f%28x%29%20%3D%20%5Cbegin%7Bcases%7D%0A%20%20%20%20%20%20%20%20%20%2B1%20%26%20x%20%5Cgeq%200%20%5C%5C%0A%20%20%20%20%20%20%20%20%20-1%20%26%20x%20%3C%200%0A%20%20%20%20%20%20%20%5Cend%7Bcases%7D%20%5C%5C%0A&bc=White&fc=Black&im=jpg&fs=12&ff=arev&edit=0" align="center" border="0" alt="f(x) = \begin{cases} +1 & x \geq 0 \\ -1 & x < 0 \end{cases} \\" width="154" height="50" / -->
<img src="../../../../../translated_images/vi/activation-func.b4924007c7ce7764.webp"/>

## Huáº¥n luyá»‡n Perceptron

Äá»ƒ huáº¥n luyá»‡n perceptron, chÃºng ta cáº§n tÃ¬m má»™t vector trá»ng sá»‘ w sao cho phÃ¢n loáº¡i Ä‘Ãºng háº§u háº¿t cÃ¡c giÃ¡ trá»‹, tá»©c lÃ  dáº«n Ä‘áº¿n **lá»—i** nhá» nháº¥t. Lá»—i E nÃ y Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a bá»Ÿi **tiÃªu chÃ­ perceptron** theo cÃ¡ch sau:

E(w) = -&sum;w<sup>T</sup>x<sub>i</sub>t<sub>i</sub>

trong Ä‘Ã³:

* tá»•ng Ä‘Æ°á»£c thá»±c hiá»‡n trÃªn cÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u huáº¥n luyá»‡n i dáº«n Ä‘áº¿n phÃ¢n loáº¡i sai
* x<sub>i</sub> lÃ  dá»¯ liá»‡u Ä‘áº§u vÃ o, vÃ  t<sub>i</sub> lÃ  -1 hoáº·c +1 tÆ°Æ¡ng á»©ng vá»›i vÃ­ dá»¥ Ã¢m vÃ  dÆ°Æ¡ng.

TiÃªu chÃ­ nÃ y Ä‘Æ°á»£c coi lÃ  má»™t hÃ m cá»§a trá»ng sá»‘ w, vÃ  chÃºng ta cáº§n tá»‘i Æ°u hÃ³a nÃ³. ThÆ°á»ng thÃ¬ má»™t phÆ°Æ¡ng phÃ¡p gá»i lÃ  **gradient descent** Ä‘Æ°á»£c sá»­ dá»¥ng, trong Ä‘Ã³ chÃºng ta báº¯t Ä‘áº§u vá»›i má»™t trá»ng sá»‘ ban Ä‘áº§u w<sup>(0)</sup>, vÃ  sau Ä‘Ã³ táº¡i má»—i bÆ°á»›c cáº­p nháº­t trá»ng sá»‘ theo cÃ´ng thá»©c:

w<sup>(t+1)</sup> = w<sup>(t)</sup> - &eta;&nabla;E(w)

á» Ä‘Ã¢y &eta; lÃ  **tá»‘c Ä‘á»™ há»c**, vÃ  &nabla;E(w) biá»ƒu thá»‹ **gradient** cá»§a E. Sau khi tÃ­nh toÃ¡n gradient, chÃºng ta cÃ³:

w<sup>(t+1)</sup> = w<sup>(t)</sup> + &sum;&eta;x<sub>i</sub>t<sub>i</sub>

Thuáº­t toÃ¡n trong Python trÃ´ng nhÆ° sau:

```python
def train(positive_examples, negative_examples, num_iterations = 100, eta = 1):

    weights = [0,0,0] # Initialize weights (almost randomly :)
        
    for i in range(num_iterations):
        pos = random.choice(positive_examples)
        neg = random.choice(negative_examples)

        z = np.dot(pos, weights) # compute perceptron output
        if z < 0: # positive example classified as negative
            weights = weights + eta*weights.shape

        z  = np.dot(neg, weights)
        if z >= 0: # negative example classified as positive
            weights = weights - eta*weights.shape

    return weights
```

## Káº¿t luáº­n

Trong bÃ i há»c nÃ y, báº¡n Ä‘Ã£ tÃ¬m hiá»ƒu vá» perceptron, má»™t mÃ´ hÃ¬nh phÃ¢n loáº¡i nhá»‹ phÃ¢n, vÃ  cÃ¡ch huáº¥n luyá»‡n nÃ³ báº±ng cÃ¡ch sá»­ dá»¥ng vector trá»ng sá»‘.

## ğŸš€ Thá»­ thÃ¡ch

Náº¿u báº¡n muá»‘n thá»­ xÃ¢y dá»±ng perceptron cá»§a riÃªng mÃ¬nh, hÃ£y thá»­ [bÃ i thá»±c hÃ nh nÃ y trÃªn Microsoft Learn](https://docs.microsoft.com/en-us/azure/machine-learning/component-reference/two-class-averaged-perceptron?WT.mc_id=academic-77998-cacaste) sá»­ dá»¥ng [Azure ML designer](https://docs.microsoft.com/en-us/azure/machine-learning/concept-designer?WT.mc_id=academic-77998-cacaste).

## [CÃ¢u há»i sau bÃ i giáº£ng](https://ff-quizzes.netlify.app/en/ai/quiz/6)

## Ã”n táº­p & Tá»± há»c

Äá»ƒ xem cÃ¡ch chÃºng ta cÃ³ thá»ƒ sá»­ dá»¥ng perceptron Ä‘á»ƒ giáº£i quyáº¿t má»™t bÃ i toÃ¡n Ä‘Æ¡n giáº£n cÅ©ng nhÆ° cÃ¡c váº¥n Ä‘á» thá»±c táº¿, vÃ  Ä‘á»ƒ tiáº¿p tá»¥c há»c - hÃ£y truy cáº­p notebook [Perceptron](Perceptron.ipynb).

ÄÃ¢y lÃ  má»™t [bÃ i viáº¿t thÃº vá»‹ vá» perceptron](https://towardsdatascience.com/what-is-a-perceptron-basics-of-neural-networks-c4cfea20c590).

## [BÃ i táº­p](lab/README.md)

Trong bÃ i há»c nÃ y, chÃºng ta Ä‘Ã£ triá»ƒn khai má»™t perceptron cho nhiá»‡m vá»¥ phÃ¢n loáº¡i nhá»‹ phÃ¢n, vÃ  Ä‘Ã£ sá»­ dá»¥ng nÃ³ Ä‘á»ƒ phÃ¢n loáº¡i giá»¯a hai chá»¯ sá»‘ viáº¿t tay. Trong bÃ i thá»±c hÃ nh nÃ y, báº¡n Ä‘Æ°á»£c yÃªu cáº§u giáº£i quyáº¿t váº¥n Ä‘á» phÃ¢n loáº¡i chá»¯ sá»‘ hoÃ n toÃ n, tá»©c lÃ  xÃ¡c Ä‘á»‹nh chá»¯ sá»‘ nÃ o cÃ³ kháº£ nÄƒng tÆ°Æ¡ng á»©ng vá»›i má»™t hÃ¬nh áº£nh cho trÆ°á»›c.

* [HÆ°á»›ng dáº«n](lab/README.md)
* [Notebook](lab/PerceptronMultiClass.ipynb)

---

