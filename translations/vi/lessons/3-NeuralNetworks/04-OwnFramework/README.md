# Giá»›i thiá»‡u vá» Máº¡ng Neural. Perceptron Äa Táº§ng

Trong pháº§n trÆ°á»›c, báº¡n Ä‘Ã£ tÃ¬m hiá»ƒu vá» mÃ´ hÃ¬nh máº¡ng neural Ä‘Æ¡n giáº£n nháº¥t - perceptron má»™t táº§ng, má»™t mÃ´ hÃ¬nh phÃ¢n loáº¡i hai lá»›p tuyáº¿n tÃ­nh.

Trong pháº§n nÃ y, chÃºng ta sáº½ má»Ÿ rá»™ng mÃ´ hÃ¬nh nÃ y thÃ nh má»™t khung linh hoáº¡t hÆ¡n, cho phÃ©p chÃºng ta:

* thá»±c hiá»‡n **phÃ¢n loáº¡i Ä‘a lá»›p** ngoÃ i phÃ¢n loáº¡i hai lá»›p
* giáº£i quyáº¿t **bÃ i toÃ¡n há»“i quy** ngoÃ i phÃ¢n loáº¡i
* phÃ¢n tÃ¡ch cÃ¡c lá»›p khÃ´ng thá»ƒ phÃ¢n tÃ¡ch tuyáº¿n tÃ­nh

ChÃºng ta cÅ©ng sáº½ phÃ¡t triá»ƒn má»™t khung mÃ´-Ä‘un riÃªng trong Python, cho phÃ©p xÃ¢y dá»±ng cÃ¡c kiáº¿n trÃºc máº¡ng neural khÃ¡c nhau.

## [CÃ¢u há»i trÆ°á»›c bÃ i giáº£ng](https://ff-quizzes.netlify.app/en/ai/quiz/7)

## Formal hÃ³a Há»c MÃ¡y

HÃ£y báº¯t Ä‘áº§u báº±ng cÃ¡ch formal hÃ³a bÃ i toÃ¡n Há»c MÃ¡y. Giáº£ sá»­ chÃºng ta cÃ³ má»™t táº­p dá»¯ liá»‡u huáº¥n luyá»‡n **X** vá»›i nhÃ£n **Y**, vÃ  cáº§n xÃ¢y dá»±ng má»™t mÃ´ hÃ¬nh *f* Ä‘á»ƒ Ä‘Æ°a ra dá»± Ä‘oÃ¡n chÃ­nh xÃ¡c nháº¥t. Cháº¥t lÆ°á»£ng cá»§a dá»± Ä‘oÃ¡n Ä‘Æ°á»£c Ä‘o lÆ°á»ng báº±ng **HÃ m máº¥t mÃ¡t** &lagran;. CÃ¡c hÃ m máº¥t mÃ¡t thÆ°á»ng Ä‘Æ°á»£c sá»­ dá»¥ng bao gá»“m:

* Äá»‘i vá»›i bÃ i toÃ¡n há»“i quy, khi cáº§n dá»± Ä‘oÃ¡n má»™t con sá»‘, chÃºng ta cÃ³ thá»ƒ sá»­ dá»¥ng **lá»—i tuyá»‡t Ä‘á»‘i** &sum;<sub>i</sub>|f(x<sup>(i)</sup>)-y<sup>(i)</sup>|, hoáº·c **lá»—i bÃ¬nh phÆ°Æ¡ng** &sum;<sub>i</sub>(f(x<sup>(i)</sup>)-y<sup>(i)</sup>)<sup>2</sup>
* Äá»‘i vá»›i phÃ¢n loáº¡i, chÃºng ta sá»­ dá»¥ng **máº¥t mÃ¡t 0-1** (vá» cÆ¡ báº£n giá»‘ng vá»›i **Ä‘á»™ chÃ­nh xÃ¡c** cá»§a mÃ´ hÃ¬nh), hoáº·c **máº¥t mÃ¡t logistic**.

Äá»‘i vá»›i perceptron má»™t táº§ng, hÃ m *f* Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a lÃ  má»™t hÃ m tuyáº¿n tÃ­nh *f(x)=wx+b* (á»Ÿ Ä‘Ã¢y *w* lÃ  ma tráº­n trá»ng sá»‘, *x* lÃ  vector cÃ¡c Ä‘áº·c trÆ°ng Ä‘áº§u vÃ o, vÃ  *b* lÃ  vector Ä‘á»™ lá»‡ch). Äá»‘i vá»›i cÃ¡c kiáº¿n trÃºc máº¡ng neural khÃ¡c nhau, hÃ m nÃ y cÃ³ thá»ƒ cÃ³ dáº¡ng phá»©c táº¡p hÆ¡n.

> Trong trÆ°á»ng há»£p phÃ¢n loáº¡i, thÆ°á»ng mong muá»‘n Ä‘áº§u ra cá»§a máº¡ng lÃ  xÃ¡c suáº¥t cá»§a cÃ¡c lá»›p tÆ°Æ¡ng á»©ng. Äá»ƒ chuyá»ƒn Ä‘á»•i cÃ¡c sá»‘ báº¥t ká»³ thÃ nh xÃ¡c suáº¥t (vÃ­ dá»¥: Ä‘á»ƒ chuáº©n hÃ³a Ä‘áº§u ra), chÃºng ta thÆ°á»ng sá»­ dá»¥ng hÃ m **softmax** &sigma;, vÃ  hÃ m *f* trá»Ÿ thÃ nh *f(x)=&sigma;(wx+b)*

Trong Ä‘á»‹nh nghÄ©a cá»§a *f* á»Ÿ trÃªn, *w* vÃ  *b* Ä‘Æ°á»£c gá»i lÃ  **tham sá»‘** &theta;=âŸ¨*w,b*âŸ©. Vá»›i táº­p dá»¯ liá»‡u âŸ¨**X**,**Y**âŸ©, chÃºng ta cÃ³ thá»ƒ tÃ­nh toÃ¡n lá»—i tá»•ng thá»ƒ trÃªn toÃ n bá»™ táº­p dá»¯ liá»‡u dÆ°á»›i dáº¡ng má»™t hÃ m cá»§a tham sá»‘ &theta;.

> âœ… **Má»¥c tiÃªu cá»§a viá»‡c huáº¥n luyá»‡n máº¡ng neural lÃ  giáº£m thiá»ƒu lá»—i báº±ng cÃ¡ch thay Ä‘á»•i tham sá»‘ &theta;**

## Tá»‘i Æ°u hÃ³a Gradient Descent

CÃ³ má»™t phÆ°Æ¡ng phÃ¡p tá»‘i Æ°u hÃ³a hÃ m ná»•i tiáº¿ng gá»i lÃ  **gradient descent**. Ã tÆ°á»Ÿng lÃ  chÃºng ta cÃ³ thá»ƒ tÃ­nh Ä‘áº¡o hÃ m (trong trÆ°á»ng há»£p Ä‘a chiá»u gá»i lÃ  **gradient**) cá»§a hÃ m máº¥t mÃ¡t Ä‘á»‘i vá»›i cÃ¡c tham sá»‘, vÃ  thay Ä‘á»•i tham sá»‘ sao cho lá»—i giáº£m Ä‘i. Äiá»u nÃ y cÃ³ thá»ƒ Ä‘Æ°á»£c formal hÃ³a nhÆ° sau:

* Khá»Ÿi táº¡o tham sá»‘ báº±ng má»™t sá»‘ giÃ¡ trá»‹ ngáº«u nhiÃªn w<sup>(0)</sup>, b<sup>(0)</sup>
* Láº·p láº¡i bÆ°á»›c sau nhiá»u láº§n:
    - w<sup>(i+1)</sup> = w<sup>(i)</sup>-&eta;&part;&lagran;/&part;w
    - b<sup>(i+1)</sup> = b<sup>(i)</sup>-&eta;&part;&lagran;/&part;b

Trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n, cÃ¡c bÆ°á»›c tá»‘i Æ°u hÃ³a Ä‘Æ°á»£c tÃ­nh toÃ¡n dá»±a trÃªn toÃ n bá»™ táº­p dá»¯ liá»‡u (nhá»› ráº±ng hÃ m máº¥t mÃ¡t Ä‘Æ°á»£c tÃ­nh báº±ng tá»•ng qua táº¥t cáº£ cÃ¡c máº«u huáº¥n luyá»‡n). Tuy nhiÃªn, trong thá»±c táº¿, chÃºng ta láº¥y cÃ¡c pháº§n nhá» cá»§a táº­p dá»¯ liá»‡u gá»i lÃ  **minibatches**, vÃ  tÃ­nh toÃ¡n gradient dá»±a trÃªn má»™t táº­p con cá»§a dá»¯ liá»‡u. VÃ¬ táº­p con Ä‘Æ°á»£c láº¥y ngáº«u nhiÃªn má»—i láº§n, phÆ°Æ¡ng phÃ¡p nÃ y Ä‘Æ°á»£c gá»i lÃ  **stochastic gradient descent** (SGD).

## Perceptron Äa Táº§ng vÃ  Backpropagation

Máº¡ng má»™t táº§ng, nhÆ° chÃºng ta Ä‘Ã£ tháº¥y á»Ÿ trÃªn, cÃ³ kháº£ nÄƒng phÃ¢n loáº¡i cÃ¡c lá»›p cÃ³ thá»ƒ phÃ¢n tÃ¡ch tuyáº¿n tÃ­nh. Äá»ƒ xÃ¢y dá»±ng má»™t mÃ´ hÃ¬nh phong phÃº hÆ¡n, chÃºng ta cÃ³ thá»ƒ káº¿t há»£p nhiá»u táº§ng cá»§a máº¡ng. Vá» máº·t toÃ¡n há»c, Ä‘iá»u nÃ y cÃ³ nghÄ©a lÃ  hÃ m *f* sáº½ cÃ³ dáº¡ng phá»©c táº¡p hÆ¡n vÃ  Ä‘Æ°á»£c tÃ­nh toÃ¡n qua nhiá»u bÆ°á»›c:
* z<sub>1</sub>=w<sub>1</sub>x+b<sub>1</sub>
* z<sub>2</sub>=w<sub>2</sub>&alpha;(z<sub>1</sub>)+b<sub>2</sub>
* f = &sigma;(z<sub>2</sub>)

á» Ä‘Ã¢y, &alpha; lÃ  má»™t **hÃ m kÃ­ch hoáº¡t phi tuyáº¿n**, &sigma; lÃ  hÃ m softmax, vÃ  tham sá»‘ &theta;=<*w<sub>1</sub>,b<sub>1</sub>,w<sub>2</sub>,b<sub>2</sub>*>.

Thuáº­t toÃ¡n gradient descent váº«n giá»¯ nguyÃªn, nhÆ°ng viá»‡c tÃ­nh toÃ¡n gradient sáº½ khÃ³ khÄƒn hÆ¡n. Dá»±a vÃ o quy táº¯c Ä‘áº¡o hÃ m chuá»—i, chÃºng ta cÃ³ thá»ƒ tÃ­nh Ä‘áº¡o hÃ m nhÆ° sau:

* &part;&lagran;/&part;w<sub>2</sub> = (&part;&lagran;/&part;&sigma;)(&part;&sigma;/&part;z<sub>2</sub>)(&part;z<sub>2</sub>/&part;w<sub>2</sub>)
* &part;&lagran;/&part;w<sub>1</sub> = (&part;&lagran;/&part;&sigma;)(&part;&sigma;/&part;z<sub>2</sub>)(&part;z<sub>2</sub>/&part;&alpha;)(&part;&alpha;/&part;z<sub>1</sub>)(&part;z<sub>1</sub>/&part;w<sub>1</sub>)

> âœ… Quy táº¯c Ä‘áº¡o hÃ m chuá»—i Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ tÃ­nh Ä‘áº¡o hÃ m cá»§a hÃ m máº¥t mÃ¡t Ä‘á»‘i vá»›i cÃ¡c tham sá»‘.

LÆ°u Ã½ ráº±ng pháº§n bÃªn trÃ¡i nháº¥t cá»§a táº¥t cáº£ cÃ¡c biá»ƒu thá»©c nÃ y lÃ  giá»‘ng nhau, vÃ  do Ä‘Ã³ chÃºng ta cÃ³ thá»ƒ tÃ­nh toÃ¡n hiá»‡u quáº£ cÃ¡c Ä‘áº¡o hÃ m báº¯t Ä‘áº§u tá»« hÃ m máº¥t mÃ¡t vÃ  Ä‘i "ngÆ°á»£c láº¡i" qua Ä‘á»“ thá»‹ tÃ­nh toÃ¡n. VÃ¬ váº­y, phÆ°Æ¡ng phÃ¡p huáº¥n luyá»‡n perceptron Ä‘a táº§ng Ä‘Æ°á»£c gá»i lÃ  **backpropagation**, hay 'backprop'.

<img alt="compute graph" src="../../../../../translated_images/vi/ComputeGraphGrad.4626252c0de03507.webp"/>

> TODO: trÃ­ch dáº«n hÃ¬nh áº£nh

> âœ… ChÃºng ta sáº½ tÃ¬m hiá»ƒu backprop chi tiáº¿t hÆ¡n trong vÃ­ dá»¥ notebook cá»§a chÃºng ta.  

## Káº¿t luáº­n

Trong bÃ i há»c nÃ y, chÃºng ta Ä‘Ã£ xÃ¢y dá»±ng thÆ° viá»‡n máº¡ng neural cá»§a riÃªng mÃ¬nh vÃ  sá»­ dá»¥ng nÃ³ cho má»™t bÃ i toÃ¡n phÃ¢n loáº¡i hai chiá»u Ä‘Æ¡n giáº£n.

## ğŸš€ Thá»­ thÃ¡ch

Trong notebook Ä‘i kÃ¨m, báº¡n sáº½ triá»ƒn khai khung cá»§a riÃªng mÃ¬nh Ä‘á»ƒ xÃ¢y dá»±ng vÃ  huáº¥n luyá»‡n perceptron Ä‘a táº§ng. Báº¡n sáº½ cÃ³ cÆ¡ há»™i tÃ¬m hiá»ƒu chi tiáº¿t cÃ¡ch cÃ¡c máº¡ng neural hiá»‡n Ä‘áº¡i hoáº¡t Ä‘á»™ng.

HÃ£y tiáº¿p tá»¥c vá»›i notebook [OwnFramework](OwnFramework.ipynb) vÃ  lÃ m viá»‡c qua nÃ³.

## [CÃ¢u há»i sau bÃ i giáº£ng](https://ff-quizzes.netlify.app/en/ai/quiz/8)

## Ã”n táº­p & Tá»± há»c

Backpropagation lÃ  má»™t thuáº­t toÃ¡n phá»• biáº¿n Ä‘Æ°á»£c sá»­ dá»¥ng trong AI vÃ  ML, Ä‘Ã¡ng Ä‘á»ƒ nghiÃªn cá»©u [chi tiáº¿t hÆ¡n](https://wikipedia.org/wiki/Backpropagation)

## [BÃ i táº­p](lab/README.md)

Trong bÃ i thá»±c hÃ nh nÃ y, báº¡n Ä‘Æ°á»£c yÃªu cáº§u sá»­ dá»¥ng khung mÃ  báº¡n Ä‘Ã£ xÃ¢y dá»±ng trong bÃ i há»c nÃ y Ä‘á»ƒ giáº£i bÃ i toÃ¡n phÃ¢n loáº¡i chá»¯ sá»‘ viáº¿t tay MNIST.

* [HÆ°á»›ng dáº«n](lab/README.md)
* [Notebook](lab/MyFW_MNIST.ipynb)

---

