<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "186bf7eeab776b36f557357ea56d4751",
  "translation_date": "2025-08-29T12:34:28+00:00",
  "source_file": "lessons/3-NeuralNetworks/04-OwnFramework/README.md",
  "language_code": "vi"
}
-->
# Giá»›i thiá»‡u vá» Máº¡ng NÆ¡-ron. Perceptron Nhiá»u Lá»›p

Trong pháº§n trÆ°á»›c, báº¡n Ä‘Ã£ tÃ¬m hiá»ƒu vá» mÃ´ hÃ¬nh máº¡ng nÆ¡-ron Ä‘Æ¡n giáº£n nháº¥t - perceptron má»™t lá»›p, má»™t mÃ´ hÃ¬nh phÃ¢n loáº¡i tuyáº¿n tÃ­nh hai lá»›p.

Trong pháº§n nÃ y, chÃºng ta sáº½ má»Ÿ rá»™ng mÃ´ hÃ¬nh nÃ y thÃ nh má»™t khung linh hoáº¡t hÆ¡n, cho phÃ©p chÃºng ta:

* thá»±c hiá»‡n **phÃ¢n loáº¡i Ä‘a lá»›p** ngoÃ i phÃ¢n loáº¡i hai lá»›p  
* giáº£i quyáº¿t cÃ¡c **bÃ i toÃ¡n há»“i quy** ngoÃ i phÃ¢n loáº¡i  
* phÃ¢n tÃ¡ch cÃ¡c lá»›p khÃ´ng thá»ƒ phÃ¢n tÃ¡ch tuyáº¿n tÃ­nh  

ChÃºng ta cÅ©ng sáº½ phÃ¡t triá»ƒn khung mÃ´-Ä‘un cá»§a riÃªng mÃ¬nh báº±ng Python Ä‘á»ƒ xÃ¢y dá»±ng cÃ¡c kiáº¿n trÃºc máº¡ng nÆ¡-ron khÃ¡c nhau.

## [CÃ¢u há»i trÆ°á»›c bÃ i giáº£ng](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/104)

## ChÃ­nh thá»©c hÃ³a Há»c MÃ¡y

HÃ£y báº¯t Ä‘áº§u báº±ng cÃ¡ch chÃ­nh thá»©c hÃ³a bÃ i toÃ¡n Há»c MÃ¡y. Giáº£ sá»­ chÃºng ta cÃ³ má»™t táº­p dá»¯ liá»‡u huáº¥n luyá»‡n **X** vá»›i nhÃ£n **Y**, vÃ  chÃºng ta cáº§n xÃ¢y dá»±ng má»™t mÃ´ hÃ¬nh *f* Ä‘á»ƒ Ä‘Æ°a ra dá»± Ä‘oÃ¡n chÃ­nh xÃ¡c nháº¥t. Cháº¥t lÆ°á»£ng cá»§a dá»± Ä‘oÃ¡n Ä‘Æ°á»£c Ä‘o lÆ°á»ng báº±ng **HÃ m máº¥t mÃ¡t** â„’. CÃ¡c hÃ m máº¥t mÃ¡t sau thÆ°á»ng Ä‘Æ°á»£c sá»­ dá»¥ng:

* Äá»‘i vá»›i bÃ i toÃ¡n há»“i quy, khi cáº§n dá»± Ä‘oÃ¡n má»™t con sá»‘, chÃºng ta cÃ³ thá»ƒ sá»­ dá»¥ng **lá»—i tuyá»‡t Ä‘á»‘i** âˆ‘<sub>i</sub>|f(x<sup>(i)</sup>)-y<sup>(i)</sup>|, hoáº·c **lá»—i bÃ¬nh phÆ°Æ¡ng** âˆ‘<sub>i</sub>(f(x<sup>(i)</sup>)-y<sup>(i)</sup>)<sup>2</sup>  
* Äá»‘i vá»›i phÃ¢n loáº¡i, chÃºng ta sá»­ dá»¥ng **máº¥t mÃ¡t 0-1** (thá»±c cháº¥t lÃ  **Ä‘á»™ chÃ­nh xÃ¡c** cá»§a mÃ´ hÃ¬nh), hoáº·c **máº¥t mÃ¡t logistic**.

Äá»‘i vá»›i perceptron má»™t lá»›p, hÃ m *f* Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a lÃ  má»™t hÃ m tuyáº¿n tÃ­nh *f(x)=wx+b* (á»Ÿ Ä‘Ã¢y *w* lÃ  ma tráº­n trá»ng sá»‘, *x* lÃ  vector cÃ¡c Ä‘áº·c trÆ°ng Ä‘áº§u vÃ o, vÃ  *b* lÃ  vector Ä‘á»™ chá»‡ch). Äá»‘i vá»›i cÃ¡c kiáº¿n trÃºc máº¡ng nÆ¡-ron khÃ¡c nhau, hÃ m nÃ y cÃ³ thá»ƒ cÃ³ dáº¡ng phá»©c táº¡p hÆ¡n.

> Trong trÆ°á»ng há»£p phÃ¢n loáº¡i, thÆ°á»ng mong muá»‘n Ä‘áº§u ra cá»§a máº¡ng lÃ  xÃ¡c suáº¥t cá»§a cÃ¡c lá»›p tÆ°Æ¡ng á»©ng. Äá»ƒ chuyá»ƒn Ä‘á»•i cÃ¡c sá»‘ báº¥t ká»³ thÃ nh xÃ¡c suáº¥t (vÃ­ dá»¥, Ä‘á»ƒ chuáº©n hÃ³a Ä‘áº§u ra), chÃºng ta thÆ°á»ng sá»­ dá»¥ng hÃ m **softmax** Ïƒ, vÃ  hÃ m *f* trá»Ÿ thÃ nh *f(x)=Ïƒ(wx+b)*.

Trong Ä‘á»‹nh nghÄ©a cá»§a *f* á»Ÿ trÃªn, *w* vÃ  *b* Ä‘Æ°á»£c gá»i lÃ  **tham sá»‘** Î¸=âŸ¨*w,b*âŸ©. Vá»›i táº­p dá»¯ liá»‡u âŸ¨**X**,**Y**âŸ©, chÃºng ta cÃ³ thá»ƒ tÃ­nh toÃ¡n lá»—i tá»•ng thá»ƒ trÃªn toÃ n bá»™ táº­p dá»¯ liá»‡u nhÆ° má»™t hÃ m cá»§a cÃ¡c tham sá»‘ Î¸.

> âœ… **Má»¥c tiÃªu cá»§a viá»‡c huáº¥n luyá»‡n máº¡ng nÆ¡-ron lÃ  giáº£m thiá»ƒu lá»—i báº±ng cÃ¡ch thay Ä‘á»•i cÃ¡c tham sá»‘ Î¸**

## Tá»‘i Æ°u hÃ³a Gradient Descent

CÃ³ má»™t phÆ°Æ¡ng phÃ¡p tá»‘i Æ°u hÃ³a hÃ m ná»•i tiáº¿ng gá»i lÃ  **gradient descent**. Ã tÆ°á»Ÿng lÃ  chÃºng ta cÃ³ thá»ƒ tÃ­nh Ä‘áº¡o hÃ m (trong trÆ°á»ng há»£p Ä‘a chiá»u gá»i lÃ  **gradient**) cá»§a hÃ m máº¥t mÃ¡t Ä‘á»‘i vá»›i cÃ¡c tham sá»‘, vÃ  thay Ä‘á»•i cÃ¡c tham sá»‘ sao cho lá»—i giáº£m Ä‘i. Äiá»u nÃ y cÃ³ thá»ƒ Ä‘Æ°á»£c chÃ­nh thá»©c hÃ³a nhÆ° sau:

* Khá»Ÿi táº¡o cÃ¡c tham sá»‘ báº±ng má»™t sá»‘ giÃ¡ trá»‹ ngáº«u nhiÃªn w<sup>(0)</sup>, b<sup>(0)</sup>  
* Láº·p láº¡i bÆ°á»›c sau nhiá»u láº§n:  
    - w<sup>(i+1)</sup> = w<sup>(i)</sup>-Î·âˆ‚â„’/âˆ‚w  
    - b<sup>(i+1)</sup> = b<sup>(i)</sup>-Î·âˆ‚â„’/âˆ‚b  

Trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n, cÃ¡c bÆ°á»›c tá»‘i Æ°u hÃ³a Ä‘Æ°á»£c tÃ­nh toÃ¡n dá»±a trÃªn toÃ n bá»™ táº­p dá»¯ liá»‡u (nhá»› ráº±ng hÃ m máº¥t mÃ¡t Ä‘Æ°á»£c tÃ­nh nhÆ° tá»•ng qua táº¥t cáº£ cÃ¡c máº«u huáº¥n luyá»‡n). Tuy nhiÃªn, trong thá»±c táº¿, chÃºng ta láº¥y cÃ¡c pháº§n nhá» cá»§a táº­p dá»¯ liá»‡u gá»i lÃ  **minibatches**, vÃ  tÃ­nh gradient dá»±a trÃªn má»™t táº­p con dá»¯ liá»‡u. VÃ¬ táº­p con Ä‘Æ°á»£c láº¥y ngáº«u nhiÃªn má»—i láº§n, phÆ°Æ¡ng phÃ¡p nÃ y Ä‘Æ°á»£c gá»i lÃ  **stochastic gradient descent** (SGD).

## Perceptron Nhiá»u Lá»›p vÃ  Backpropagation

Máº¡ng má»™t lá»›p, nhÆ° chÃºng ta Ä‘Ã£ tháº¥y á»Ÿ trÃªn, cÃ³ kháº£ nÄƒng phÃ¢n loáº¡i cÃ¡c lá»›p cÃ³ thá»ƒ phÃ¢n tÃ¡ch tuyáº¿n tÃ­nh. Äá»ƒ xÃ¢y dá»±ng má»™t mÃ´ hÃ¬nh phong phÃº hÆ¡n, chÃºng ta cÃ³ thá»ƒ káº¿t há»£p nhiá»u lá»›p cá»§a máº¡ng. Vá» máº·t toÃ¡n há»c, Ä‘iá»u nÃ y cÃ³ nghÄ©a lÃ  hÃ m *f* sáº½ cÃ³ dáº¡ng phá»©c táº¡p hÆ¡n vÃ  Ä‘Æ°á»£c tÃ­nh toÃ¡n qua nhiá»u bÆ°á»›c:
* z<sub>1</sub>=w<sub>1</sub>x+b<sub>1</sub>  
* z<sub>2</sub>=w<sub>2</sub>Î±(z<sub>1</sub>)+b<sub>2</sub>  
* f = Ïƒ(z<sub>2</sub>)  

á» Ä‘Ã¢y, Î± lÃ  má»™t **hÃ m kÃ­ch hoáº¡t phi tuyáº¿n**, Ïƒ lÃ  hÃ m softmax, vÃ  cÃ¡c tham sá»‘ Î¸=<*w<sub>1</sub>,b<sub>1</sub>,w<sub>2</sub>,b<sub>2</sub>*>.

Thuáº­t toÃ¡n gradient descent váº«n giá»¯ nguyÃªn, nhÆ°ng viá»‡c tÃ­nh toÃ¡n gradient sáº½ phá»©c táº¡p hÆ¡n. Dá»±a trÃªn quy táº¯c Ä‘áº¡o hÃ m chuá»—i, chÃºng ta cÃ³ thá»ƒ tÃ­nh cÃ¡c Ä‘áº¡o hÃ m nhÆ° sau:

* âˆ‚â„’/âˆ‚w<sub>2</sub> = (âˆ‚â„’/âˆ‚Ïƒ)(âˆ‚Ïƒ/âˆ‚z<sub>2</sub>)(âˆ‚z<sub>2</sub>/âˆ‚w<sub>2</sub>)  
* âˆ‚â„’/âˆ‚w<sub>1</sub> = (âˆ‚â„’/âˆ‚Ïƒ)(âˆ‚Ïƒ/âˆ‚z<sub>2</sub>)(âˆ‚z<sub>2</sub>/âˆ‚Î±)(âˆ‚Î±/âˆ‚z<sub>1</sub>)(âˆ‚z<sub>1</sub>/âˆ‚w<sub>1</sub>)  

> âœ… Quy táº¯c Ä‘áº¡o hÃ m chuá»—i Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ tÃ­nh Ä‘áº¡o hÃ m cá»§a hÃ m máº¥t mÃ¡t Ä‘á»‘i vá»›i cÃ¡c tham sá»‘.

LÆ°u Ã½ ráº±ng pháº§n bÃªn trÃ¡i nháº¥t cá»§a táº¥t cáº£ cÃ¡c biá»ƒu thá»©c nÃ y lÃ  giá»‘ng nhau, do Ä‘Ã³ chÃºng ta cÃ³ thá»ƒ tÃ­nh toÃ¡n hiá»‡u quáº£ cÃ¡c Ä‘áº¡o hÃ m báº¯t Ä‘áº§u tá»« hÃ m máº¥t mÃ¡t vÃ  Ä‘i "ngÆ°á»£c láº¡i" qua Ä‘á»“ thá»‹ tÃ­nh toÃ¡n. VÃ¬ váº­y, phÆ°Æ¡ng phÃ¡p huáº¥n luyá»‡n perceptron nhiá»u lá»›p Ä‘Æ°á»£c gá»i lÃ  **backpropagation**, hoáº·c 'backprop'.

<img alt="compute graph" src="images/ComputeGraphGrad.png"/>

> TODO: trÃ­ch dáº«n hÃ¬nh áº£nh

> âœ… ChÃºng ta sáº½ tÃ¬m hiá»ƒu chi tiáº¿t hÆ¡n vá» backprop trong vÃ­ dá»¥ notebook cá»§a chÃºng ta.

## Káº¿t luáº­n

Trong bÃ i há»c nÃ y, chÃºng ta Ä‘Ã£ xÃ¢y dá»±ng thÆ° viá»‡n máº¡ng nÆ¡-ron cá»§a riÃªng mÃ¬nh vÃ  sá»­ dá»¥ng nÃ³ cho má»™t bÃ i toÃ¡n phÃ¢n loáº¡i hai chiá»u Ä‘Æ¡n giáº£n.

## ğŸš€ Thá»­ thÃ¡ch

Trong notebook Ä‘i kÃ¨m, báº¡n sáº½ triá»ƒn khai khung cá»§a riÃªng mÃ¬nh Ä‘á»ƒ xÃ¢y dá»±ng vÃ  huáº¥n luyá»‡n perceptron nhiá»u lá»›p. Báº¡n sáº½ cÃ³ thá»ƒ tháº¥y chi tiáº¿t cÃ¡ch cÃ¡c máº¡ng nÆ¡-ron hiá»‡n Ä‘áº¡i hoáº¡t Ä‘á»™ng.

HÃ£y tiáº¿p tá»¥c vá»›i notebook [OwnFramework](OwnFramework.ipynb) vÃ  lÃ m viá»‡c qua nÃ³.

## [CÃ¢u há»i sau bÃ i giáº£ng](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/204)

## Ã”n táº­p & Tá»± há»c

Backpropagation lÃ  má»™t thuáº­t toÃ¡n phá»• biáº¿n Ä‘Æ°á»£c sá»­ dá»¥ng trong AI vÃ  ML, Ä‘Ã¡ng Ä‘á»ƒ nghiÃªn cá»©u [chi tiáº¿t hÆ¡n](https://wikipedia.org/wiki/Backpropagation).

## [BÃ i táº­p](lab/README.md)

Trong bÃ i thá»±c hÃ nh nÃ y, báº¡n Ä‘Æ°á»£c yÃªu cáº§u sá»­ dá»¥ng khung mÃ  báº¡n Ä‘Ã£ xÃ¢y dá»±ng trong bÃ i há»c nÃ y Ä‘á»ƒ giáº£i bÃ i toÃ¡n phÃ¢n loáº¡i chá»¯ sá»‘ viáº¿t tay MNIST.

* [HÆ°á»›ng dáº«n](lab/README.md)  
* [Notebook](lab/MyFW_MNIST.ipynb)  

---

**TuyÃªn bá»‘ miá»…n trá»« trÃ¡ch nhiá»‡m**:  
TÃ i liá»‡u nÃ y Ä‘Ã£ Ä‘Æ°á»£c dá»‹ch báº±ng dá»‹ch vá»¥ dá»‹ch thuáº­t AI [Co-op Translator](https://github.com/Azure/co-op-translator). Máº·c dÃ¹ chÃºng tÃ´i cá»‘ gáº¯ng Ä‘áº£m báº£o Ä‘á»™ chÃ­nh xÃ¡c, xin lÆ°u Ã½ ráº±ng cÃ¡c báº£n dá»‹ch tá»± Ä‘á»™ng cÃ³ thá»ƒ chá»©a lá»—i hoáº·c khÃ´ng chÃ­nh xÃ¡c. TÃ i liá»‡u gá»‘c báº±ng ngÃ´n ngá»¯ báº£n Ä‘á»‹a nÃªn Ä‘Æ°á»£c coi lÃ  nguá»“n thÃ´ng tin chÃ­nh thá»©c. Äá»‘i vá»›i cÃ¡c thÃ´ng tin quan trá»ng, khuyáº¿n nghá»‹ sá»­ dá»¥ng dá»‹ch vá»¥ dá»‹ch thuáº­t chuyÃªn nghiá»‡p bá»Ÿi con ngÆ°á»i. ChÃºng tÃ´i khÃ´ng chá»‹u trÃ¡ch nhiá»‡m cho báº¥t ká»³ sá»± hiá»ƒu láº§m hoáº·c diá»…n giáº£i sai nÃ o phÃ¡t sinh tá»« viá»‡c sá»­ dá»¥ng báº£n dá»‹ch nÃ y.