# آموزش مدل Skip-Gram

تکلیف آزمایشگاهی از [برنامه درسی هوش مصنوعی برای مبتدیان](https://github.com/microsoft/ai-for-beginners).

## وظیفه

در این آزمایشگاه، از شما خواسته می‌شود که مدل Word2Vec را با استفاده از تکنیک Skip-Gram آموزش دهید. یک شبکه با embedding آموزش دهید تا کلمات همسایه را در یک پنجره Skip-Gram با عرض $N$ توکن پیش‌بینی کند. می‌توانید از [کد این درس](../../../../../../lessons/5-NLP/15-LanguageModeling/CBoW-TF.ipynb) استفاده کرده و آن را کمی تغییر دهید.

## مجموعه داده

می‌توانید از هر کتابی استفاده کنید. تعداد زیادی متن رایگان در [پروژه گوتنبرگ](https://www.gutenberg.org/) پیدا می‌شود. به عنوان مثال، اینجا یک لینک مستقیم به [ماجراهای آلیس در سرزمین عجایب](https://www.gutenberg.org/files/11/11-0.txt) نوشته لوئیس کارول وجود دارد. یا می‌توانید از نمایشنامه‌های شکسپیر استفاده کنید که می‌توانید با استفاده از کد زیر دریافت کنید:

```python
path_to_file = tf.keras.utils.get_file(
   'shakespeare.txt', 
   'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')
text = open(path_to_file, 'rb').read().decode(encoding='utf-8')
```

## کاوش کنید!

اگر زمان دارید و می‌خواهید عمیق‌تر به موضوع بپردازید، چند مورد را بررسی کنید:

* اندازه embedding چگونه بر نتایج تأثیر می‌گذارد؟
* سبک‌های مختلف متن چگونه بر نتیجه تأثیر می‌گذارند؟
* چند کلمه بسیار متفاوت و مترادف‌های آن‌ها را انتخاب کنید، نمایش‌های برداری آن‌ها را به دست آورید، PCA را برای کاهش ابعاد به ۲ اعمال کنید و آن‌ها را در فضای دوبعدی رسم کنید. آیا الگوهایی مشاهده می‌کنید؟

**سلب مسئولیت**:  
این سند با استفاده از سرویس ترجمه هوش مصنوعی [Co-op Translator](https://github.com/Azure/co-op-translator) ترجمه شده است. در حالی که ما تلاش می‌کنیم دقت را حفظ کنیم، لطفاً توجه داشته باشید که ترجمه‌های خودکار ممکن است حاوی خطاها یا نادرستی‌هایی باشند. سند اصلی به زبان اصلی آن باید به عنوان منبع معتبر در نظر گرفته شود. برای اطلاعات حساس، ترجمه حرفه‌ای انسانی توصیه می‌شود. ما هیچ مسئولیتی در قبال سوءتفاهم‌ها یا تفسیرهای نادرست ناشی از استفاده از این ترجمه نداریم.