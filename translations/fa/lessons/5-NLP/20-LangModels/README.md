# مدل‌های زبانی بزرگ از پیش آموزش‌دیده

در تمام وظایف قبلی، ما یک شبکه عصبی را برای انجام یک وظیفه خاص با استفاده از مجموعه داده‌های برچسب‌دار آموزش می‌دادیم. اما با مدل‌های بزرگ ترانسفورمر، مانند BERT، از مدل‌سازی زبان به صورت خودنظارتی استفاده می‌کنیم تا یک مدل زبانی بسازیم که سپس با آموزش‌های خاص دامنه برای وظایف پایین‌دستی تخصصی می‌شود. با این حال، نشان داده شده است که مدل‌های زبانی بزرگ می‌توانند بسیاری از وظایف را بدون هیچ‌گونه آموزش خاص دامنه‌ای حل کنند. خانواده‌ای از مدل‌ها که قادر به انجام این کار هستند، **GPT** نامیده می‌شوند: ترانسفورمر از پیش آموزش‌دیده مولد.

## [آزمون پیش از درس](https://ff-quizzes.netlify.app/en/ai/quiz/39)

## تولید متن و پیچیدگی

ایده اینکه یک شبکه عصبی بتواند وظایف عمومی را بدون آموزش پایین‌دستی انجام دهد، در مقاله [مدل‌های زبانی یادگیرندگان چندوظیفه‌ای بدون نظارت هستند](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) ارائه شده است. ایده اصلی این است که بسیاری از وظایف دیگر را می‌توان با استفاده از **تولید متن** مدل‌سازی کرد، زیرا درک متن اساساً به معنای توانایی تولید آن است. از آنجا که مدل بر روی حجم عظیمی از متنی که دانش انسانی را در بر می‌گیرد آموزش دیده است، درباره طیف گسترده‌ای از موضوعات نیز آگاه می‌شود.

> درک و توانایی تولید متن همچنین مستلزم دانستن چیزی درباره دنیای اطراف ما است. انسان‌ها نیز تا حد زیادی با خواندن یاد می‌گیرند و شبکه GPT در این زمینه مشابه است.

شبکه‌های تولید متن با پیش‌بینی احتمال کلمه بعدی $$P(w_N)$$ کار می‌کنند. با این حال، احتمال غیرشرطی کلمه بعدی برابر با فراوانی آن کلمه در مجموعه متنی است. GPT می‌تواند **احتمال شرطی** کلمه بعدی را با توجه به کلمات قبلی به ما بدهد: $$P(w_N | w_{n-1}, ..., w_0)$$

> می‌توانید درباره احتمالات بیشتر در [برنامه درسی علم داده برای مبتدیان](https://github.com/microsoft/Data-Science-For-Beginners/tree/main/1-Introduction/04-stats-and-probability) ما بخوانید.

کیفیت مدل تولید زبان را می‌توان با استفاده از **پیچیدگی** تعریف کرد. این یک معیار ذاتی است که به ما اجازه می‌دهد کیفیت مدل را بدون هیچ مجموعه داده خاص وظیفه‌ای اندازه‌گیری کنیم. این معیار بر اساس مفهوم *احتمال یک جمله* است - مدل به جمله‌ای که احتمال واقعی بودن آن زیاد است (یعنی مدل از آن **گیج نشده است**) احتمال بالایی اختصاص می‌دهد و به جملاتی که کمتر منطقی هستند (مثلاً *آیا می‌تواند چه چیزی انجام دهد؟*) احتمال پایینی می‌دهد. وقتی جملاتی از مجموعه متنی واقعی به مدل خود بدهیم، انتظار داریم که احتمال بالایی داشته باشند و **پیچیدگی** پایینی داشته باشند. به صورت ریاضی، پیچیدگی به عنوان احتمال معکوس نرمال‌شده مجموعه آزمون تعریف می‌شود:
$$
\mathrm{Perplexity}(W) = \sqrt[N]{1\over P(W_1,...,W_N)}
$$ 

**می‌توانید با استفاده از [ویرایشگر متن مبتنی بر GPT از Hugging Face](https://transformer.huggingface.co/doc/gpt2-large)** با تولید متن آزمایش کنید. در این ویرایشگر، نوشتن متن خود را شروع کنید و با فشار دادن **[TAB]** چند گزینه تکمیل به شما پیشنهاد می‌شود. اگر این گزینه‌ها خیلی کوتاه هستند یا از آن‌ها راضی نیستید، دوباره [TAB] را فشار دهید و گزینه‌های بیشتری، از جمله قطعات طولانی‌تر متن، خواهید داشت.

## GPT یک خانواده است

GPT یک مدل واحد نیست، بلکه مجموعه‌ای از مدل‌ها است که توسط [OpenAI](https://openai.com) توسعه و آموزش داده شده‌اند.

در زیر مدل‌های GPT داریم:

| [GPT-2](https://huggingface.co/docs/transformers/model_doc/gpt2#openai-gpt2) | [GPT-3](https://openai.com/research/language-models-are-few-shot-learners) | [GPT-4](https://openai.com/gpt-4) |
| -- | -- | -- |
| مدل زبانی با حداکثر ۱.۵ میلیارد پارامتر | مدل زبانی با حداکثر ۱۷۵ میلیارد پارامتر | ۱۰۰ تریلیون پارامتر که ورودی‌های تصویری و متنی را می‌پذیرد و خروجی متنی تولید می‌کند |

مدل‌های GPT-3 و GPT-4 به عنوان [یک سرویس شناختی از Microsoft Azure](https://azure.microsoft.com/en-us/services/cognitive-services/openai-service/#overview?WT.mc_id=academic-77998-cacaste) و همچنین به عنوان [API از OpenAI](https://openai.com/api/) در دسترس هستند.

## مهندسی درخواست (Prompt Engineering)

از آنجا که GPT بر روی حجم عظیمی از داده‌ها برای درک زبان و کد آموزش دیده است، در پاسخ به ورودی‌ها (درخواست‌ها) خروجی ارائه می‌دهد. درخواست‌ها ورودی‌ها یا پرسش‌هایی هستند که در آن‌ها دستورالعمل‌هایی به مدل‌ها داده می‌شود تا وظایف بعدی را تکمیل کنند. برای دستیابی به نتیجه مطلوب، باید مؤثرترین درخواست را ایجاد کنید که شامل انتخاب کلمات، قالب‌ها، عبارات یا حتی نمادهای مناسب است. این رویکرد [مهندسی درخواست](https://learn.microsoft.com/en-us/shows/ai-show/the-basics-of-prompt-engineering-with-azure-openai-service?WT.mc_id=academic-77998-bethanycheum) نامیده می‌شود.

[این مستندات](https://learn.microsoft.com/en-us/semantic-kernel/prompt-engineering/?WT.mc_id=academic-77998-bethanycheum) اطلاعات بیشتری درباره مهندسی درخواست به شما ارائه می‌دهد.

## ✍️ دفترچه مثال: [بازی با OpenAI-GPT](GPT-PyTorch.ipynb)

یادگیری خود را در دفترچه‌های زیر ادامه دهید:

* [تولید متن با OpenAI-GPT و Hugging Face Transformers](GPT-PyTorch.ipynb)

## نتیجه‌گیری

مدل‌های زبانی عمومی از پیش آموزش‌دیده جدید نه تنها ساختار زبان را مدل‌سازی می‌کنند، بلکه حجم عظیمی از زبان طبیعی را نیز در بر می‌گیرند. بنابراین، می‌توانند به طور مؤثری برای حل برخی از وظایف پردازش زبان طبیعی در تنظیمات بدون نمونه یا با نمونه کم استفاده شوند.

## [آزمون پس از درس](https://ff-quizzes.netlify.app/en/ai/quiz/40)

---

