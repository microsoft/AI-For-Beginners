<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "58bf4adb210aab53e8f78c8082040e7c",
  "translation_date": "2025-08-24T10:12:39+00:00",
  "source_file": "lessons/5-NLP/16-RNN/README.md",
  "language_code": "pl"
}
-->
# Recurrent Neural Networks

## [Pre-lecture quiz](https://ff-quizzes.netlify.app/en/ai/quiz/31)

W poprzednich sekcjach korzystaliÅ›my z bogatych semantycznych reprezentacji tekstu oraz prostego klasyfikatora liniowego na bazie osadzeÅ„. Taka architektura pozwala uchwyciÄ‡ zagregowane znaczenie sÅ‚Ã³w w zdaniu, ale nie uwzglÄ™dnia **kolejnoÅ›ci** sÅ‚Ã³w, poniewaÅ¼ operacja agregacji na osadzeniach usuwa tÄ™ informacjÄ™ z oryginalnego tekstu. Z tego powodu modele te nie sÄ… w stanie modelowaÄ‡ kolejnoÅ›ci sÅ‚Ã³w, co uniemoÅ¼liwia im rozwiÄ…zywanie bardziej zÅ‚oÅ¼onych lub niejednoznacznych zadaÅ„, takich jak generowanie tekstu czy odpowiadanie na pytania.

Aby uchwyciÄ‡ znaczenie sekwencji tekstu, musimy uÅ¼yÄ‡ innej architektury sieci neuronowej, zwanej **rekurencyjnÄ… sieciÄ… neuronowÄ…** (RNN). W RNN przekazujemy nasze zdanie przez sieÄ‡ symbol po symbolu, a sieÄ‡ generuje pewien **stan**, ktÃ³ry nastÄ™pnie przekazujemy do sieci wraz z kolejnym symbolem.

![RNN](../../../../../lessons/5-NLP/16-RNN/images/rnn.png)

> Obraz autorstwa autora

Dla wejÅ›ciowej sekwencji tokenÃ³w X<sub>0</sub>,...,X<sub>n</sub>, RNN tworzy sekwencjÄ™ blokÃ³w sieci neuronowej i trenuje tÄ™ sekwencjÄ™ end-to-end za pomocÄ… propagacji wstecznej. KaÅ¼dy blok sieci przyjmuje parÄ™ (X<sub>i</sub>,S<sub>i</sub>) jako wejÅ›cie i generuje S<sub>i+1</sub> jako wynik. KoÅ„cowy stan S<sub>n</sub> lub (wyjÅ›cie Y<sub>n</sub>) trafia do klasyfikatora liniowego, aby wygenerowaÄ‡ wynik. Wszystkie bloki sieci majÄ… te same wagi i sÄ… trenowane end-to-end za pomocÄ… jednej propagacji wstecznej.

DziÄ™ki temu, Å¼e wektory stanÃ³w S<sub>0</sub>,...,S<sub>n</sub> sÄ… przekazywane przez sieÄ‡, jest ona w stanie nauczyÄ‡ siÄ™ zaleÅ¼noÅ›ci sekwencyjnych miÄ™dzy sÅ‚owami. Na przykÅ‚ad, gdy sÅ‚owo *not* pojawia siÄ™ gdzieÅ› w sekwencji, sieÄ‡ moÅ¼e nauczyÄ‡ siÄ™ negowaÄ‡ pewne elementy wektora stanu, co prowadzi do negacji.

> âœ… PoniewaÅ¼ wagi wszystkich blokÃ³w RNN na powyÅ¼szym obrazku sÄ… wspÃ³lne, ten sam obrazek moÅ¼na przedstawiÄ‡ jako jeden blok (po prawej) z pÄ™tlÄ… sprzÄ™Å¼enia zwrotnego, ktÃ³ra przekazuje wyjÅ›ciowy stan sieci z powrotem na wejÅ›cie.

## Anatomia komÃ³rki RNN

Przyjrzyjmy siÄ™, jak zorganizowana jest prosta komÃ³rka RNN. Przyjmuje ona poprzedni stan S<sub>i-1</sub> oraz bieÅ¼Ä…cy symbol X<sub>i</sub> jako wejÅ›cia i musi wygenerowaÄ‡ wyjÅ›ciowy stan S<sub>i</sub> (a czasami interesuje nas rÃ³wnieÅ¼ inne wyjÅ›cie Y<sub>i</sub>, jak w przypadku sieci generatywnych).

Prosta komÃ³rka RNN ma wewnÄ…trz dwie macierze wag: jedna przeksztaÅ‚ca symbol wejÅ›ciowy (nazwijmy jÄ… W), a druga przeksztaÅ‚ca stan wejÅ›ciowy (H). W takim przypadku wyjÅ›cie sieci obliczane jest jako Ïƒ(WÃ—X<sub>i</sub>+HÃ—S<sub>i-1</sub>+b), gdzie Ïƒ to funkcja aktywacji, a b to dodatkowe przesuniÄ™cie.

<img alt="Anatomia komÃ³rki RNN" src="images/rnn-anatomy.png" width="50%"/>

> Obraz autorstwa autora

W wielu przypadkach tokeny wejÅ›ciowe sÄ… przekazywane przez warstwÄ™ osadzeÅ„ przed wejÅ›ciem do RNN, aby zmniejszyÄ‡ wymiarowoÅ›Ä‡. W takim przypadku, jeÅ›li wymiar wektorÃ³w wejÅ›ciowych to *emb_size*, a wektor stanu to *hid_size* - rozmiar W wynosi *emb_size*Ã—*hid_size*, a rozmiar H wynosi *hid_size*Ã—*hid_size*.

## Long Short Term Memory (LSTM)

Jednym z gÅ‚Ã³wnych problemÃ³w klasycznych RNN jest tzw. **problem zanikajÄ…cych gradientÃ³w**. PoniewaÅ¼ RNN sÄ… trenowane end-to-end w jednej propagacji wstecznej, majÄ… trudnoÅ›ci z propagowaniem bÅ‚Ä™du do pierwszych warstw sieci, co uniemoÅ¼liwia sieci naukÄ™ relacji miÄ™dzy odlegÅ‚ymi tokenami. Jednym ze sposobÃ³w unikniÄ™cia tego problemu jest wprowadzenie **jawnego zarzÄ…dzania stanem** za pomocÄ… tzw. **bramek**. IstniejÄ… dwie dobrze znane architektury tego typu: **Long Short Term Memory** (LSTM) oraz **Gated Relay Unit** (GRU).

![Obraz przedstawiajÄ…cy przykÅ‚ad komÃ³rki LSTM](../../../../../lessons/5-NLP/16-RNN/images/long-short-term-memory-cell.svg)

> Å¹rÃ³dÅ‚o obrazu TBD

SieÄ‡ LSTM jest zorganizowana w sposÃ³b podobny do RNN, ale istniejÄ… dwa stany, ktÃ³re sÄ… przekazywane z warstwy do warstwy: rzeczywisty stan C oraz ukryty wektor H. W kaÅ¼dej jednostce ukryty wektor H<sub>i</sub> jest konkatenowany z wejÅ›ciem X<sub>i</sub>, a one kontrolujÄ…, co dzieje siÄ™ ze stanem C za pomocÄ… **bramek**. KaÅ¼da bramka to sieÄ‡ neuronowa z aktywacjÄ… sigmoid (wyjÅ›cie w zakresie [0,1]), ktÃ³rÄ… moÅ¼na traktowaÄ‡ jako maskÄ™ bitowÄ… przy mnoÅ¼eniu przez wektor stanu. Na obrazku powyÅ¼ej znajdujÄ… siÄ™ nastÄ™pujÄ…ce bramki (od lewej do prawej):

* **Bramka zapominania** bierze ukryty wektor i okreÅ›la, ktÃ³re komponenty wektora C naleÅ¼y zapomnieÄ‡, a ktÃ³re przekazaÄ‡ dalej.
* **Bramka wejÅ›ciowa** pobiera pewne informacje z wejÅ›cia i ukrytych wektorÃ³w i wprowadza je do stanu.
* **Bramka wyjÅ›ciowa** przeksztaÅ‚ca stan za pomocÄ… warstwy liniowej z aktywacjÄ… *tanh*, a nastÄ™pnie wybiera niektÃ³re z jego komponentÃ³w za pomocÄ… ukrytego wektora H<sub>i</sub>, aby wygenerowaÄ‡ nowy stan C<sub>i+1</sub>.

Komponenty stanu C moÅ¼na traktowaÄ‡ jako pewne flagi, ktÃ³re moÅ¼na wÅ‚Ä…czaÄ‡ i wyÅ‚Ä…czaÄ‡. Na przykÅ‚ad, gdy w sekwencji napotkamy imiÄ™ *Alice*, moÅ¼emy zaÅ‚oÅ¼yÄ‡, Å¼e odnosi siÄ™ ono do postaci kobiecej i podnieÅ›Ä‡ flagÄ™ w stanie, Å¼e mamy rzeczownik Å¼eÅ„ski w zdaniu. Gdy dalej napotkamy frazÄ™ *and Tom*, podniesiemy flagÄ™, Å¼e mamy rzeczownik w liczbie mnogiej. W ten sposÃ³b manipulujÄ…c stanem moÅ¼emy teoretycznie Å›ledziÄ‡ wÅ‚aÅ›ciwoÅ›ci gramatyczne czÄ™Å›ci zdania.

> âœ… DoskonaÅ‚ym ÅºrÃ³dÅ‚em do zrozumienia wewnÄ™trznego dziaÅ‚ania LSTM jest ten Å›wietny artykuÅ‚ [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) autorstwa Christophera Olaha.

## Dwukierunkowe i wielowarstwowe RNN

OmÃ³wiliÅ›my sieci rekurencyjne, ktÃ³re dziaÅ‚ajÄ… w jednym kierunku, od poczÄ…tku sekwencji do koÅ„ca. Wydaje siÄ™ to naturalne, poniewaÅ¼ przypomina sposÃ³b, w jaki czytamy i sÅ‚uchamy mowy. JednakÅ¼e, poniewaÅ¼ w wielu praktycznych przypadkach mamy losowy dostÄ™p do sekwencji wejÅ›ciowej, moÅ¼e mieÄ‡ sens uruchomienie obliczeÅ„ rekurencyjnych w obu kierunkach. Takie sieci nazywane sÄ… **dwukierunkowymi** RNN. W przypadku sieci dwukierunkowej potrzebne bÄ™dÄ… dwa ukryte wektory stanu, po jednym dla kaÅ¼dego kierunku.

SieÄ‡ rekurencyjna, jedno- lub dwukierunkowa, wychwytuje pewne wzorce w sekwencji i moÅ¼e je przechowywaÄ‡ w wektorze stanu lub przekazywaÄ‡ na wyjÅ›cie. Podobnie jak w przypadku sieci konwolucyjnych, moÅ¼emy zbudowaÄ‡ kolejnÄ… warstwÄ™ rekurencyjnÄ… na szczycie pierwszej, aby uchwyciÄ‡ wzorce wyÅ¼szego poziomu i budowaÄ‡ na bazie wzorcÃ³w niskiego poziomu wyodrÄ™bnionych przez pierwszÄ… warstwÄ™. Prowadzi to do pojÄ™cia **wielowarstwowego RNN**, ktÃ³ry skÅ‚ada siÄ™ z dwÃ³ch lub wiÄ™cej sieci rekurencyjnych, gdzie wyjÅ›cie poprzedniej warstwy jest przekazywane do nastÄ™pnej warstwy jako wejÅ›cie.

![Obraz przedstawiajÄ…cy wielowarstwowy LSTM RNN](../../../../../lessons/5-NLP/16-RNN/images/multi-layer-lstm.jpg)

*Obraz z [tego wspaniaÅ‚ego artykuÅ‚u](https://towardsdatascience.com/from-a-lstm-cell-to-a-multilayer-lstm-network-with-pytorch-2899eb5696f3) autorstwa Fernando LÃ³peza*

## âœï¸ Ä†wiczenia: Osadzenia

Kontynuuj naukÄ™ w poniÅ¼szych notatnikach:

* [RNNs with PyTorch](../../../../../lessons/5-NLP/16-RNN/RNNPyTorch.ipynb)
* [RNNs with TensorFlow](../../../../../lessons/5-NLP/16-RNN/RNNTF.ipynb)

## Podsumowanie

W tej jednostce zobaczyliÅ›my, Å¼e RNN mogÄ… byÄ‡ uÅ¼ywane do klasyfikacji sekwencji, ale w rzeczywistoÅ›ci mogÄ… obsÅ‚ugiwaÄ‡ wiele innych zadaÅ„, takich jak generowanie tekstu, tÅ‚umaczenie maszynowe i inne. RozwaÅ¼ymy te zadania w nastÄ™pnej jednostce.

## ğŸš€ Wyzwanie

Przeczytaj literaturÄ™ na temat LSTM i rozwaÅ¼ ich zastosowania:

- [Grid Long Short-Term Memory](https://arxiv.org/pdf/1507.01526v1.pdf)
- [Show, Attend and Tell: Neural Image Caption
Generation with Visual Attention](https://arxiv.org/pdf/1502.03044v2.pdf)

## [Post-lecture quiz](https://ff-quizzes.netlify.app/en/ai/quiz/32)

## PrzeglÄ…d i samodzielna nauka

- [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) autorstwa Christophera Olaha.

## [Zadanie: Notatniki](assignment.md)

**ZastrzeÅ¼enie**:  
Ten dokument zostaÅ‚ przetÅ‚umaczony za pomocÄ… usÅ‚ugi tÅ‚umaczenia AI [Co-op Translator](https://github.com/Azure/co-op-translator). ChociaÅ¼ staramy siÄ™ zapewniÄ‡ dokÅ‚adnoÅ›Ä‡, prosimy mieÄ‡ na uwadze, Å¼e automatyczne tÅ‚umaczenia mogÄ… zawieraÄ‡ bÅ‚Ä™dy lub nieÅ›cisÅ‚oÅ›ci. Oryginalny dokument w jego rodzimym jÄ™zyku powinien byÄ‡ uznawany za wiarygodne ÅºrÃ³dÅ‚o. W przypadku informacji krytycznych zaleca siÄ™ skorzystanie z profesjonalnego tÅ‚umaczenia przez czÅ‚owieka. Nie ponosimy odpowiedzialnoÅ›ci za jakiekolwiek nieporozumienia lub bÅ‚Ä™dne interpretacje wynikajÄ…ce z uÅ¼ycia tego tÅ‚umaczenia.