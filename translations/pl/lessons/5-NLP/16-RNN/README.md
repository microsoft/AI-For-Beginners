# Sieci Neuronowe Rekurencyjne

## [Quiz przed wykÅ‚adem](https://ff-quizzes.netlify.app/en/ai/quiz/31)

W poprzednich sekcjach korzystaliÅ›my z bogatych semantycznych reprezentacji tekstu oraz prostego klasyfikatora liniowego na bazie osadzeÅ„. Taka architektura pozwala uchwyciÄ‡ zagregowane znaczenie sÅ‚Ã³w w zdaniu, ale nie uwzglÄ™dnia **kolejnoÅ›ci** sÅ‚Ã³w, poniewaÅ¼ operacja agregacji na osadzeniach usuwa tÄ™ informacjÄ™ z oryginalnego tekstu. Z tego powodu modele te nie sÄ… w stanie modelowaÄ‡ kolejnoÅ›ci sÅ‚Ã³w, co uniemoÅ¼liwia rozwiÄ…zywanie bardziej zÅ‚oÅ¼onych lub niejednoznacznych zadaÅ„, takich jak generowanie tekstu czy odpowiadanie na pytania.

Aby uchwyciÄ‡ znaczenie sekwencji tekstu, musimy uÅ¼yÄ‡ innej architektury sieci neuronowej, zwanej **sieciÄ… neuronowÄ… rekurencyjnÄ…** (RNN). W RNN przekazujemy nasze zdanie przez sieÄ‡ symbol po symbolu, a sieÄ‡ generuje pewien **stan**, ktÃ³ry nastÄ™pnie przekazujemy z kolejnym symbolem.

![RNN](../../../../../translated_images/pl/rnn.27f5c29c53d727b5.webp)

> Obraz autorstwa autora

Dla wejÅ›ciowej sekwencji tokenÃ³w X<sub>0</sub>,...,X<sub>n</sub>, RNN tworzy sekwencjÄ™ blokÃ³w sieci neuronowej i trenuje tÄ™ sekwencjÄ™ end-to-end za pomocÄ… propagacji wstecznej. KaÅ¼dy blok sieci przyjmuje parÄ™ (X<sub>i</sub>,S<sub>i</sub>) jako wejÅ›cie i generuje S<sub>i+1</sub> jako wynik. KoÅ„cowy stan S<sub>n</sub> (lub wyjÅ›cie Y<sub>n</sub>) trafia do klasyfikatora liniowego, ktÃ³ry generuje wynik. Wszystkie bloki sieci majÄ… te same wagi i sÄ… trenowane end-to-end w jednym przebiegu propagacji wstecznej.

PoniewaÅ¼ wektory stanÃ³w S<sub>0</sub>,...,S<sub>n</sub> sÄ… przekazywane przez sieÄ‡, jest ona w stanie nauczyÄ‡ siÄ™ zaleÅ¼noÅ›ci sekwencyjnych miÄ™dzy sÅ‚owami. Na przykÅ‚ad, gdy w sekwencji pojawia siÄ™ sÅ‚owo *nie*, sieÄ‡ moÅ¼e nauczyÄ‡ siÄ™ negowaÄ‡ pewne elementy wektora stanu, co prowadzi do negacji.

> âœ… PoniewaÅ¼ wagi wszystkich blokÃ³w RNN na powyÅ¼szym obrazku sÄ… wspÃ³lne, ten sam obrazek moÅ¼na przedstawiÄ‡ jako jeden blok (po prawej) z pÄ™tlÄ… sprzÄ™Å¼enia zwrotnego, ktÃ³ra przekazuje wyjÅ›ciowy stan sieci z powrotem na wejÅ›cie.

## Anatomia komÃ³rki RNN

Przyjrzyjmy siÄ™, jak zorganizowana jest prosta komÃ³rka RNN. Przyjmuje ona poprzedni stan S<sub>i-1</sub> oraz bieÅ¼Ä…cy symbol X<sub>i</sub> jako wejÅ›cia i musi wygenerowaÄ‡ wyjÅ›ciowy stan S<sub>i</sub> (a czasami interesuje nas rÃ³wnieÅ¼ inne wyjÅ›cie Y<sub>i</sub>, jak w przypadku sieci generatywnych).

Prosta komÃ³rka RNN ma wewnÄ…trz dwie macierze wag: jedna przeksztaÅ‚ca symbol wejÅ›ciowy (nazwijmy jÄ… W), a druga przeksztaÅ‚ca stan wejÅ›ciowy (H). W takim przypadku wyjÅ›cie sieci obliczane jest jako &sigma;(W&times;X<sub>i</sub>+H&times;S<sub>i-1</sub>+b), gdzie &sigma; to funkcja aktywacji, a b to dodatkowe przesuniÄ™cie.

<img alt="Anatomia komÃ³rki RNN" src="../../../../../translated_images/pl/rnn-anatomy.79ee3f3920b3294b.webp" width="50%"/>

> Obraz autorstwa autora

W wielu przypadkach tokeny wejÅ›ciowe sÄ… przekazywane przez warstwÄ™ osadzeÅ„ przed wejÅ›ciem do RNN, aby zmniejszyÄ‡ wymiarowoÅ›Ä‡. W takim przypadku, jeÅ›li wymiar wektorÃ³w wejÅ›ciowych wynosi *emb_size*, a wektor stanu wynosi *hid_size* - rozmiar W to *emb_size*&times;*hid_size*, a rozmiar H to *hid_size*&times;*hid_size*.

## DÅ‚ugie KrÃ³tkoterminowe PamiÄ™ci (LSTM)

Jednym z gÅ‚Ã³wnych problemÃ³w klasycznych RNN jest tzw. problem **zanikajÄ…cych gradientÃ³w**. PoniewaÅ¼ RNN sÄ… trenowane end-to-end w jednym przebiegu propagacji wstecznej, majÄ… trudnoÅ›ci z propagowaniem bÅ‚Ä™du do pierwszych warstw sieci, co uniemoÅ¼liwia naukÄ™ relacji miÄ™dzy odlegÅ‚ymi tokenami. Jednym ze sposobÃ³w unikniÄ™cia tego problemu jest wprowadzenie **jawnego zarzÄ…dzania stanem** za pomocÄ… tzw. **bramek**. IstniejÄ… dwie dobrze znane architektury tego typu: **DÅ‚ugie KrÃ³tkoterminowe PamiÄ™ci** (LSTM) oraz **Jednostka PrzekaÅºnikowa Bramkowana** (GRU).

![Obraz przedstawiajÄ…cy przykÅ‚ad komÃ³rki LSTM](../../../../../lessons/5-NLP/16-RNN/images/long-short-term-memory-cell.svg)

> Å¹rÃ³dÅ‚o obrazu TBD

SieÄ‡ LSTM jest zorganizowana w sposÃ³b podobny do RNN, ale istniejÄ… dwa stany, ktÃ³re sÄ… przekazywane z warstwy do warstwy: rzeczywisty stan C oraz wektor ukryty H. W kaÅ¼dej jednostce wektor ukryty H<sub>i</sub> jest konkatenowany z wejÅ›ciem X<sub>i</sub>, a nastÄ™pnie kontrolujÄ… one, co dzieje siÄ™ ze stanem C za pomocÄ… **bramek**. KaÅ¼da bramka jest sieciÄ… neuronowÄ… z aktywacjÄ… sigmoid (wyjÅ›cie w zakresie [0,1]), ktÃ³rÄ… moÅ¼na traktowaÄ‡ jako maskÄ™ bitowÄ… przy mnoÅ¼eniu przez wektor stanu. Na powyÅ¼szym obrazku znajdujÄ… siÄ™ nastÄ™pujÄ…ce bramki (od lewej do prawej):

* **Bramka zapominania** przyjmuje wektor ukryty i okreÅ›la, ktÃ³re komponenty wektora C naleÅ¼y zapomnieÄ‡, a ktÃ³re przekazaÄ‡ dalej.
* **Bramka wejÅ›ciowa** pobiera pewne informacje z wektorÃ³w wejÅ›ciowego i ukrytego i wprowadza je do stanu.
* **Bramka wyjÅ›ciowa** przeksztaÅ‚ca stan za pomocÄ… warstwy liniowej z aktywacjÄ… *tanh*, a nastÄ™pnie wybiera niektÃ³re z jego komponentÃ³w za pomocÄ… wektora ukrytego H<sub>i</sub>, aby wygenerowaÄ‡ nowy stan C<sub>i+1</sub>.

Komponenty stanu C moÅ¼na traktowaÄ‡ jako pewne flagi, ktÃ³re moÅ¼na wÅ‚Ä…czaÄ‡ i wyÅ‚Ä…czaÄ‡. Na przykÅ‚ad, gdy w sekwencji napotkamy imiÄ™ *Alice*, moÅ¼emy zaÅ‚oÅ¼yÄ‡, Å¼e odnosi siÄ™ ono do postaci kobiecej i podnieÅ›Ä‡ flagÄ™ w stanie, Å¼e mamy rzeczownik Å¼eÅ„ski w zdaniu. Gdy dalej napotkamy frazÄ™ *i Tom*, podniesiemy flagÄ™, Å¼e mamy rzeczownik w liczbie mnogiej. W ten sposÃ³b manipulujÄ…c stanem moÅ¼emy teoretycznie Å›ledziÄ‡ wÅ‚aÅ›ciwoÅ›ci gramatyczne czÄ™Å›ci zdania.

> âœ… DoskonaÅ‚ym ÅºrÃ³dÅ‚em do zrozumienia wewnÄ™trznej struktury LSTM jest ten Å›wietny artykuÅ‚ [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) autorstwa Christophera Olaha.

## Dwukierunkowe i wielowarstwowe RNN

OmÃ³wiliÅ›my sieci rekurencyjne, ktÃ³re dziaÅ‚ajÄ… w jednym kierunku, od poczÄ…tku sekwencji do koÅ„ca. Wydaje siÄ™ to naturalne, poniewaÅ¼ przypomina sposÃ³b, w jaki czytamy i sÅ‚uchamy mowy. JednakÅ¼e, poniewaÅ¼ w wielu praktycznych przypadkach mamy losowy dostÄ™p do sekwencji wejÅ›ciowej, moÅ¼e mieÄ‡ sens uruchomienie obliczeÅ„ rekurencyjnych w obu kierunkach. Takie sieci nazywane sÄ… **dwukierunkowymi** RNN. W przypadku sieci dwukierunkowej potrzebne bÄ™dÄ… dwa wektory ukryte, po jednym dla kaÅ¼dego kierunku.

SieÄ‡ rekurencyjna, czy to jednokierunkowa, czy dwukierunkowa, wychwytuje pewne wzorce w sekwencji i moÅ¼e je przechowywaÄ‡ w wektorze stanu lub przekazywaÄ‡ na wyjÅ›cie. Podobnie jak w przypadku sieci konwolucyjnych, moÅ¼emy zbudowaÄ‡ kolejnÄ… warstwÄ™ rekurencyjnÄ… na szczycie pierwszej, aby uchwyciÄ‡ wzorce wyÅ¼szego poziomu i budowaÄ‡ na bazie wzorcÃ³w niskiego poziomu wyodrÄ™bnionych przez pierwszÄ… warstwÄ™. Prowadzi to do pojÄ™cia **wielowarstwowego RNN**, ktÃ³ry skÅ‚ada siÄ™ z dwÃ³ch lub wiÄ™cej sieci rekurencyjnych, gdzie wyjÅ›cie poprzedniej warstwy jest przekazywane jako wejÅ›cie do nastÄ™pnej warstwy.

![Obraz przedstawiajÄ…cy wielowarstwowy LSTM RNN](../../../../../translated_images/pl/multi-layer-lstm.dd975e29bb2a59fe.webp)

*Obraz z [tego wspaniaÅ‚ego artykuÅ‚u](https://towardsdatascience.com/from-a-lstm-cell-to-a-multilayer-lstm-network-with-pytorch-2899eb5696f3) autorstwa Fernando LÃ³peza*

## âœï¸ Ä†wiczenia: Osadzenia

Kontynuuj naukÄ™ w poniÅ¼szych notatnikach:

* [RNN z PyTorch](RNNPyTorch.ipynb)
* [RNN z TensorFlow](RNNTF.ipynb)

## Podsumowanie

W tej jednostce zobaczyliÅ›my, Å¼e RNN mogÄ… byÄ‡ uÅ¼ywane do klasyfikacji sekwencji, ale w rzeczywistoÅ›ci mogÄ… obsÅ‚ugiwaÄ‡ wiele innych zadaÅ„, takich jak generowanie tekstu, tÅ‚umaczenie maszynowe i inne. RozwaÅ¼ymy te zadania w nastÄ™pnej jednostce.

## ğŸš€ Wyzwanie

Przeczytaj literaturÄ™ na temat LSTM i rozwaÅ¼ ich zastosowania:

- [Grid Long Short-Term Memory](https://arxiv.org/pdf/1507.01526v1.pdf)
- [Show, Attend and Tell: Neural Image Caption
Generation with Visual Attention](https://arxiv.org/pdf/1502.03044v2.pdf)

## [Quiz po wykÅ‚adzie](https://ff-quizzes.netlify.app/en/ai/quiz/32)

## PrzeglÄ…d i samodzielna nauka

- [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) autorstwa Christophera Olaha.

## [Zadanie: Notatniki](assignment.md)

---

