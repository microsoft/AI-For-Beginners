<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "7e617f0b8de85a43957a853aba09bfeb",
  "translation_date": "2025-08-24T10:18:50+00:00",
  "source_file": "lessons/5-NLP/18-Transformers/README.md",
  "language_code": "pl"
}
-->
# Mechanizmy uwagi i modele Transformer

## [Quiz przed wykadem](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/118)

Jednym z najwa偶niejszych problem贸w w dziedzinie NLP jest **tumaczenie maszynowe**, kluczowe zadanie, kt贸re stanowi podstaw narzdzi takich jak Google Translate. W tej sekcji skupimy si na tumaczeniu maszynowym, a bardziej og贸lnie na ka偶dym zadaniu typu *sequence-to-sequence* (nazywanym r贸wnie偶 **transdukcj zda**).

W przypadku RNN, zadania sequence-to-sequence s realizowane za pomoc dw贸ch sieci rekurencyjnych, gdzie jedna sie, **enkoder**, przeksztaca sekwencj wejciow w stan ukryty, a druga sie, **dekoder**, rozwija ten stan ukryty w przetumaczony wynik. Istnieje kilka problem贸w zwizanych z tym podejciem:

* Kocowy stan sieci enkodera ma trudnoci z zapamitaniem pocztku zdania, co prowadzi do niskiej jakoci modelu dla dugich zda.
* Wszystkie sowa w sekwencji maj taki sam wpyw na wynik. W rzeczywistoci jednak konkretne sowa w sekwencji wejciowej czsto maj wikszy wpyw na sekwencyjne wyniki ni偶 inne.

**Mechanizmy uwagi** umo偶liwiaj wa偶enie kontekstowego wpywu ka偶dego wektora wejciowego na ka偶d prognoz wyjciow RNN. Implementuje si to poprzez tworzenie skr贸t贸w midzy stanami porednimi wejciowego RNN a wyjciowego RNN. W ten spos贸b, generujc symbol wyjciowy y<sub>t</sub>, uwzgldniamy wszystkie stany ukryte wejciowe h<sub>i</sub>, z r贸偶nymi wsp贸czynnikami wag 伪<sub>t,i</sub>.

![Obraz przedstawiajcy model enkoder/dekoder z warstw uwagi addytywnej](../../../../../lessons/5-NLP/18-Transformers/images/encoder-decoder-attention.png)

> Model enkoder-dekoder z mechanizmem uwagi addytywnej w [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf), cytowany z [tego wpisu na blogu](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)

Macierz uwagi {伪<sub>i,j</sub>} reprezentuje stopie, w jakim okrelone sowa wejciowe wpywaj na generowanie danego sowa w sekwencji wyjciowej. Poni偶ej znajduje si przykad takiej macierzy:

![Obraz przedstawiajcy przykadowe dopasowanie znalezione przez RNNsearch-50, zaczerpnite z Bahdanau - arviz.org](../../../../../lessons/5-NLP/18-Transformers/images/bahdanau-fig3.png)

> Rysunek z [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) (Fig.3)

Mechanizmy uwagi s odpowiedzialne za du偶 cz obecnego lub bliskiego obecnemu stanu sztuki w NLP. Dodanie uwagi znacznie zwiksza jednak liczb parametr贸w modelu, co prowadzi do problem贸w ze skalowaniem RNN. Kluczowym ograniczeniem skalowania RNN jest to, 偶e rekurencyjny charakter modeli utrudnia grupowanie i r贸wnolege trenowanie. W RNN ka偶dy element sekwencji musi by przetwarzany w kolejnoci sekwencyjnej, co oznacza, 偶e nie mo偶na go atwo zr贸wnolegli.

![Enkoder Dekoder z Uwag](../../../../../lessons/5-NLP/18-Transformers/images/EncDecAttention.gif)

> Rysunek z [Bloga Google](https://research.googleblog.com/2016/09/a-neural-network-for-machine.html)

Zastosowanie mechanizm贸w uwagi w poczeniu z tym ograniczeniem doprowadzio do powstania obecnych modeli Transformer, takich jak BERT czy Open-GPT3, kt贸re znamy i u偶ywamy dzisiaj.

## Modele Transformer

Jednym z g贸wnych pomys贸w stojcych za modelami Transformer jest unikanie sekwencyjnego charakteru RNN i stworzenie modelu, kt贸ry mo偶na zr贸wnolegli podczas trenowania. Osiga si to poprzez wdro偶enie dw贸ch pomys贸w:

* kodowanie pozycji
* zastosowanie mechanizmu samouwagi do wychwytywania wzorc贸w zamiast RNN (lub CNN) (dlatego artyku wprowadzajcy transformery nosi tytu *[Attention is all you need](https://arxiv.org/abs/1706.03762)*)

### Kodowanie/Osadzanie Pozycji

Pomys kodowania pozycji jest nastpujcy:  
1. W przypadku RNN wzgldna pozycja token贸w jest reprezentowana przez liczb krok贸w, wic nie musi by jawnie reprezentowana.  
2. Jednak po przejciu na uwag musimy zna wzgldne pozycje token贸w w sekwencji.  
3. Aby uzyska kodowanie pozycji, uzupeniamy nasz sekwencj token贸w o sekwencj pozycji token贸w w sekwencji (np. sekwencj liczb 0,1, ...).  
4. Nastpnie mieszamy pozycj tokenu z wektorem osadzania tokenu. Aby przeksztaci pozycj (liczb cakowit) w wektor, mo偶emy u偶y r贸偶nych podej:

* Osadzanie trenowalne, podobne do osadzania token贸w. To podejcie rozwa偶amy tutaj. Stosujemy warstwy osadzania zar贸wno na tokenach, jak i ich pozycjach, uzyskujc wektory osadzania o tych samych wymiarach, kt贸re nastpnie dodajemy do siebie.
* Funkcja kodowania pozycji staa, jak zaproponowano w oryginalnym artykule.

<img src="images/pos-embedding.png" width="50%"/>

> Obraz autorstwa autora

Rezultat, kt贸ry uzyskujemy dziki osadzaniu pozycji, osadza zar贸wno oryginalny token, jak i jego pozycj w sekwencji.

### Samouwaga z Wieloma Gowami

Nastpnie musimy wychwyci pewne wzorce w naszej sekwencji. Aby to zrobi, transformery u偶ywaj mechanizmu **samouwagi**, kt贸ry w zasadzie jest uwag zastosowan do tej samej sekwencji jako wejcie i wyjcie. Zastosowanie samouwagi pozwala nam uwzgldni **kontekst** w zdaniu i zobaczy, kt贸re sowa s ze sob powizane. Na przykad pozwala nam zobaczy, kt贸re sowa s odniesieniami do innych, takich jak *to*, oraz uwzgldni kontekst:

![](../../../../../lessons/5-NLP/18-Transformers/images/CoreferenceResolution.png)

> Obraz z [Bloga Google](https://research.googleblog.com/2017/08/transformer-novel-neural-network.html)

W transformatorach u偶ywamy **uwagi z wieloma gowami**, aby da sieci mo偶liwo wychwytywania r贸偶nych typ贸w zale偶noci, np. relacji midzy sowami dugoterminowych vs. kr贸tkoterminowych, koreferencji vs. czego innego itd.

[Notebook TensorFlow](../../../../../lessons/5-NLP/18-Transformers/TransformersTF.ipynb) zawiera wicej szczeg贸贸w na temat implementacji warstw transformera.

### Uwagi Enkoder-Dekoder

W transformatorach uwaga jest stosowana w dw贸ch miejscach:

* Do wychwytywania wzorc贸w w tekcie wejciowym za pomoc samouwagi
* Do tumaczenia sekwencji - jest to warstwa uwagi midzy enkoderem a dekoderem.

Uwagi enkoder-dekoder s bardzo podobne do mechanizmu uwagi stosowanego w RNN, jak opisano na pocztku tej sekcji. Ten animowany diagram wyjania rol uwagi enkoder-dekoder.

![Animowany GIF pokazujcy, jak przeprowadzane s oceny w modelach transformera.](../../../../../lessons/5-NLP/18-Transformers/images/transformer-animated-explanation.gif)

Poniewa偶 ka偶da pozycja wejciowa jest mapowana niezale偶nie na ka偶d pozycj wyjciow, transformery mog lepiej zr贸wnolegli ni偶 RNN, co umo偶liwia tworzenie znacznie wikszych i bardziej ekspresyjnych modeli jzykowych. Ka偶da gowa uwagi mo偶e by u偶ywana do nauki r贸偶nych relacji midzy sowami, co poprawia zadania przetwarzania jzyka naturalnego.

## BERT

**BERT** (Bidirectional Encoder Representations from Transformers) to bardzo du偶a wielowarstwowa sie transformera z 12 warstwami dla *BERT-base* i 24 dla *BERT-large*. Model jest najpierw wstpnie trenowany na du偶ym korpusie danych tekstowych (Wikipedia + ksi偶ki) za pomoc uczenia niesuperwizowanego (przewidywanie zamaskowanych s贸w w zdaniu). Podczas wstpnego trenowania model przyswaja znaczce poziomy zrozumienia jzyka, kt贸re mo偶na nastpnie wykorzysta z innymi zestawami danych za pomoc dostrajania. Ten proces nazywa si **uczeniem transferowym**.

![obrazek z http://jalammar.github.io/illustrated-bert/](../../../../../lessons/5-NLP/18-Transformers/images/jalammarBERT-language-modeling-masked-lm.png)

> Obraz [藕r贸do](http://jalammar.github.io/illustrated-bert/)

## 锔 wiczenia: Transformery

Kontynuuj nauk w poni偶szych notebookach:

* [Transformery w PyTorch](../../../../../lessons/5-NLP/18-Transformers/TransformersPyTorch.ipynb)
* [Transformery w TensorFlow](../../../../../lessons/5-NLP/18-Transformers/TransformersTF.ipynb)

## Podsumowanie

W tej lekcji dowiedziae si o Transformerach i Mechanizmach Uwagi, kt贸re s niezbdnymi narzdziami w zestawie narzdzi NLP. Istnieje wiele wariacji architektur Transformer, w tym BERT, DistilBERT, BigBird, OpenGPT3 i inne, kt贸re mo偶na dostraja. Pakiet [HuggingFace](https://github.com/huggingface/) zapewnia repozytorium do trenowania wielu z tych architektur zar贸wno w PyTorch, jak i TensorFlow.

##  Wyzwanie

## [Quiz po wykadzie](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/218)

## Przegld i Samodzielna Nauka

* [Wpis na blogu](https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/), wyjaniajcy klasyczny artyku [Attention is all you need](https://arxiv.org/abs/1706.03762) o transformatorach.
* [Seria wpis贸w na blogu](https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452) o transformatorach, wyjaniajca szczeg贸y architektury.

## [Zadanie](assignment.md)

**Zastrze偶enie**:  
Ten dokument zosta przetumaczony za pomoc usugi tumaczenia AI [Co-op Translator](https://github.com/Azure/co-op-translator). Chocia偶 staramy si zapewni dokadno, prosimy mie na uwadze, 偶e automatyczne tumaczenia mog zawiera bdy lub niecisoci. Oryginalny dokument w jego rodzimym jzyku powinien by uznawany za wiarygodne 藕r贸do. W przypadku informacji krytycznych zaleca si skorzystanie z profesjonalnego tumaczenia przez czowieka. Nie ponosimy odpowiedzialnoci za jakiekolwiek nieporozumienia lub bdne interpretacje wynikajce z u偶ycia tego tumaczenia.