<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "e40b47ac3fd48f71304ede1474e66293",
  "translation_date": "2025-08-24T10:13:41+00:00",
  "source_file": "lessons/5-NLP/14-Embeddings/README.md",
  "language_code": "pl"
}
-->
# Osadzenia

## [Quiz przed wykÅ‚adem](https://ff-quizzes.netlify.app/en/ai/quiz/27)

Podczas trenowania klasyfikatorÃ³w opartych na BoW lub TF/IDF operowaliÅ›my na wektorach worka sÅ‚Ã³w o wysokiej wymiarowoÅ›ci o dÅ‚ugoÅ›ci `vocab_size`, a takÅ¼e jawnie konwertowaliÅ›my z wektorÃ³w reprezentacji pozycyjnej o niskiej wymiarowoÅ›ci na rzadkie wektory one-hot. JednakÅ¼e, reprezentacja one-hot nie jest efektywna pod wzglÄ™dem pamiÄ™ci. Dodatkowo, kaÅ¼de sÅ‚owo jest traktowane niezaleÅ¼nie od innych, tzn. wektory zakodowane one-hot nie wyraÅ¼ajÄ… Å¼adnego podobieÅ„stwa semantycznego miÄ™dzy sÅ‚owami.

PomysÅ‚ **osadzenia** polega na reprezentowaniu sÅ‚Ã³w za pomocÄ… gÄ™stych wektorÃ³w o niÅ¼szej wymiarowoÅ›ci, ktÃ³re w pewien sposÃ³b odzwierciedlajÄ… semantyczne znaczenie sÅ‚owa. PÃ³Åºniej omÃ³wimy, jak budowaÄ‡ znaczÄ…ce osadzenia sÅ‚Ã³w, ale na razie pomyÅ›lmy o osadzeniach jako o sposobie na zmniejszenie wymiarowoÅ›ci wektora sÅ‚owa.

Warstwa osadzenia przyjmuje sÅ‚owo jako wejÅ›cie i generuje wektor wyjÅ›ciowy o okreÅ›lonym `embedding_size`. W pewnym sensie jest to bardzo podobne do warstwy `Linear`, ale zamiast przyjmowaÄ‡ wektor zakodowany one-hot, moÅ¼e przyjÄ…Ä‡ numer sÅ‚owa jako wejÅ›cie, co pozwala uniknÄ…Ä‡ tworzenia duÅ¼ych wektorÃ³w zakodowanych one-hot.

UÅ¼ywajÄ…c warstwy osadzenia jako pierwszej warstwy w naszej sieci klasyfikatora, moÅ¼emy przejÅ›Ä‡ od modelu worka sÅ‚Ã³w do modelu **embedding bag**, w ktÃ³rym najpierw konwertujemy kaÅ¼de sÅ‚owo w naszym tekÅ›cie na odpowiadajÄ…ce mu osadzenie, a nastÄ™pnie obliczamy pewnÄ… funkcjÄ™ agregujÄ…cÄ… dla wszystkich tych osadzeÅ„, takÄ… jak `sum`, `average` lub `max`.  

![Obraz przedstawiajÄ…cy klasyfikator osadzeÅ„ dla piÄ™ciu sÅ‚Ã³w w sekwencji.](../../../../../lessons/5-NLP/14-Embeddings/images/embedding-classifier-example.png)

> Obraz autorstwa autora

## âœï¸ Ä†wiczenia: Osadzenia

Kontynuuj naukÄ™ w poniÅ¼szych notatnikach:
* [Osadzenia z PyTorch](../../../../../lessons/5-NLP/14-Embeddings/EmbeddingsPyTorch.ipynb)
* [Osadzenia z TensorFlow](../../../../../lessons/5-NLP/14-Embeddings/EmbeddingsTF.ipynb)

## Semantyczne osadzenia: Word2Vec

ChociaÅ¼ warstwa osadzenia nauczyÅ‚a siÄ™ mapowaÄ‡ sÅ‚owa na reprezentacje wektorowe, to jednak ta reprezentacja niekoniecznie miaÅ‚a wiele wspÃ³lnego z semantykÄ…. ByÅ‚oby dobrze nauczyÄ‡ siÄ™ reprezentacji wektorowej, w ktÃ³rej podobne sÅ‚owa lub synonimy odpowiadajÄ… wektorom bliskim sobie pod wzglÄ™dem pewnej odlegÅ‚oÅ›ci wektorowej (np. odlegÅ‚oÅ›ci euklidesowej).

Aby to osiÄ…gnÄ…Ä‡, musimy wstÄ™pnie wytrenowaÄ‡ nasz model osadzeÅ„ na duÅ¼ym zbiorze tekstÃ³w w okreÅ›lony sposÃ³b. Jednym ze sposobÃ³w trenowania semantycznych osadzeÅ„ jest metoda [Word2Vec](https://en.wikipedia.org/wiki/Word2vec). Opiera siÄ™ ona na dwÃ³ch gÅ‚Ã³wnych architekturach uÅ¼ywanych do tworzenia rozproszonych reprezentacji sÅ‚Ã³w:

 - **CiÄ…gÅ‚y worek sÅ‚Ã³w** (CBoW) â€” w tej architekturze trenujemy model, aby przewidywaÅ‚ sÅ‚owo na podstawie otaczajÄ…cego kontekstu. MajÄ…c n-gram $(W_{-2},W_{-1},W_0,W_1,W_2)$, celem modelu jest przewidzenie $W_0$ na podstawie $(W_{-2},W_{-1},W_1,W_2)$.
 - **CiÄ…gÅ‚y skip-gram** jest odwrotnoÅ›ciÄ… CBoW. Model uÅ¼ywa otaczajÄ…cego okna sÅ‚Ã³w kontekstowych, aby przewidzieÄ‡ bieÅ¼Ä…ce sÅ‚owo.

CBoW jest szybszy, podczas gdy skip-gram jest wolniejszy, ale lepiej radzi sobie z reprezentowaniem rzadkich sÅ‚Ã³w.

![Obraz przedstawiajÄ…cy algorytmy CBoW i Skip-Gram do konwersji sÅ‚Ã³w na wektory.](../../../../../lessons/5-NLP/14-Embeddings/images/example-algorithms-for-converting-words-to-vectors.png)

> Obraz z [tego artykuÅ‚u](https://arxiv.org/pdf/1301.3781.pdf)

WstÄ™pnie wytrenowane osadzenia Word2Vec (jak rÃ³wnieÅ¼ inne podobne modele, takie jak GloVe) mogÄ… byÄ‡ rÃ³wnieÅ¼ uÅ¼ywane zamiast warstwy osadzenia w sieciach neuronowych. Musimy jednak poradziÄ‡ sobie ze sÅ‚ownikami, poniewaÅ¼ sÅ‚ownik uÅ¼ywany do wstÄ™pnego trenowania Word2Vec/GloVe prawdopodobnie rÃ³Å¼ni siÄ™ od sÅ‚ownika w naszym korpusie tekstowym. Zajrzyj do powyÅ¼szych notatnikÃ³w, aby zobaczyÄ‡, jak moÅ¼na rozwiÄ…zaÄ‡ ten problem.

## Kontekstowe osadzenia

Jednym z kluczowych ograniczeÅ„ tradycyjnych wstÄ™pnie wytrenowanych reprezentacji osadzeÅ„, takich jak Word2Vec, jest problem rozrÃ³Å¼niania znaczeÅ„ sÅ‚Ã³w. ChociaÅ¼ wstÄ™pnie wytrenowane osadzenia mogÄ… uchwyciÄ‡ czÄ™Å›Ä‡ znaczenia sÅ‚Ã³w w kontekÅ›cie, kaÅ¼de moÅ¼liwe znaczenie sÅ‚owa jest zakodowane w tym samym osadzeniu. MoÅ¼e to powodowaÄ‡ problemy w modelach koÅ„cowych, poniewaÅ¼ wiele sÅ‚Ã³w, takich jak sÅ‚owo â€playâ€, ma rÃ³Å¼ne znaczenia w zaleÅ¼noÅ›ci od kontekstu, w jakim jest uÅ¼ywane.

Na przykÅ‚ad sÅ‚owo â€playâ€ w tych dwÃ³ch zdaniach ma zupeÅ‚nie inne znaczenie:

- PoszedÅ‚em na **sztukÄ™** do teatru.
- John chce siÄ™ **bawiÄ‡** z przyjaciÃ³Å‚mi.

WstÄ™pnie wytrenowane osadzenia reprezentujÄ… oba te znaczenia sÅ‚owa â€playâ€ w tym samym osadzeniu. Aby przezwyciÄ™Å¼yÄ‡ to ograniczenie, musimy budowaÄ‡ osadzenia oparte na **modelu jÄ™zykowym**, ktÃ³ry jest trenowany na duÅ¼ym korpusie tekstÃ³w i *wie*, jak sÅ‚owa mogÄ… byÄ‡ uÅ¼ywane w rÃ³Å¼nych kontekstach. OmÃ³wienie kontekstowych osadzeÅ„ wykracza poza zakres tego samouczka, ale wrÃ³cimy do nich, gdy bÄ™dziemy omawiaÄ‡ modele jÄ™zykowe w dalszej czÄ™Å›ci kursu.

## Podsumowanie

W tej lekcji dowiedziaÅ‚eÅ› siÄ™, jak budowaÄ‡ i uÅ¼ywaÄ‡ warstw osadzeÅ„ w TensorFlow i PyTorch, aby lepiej odzwierciedlaÄ‡ semantyczne znaczenia sÅ‚Ã³w.

## ğŸš€ Wyzwanie

Word2Vec zostaÅ‚ wykorzystany w ciekawych zastosowaniach, takich jak generowanie tekstÃ³w piosenek i poezji. Zajrzyj do [tego artykuÅ‚u](https://www.politetype.com/blog/word2vec-color-poems), ktÃ³ry opisuje, jak autor uÅ¼yÅ‚ Word2Vec do generowania poezji. Obejrzyj rÃ³wnieÅ¼ [ten film Dana Shiffmanna](https://www.youtube.com/watch?v=LSS_bos_TPI&ab_channel=TheCodingTrain), aby poznaÄ‡ inne wyjaÅ›nienie tej techniki. NastÄ™pnie sprÃ³buj zastosowaÄ‡ te techniki do wÅ‚asnego korpusu tekstowego, byÄ‡ moÅ¼e pochodzÄ…cego z Kaggle.

## [Quiz po wykÅ‚adzie](https://ff-quizzes.netlify.app/en/ai/quiz/28)

## PrzeglÄ…d i samodzielna nauka

Przeczytaj ten artykuÅ‚ o Word2Vec: [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf)

## [Zadanie: Notatniki](assignment.md)

**ZastrzeÅ¼enie**:  
Ten dokument zostaÅ‚ przetÅ‚umaczony za pomocÄ… usÅ‚ugi tÅ‚umaczeniowej AI [Co-op Translator](https://github.com/Azure/co-op-translator). ChociaÅ¼ dokÅ‚adamy wszelkich staraÅ„, aby zapewniÄ‡ poprawnoÅ›Ä‡ tÅ‚umaczenia, prosimy pamiÄ™taÄ‡, Å¼e automatyczne tÅ‚umaczenia mogÄ… zawieraÄ‡ bÅ‚Ä™dy lub nieÅ›cisÅ‚oÅ›ci. Oryginalny dokument w jego rodzimym jÄ™zyku powinien byÄ‡ uznawany za autorytatywne ÅºrÃ³dÅ‚o. W przypadku informacji o kluczowym znaczeniu zaleca siÄ™ skorzystanie z profesjonalnego tÅ‚umaczenia wykonanego przez czÅ‚owieka. Nie ponosimy odpowiedzialnoÅ›ci za jakiekolwiek nieporozumienia lub bÅ‚Ä™dne interpretacje wynikajÄ…ce z uÅ¼ycia tego tÅ‚umaczenia.