# Osadzenia

## [Quiz przed wykÅ‚adem](https://ff-quizzes.netlify.app/en/ai/quiz/27)

Podczas trenowania klasyfikatorÃ³w opartych na BoW lub TF/IDF operowaliÅ›my na wysokowymiarowych wektorach bag-of-words o dÅ‚ugoÅ›ci `vocab_size`, a takÅ¼e jawnie konwertowaliÅ›my niskowymiarowe wektory reprezentacji pozycyjnej na rzadkie wektory one-hot. JednakÅ¼e reprezentacja one-hot nie jest efektywna pod wzglÄ™dem pamiÄ™ci. Dodatkowo, kaÅ¼de sÅ‚owo jest traktowane niezaleÅ¼nie od innych, tj. zakodowane wektory one-hot nie wyraÅ¼ajÄ… Å¼adnego semantycznego podobieÅ„stwa miÄ™dzy sÅ‚owami.

PomysÅ‚ **osadzenia** polega na reprezentowaniu sÅ‚Ã³w za pomocÄ… niskowymiarowych, gÄ™stych wektorÃ³w, ktÃ³re w pewien sposÃ³b odzwierciedlajÄ… semantyczne znaczenie sÅ‚owa. PÃ³Åºniej omÃ³wimy, jak budowaÄ‡ znaczÄ…ce osadzenia sÅ‚Ã³w, ale na razie pomyÅ›lmy o osadzeniach jako o sposobie na zmniejszenie wymiarowoÅ›ci wektora sÅ‚owa.

Warstwa osadzenia przyjmuje sÅ‚owo jako wejÅ›cie i generuje wektor wyjÅ›ciowy o okreÅ›lonym `embedding_size`. W pewnym sensie jest bardzo podobna do warstwy `Linear`, ale zamiast przyjmowaÄ‡ zakodowany wektor one-hot, moÅ¼e przyjmowaÄ‡ numer sÅ‚owa jako wejÅ›cie, co pozwala nam uniknÄ…Ä‡ tworzenia duÅ¼ych wektorÃ³w zakodowanych one-hot.

UÅ¼ywajÄ…c warstwy osadzenia jako pierwszej warstwy w naszej sieci klasyfikatora, moÅ¼emy przejÅ›Ä‡ od modelu bag-of-words do modelu **embedding bag**, gdzie najpierw konwertujemy kaÅ¼de sÅ‚owo w naszym tekÅ›cie na odpowiadajÄ…ce mu osadzenie, a nastÄ™pnie obliczamy pewnÄ… funkcjÄ™ agregujÄ…cÄ… dla wszystkich tych osadzeÅ„, takÄ… jak `sum`, `average` lub `max`.

![Obraz przedstawiajÄ…cy klasyfikator osadzeÅ„ dla piÄ™ciu sÅ‚Ã³w w sekwencji.](../../../../../translated_images/pl/embedding-classifier-example.b77f021a7ee67eee.webp)

> Obraz autorstwa autora

## âœï¸ Ä†wiczenia: Osadzenia

Kontynuuj naukÄ™ w poniÅ¼szych notatnikach:
* [Osadzenia z PyTorch](EmbeddingsPyTorch.ipynb)
* [Osadzenia TensorFlow](EmbeddingsTF.ipynb)

## Semantyczne osadzenia: Word2Vec

ChociaÅ¼ warstwa osadzenia nauczyÅ‚a siÄ™ mapowaÄ‡ sÅ‚owa na reprezentacjÄ™ wektorowÄ…, ta reprezentacja niekoniecznie miaÅ‚a wiele semantycznego znaczenia. ByÅ‚oby dobrze nauczyÄ‡ siÄ™ reprezentacji wektorowej, w ktÃ³rej podobne sÅ‚owa lub synonimy odpowiadajÄ… wektorom bliskim sobie pod wzglÄ™dem pewnej odlegÅ‚oÅ›ci wektorowej (np. odlegÅ‚oÅ›ci euklidesowej).

Aby to osiÄ…gnÄ…Ä‡, musimy wstÄ™pnie wytrenowaÄ‡ nasz model osadzenia na duÅ¼ym zbiorze tekstÃ³w w okreÅ›lony sposÃ³b. Jednym ze sposobÃ³w trenowania semantycznych osadzeÅ„ jest [Word2Vec](https://en.wikipedia.org/wiki/Word2vec). Opiera siÄ™ on na dwÃ³ch gÅ‚Ã³wnych architekturach uÅ¼ywanych do tworzenia rozproszonej reprezentacji sÅ‚Ã³w:

 - **Continuous bag-of-words** (CBoW) â€” w tej architekturze uczymy model przewidywania sÅ‚owa na podstawie otaczajÄ…cego kontekstu. MajÄ…c ngram $(W_{-2},W_{-1},W_0,W_1,W_2)$, celem modelu jest przewidzenie $W_0$ na podstawie $(W_{-2},W_{-1},W_1,W_2)$.
 - **Continuous skip-gram** â€” jest przeciwieÅ„stwem CBoW. Model uÅ¼ywa otaczajÄ…cego okna sÅ‚Ã³w kontekstowych do przewidywania bieÅ¼Ä…cego sÅ‚owa.

CBoW dziaÅ‚a szybciej, podczas gdy skip-gram jest wolniejszy, ale lepiej reprezentuje rzadkie sÅ‚owa.

![Obraz przedstawiajÄ…cy algorytmy CBoW i Skip-Gram do konwersji sÅ‚Ã³w na wektory.](../../../../../translated_images/pl/example-algorithms-for-converting-words-to-vectors.fbe9207a726922f6.webp)

> Obraz z [tego artykuÅ‚u](https://arxiv.org/pdf/1301.3781.pdf)

WstÄ™pnie wytrenowane osadzenia Word2Vec (oraz inne podobne modele, takie jak GloVe) mogÄ… byÄ‡ rÃ³wnieÅ¼ uÅ¼ywane zamiast warstwy osadzenia w sieciach neuronowych. Jednak musimy poradziÄ‡ sobie ze sÅ‚ownikami, poniewaÅ¼ sÅ‚ownik uÅ¼ywany do wstÄ™pnego trenowania Word2Vec/GloVe prawdopodobnie rÃ³Å¼ni siÄ™ od sÅ‚ownika w naszym korpusie tekstowym. Zajrzyj do powyÅ¼szych notatnikÃ³w, aby zobaczyÄ‡, jak moÅ¼na rozwiÄ…zaÄ‡ ten problem.

## Kontekstowe osadzenia

Jednym z kluczowych ograniczeÅ„ tradycyjnych wstÄ™pnie wytrenowanych reprezentacji osadzeÅ„, takich jak Word2Vec, jest problem rozrÃ³Å¼niania znaczeÅ„ sÅ‚Ã³w. ChociaÅ¼ wstÄ™pnie wytrenowane osadzenia mogÄ… uchwyciÄ‡ czÄ™Å›Ä‡ znaczenia sÅ‚Ã³w w kontekÅ›cie, kaÅ¼de moÅ¼liwe znaczenie sÅ‚owa jest kodowane w tym samym osadzeniu. MoÅ¼e to powodowaÄ‡ problemy w modelach downstream, poniewaÅ¼ wiele sÅ‚Ã³w, takich jak sÅ‚owo 'play', ma rÃ³Å¼ne znaczenia w zaleÅ¼noÅ›ci od kontekstu, w ktÃ³rym sÄ… uÅ¼ywane.

Na przykÅ‚ad sÅ‚owo 'play' w tych dwÃ³ch rÃ³Å¼nych zdaniach ma zupeÅ‚nie inne znaczenie:

- PoszedÅ‚em na **sztukÄ™** do teatru.
- John chce siÄ™ **bawiÄ‡** ze swoimi przyjaciÃ³Å‚mi.

WstÄ™pnie wytrenowane osadzenia powyÅ¼ej reprezentujÄ… oba te znaczenia sÅ‚owa 'play' w tym samym osadzeniu. Aby przezwyciÄ™Å¼yÄ‡ to ograniczenie, musimy budowaÄ‡ osadzenia oparte na **modelu jÄ™zykowym**, ktÃ³ry jest trenowany na duÅ¼ym korpusie tekstÃ³w i *wie*, jak sÅ‚owa mogÄ… byÄ‡ uÅ¼ywane w rÃ³Å¼nych kontekstach. OmÃ³wienie kontekstowych osadzeÅ„ wykracza poza zakres tego tutorialu, ale wrÃ³cimy do nich, gdy bÄ™dziemy omawiaÄ‡ modele jÄ™zykowe pÃ³Åºniej w kursie.

## Podsumowanie

W tej lekcji odkryÅ‚eÅ›, jak budowaÄ‡ i uÅ¼ywaÄ‡ warstw osadzeÅ„ w TensorFlow i Pytorch, aby lepiej odzwierciedlaÄ‡ semantyczne znaczenia sÅ‚Ã³w.

## ğŸš€ Wyzwanie

Word2Vec byÅ‚ uÅ¼ywany w ciekawych aplikacjach, w tym do generowania tekstÃ³w piosenek i poezji. Zajrzyj do [tego artykuÅ‚u](https://www.politetype.com/blog/word2vec-color-poems), ktÃ³ry opisuje, jak autor uÅ¼yÅ‚ Word2Vec do generowania poezji. Obejrzyj rÃ³wnieÅ¼ [ten film Dana Shiffmanna](https://www.youtube.com/watch?v=LSS_bos_TPI&ab_channel=TheCodingTrain), aby odkryÄ‡ inne wyjaÅ›nienie tej techniki. NastÄ™pnie sprÃ³buj zastosowaÄ‡ te techniki do wÅ‚asnego korpusu tekstowego, byÄ‡ moÅ¼e pochodzÄ…cego z Kaggle.

## [Quiz po wykÅ‚adzie](https://ff-quizzes.netlify.app/en/ai/quiz/28)

## PrzeglÄ…d i samodzielna nauka

Przeczytaj ten artykuÅ‚ o Word2Vec: [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf)

## [Zadanie: Notatniki](assignment.md)

---

