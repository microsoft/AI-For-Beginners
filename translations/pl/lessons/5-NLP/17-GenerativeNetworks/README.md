# Generatywne sieci

## [Quiz przed wykÅ‚adem](https://ff-quizzes.netlify.app/en/ai/quiz/33)

Rekurencyjne sieci neuronowe (RNN) oraz ich warianty z komÃ³rkami bramkowymi, takie jak komÃ³rki pamiÄ™ci dÅ‚ugoterminowej (LSTM) i jednostki rekurencyjne z bramkami (GRU), umoÅ¼liwiajÄ… modelowanie jÄ™zyka, poniewaÅ¼ potrafiÄ… uczyÄ‡ siÄ™ kolejnoÅ›ci sÅ‚Ã³w i przewidywaÄ‡ nastÄ™pne sÅ‚owo w sekwencji. DziÄ™ki temu moÅ¼emy uÅ¼ywaÄ‡ RNN do **zadaÅ„ generatywnych**, takich jak generowanie tekstu, tÅ‚umaczenie maszynowe, a nawet opisywanie obrazÃ³w.

> âœ… PomyÅ›l o wszystkich sytuacjach, w ktÃ³rych korzystaÅ‚eÅ› z zadaÅ„ generatywnych, takich jak uzupeÅ‚nianie tekstu podczas pisania. Poszukaj informacji o swoich ulubionych aplikacjach, aby sprawdziÄ‡, czy wykorzystujÄ… RNN.

W architekturze RNN, ktÃ³rÄ… omawialiÅ›my w poprzedniej jednostce, kaÅ¼da jednostka RNN generowaÅ‚a kolejny ukryty stan jako wyjÅ›cie. MoÅ¼emy jednak dodaÄ‡ kolejne wyjÅ›cie do kaÅ¼dej jednostki rekurencyjnej, co pozwoli na generowanie **sekwencji** (o dÅ‚ugoÅ›ci rÃ³wnej oryginalnej sekwencji). Ponadto moÅ¼emy uÅ¼ywaÄ‡ jednostek RNN, ktÃ³re nie przyjmujÄ… danych wejÅ›ciowych na kaÅ¼dym kroku, a jedynie poczÄ…tkowy wektor stanu, aby generowaÄ‡ sekwencjÄ™ wyjÅ›Ä‡.

To umoÅ¼liwia rÃ³Å¼ne architektury sieci neuronowych, ktÃ³re przedstawiono na poniÅ¼szym obrazku:

![Obraz przedstawiajÄ…cy typowe wzorce rekurencyjnych sieci neuronowych.](../../../../../translated_images/pl/unreasonable-effectiveness-of-rnn.541ead816778f42d.webp)

> Obraz z wpisu na blogu [Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) autorstwa [Andreja Karpaty](http://karpathy.github.io/)

* **One-to-one** to tradycyjna sieÄ‡ neuronowa z jednym wejÅ›ciem i jednym wyjÅ›ciem
* **One-to-many** to architektura generatywna, ktÃ³ra przyjmuje jednÄ… wartoÅ›Ä‡ wejÅ›ciowÄ… i generuje sekwencjÄ™ wartoÅ›ci wyjÅ›ciowych. Na przykÅ‚ad, jeÅ›li chcemy wytrenowaÄ‡ sieÄ‡ do **opisywania obrazÃ³w**, ktÃ³ra generuje tekstowy opis obrazu, moÅ¼emy podaÄ‡ obraz jako wejÅ›cie, przepuÅ›ciÄ‡ go przez CNN, aby uzyskaÄ‡ ukryty stan, a nastÄ™pnie uÅ¼yÄ‡ Å‚aÅ„cucha rekurencyjnego do generowania opisu sÅ‚owo po sÅ‚owie
* **Many-to-one** odpowiada architekturom RNN, ktÃ³re opisaliÅ›my w poprzedniej jednostce, takich jak klasyfikacja tekstu
* **Many-to-many**, czyli **sequence-to-sequence**, odpowiada zadaniom takim jak **tÅ‚umaczenie maszynowe**, gdzie pierwsza RNN zbiera wszystkie informacje z sekwencji wejÅ›ciowej do ukrytego stanu, a kolejny Å‚aÅ„cuch RNN rozwija ten stan w sekwencjÄ™ wyjÅ›ciowÄ….

W tej jednostce skupimy siÄ™ na prostych modelach generatywnych, ktÃ³re pomagajÄ… generowaÄ‡ tekst. Dla uproszczenia uÅ¼yjemy tokenizacji na poziomie znakÃ³w.

Wytrenujemy tÄ™ RNN do generowania tekstu krok po kroku. Na kaÅ¼dym kroku weÅºmiemy sekwencjÄ™ znakÃ³w o dÅ‚ugoÅ›ci `nchars` i poprosimy sieÄ‡ o wygenerowanie kolejnego znaku wyjÅ›ciowego dla kaÅ¼dego znaku wejÅ›ciowego:

![Obraz przedstawiajÄ…cy przykÅ‚ad generowania sÅ‚owa 'HELLO' przez RNN.](../../../../../translated_images/pl/rnn-generate.56c54afb52f9781d.webp)

Podczas generowania tekstu (w trakcie inferencji) zaczynamy od jakiegoÅ› **podpowiedzi** (prompt), ktÃ³ra jest przepuszczana przez komÃ³rki RNN, aby wygenerowaÄ‡ jej stan poÅ›redni, a nastÄ™pnie z tego stanu rozpoczyna siÄ™ generowanie. Generujemy jeden znak na raz, przekazujemy stan i wygenerowany znak do kolejnej komÃ³rki RNN, aby wygenerowaÄ‡ nastÄ™pny znak, aÅ¼ wygenerujemy wystarczajÄ…cÄ… liczbÄ™ znakÃ³w.

<img src="../../../../../translated_images/pl/rnn-generate-inf.5168dc65e0370eea.webp" width="60%"/>

> Obraz autorstwa autora

## âœï¸ Ä†wiczenia: Generatywne sieci

Kontynuuj naukÄ™ w poniÅ¼szych notatnikach:

* [Generatywne sieci w PyTorch](GenerativePyTorch.ipynb)
* [Generatywne sieci w TensorFlow](GenerativeTF.ipynb)

## MiÄ™kkie generowanie tekstu i temperatura

WyjÅ›cie kaÅ¼dej komÃ³rki RNN to rozkÅ‚ad prawdopodobieÅ„stwa znakÃ³w. JeÅ›li zawsze wybieramy znak o najwyÅ¼szym prawdopodobieÅ„stwie jako kolejny znak w generowanym tekÅ›cie, tekst czÄ™sto moÅ¼e "cyklicznie" powtarzaÄ‡ te same sekwencje znakÃ³w, jak w tym przykÅ‚adzie:

```
today of the second the company and a second the company ...
```

Jednak jeÅ›li spojrzymy na rozkÅ‚ad prawdopodobieÅ„stwa dla kolejnego znaku, moÅ¼e siÄ™ okazaÄ‡, Å¼e rÃ³Å¼nica miÄ™dzy kilkoma najwyÅ¼szymi prawdopodobieÅ„stwami nie jest duÅ¼a, np. jeden znak moÅ¼e mieÄ‡ prawdopodobieÅ„stwo 0.2, a inny 0.19 itd. Na przykÅ‚ad, gdy szukamy kolejnego znaku w sekwencji '*play*', kolejnym znakiem moÅ¼e byÄ‡ zarÃ³wno spacja, jak i **e** (jak w sÅ‚owie *player*).

To prowadzi nas do wniosku, Å¼e nie zawsze "uczciwe" jest wybieranie znaku o najwyÅ¼szym prawdopodobieÅ„stwie, poniewaÅ¼ wybÃ³r drugiego najwyÅ¼szego moÅ¼e rÃ³wnieÅ¼ prowadziÄ‡ do sensownego tekstu. MÄ…drzejszym podejÅ›ciem jest **prÃ³bkowanie** znakÃ³w z rozkÅ‚adu prawdopodobieÅ„stwa podanego przez wyjÅ›cie sieci. MoÅ¼emy rÃ³wnieÅ¼ uÅ¼yÄ‡ parametru **temperatura**, ktÃ³ry spÅ‚aszczy rozkÅ‚ad prawdopodobieÅ„stwa, jeÅ›li chcemy dodaÄ‡ wiÄ™cej losowoÅ›ci, lub uczyni go bardziej stromym, jeÅ›li chcemy trzymaÄ‡ siÄ™ znakÃ³w o najwyÅ¼szym prawdopodobieÅ„stwie.

Zbadaj, jak to miÄ™kkie generowanie tekstu jest zaimplementowane w notatnikach podlinkowanych powyÅ¼ej.

## Podsumowanie

ChociaÅ¼ generowanie tekstu moÅ¼e byÄ‡ uÅ¼yteczne samo w sobie, gÅ‚Ã³wne korzyÅ›ci wynikajÄ… z moÅ¼liwoÅ›ci generowania tekstu za pomocÄ… RNN z poczÄ…tkowego wektora cech. Na przykÅ‚ad generowanie tekstu jest uÅ¼ywane jako czÄ™Å›Ä‡ tÅ‚umaczenia maszynowego (sequence-to-sequence, w tym przypadku wektor stanu z *enkodera* jest uÅ¼ywany do generowania lub *dekodowania* przetÅ‚umaczonej wiadomoÅ›ci) lub generowania tekstowego opisu obrazu (w takim przypadku wektor cech pochodzi z ekstraktora CNN).

## ğŸš€ Wyzwanie

WeÅº udziaÅ‚ w lekcjach na Microsoft Learn na ten temat:

* Generowanie tekstu w [PyTorch](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-pytorch/6-generative-networks/?WT.mc_id=academic-77998-cacaste)/[TensorFlow](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-tensorflow/5-generative-networks/?WT.mc_id=academic-77998-cacaste)

## [Quiz po wykÅ‚adzie](https://ff-quizzes.netlify.app/en/ai/quiz/34)

## PrzeglÄ…d i samodzielna nauka

Oto kilka artykuÅ‚Ã³w, ktÃ³re poszerzÄ… TwojÄ… wiedzÄ™:

* RÃ³Å¼ne podejÅ›cia do generowania tekstu z Markov Chain, LSTM i GPT-2: [wpis na blogu](https://towardsdatascience.com/text-generation-gpt-2-lstm-markov-chain-9ea371820e1e)
* PrzykÅ‚ad generowania tekstu w [dokumentacji Keras](https://keras.io/examples/generative/lstm_character_level_text_generation/)

## [Zadanie](lab/README.md)

WidzieliÅ›my, jak generowaÄ‡ tekst znak po znaku. W laboratorium bÄ™dziesz eksplorowaÄ‡ generowanie tekstu na poziomie sÅ‚Ã³w.

---

