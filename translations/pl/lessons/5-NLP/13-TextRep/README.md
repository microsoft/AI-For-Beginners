# Reprezentowanie tekstu jako tensory

## [Pre-quiz przed wykÅ‚adem](https://ff-quizzes.netlify.app/en/ai/quiz/25)

## Klasyfikacja tekstu

W pierwszej czÄ™Å›ci tego rozdziaÅ‚u skupimy siÄ™ na zadaniu **klasyfikacji tekstu**. Wykorzystamy zbiÃ³r danych [AG News](https://www.kaggle.com/amananandrai/ag-news-classification-dataset), ktÃ³ry zawiera artykuÅ‚y prasowe, takie jak poniÅ¼szy:

* Kategoria: Nauka/Technologia
* TytuÅ‚: Firma z Kentucky zdobywa grant na badanie peptydÃ³w (AP)
* TreÅ›Ä‡: AP - Firma zaÅ‚oÅ¼ona przez badacza chemii z Uniwersytetu w Louisville zdobyÅ‚a grant na rozwÃ³j...

Naszym celem bÄ™dzie sklasyfikowanie artykuÅ‚u prasowego do jednej z kategorii na podstawie tekstu.

## Reprezentowanie tekstu

Aby rozwiÄ…zywaÄ‡ zadania zwiÄ…zane z przetwarzaniem jÄ™zyka naturalnego (NLP) za pomocÄ… sieci neuronowych, musimy znaleÅºÄ‡ sposÃ³b na reprezentowanie tekstu jako tensory. Komputery juÅ¼ teraz reprezentujÄ… znaki tekstowe jako liczby, ktÃ³re mapujÄ… na czcionki na ekranie, uÅ¼ywajÄ…c kodowaÅ„ takich jak ASCII czy UTF-8.

<img alt="Obraz przedstawiajÄ…cy diagram mapujÄ…cy znak na reprezentacjÄ™ ASCII i binarnÄ…" src="../../../../../translated_images/pl/ascii-character-map.18ed6aa7f3b0a7ff.webp" width="50%"/>

> [Å¹rÃ³dÅ‚o obrazu](https://www.seobility.net/en/wiki/ASCII)

Jako ludzie rozumiemy, co kaÅ¼dy znak **reprezentuje** i jak wszystkie znaki Å‚Ä…czÄ… siÄ™, tworzÄ…c sÅ‚owa w zdaniu. Jednak komputery same w sobie nie majÄ… takiego zrozumienia, a sieÄ‡ neuronowa musi nauczyÄ‡ siÄ™ znaczenia podczas treningu.

Dlatego moÅ¼emy uÅ¼ywaÄ‡ rÃ³Å¼nych podejÅ›Ä‡ do reprezentowania tekstu:

* **Reprezentacja na poziomie znakÃ³w**, gdzie tekst reprezentujemy, traktujÄ…c kaÅ¼dy znak jako liczbÄ™. ZakÅ‚adajÄ…c, Å¼e mamy *C* rÃ³Å¼nych znakÃ³w w naszym korpusie tekstowym, sÅ‚owo *Hello* byÅ‚oby reprezentowane jako tensor 5x*C*. KaÅ¼da litera odpowiadaÅ‚aby kolumnie tensora w kodowaniu one-hot.
* **Reprezentacja na poziomie sÅ‚Ã³w**, w ktÃ³rej tworzymy **sÅ‚ownik** wszystkich sÅ‚Ã³w w naszym tekÅ›cie, a nastÄ™pnie reprezentujemy sÅ‚owa za pomocÄ… kodowania one-hot. To podejÅ›cie jest nieco lepsze, poniewaÅ¼ kaÅ¼da litera sama w sobie nie ma duÅ¼ego znaczenia, a uÅ¼ywajÄ…c wyÅ¼szych poziomÃ³w semantycznych - sÅ‚Ã³w - upraszczamy zadanie dla sieci neuronowej. Jednak ze wzglÄ™du na duÅ¼Ä… wielkoÅ›Ä‡ sÅ‚ownika musimy radziÄ‡ sobie z tensorami o wysokiej wymiarowoÅ›ci i duÅ¼ej rzadkoÅ›ci.

NiezaleÅ¼nie od wybranej reprezentacji, najpierw musimy przeksztaÅ‚ciÄ‡ tekst w sekwencjÄ™ **tokenÃ³w**, gdzie jeden token to znak, sÅ‚owo lub czasami nawet czÄ™Å›Ä‡ sÅ‚owa. NastÄ™pnie konwertujemy token na liczbÄ™, zazwyczaj uÅ¼ywajÄ…c **sÅ‚ownika**, a tÄ™ liczbÄ™ moÅ¼na wprowadziÄ‡ do sieci neuronowej za pomocÄ… kodowania one-hot.

## N-Gramy

W jÄ™zyku naturalnym precyzyjne znaczenie sÅ‚Ã³w moÅ¼na okreÅ›liÄ‡ tylko w kontekÅ›cie. Na przykÅ‚ad znaczenia *sieÄ‡ neuronowa* i *sieÄ‡ rybacka* sÄ… zupeÅ‚nie rÃ³Å¼ne. Jednym ze sposobÃ³w uwzglÄ™dnienia tego jest budowanie modelu na parach sÅ‚Ã³w i traktowanie par sÅ‚Ã³w jako oddzielnych tokenÃ³w sÅ‚ownika. W ten sposÃ³b zdanie *LubiÄ™ chodziÄ‡ na ryby* bÄ™dzie reprezentowane przez nastÄ™pujÄ…cÄ… sekwencjÄ™ tokenÃ³w: *LubiÄ™ chodziÄ‡*, *chodziÄ‡ na*, *na ryby*. Problem z tym podejÅ›ciem polega na tym, Å¼e rozmiar sÅ‚ownika znacznie roÅ›nie, a kombinacje takie jak *na ryby* i *na zakupy* sÄ… reprezentowane przez rÃ³Å¼ne tokeny, ktÃ³re nie dzielÄ… Å¼adnego podobieÅ„stwa semantycznego mimo tego samego czasownika.

W niektÃ³rych przypadkach moÅ¼emy rozwaÅ¼yÄ‡ uÅ¼ycie tri-gramÃ³w â€” kombinacji trzech sÅ‚Ã³w. Takie podejÅ›cie nazywane jest **n-gramami**. Warto rÃ³wnieÅ¼ uÅ¼ywaÄ‡ n-gramÃ³w w reprezentacji na poziomie znakÃ³w, gdzie n-gramy bÄ™dÄ… mniej wiÄ™cej odpowiadaÄ‡ rÃ³Å¼nym sylabom.

## Bag-of-Words i TF/IDF

Podczas rozwiÄ…zywania zadaÅ„ takich jak klasyfikacja tekstu musimy byÄ‡ w stanie reprezentowaÄ‡ tekst za pomocÄ… jednego wektora o staÅ‚ym rozmiarze, ktÃ³ry bÄ™dzie uÅ¼ywany jako wejÅ›cie do koÅ„cowego klasyfikatora gÄ™stego. Jednym z najprostszych sposobÃ³w na to jest poÅ‚Ä…czenie wszystkich indywidualnych reprezentacji sÅ‚Ã³w, np. przez ich dodanie. JeÅ›li dodamy kodowania one-hot kaÅ¼dego sÅ‚owa, otrzymamy wektor czÄ™stotliwoÅ›ci, pokazujÄ…cy, ile razy kaÅ¼de sÅ‚owo pojawia siÄ™ w tekÅ›cie. Taka reprezentacja tekstu nazywana jest **bag-of-words** (BoW).

<img src="../../../../../translated_images/pl/bow.3811869cff59368d.webp" width="90%"/>

> Obraz autorstwa autora

BoW zasadniczo reprezentuje, ktÃ³re sÅ‚owa pojawiajÄ… siÄ™ w tekÅ›cie i w jakich iloÅ›ciach, co moÅ¼e byÄ‡ dobrym wskaÅºnikiem tego, o czym jest tekst. Na przykÅ‚ad artykuÅ‚ prasowy o polityce prawdopodobnie zawiera sÅ‚owa takie jak *prezydent* i *kraj*, podczas gdy publikacja naukowa moÅ¼e zawieraÄ‡ sÅ‚owa takie jak *zderzacz*, *odkryto*, itd. Zatem czÄ™stotliwoÅ›ci sÅ‚Ã³w mogÄ… w wielu przypadkach byÄ‡ dobrym wskaÅºnikiem treÅ›ci tekstu.

Problem z BoW polega na tym, Å¼e pewne powszechne sÅ‚owa, takie jak *i*, *jest*, itd., pojawiajÄ… siÄ™ w wiÄ™kszoÅ›ci tekstÃ³w i majÄ… najwyÅ¼sze czÄ™stotliwoÅ›ci, maskujÄ…c sÅ‚owa, ktÃ³re sÄ… naprawdÄ™ waÅ¼ne. MoÅ¼emy obniÅ¼yÄ‡ znaczenie tych sÅ‚Ã³w, uwzglÄ™dniajÄ…c czÄ™stotliwoÅ›Ä‡, z jakÄ… sÅ‚owa wystÄ™pujÄ… w caÅ‚ej kolekcji dokumentÃ³w. To jest gÅ‚Ã³wna idea podejÅ›cia TF/IDF, ktÃ³re jest szczegÃ³Å‚owo omÃ³wione w notatnikach doÅ‚Ä…czonych do tej lekcji.

Jednak Å¼adne z tych podejÅ›Ä‡ nie jest w stanie w peÅ‚ni uwzglÄ™dniÄ‡ **semantyki** tekstu. Potrzebujemy bardziej zaawansowanych modeli sieci neuronowych, aby to zrobiÄ‡, co omÃ³wimy pÃ³Åºniej w tym rozdziale.

## âœï¸ Ä†wiczenia: Reprezentacja tekstu

Kontynuuj naukÄ™ w poniÅ¼szych notatnikach:

* [Reprezentacja tekstu w PyTorch](TextRepresentationPyTorch.ipynb)
* [Reprezentacja tekstu w TensorFlow](TextRepresentationTF.ipynb)

## Podsumowanie

Do tej pory poznaliÅ›my techniki, ktÃ³re mogÄ… dodawaÄ‡ wagÄ™ czÄ™stotliwoÅ›ci do rÃ³Å¼nych sÅ‚Ã³w. Nie sÄ… one jednak w stanie reprezentowaÄ‡ znaczenia ani kolejnoÅ›ci. Jak powiedziaÅ‚ sÅ‚ynny jÄ™zykoznawca J. R. Firth w 1935 roku: "PeÅ‚ne znaczenie sÅ‚owa zawsze jest kontekstowe, a Å¼adna analiza znaczenia poza kontekstem nie moÅ¼e byÄ‡ traktowana powaÅ¼nie." W dalszej czÄ™Å›ci kursu nauczymy siÄ™, jak uchwyciÄ‡ informacje kontekstowe z tekstu za pomocÄ… modelowania jÄ™zyka.

## ğŸš€ Wyzwanie

WyprÃ³buj inne Ä‡wiczenia, korzystajÄ…c z bag-of-words i rÃ³Å¼nych modeli danych. MoÅ¼esz zainspirowaÄ‡ siÄ™ tym [konkursem na Kaggle](https://www.kaggle.com/competitions/word2vec-nlp-tutorial/overview/part-1-for-beginners-bag-of-words)

## [Quiz po wykÅ‚adzie](https://ff-quizzes.netlify.app/en/ai/quiz/26)

## PrzeglÄ…d i samodzielna nauka

Ä†wicz swoje umiejÄ™tnoÅ›ci z technikami osadzania tekstu i bag-of-words na [Microsoft Learn](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-pytorch/?WT.mc_id=academic-77998-cacaste)

## [Zadanie: Notatniki](assignment.md)

---

