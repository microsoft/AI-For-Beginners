<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "4522e22e150be0845e03aa41209a39d5",
  "translation_date": "2025-08-24T10:15:35+00:00",
  "source_file": "lessons/5-NLP/13-TextRep/README.md",
  "language_code": "pl"
}
-->
# Reprezentacja tekstu jako tensory

## [Quiz przed wykÅ‚adem](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/113)

## Klasyfikacja tekstu

W pierwszej czÄ™Å›ci tego rozdziaÅ‚u skupimy siÄ™ na zadaniu **klasyfikacji tekstu**. Wykorzystamy zbiÃ³r danych [AG News](https://www.kaggle.com/amananandrai/ag-news-classification-dataset), ktÃ³ry zawiera artykuÅ‚y prasowe, takie jak poniÅ¼szy przykÅ‚ad:

* Kategoria: Nauka/Technologia  
* TytuÅ‚: Firma z Kentucky zdobywa grant na badania nad peptydami (AP)  
* TreÅ›Ä‡: AP - Firma zaÅ‚oÅ¼ona przez badacza chemii z Uniwersytetu w Louisville zdobyÅ‚a grant na rozwÃ³j...

Naszym celem bÄ™dzie sklasyfikowanie artykuÅ‚u prasowego do jednej z kategorii na podstawie tekstu.

## Reprezentacja tekstu

Aby rozwiÄ…zywaÄ‡ zadania z zakresu przetwarzania jÄ™zyka naturalnego (NLP) za pomocÄ… sieci neuronowych, musimy znaleÅºÄ‡ sposÃ³b na reprezentacjÄ™ tekstu jako tensory. Komputery juÅ¼ teraz reprezentujÄ… znaki tekstowe jako liczby, ktÃ³re sÄ… mapowane na czcionki na ekranie, uÅ¼ywajÄ…c kodowaÅ„ takich jak ASCII czy UTF-8.

<img alt="Obraz przedstawiajÄ…cy diagram mapujÄ…cy znak na reprezentacjÄ™ ASCII i binarnÄ…" src="images/ascii-character-map.png" width="50%"/>

> [Å¹rÃ³dÅ‚o obrazu](https://www.seobility.net/en/wiki/ASCII)

Jako ludzie rozumiemy, co kaÅ¼dy znak **reprezentuje**, i jak wszystkie znaki Å‚Ä…czÄ… siÄ™, tworzÄ…c sÅ‚owa w zdaniu. Jednak komputery same w sobie nie majÄ… takiego zrozumienia, a sieÄ‡ neuronowa musi nauczyÄ‡ siÄ™ znaczenia podczas treningu.

Dlatego moÅ¼emy uÅ¼ywaÄ‡ rÃ³Å¼nych podejÅ›Ä‡ do reprezentacji tekstu:

* **Reprezentacja na poziomie znakÃ³w**, gdzie traktujemy kaÅ¼dy znak jako liczbÄ™. ZakÅ‚adajÄ…c, Å¼e mamy *C* rÃ³Å¼nych znakÃ³w w naszym korpusie tekstowym, sÅ‚owo *Hello* byÅ‚oby reprezentowane jako tensor o wymiarach 5x*C*. KaÅ¼da litera odpowiadaÅ‚aby kolumnie tensora w kodowaniu one-hot.  
* **Reprezentacja na poziomie sÅ‚Ã³w**, w ktÃ³rej tworzymy **sÅ‚ownik** wszystkich sÅ‚Ã³w w naszym tekÅ›cie, a nastÄ™pnie reprezentujemy sÅ‚owa za pomocÄ… kodowania one-hot. To podejÅ›cie jest nieco lepsze, poniewaÅ¼ pojedyncze litery same w sobie nie majÄ… duÅ¼ego znaczenia, a uÅ¼ywajÄ…c wyÅ¼szych poziomÃ³w semantycznych - sÅ‚Ã³w - upraszczamy zadanie dla sieci neuronowej. Jednak ze wzglÄ™du na duÅ¼Ä… wielkoÅ›Ä‡ sÅ‚ownika musimy radziÄ‡ sobie z tensorami o wysokiej wymiarowoÅ›ci i duÅ¼ej rzadkoÅ›ci.

NiezaleÅ¼nie od wybranej reprezentacji, najpierw musimy przeksztaÅ‚ciÄ‡ tekst w sekwencjÄ™ **tokenÃ³w**, gdzie jeden token to znak, sÅ‚owo lub czasami nawet czÄ™Å›Ä‡ sÅ‚owa. NastÄ™pnie konwertujemy token na liczbÄ™, zazwyczaj uÅ¼ywajÄ…c **sÅ‚ownika**, a ta liczba moÅ¼e byÄ‡ wprowadzona do sieci neuronowej za pomocÄ… kodowania one-hot.

## N-Gramy

W jÄ™zyku naturalnym precyzyjne znaczenie sÅ‚Ã³w moÅ¼na okreÅ›liÄ‡ tylko w kontekÅ›cie. Na przykÅ‚ad znaczenia *neural network* i *fishing network* sÄ… zupeÅ‚nie rÃ³Å¼ne. Jednym ze sposobÃ³w uwzglÄ™dnienia tego jest budowanie modelu na podstawie par sÅ‚Ã³w i traktowanie par sÅ‚Ã³w jako oddzielnych tokenÃ³w sÅ‚ownika. W ten sposÃ³b zdanie *I like to go fishing* bÄ™dzie reprezentowane przez nastÄ™pujÄ…cÄ… sekwencjÄ™ tokenÃ³w: *I like*, *like to*, *to go*, *go fishing*. Problem z tym podejÅ›ciem polega na tym, Å¼e rozmiar sÅ‚ownika znacznie roÅ›nie, a kombinacje takie jak *go fishing* i *go shopping* sÄ… reprezentowane przez rÃ³Å¼ne tokeny, ktÃ³re nie dzielÄ… Å¼adnego podobieÅ„stwa semantycznego, mimo Å¼e zawierajÄ… to samo czasownik.

W niektÃ³rych przypadkach moÅ¼emy rozwaÅ¼yÄ‡ uÅ¼ycie tri-gramÃ³w â€” kombinacji trzech sÅ‚Ã³w. Dlatego podejÅ›cie to czÄ™sto nazywane jest **n-gramami**. Sensowne jest rÃ³wnieÅ¼ uÅ¼ycie n-gramÃ³w w reprezentacji na poziomie znakÃ³w, gdzie n-gramy bÄ™dÄ… mniej wiÄ™cej odpowiadaÄ‡ rÃ³Å¼nym sylabom.

## Bag-of-Words i TF/IDF

Podczas rozwiÄ…zywania zadaÅ„ takich jak klasyfikacja tekstu musimy byÄ‡ w stanie reprezentowaÄ‡ tekst za pomocÄ… jednego wektora o staÅ‚ym rozmiarze, ktÃ³ry bÄ™dzie uÅ¼ywany jako wejÅ›cie do koÅ„cowego klasyfikatora gÄ™stego. Jednym z najprostszych sposobÃ³w na to jest poÅ‚Ä…czenie wszystkich indywidualnych reprezentacji sÅ‚Ã³w, np. przez ich dodanie. JeÅ›li dodamy kodowania one-hot kaÅ¼dego sÅ‚owa, otrzymamy wektor czÄ™stotliwoÅ›ci, pokazujÄ…cy, ile razy kaÅ¼de sÅ‚owo pojawia siÄ™ w tekÅ›cie. Taka reprezentacja tekstu nazywana jest **bag of words** (BoW).

<img src="images/bow.png" width="90%"/>

> Obraz autora

BoW zasadniczo reprezentuje, ktÃ³re sÅ‚owa pojawiajÄ… siÄ™ w tekÅ›cie i w jakich iloÅ›ciach, co moÅ¼e byÄ‡ dobrym wskaÅºnikiem tego, o czym jest tekst. Na przykÅ‚ad artykuÅ‚ prasowy o polityce prawdopodobnie zawiera sÅ‚owa takie jak *prezydent* i *kraj*, podczas gdy publikacja naukowa moÅ¼e zawieraÄ‡ sÅ‚owa takie jak *zderzacz*, *odkryto* itp. Dlatego czÄ™stotliwoÅ›ci sÅ‚Ã³w mogÄ… w wielu przypadkach byÄ‡ dobrym wskaÅºnikiem treÅ›ci tekstu.

Problem z BoW polega na tym, Å¼e pewne powszechne sÅ‚owa, takie jak *i*, *jest* itp., pojawiajÄ… siÄ™ w wiÄ™kszoÅ›ci tekstÃ³w i majÄ… najwyÅ¼sze czÄ™stotliwoÅ›ci, maskujÄ…c sÅ‚owa, ktÃ³re sÄ… naprawdÄ™ waÅ¼ne. MoÅ¼emy obniÅ¼yÄ‡ znaczenie tych sÅ‚Ã³w, biorÄ…c pod uwagÄ™ czÄ™stotliwoÅ›Ä‡ ich wystÄ™powania w caÅ‚ej kolekcji dokumentÃ³w. To gÅ‚Ã³wna idea podejÅ›cia TF/IDF, ktÃ³re jest omÃ³wione bardziej szczegÃ³Å‚owo w notatnikach doÅ‚Ä…czonych do tej lekcji.

Jednak Å¼adne z tych podejÅ›Ä‡ nie jest w stanie w peÅ‚ni uwzglÄ™dniÄ‡ **semantyki** tekstu. Do tego potrzebujemy bardziej zaawansowanych modeli sieci neuronowych, ktÃ³re omÃ³wimy pÃ³Åºniej w tym rozdziale.

## âœï¸ Ä†wiczenia: Reprezentacja tekstu

Kontynuuj naukÄ™ w poniÅ¼szych notatnikach:

* [Reprezentacja tekstu z PyTorch](../../../../../lessons/5-NLP/13-TextRep/TextRepresentationPyTorch.ipynb)  
* [Reprezentacja tekstu z TensorFlow](../../../../../lessons/5-NLP/13-TextRep/TextRepresentationTF.ipynb)

## Podsumowanie

Jak dotÄ…d poznaliÅ›my techniki, ktÃ³re mogÄ… dodaÄ‡ wagÄ™ czÄ™stotliwoÅ›ci do rÃ³Å¼nych sÅ‚Ã³w. Nie sÄ… one jednak w stanie reprezentowaÄ‡ znaczenia ani kolejnoÅ›ci. Jak powiedziaÅ‚ sÅ‚ynny jÄ™zykoznawca J. R. Firth w 1935 roku: "PeÅ‚ne znaczenie sÅ‚owa zawsze jest kontekstowe, a Å¼adna analiza znaczenia poza kontekstem nie moÅ¼e byÄ‡ traktowana powaÅ¼nie." W dalszej czÄ™Å›ci kursu nauczymy siÄ™, jak uchwyciÄ‡ informacje kontekstowe z tekstu za pomocÄ… modelowania jÄ™zyka.

## ğŸš€ Wyzwanie

SprÃ³buj innych Ä‡wiczeÅ„ z uÅ¼yciem bag-of-words i rÃ³Å¼nych modeli danych. MoÅ¼esz zainspirowaÄ‡ siÄ™ tÄ… [konkursem na Kaggle](https://www.kaggle.com/competitions/word2vec-nlp-tutorial/overview/part-1-for-beginners-bag-of-words)

## [Quiz po wykÅ‚adzie](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/213)

## PrzeglÄ…d i samodzielna nauka

Ä†wicz swoje umiejÄ™tnoÅ›ci z technikami osadzania tekstu i bag-of-words na [Microsoft Learn](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-pytorch/?WT.mc_id=academic-77998-cacaste)

## [Zadanie: Notatniki](assignment.md)

**ZastrzeÅ¼enie**:  
Ten dokument zostaÅ‚ przetÅ‚umaczony za pomocÄ… usÅ‚ugi tÅ‚umaczenia AI [Co-op Translator](https://github.com/Azure/co-op-translator). ChociaÅ¼ staramy siÄ™ zapewniÄ‡ dokÅ‚adnoÅ›Ä‡, prosimy mieÄ‡ na uwadze, Å¼e automatyczne tÅ‚umaczenia mogÄ… zawieraÄ‡ bÅ‚Ä™dy lub nieÅ›cisÅ‚oÅ›ci. Oryginalny dokument w jego rodzimym jÄ™zyku powinien byÄ‡ uznawany za wiarygodne ÅºrÃ³dÅ‚o. W przypadku informacji krytycznych zaleca siÄ™ skorzystanie z profesjonalnego tÅ‚umaczenia przez czÅ‚owieka. Nie ponosimy odpowiedzialnoÅ›ci za jakiekolwiek nieporozumienia lub bÅ‚Ä™dne interpretacje wynikajÄ…ce z uÅ¼ycia tego tÅ‚umaczenia.