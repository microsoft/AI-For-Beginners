<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "31b46ba1f3aa78578134d4829f88be53",
  "translation_date": "2025-08-24T10:16:32+00:00",
  "source_file": "lessons/5-NLP/15-LanguageModeling/README.md",
  "language_code": "pl"
}
-->
# Modelowanie jÄ™zyka

Semantyczne osadzenia, takie jak Word2Vec i GloVe, to w rzeczywistoÅ›ci pierwszy krok w kierunku **modelowania jÄ™zyka** â€“ tworzenia modeli, ktÃ³re w pewien sposÃ³b *rozumiejÄ…* (lub *reprezentujÄ…*) naturÄ™ jÄ™zyka.

## [Quiz przed wykÅ‚adem](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/115)

GÅ‚Ã³wna idea modelowania jÄ™zyka polega na trenowaniu modeli na niezlabelowanych zbiorach danych w sposÃ³b nienadzorowany. Jest to istotne, poniewaÅ¼ mamy dostÄ™p do ogromnych iloÅ›ci niezlabelowanego tekstu, podczas gdy iloÅ›Ä‡ tekstu zlabelowanego zawsze bÄ™dzie ograniczona wysiÅ‚kiem, jaki moÅ¼emy poÅ›wiÄ™ciÄ‡ na jego oznaczanie. NajczÄ™Å›ciej moÅ¼emy budowaÄ‡ modele jÄ™zykowe, ktÃ³re potrafiÄ… **przewidywaÄ‡ brakujÄ…ce sÅ‚owa** w tekÅ›cie, poniewaÅ¼ Å‚atwo jest zamaskowaÄ‡ losowe sÅ‚owo w tekÅ›cie i uÅ¼yÄ‡ go jako prÃ³bki treningowej.

## Trenowanie osadzeÅ„

W poprzednich przykÅ‚adach korzystaliÅ›my z wczeÅ›niej wytrenowanych semantycznych osadzeÅ„, ale warto zobaczyÄ‡, jak moÅ¼na trenowaÄ‡ takie osadzenia samodzielnie. Istnieje kilka moÅ¼liwych podejÅ›Ä‡:

* **Modelowanie jÄ™zyka N-Gram**, gdzie przewidujemy token, patrzÄ…c na N poprzednich tokenÃ³w (N-gramy).
* **Continuous Bag-of-Words** (CBoW), gdzie przewidujemy Å›rodkowy token $W_0$ w sekwencji tokenÃ³w $W_{-N}$, ..., $W_N$.
* **Skip-gram**, gdzie przewidujemy zestaw sÄ…siednich tokenÃ³w {$W_{-N},\dots, W_{-1}, W_1,\dots, W_N$} na podstawie Å›rodkowego tokena $W_0$.

![obraz z artykuÅ‚u o konwersji sÅ‚Ã³w na wektory](../../../../../lessons/5-NLP/14-Embeddings/images/example-algorithms-for-converting-words-to-vectors.png)

> Obraz z [tego artykuÅ‚u](https://arxiv.org/pdf/1301.3781.pdf)

## âœï¸ PrzykÅ‚adowe notatniki: Trenowanie modelu CBoW

Kontynuuj naukÄ™, korzystajÄ…c z poniÅ¼szych notatnikÃ³w:

* [Trenowanie CBoW Word2Vec z TensorFlow](../../../../../lessons/5-NLP/15-LanguageModeling/CBoW-TF.ipynb)
* [Trenowanie CBoW Word2Vec z PyTorch](../../../../../lessons/5-NLP/15-LanguageModeling/CBoW-PyTorch.ipynb)

## Podsumowanie

W poprzedniej lekcji widzieliÅ›my, Å¼e osadzenia sÅ‚Ã³w dziaÅ‚ajÄ… jak magia! Teraz wiemy, Å¼e trenowanie osadzeÅ„ sÅ‚Ã³w nie jest bardzo skomplikowanym zadaniem i powinniÅ›my byÄ‡ w stanie wytrenowaÄ‡ wÅ‚asne osadzenia sÅ‚Ã³w dla tekstÃ³w specyficznych dla danej dziedziny, jeÅ›li zajdzie taka potrzeba.

## [Quiz po wykÅ‚adzie](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/215)

## PrzeglÄ…d i samodzielna nauka

* [Oficjalny samouczek PyTorch dotyczÄ…cy modelowania jÄ™zyka](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html).
* [Oficjalny samouczek TensorFlow dotyczÄ…cy trenowania modelu Word2Vec](https://www.TensorFlow.org/tutorials/text/word2vec).
* UÅ¼ycie frameworka **gensim** do trenowania najczÄ™Å›ciej uÅ¼ywanych osadzeÅ„ w kilku liniach kodu opisano [w tej dokumentacji](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html).

## ğŸš€ [Zadanie: Wytrenuj model Skip-Gram](lab/README.md)

W laboratorium zachÄ™camy CiÄ™ do zmodyfikowania kodu z tej lekcji, aby wytrenowaÄ‡ model skip-gram zamiast CBoW. [Przeczytaj szczegÃ³Å‚y](lab/README.md)

**ZastrzeÅ¼enie**:  
Ten dokument zostaÅ‚ przetÅ‚umaczony za pomocÄ… usÅ‚ugi tÅ‚umaczenia AI [Co-op Translator](https://github.com/Azure/co-op-translator). ChociaÅ¼ dokÅ‚adamy wszelkich staraÅ„, aby tÅ‚umaczenie byÅ‚o precyzyjne, prosimy pamiÄ™taÄ‡, Å¼e automatyczne tÅ‚umaczenia mogÄ… zawieraÄ‡ bÅ‚Ä™dy lub nieÅ›cisÅ‚oÅ›ci. Oryginalny dokument w jego rodzimym jÄ™zyku powinien byÄ‡ uznawany za autorytatywne ÅºrÃ³dÅ‚o. W przypadku informacji o kluczowym znaczeniu zaleca siÄ™ skorzystanie z profesjonalnego tÅ‚umaczenia przez czÅ‚owieka. Nie ponosimy odpowiedzialnoÅ›ci za jakiekolwiek nieporozumienia lub bÅ‚Ä™dne interpretacje wynikajÄ…ce z uÅ¼ycia tego tÅ‚umaczenia.