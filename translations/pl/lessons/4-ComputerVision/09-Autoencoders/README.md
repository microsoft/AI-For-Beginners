# Autoenkodery

Podczas trenowania CNN jednym z problemÃ³w jest potrzeba duÅ¼ej iloÅ›ci danych oznaczonych. W przypadku klasyfikacji obrazÃ³w musimy podzieliÄ‡ obrazy na rÃ³Å¼ne klasy, co wymaga rÄ™cznego wysiÅ‚ku.

## [Quiz przed wykÅ‚adem](https://ff-quizzes.netlify.app/en/ai/quiz/17)

MoÅ¼emy jednak chcieÄ‡ wykorzystaÄ‡ surowe (nieoznaczone) dane do trenowania ekstraktorÃ³w cech CNN, co nazywa siÄ™ **uczeniem samonadzorowanym**. Zamiast etykiet, uÅ¼yjemy obrazÃ³w treningowych zarÃ³wno jako wejÅ›cia, jak i wyjÅ›cia sieci. GÅ‚Ã³wna idea **autoenkodera** polega na tym, Å¼e bÄ™dziemy mieÄ‡ **sieÄ‡ enkodera**, ktÃ³ra przeksztaÅ‚ca obraz wejÅ›ciowy w pewnÄ… **przestrzeÅ„ latentnÄ…** (zwykle jest to wektor o mniejszym rozmiarze), a nastÄ™pnie **sieÄ‡ dekodera**, ktÃ³rej celem bÄ™dzie odtworzenie oryginalnego obrazu.

> âœ… [Autoenkoder](https://wikipedia.org/wiki/Autoencoder) to "rodzaj sztucznej sieci neuronowej uÅ¼ywanej do nauki efektywnego kodowania nieoznaczonych danych."

PoniewaÅ¼ trenujemy autoenkoder, aby uchwyciÄ‡ jak najwiÄ™cej informacji z oryginalnego obrazu w celu dokÅ‚adnej rekonstrukcji, sieÄ‡ stara siÄ™ znaleÅºÄ‡ najlepsze **osadzenie** obrazÃ³w wejÅ›ciowych, aby uchwyciÄ‡ ich znaczenie.

![Schemat Autoenkodera](../../../../../translated_images/pl/autoencoder_schema.5e6fc9ad98a5eb61.webp)

> Obraz z [blogu Keras](https://blog.keras.io/building-autoencoders-in-keras.html)

## Scenariusze uÅ¼ycia AutoenkoderÃ³w

ChociaÅ¼ samo odtwarzanie oryginalnych obrazÃ³w moÅ¼e wydawaÄ‡ siÄ™ maÅ‚o uÅ¼yteczne, istnieje kilka scenariuszy, w ktÃ³rych autoenkodery sÄ… szczegÃ³lnie przydatne:

* **ObniÅ¼anie wymiaru obrazÃ³w do wizualizacji** lub **trenowanie osadzeÅ„ obrazÃ³w**. Zwykle autoenkodery dajÄ… lepsze wyniki niÅ¼ PCA, poniewaÅ¼ uwzglÄ™dniajÄ… przestrzennÄ… naturÄ™ obrazÃ³w i hierarchiczne cechy.
* **Usuwanie szumÃ³w**, czyli eliminowanie zakÅ‚Ã³ceÅ„ z obrazu. PoniewaÅ¼ szum zawiera wiele niepotrzebnych informacji, autoenkoder nie jest w stanie zmieÅ›ciÄ‡ ich wszystkich w stosunkowo maÅ‚ej przestrzeni latentnej, dziÄ™ki czemu uchwyca tylko istotne czÄ™Å›ci obrazu. Podczas trenowania usuwania szumÃ³w zaczynamy od oryginalnych obrazÃ³w i uÅ¼ywamy obrazÃ³w z sztucznie dodanym szumem jako wejÅ›cia dla autoenkodera.
* **Super-rozdzielczoÅ›Ä‡**, czyli zwiÄ™kszanie rozdzielczoÅ›ci obrazu. Zaczynamy od obrazÃ³w o wysokiej rozdzielczoÅ›ci i uÅ¼ywamy obrazu o niÅ¼szej rozdzielczoÅ›ci jako wejÅ›cia dla autoenkodera.
* **Modele generatywne**. Po przeszkoleniu autoenkodera, czÄ™Å›Ä‡ dekodera moÅ¼e byÄ‡ uÅ¼ywana do tworzenia nowych obiektÃ³w, zaczynajÄ…c od losowych wektorÃ³w latentnych.

## Wariacyjne Autoenkodery (VAE)

Tradycyjne autoenkodery redukujÄ… wymiar danych wejÅ›ciowych, identyfikujÄ…c waÅ¼ne cechy obrazÃ³w wejÅ›ciowych. Jednak wektory latentne czÄ™sto nie majÄ… wiÄ™kszego sensu. Innymi sÅ‚owy, biorÄ…c za przykÅ‚ad zbiÃ³r danych MNIST, ustalenie, ktÃ³re cyfry odpowiadajÄ… rÃ³Å¼nym wektorom latentnym, nie jest Å‚atwym zadaniem, poniewaÅ¼ bliskie wektory latentne niekoniecznie odpowiadajÄ… tym samym cyfrom.

Z drugiej strony, aby trenowaÄ‡ modele *generatywne*, lepiej mieÄ‡ pewne zrozumienie przestrzeni latentnej. Ta idea prowadzi nas do **wariacyjnego autoenkodera** (VAE).

VAE to autoenkoder, ktÃ³ry uczy siÄ™ przewidywaÄ‡ *rozkÅ‚ad statystyczny* parametrÃ³w latentnych, tzw. **rozkÅ‚ad latentny**. Na przykÅ‚ad moÅ¼emy chcieÄ‡, aby wektory latentne byÅ‚y rozÅ‚oÅ¼one normalnie z pewnÄ… Å›redniÄ… z<sub>mean</sub> i odchyleniem standardowym z<sub>sigma</sub> (zarÃ³wno Å›rednia, jak i odchylenie standardowe sÄ… wektorami o pewnej wymiarowoÅ›ci d). Enkoder w VAE uczy siÄ™ przewidywaÄ‡ te parametry, a nastÄ™pnie dekoder bierze losowy wektor z tego rozkÅ‚adu, aby odtworzyÄ‡ obiekt.

PodsumowujÄ…c:

 * Z wektora wejÅ›ciowego przewidujemy `z_mean` i `z_log_sigma` (zamiast przewidywaÄ‡ samo odchylenie standardowe, przewidujemy jego logarytm)
 * Pobieramy prÃ³bkÄ™ wektora `sample` z rozkÅ‚adu N(z<sub>mean</sub>,exp(z<sub>log\_sigma</sub>))
 * Dekoder prÃ³buje odtworzyÄ‡ oryginalny obraz, uÅ¼ywajÄ…c `sample` jako wektora wejÅ›ciowego

 <img src="../../../../../translated_images/pl/vae.464c465a5b6a9e25.webp" width="50%">

> Obraz z [tego wpisu na blogu](https://ijdykeman.github.io/ml/2016/12/21/cvae.html) autorstwa Isaaka Dykemana

Wariacyjne autoenkodery uÅ¼ywajÄ… zÅ‚oÅ¼onej funkcji strat, ktÃ³ra skÅ‚ada siÄ™ z dwÃ³ch czÄ™Å›ci:

* **Strata rekonstrukcji** to funkcja strat, ktÃ³ra pokazuje, jak blisko odtworzony obraz jest do celu (moÅ¼e to byÄ‡ Mean Squared Error, czyli MSE). Jest to ta sama funkcja strat, co w zwykÅ‚ych autoenkoderach.
* **Strata KL**, ktÃ³ra zapewnia, Å¼e rozkÅ‚ady zmiennych latentnych pozostajÄ… bliskie rozkÅ‚adowi normalnemu. Opiera siÄ™ na pojÄ™ciu [dywergencji Kullbacka-Leiblera](https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained) - metryki do oszacowania, jak podobne sÄ… dwa rozkÅ‚ady statystyczne.

JednÄ… z waÅ¼nych zalet VAE jest to, Å¼e pozwalajÄ… na stosunkowo Å‚atwe generowanie nowych obrazÃ³w, poniewaÅ¼ wiemy, z jakiego rozkÅ‚adu pobieraÄ‡ wektory latentne. Na przykÅ‚ad, jeÅ›li przeszkolimy VAE z 2-wymiarowym wektorem latentnym na MNIST, moÅ¼emy nastÄ™pnie zmieniaÄ‡ komponenty wektora latentnego, aby uzyskaÄ‡ rÃ³Å¼ne cyfry:

<img alt="vaemnist" src="../../../../../translated_images/pl/vaemnist.cab9e602dc08dc50.webp" width="50%"/>

> Obraz autorstwa [Dmitrija Soshnikova](http://soshnikov.com)

ZauwaÅ¼, jak obrazy pÅ‚ynnie przechodzÄ… jeden w drugi, gdy zaczynamy pobieraÄ‡ wektory latentne z rÃ³Å¼nych czÄ™Å›ci przestrzeni parametrÃ³w latentnych. MoÅ¼emy rÃ³wnieÅ¼ zwizualizowaÄ‡ tÄ™ przestrzeÅ„ w 2D:

<img alt="vaemnist cluster" src="../../../../../translated_images/pl/vaemnist-diag.694315f775d5d666.webp" width="50%"/> 

> Obraz autorstwa [Dmitrija Soshnikova](http://soshnikov.com)

## âœï¸ Ä†wiczenia: Autoenkodery

Dowiedz siÄ™ wiÄ™cej o autoenkoderach w tych odpowiednich notatnikach:

* [Autoenkodery w TensorFlow](AutoencodersTF.ipynb)
* [Autoenkodery w PyTorch](AutoEncodersPyTorch.ipynb)

## WÅ‚aÅ›ciwoÅ›ci AutoenkoderÃ³w

* **Specyficzne dla danych** - dziaÅ‚ajÄ… dobrze tylko na typie obrazÃ³w, na ktÃ³rych zostaÅ‚y przeszkolone. Na przykÅ‚ad, jeÅ›li przeszkolimy sieÄ‡ super-rozdzielczoÅ›ci na kwiatach, nie bÄ™dzie dobrze dziaÅ‚aÄ‡ na portretach. Dzieje siÄ™ tak, poniewaÅ¼ sieÄ‡ moÅ¼e tworzyÄ‡ obrazy o wyÅ¼szej rozdzielczoÅ›ci, wykorzystujÄ…c szczegÃ³Å‚y z cech wyuczonych na zbiorze danych treningowych.
* **Stratne** - odtworzony obraz nie jest identyczny z oryginalnym obrazem. Charakter strat jest definiowany przez *funkcjÄ™ strat* uÅ¼ywanÄ… podczas treningu.
* DziaÅ‚a na **danych nieoznaczonych**

## [Quiz po wykÅ‚adzie](https://ff-quizzes.netlify.app/en/ai/quiz/18)

## Podsumowanie

W tej lekcji dowiedziaÅ‚eÅ› siÄ™ o rÃ³Å¼nych typach autoenkoderÃ³w dostÄ™pnych dla naukowca AI. NauczyÅ‚eÅ› siÄ™, jak je budowaÄ‡ i jak uÅ¼ywaÄ‡ ich do odtwarzania obrazÃ³w. DowiedziaÅ‚eÅ› siÄ™ rÃ³wnieÅ¼ o VAE i jak uÅ¼ywaÄ‡ go do generowania nowych obrazÃ³w.

## ğŸš€ Wyzwanie

W tej lekcji dowiedziaÅ‚eÅ› siÄ™ o uÅ¼ywaniu autoenkoderÃ³w do obrazÃ³w. Ale mogÄ… byÄ‡ rÃ³wnieÅ¼ uÅ¼ywane do muzyki! SprawdÅº projekt Magenta [MusicVAE](https://magenta.tensorflow.org/music-vae), ktÃ³ry wykorzystuje autoenkodery do nauki odtwarzania muzyki. PrzeprowadÅº kilka [eksperymentÃ³w](https://colab.research.google.com/github/magenta/magenta-demos/blob/master/colab-notebooks/Multitrack_MusicVAE.ipynb) z tÄ… bibliotekÄ…, aby zobaczyÄ‡, co moÅ¼esz stworzyÄ‡.

## [Quiz po wykÅ‚adzie](https://ff-quizzes.netlify.app/en/ai/quiz/16)

## PrzeglÄ…d i samodzielna nauka

Dla odniesienia, przeczytaj wiÄ™cej o autoenkoderach w tych zasobach:

* [Budowanie AutoenkoderÃ³w w Keras](https://blog.keras.io/building-autoencoders-in-keras.html)
* [Wpis na blogu NeuroHive](https://neurohive.io/ru/osnovy-data-science/variacionnyj-avtojenkoder-vae/)
* [Wariacyjne Autoenkodery WyjaÅ›nione](https://kvfrans.com/variational-autoencoders-explained/)
* [Wariacyjne Autoenkodery Warunkowe](https://ijdykeman.github.io/ml/2016/12/21/cvae.html)

## Zadanie

Na koÅ„cu [tego notatnika uÅ¼ywajÄ…cego TensorFlow](AutoencodersTF.ipynb) znajdziesz 'zadanie' - uÅ¼yj tego jako swojego zadania.

---

