# Generative Adversarial Networks

W poprzedniej sekcji poznaliÅ›my **modele generatywne**: modele, ktÃ³re potrafiÄ… generowaÄ‡ nowe obrazy podobne do tych z zestawu treningowego. VAE byÅ‚ dobrym przykÅ‚adem modelu generatywnego.

## [Pre-lecture quiz](https://ff-quizzes.netlify.app/en/ai/quiz/19)

JednakÅ¼e, jeÅ›li sprÃ³bujemy wygenerowaÄ‡ coÅ› naprawdÄ™ znaczÄ…cego, na przykÅ‚ad obraz w rozsÄ…dnej rozdzielczoÅ›ci, za pomocÄ… VAE, zauwaÅ¼ymy, Å¼e trening nie przebiega dobrze. W takim przypadku warto poznaÄ‡ innÄ… architekturÄ™, specjalnie zaprojektowanÄ… dla modeli generatywnych - **Generative Adversarial Networks**, czyli GAN.

GÅ‚Ã³wna idea GAN polega na wykorzystaniu dwÃ³ch sieci neuronowych, ktÃ³re bÄ™dÄ… trenowane przeciwko sobie:

<img src="../../../../../translated_images/pl/gan_architecture.8f3a5ab62b8d5d69.webp" width="70%"/>

> Obraz autorstwa [Dmitry Soshnikov](http://soshnikov.com)

> âœ… MaÅ‚y sÅ‚owniczek:
> * **Generator** to sieÄ‡, ktÃ³ra przyjmuje losowy wektor i generuje obraz jako wynik.
> * **Dyskryminator** to sieÄ‡, ktÃ³ra przyjmuje obraz i ma za zadanie okreÅ›liÄ‡, czy jest to prawdziwy obraz (z zestawu treningowego), czy zostaÅ‚ wygenerowany przez generator. W zasadzie jest to klasyfikator obrazÃ³w.

### Dyskryminator

Architektura dyskryminatora nie rÃ³Å¼ni siÄ™ od zwykÅ‚ej sieci klasyfikacji obrazÃ³w. W najprostszej wersji moÅ¼e to byÄ‡ klasyfikator w peÅ‚ni poÅ‚Ä…czony, ale najprawdopodobniej bÄ™dzie to [sieÄ‡ konwolucyjna](../07-ConvNets/README.md).

> âœ… GAN oparty na sieciach konwolucyjnych nazywany jest [DCGAN](https://arxiv.org/pdf/1511.06434.pdf)

Dyskryminator CNN skÅ‚ada siÄ™ z nastÄ™pujÄ…cych warstw: kilku konwolucji+poolingÃ³w (z malejÄ…cym rozmiarem przestrzennym) oraz jednej lub wiÄ™cej warstw w peÅ‚ni poÅ‚Ä…czonych, aby uzyskaÄ‡ "wektor cech", a na koÅ„cu klasyfikator binarny.

> âœ… 'Pooling' w tym kontekÅ›cie to technika zmniejszania rozmiaru obrazu. "Warstwy pooling zmniejszajÄ… wymiary danych, Å‚Ä…czÄ…c wyniki klastrÃ³w neuronÃ³w na jednej warstwie w pojedynczy neuron na nastÄ™pnej warstwie." - [ÅºrÃ³dÅ‚o](https://wikipedia.org/wiki/Convolutional_neural_network#Pooling_layers)

### Generator

Generator jest nieco bardziej skomplikowany. MoÅ¼na go traktowaÄ‡ jako odwrÃ³cony dyskryminator. ZaczynajÄ…c od wektora latentnego (zamiast wektora cech), ma warstwÄ™ w peÅ‚ni poÅ‚Ä…czonÄ…, ktÃ³ra przeksztaÅ‚ca go w wymagany rozmiar/ksztaÅ‚t, a nastÄ™pnie dekonwolucje+skalowanie w gÃ³rÄ™. Jest to podobne do czÄ™Å›ci *dekodera* w [autoenkoderze](../09-Autoencoders/README.md).

> âœ… PoniewaÅ¼ warstwa konwolucyjna jest implementowana jako filtr liniowy przesuwajÄ…cy siÄ™ po obrazie, dekonwolucja jest zasadniczo podobna do konwolucji i moÅ¼e byÄ‡ zaimplementowana przy uÅ¼yciu tej samej logiki warstwy.

<img src="../../../../../translated_images/pl/gan_arch_detail.46b95fd366f8e543.webp" width="70%"/>

> Obraz autorstwa [Dmitry Soshnikov](http://soshnikov.com)

### Trenowanie GAN

GAN nazywane sÄ… **adwersarialnymi**, poniewaÅ¼ istnieje ciÄ…gÅ‚a rywalizacja miÄ™dzy generatorem a dyskryminatorem. W trakcie tej rywalizacji zarÃ³wno generator, jak i dyskryminator poprawiajÄ… siÄ™, dziÄ™ki czemu sieÄ‡ uczy siÄ™ generowaÄ‡ coraz lepsze obrazy.

Trening odbywa siÄ™ w dwÃ³ch etapach:

* **Trenowanie dyskryminatora**. To zadanie jest doÅ›Ä‡ proste: generujemy partiÄ™ obrazÃ³w za pomocÄ… generatora, oznaczajÄ…c je jako 0, co oznacza faÅ‚szywy obraz, oraz bierzemy partiÄ™ obrazÃ³w z zestawu wejÅ›ciowego (z etykietÄ… 1, prawdziwy obraz). Obliczamy *stratÄ™ dyskryminatora* i wykonujemy backprop.
* **Trenowanie generatora**. To jest nieco bardziej skomplikowane, poniewaÅ¼ nie znamy oczekiwanego wyniku dla generatora bezpoÅ›rednio. Bierzemy caÅ‚Ä… sieÄ‡ GAN skÅ‚adajÄ…cÄ… siÄ™ z generatora i dyskryminatora, podajemy jej losowe wektory i oczekujemy, Å¼e wynik bÄ™dzie 1 (odpowiadajÄ…cy prawdziwym obrazom). NastÄ™pnie zamraÅ¼amy parametry dyskryminatora (nie chcemy, aby byÅ‚ trenowany na tym etapie) i wykonujemy backprop.

Podczas tego procesu zarÃ³wno straty generatora, jak i dyskryminatora nie zmniejszajÄ… siÄ™ znaczÄ…co. W idealnej sytuacji powinny oscylowaÄ‡, co odpowiada poprawie wydajnoÅ›ci obu sieci.

## âœï¸ Ä†wiczenia: GAN

* [Notebook GAN w TensorFlow/Keras](GANTF.ipynb)
* [Notebook GAN w PyTorch](GANPyTorch.ipynb)

### Problemy z trenowaniem GAN

GAN sÄ… znane z tego, Å¼e sÄ… szczegÃ³lnie trudne do trenowania. Oto kilka problemÃ³w:

* **Mode Collapse**. Termin ten oznacza, Å¼e generator uczy siÄ™ generowaÄ‡ jeden udany obraz, ktÃ³ry oszukuje dyskryminator, zamiast rÃ³Å¼norodnych obrazÃ³w.
* **WraÅ¼liwoÅ›Ä‡ na hiperparametry**. CzÄ™sto moÅ¼na zauwaÅ¼yÄ‡, Å¼e GAN w ogÃ³le siÄ™ nie zbiega, a potem nagle zmniejszenie wspÃ³Å‚czynnika uczenia prowadzi do zbieÅ¼noÅ›ci.
* Utrzymanie **rÃ³wnowagi** miÄ™dzy generatorem a dyskryminatorem. W wielu przypadkach strata dyskryminatora moÅ¼e szybko spaÅ›Ä‡ do zera, co powoduje, Å¼e generator nie jest w stanie dalej siÄ™ uczyÄ‡. Aby temu zaradziÄ‡, moÅ¼na sprÃ³bowaÄ‡ ustawiÄ‡ rÃ³Å¼ne wspÃ³Å‚czynniki uczenia dla generatora i dyskryminatora lub pominÄ…Ä‡ trening dyskryminatora, jeÅ›li strata jest juÅ¼ zbyt niska.
* Trenowanie dla **wysokiej rozdzielczoÅ›ci**. OdzwierciedlajÄ…c ten sam problem co w autoenkoderach, problem ten pojawia siÄ™, poniewaÅ¼ odtworzenie zbyt wielu warstw sieci konwolucyjnej prowadzi do artefaktÃ³w. Problem ten jest zazwyczaj rozwiÄ…zywany za pomocÄ… tzw. **progresywnego wzrostu**, gdzie najpierw kilka warstw jest trenowanych na obrazach o niskiej rozdzielczoÅ›ci, a nastÄ™pnie warstwy sÄ… "odblokowywane" lub dodawane. Innym rozwiÄ…zaniem byÅ‚oby dodanie dodatkowych poÅ‚Ä…czeÅ„ miÄ™dzy warstwami i trenowanie kilku rozdzielczoÅ›ci jednoczeÅ›nie - szczegÃ³Å‚y moÅ¼na znaleÅºÄ‡ w artykule [Multi-Scale Gradient GANs](https://arxiv.org/abs/1903.06048).

## Transfer stylu

GAN to Å›wietny sposÃ³b na generowanie artystycznych obrazÃ³w. InnÄ… interesujÄ…cÄ… technikÄ… jest tzw. **transfer stylu**, ktÃ³ry bierze jeden **obraz treÅ›ci** i przeksztaÅ‚ca go w inny styl, stosujÄ…c filtry z **obrazu stylu**.

Jak to dziaÅ‚a:
* Zaczynamy od losowego obrazu szumu (lub od obrazu treÅ›ci, ale dla lepszego zrozumienia Å‚atwiej zaczÄ…Ä‡ od losowego szumu).
* Naszym celem jest stworzenie takiego obrazu, ktÃ³ry bÄ™dzie bliski zarÃ³wno obrazowi treÅ›ci, jak i obrazowi stylu. To bÄ™dzie okreÅ›lone przez dwie funkcje strat:
   - **Strata treÅ›ci** jest obliczana na podstawie cech wyodrÄ™bnionych przez CNN na niektÃ³rych warstwach z bieÅ¼Ä…cego obrazu i obrazu treÅ›ci.
   - **Strata stylu** jest obliczana miÄ™dzy bieÅ¼Ä…cym obrazem a obrazem stylu w sprytny sposÃ³b za pomocÄ… macierzy Grama (wiÄ™cej szczegÃ³Å‚Ã³w w [przykÅ‚adowym notebooku](StyleTransfer.ipynb)).
* Aby wygÅ‚adziÄ‡ obraz i usunÄ…Ä‡ szum, wprowadzamy rÃ³wnieÅ¼ **StratÄ™ wariacji**, ktÃ³ra oblicza Å›redniÄ… odlegÅ‚oÅ›Ä‡ miÄ™dzy sÄ…siednimi pikselami.
* GÅ‚Ã³wna pÄ™tla optymalizacyjna dostosowuje bieÅ¼Ä…cy obraz za pomocÄ… gradientu (lub innego algorytmu optymalizacji), aby zminimalizowaÄ‡ caÅ‚kowitÄ… stratÄ™, ktÃ³ra jest waÅ¼onÄ… sumÄ… wszystkich trzech strat.

## âœï¸ PrzykÅ‚ad: [Transfer stylu](StyleTransfer.ipynb)

## [Post-lecture quiz](https://ff-quizzes.netlify.app/en/ai/quiz/20)

## Podsumowanie

W tej lekcji dowiedziaÅ‚eÅ› siÄ™ o GAN i jak je trenowaÄ‡. PoznaÅ‚eÅ› rÃ³wnieÅ¼ szczegÃ³lne wyzwania, z ktÃ³rymi moÅ¼e siÄ™ zmierzyÄ‡ ten typ sieci neuronowej, oraz strategie, jak je pokonaÄ‡.

## ğŸš€ Wyzwanie

PrzejdÅº przez [notebook Transfer stylu](StyleTransfer.ipynb), uÅ¼ywajÄ…c wÅ‚asnych obrazÃ³w.

## PrzeglÄ…d i samodzielna nauka

Dla odniesienia, przeczytaj wiÄ™cej o GAN w tych zasobach:

* Marco Pasini, [10 Lessons I Learned Training GANs for one Year](https://towardsdatascience.com/10-lessons-i-learned-training-generative-adversarial-networks-gans-for-a-year-c9071159628)
* [StyleGAN](https://en.wikipedia.org/wiki/StyleGAN), architektura GAN, ktÃ³rÄ… warto rozwaÅ¼yÄ‡
* [Creating Generative Art using GANs on Azure ML](https://soshnikov.com/scienceart/creating-generative-art-using-gan-on-azureml/)

## Zadanie

Przejrzyj jeden z dwÃ³ch notebookÃ³w zwiÄ…zanych z tÄ… lekcjÄ… i ponownie wytrenuj GAN na wÅ‚asnych obrazach. Co moÅ¼esz stworzyÄ‡?

---

