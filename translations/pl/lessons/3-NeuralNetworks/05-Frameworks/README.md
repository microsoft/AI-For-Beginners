# Frameworky Sieci Neuronowych

Jak juÅ¼ siÄ™ nauczyliÅ›my, aby efektywnie trenowaÄ‡ sieci neuronowe, musimy zrobiÄ‡ dwie rzeczy:

* OperowaÄ‡ na tensorach, np. mnoÅ¼yÄ‡, dodawaÄ‡ i obliczaÄ‡ funkcje takie jak sigmoid czy softmax
* ObliczaÄ‡ gradienty wszystkich wyraÅ¼eÅ„, aby przeprowadzaÄ‡ optymalizacjÄ™ metodÄ… gradientu prostego

## [Quiz przed wykÅ‚adem](https://ff-quizzes.netlify.app/en/ai/quiz/9)

Biblioteka `numpy` moÅ¼e zrealizowaÄ‡ pierwszy punkt, ale potrzebujemy mechanizmu do obliczania gradientÃ³w. W [naszym frameworku](../04-OwnFramework/OwnFramework.ipynb), ktÃ³ry opracowaliÅ›my w poprzedniej sekcji, musieliÅ›my rÄ™cznie programowaÄ‡ wszystkie funkcje pochodne w metodzie `backward`, ktÃ³ra wykonuje propagacjÄ™ wstecznÄ…. Idealnie byÅ‚oby, gdyby framework umoÅ¼liwiaÅ‚ obliczanie gradientÃ³w *dowolnego wyraÅ¼enia*, ktÃ³re moÅ¼emy zdefiniowaÄ‡.

KolejnÄ… waÅ¼nÄ… rzeczÄ… jest moÅ¼liwoÅ›Ä‡ wykonywania obliczeÅ„ na GPU lub innych wyspecjalizowanych jednostkach obliczeniowych, takich jak [TPU](https://en.wikipedia.org/wiki/Tensor_Processing_Unit). Trenowanie gÅ‚Ä™bokich sieci neuronowych wymaga *bardzo duÅ¼ej* iloÅ›ci obliczeÅ„, a moÅ¼liwoÅ›Ä‡ ich rÃ³wnolegÅ‚ego wykonywania na GPU jest niezwykle istotna.

> âœ… Termin 'rÃ³wnolegÅ‚oÅ›Ä‡' oznacza rozdzielenie obliczeÅ„ na wiele urzÄ…dzeÅ„.

Obecnie dwa najpopularniejsze frameworki sieci neuronowych to: [TensorFlow](http://TensorFlow.org) i [PyTorch](https://pytorch.org/). Oba oferujÄ… niskopoziomowe API do operowania na tensorach zarÃ³wno na CPU, jak i GPU. Na bazie niskopoziomowego API istnieje rÃ³wnieÅ¼ wyÅ¼szy poziom API, zwany odpowiednio [Keras](https://keras.io/) i [PyTorch Lightning](https://pytorchlightning.ai/).

Niskopoziomowe API | [TensorFlow](http://TensorFlow.org) | [PyTorch](https://pytorch.org/)
-------------------|-------------------------------------|--------------------------------
Wysokopoziomowe API| [Keras](https://keras.io/) | [PyTorch Lightning](https://pytorchlightning.ai/)

**Niskopoziomowe API** w obu frameworkach pozwala na budowanie tzw. **grafÃ³w obliczeniowych**. Graf ten definiuje, jak obliczyÄ‡ wynik (zazwyczaj funkcjÄ™ straty) dla danych wejÅ›ciowych i moÅ¼e byÄ‡ przesÅ‚any do obliczeÅ„ na GPU, jeÅ›li jest dostÄ™pne. IstniejÄ… funkcje do rÃ³Å¼niczkowania tego grafu obliczeniowego i obliczania gradientÃ³w, ktÃ³re mogÄ… byÄ‡ nastÄ™pnie uÅ¼yte do optymalizacji parametrÃ³w modelu.

**Wysokopoziomowe API** traktuje sieci neuronowe jako **sekwencjÄ™ warstw**, co znacznie uÅ‚atwia konstruowanie wiÄ™kszoÅ›ci sieci neuronowych. Trenowanie modelu zazwyczaj wymaga przygotowania danych, a nastÄ™pnie wywoÅ‚ania funkcji `fit`, ktÃ³ra wykonuje caÅ‚Ä… pracÄ™.

Wysokopoziomowe API pozwala na szybkie konstruowanie typowych sieci neuronowych bez koniecznoÅ›ci martwienia siÄ™ o wiele szczegÃ³Å‚Ã³w. Z kolei niskopoziomowe API daje duÅ¼o wiÄ™kszÄ… kontrolÄ™ nad procesem trenowania, dlatego jest czÄ™sto uÅ¼ywane w badaniach, gdy pracujemy z nowymi architekturami sieci neuronowych.

Warto rÃ³wnieÅ¼ zrozumieÄ‡, Å¼e oba API moÅ¼na uÅ¼ywaÄ‡ razem, np. moÅ¼na opracowaÄ‡ wÅ‚asnÄ… architekturÄ™ warstwy sieci za pomocÄ… niskopoziomowego API, a nastÄ™pnie uÅ¼yÄ‡ jej w wiÄ™kszej sieci skonstruowanej i trenowanej za pomocÄ… wysokopoziomowego API. MoÅ¼na teÅ¼ zdefiniowaÄ‡ sieÄ‡ za pomocÄ… wysokopoziomowego API jako sekwencjÄ™ warstw, a nastÄ™pnie uÅ¼yÄ‡ wÅ‚asnej pÄ™tli trenowania na niskim poziomie do optymalizacji. Oba API opierajÄ… siÄ™ na tych samych podstawowych koncepcjach i sÄ… zaprojektowane tak, aby dobrze ze sobÄ… wspÃ³Å‚pracowaÄ‡.

## Nauka

W tym kursie oferujemy wiÄ™kszoÅ›Ä‡ treÅ›ci zarÃ³wno dla PyTorch, jak i TensorFlow. MoÅ¼esz wybraÄ‡ preferowany framework i przejÅ›Ä‡ tylko przez odpowiednie notatniki. JeÅ›li nie jesteÅ› pewien, ktÃ³ry framework wybraÄ‡, przeczytaj dyskusje w internecie na temat **PyTorch vs. TensorFlow**. MoÅ¼esz rÃ³wnieÅ¼ zapoznaÄ‡ siÄ™ z oboma frameworkami, aby lepiej je zrozumieÄ‡.

Tam, gdzie to moÅ¼liwe, bÄ™dziemy uÅ¼ywaÄ‡ wysokopoziomowych API dla uproszczenia. Jednak uwaÅ¼amy, Å¼e waÅ¼ne jest zrozumienie, jak dziaÅ‚ajÄ… sieci neuronowe od podstaw, dlatego na poczÄ…tku pracujemy z niskopoziomowym API i tensorami. JeÅ›li jednak chcesz szybko zaczÄ…Ä‡ i nie chcesz poÅ›wiÄ™caÄ‡ duÅ¼o czasu na naukÄ™ tych szczegÃ³Å‚Ã³w, moÅ¼esz je pominÄ…Ä‡ i przejÅ›Ä‡ od razu do notatnikÃ³w z wysokopoziomowym API.

## âœï¸ Ä†wiczenia: Frameworki

Kontynuuj naukÄ™ w nastÄ™pujÄ…cych notatnikach:

Niskopoziomowe API | [TensorFlow+Keras Notebook](IntroKerasTF.ipynb) | [PyTorch](IntroPyTorch.ipynb)
-------------------|-------------------------------------|--------------------------------
Wysokopoziomowe API| [Keras](IntroKeras.ipynb) | *PyTorch Lightning*

Po opanowaniu frameworkÃ³w, przypomnijmy sobie pojÄ™cie nadmiernego dopasowania.

# Nadmierne dopasowanie (Overfitting)

Nadmierne dopasowanie to niezwykle waÅ¼ne pojÄ™cie w uczeniu maszynowym i bardzo waÅ¼ne jest, aby je dobrze zrozumieÄ‡!

RozwaÅ¼my nastÄ™pujÄ…cy problem aproksymacji 5 punktÃ³w (reprezentowanych przez `x` na poniÅ¼szych wykresach):

![linear](../../../../../translated_images/pl/overfit1.f24b71c6f652e59e.webp) | ![overfit](../../../../../translated_images/pl/overfit2.131f5800ae10ca5e.webp)
-------------------------|--------------------------
**Model liniowy, 2 parametry** | **Model nieliniowy, 7 parametrÃ³w**
BÅ‚Ä…d treningowy = 5.3 | BÅ‚Ä…d treningowy = 0
BÅ‚Ä…d walidacyjny = 5.1 | BÅ‚Ä…d walidacyjny = 20

* Po lewej widzimy dobrÄ… aproksymacjÄ™ prostÄ… liniÄ…. PoniewaÅ¼ liczba parametrÃ³w jest odpowiednia, model dobrze oddaje rozkÅ‚ad punktÃ³w.
* Po prawej model jest zbyt potÄ™Å¼ny. PoniewaÅ¼ mamy tylko 5 punktÃ³w, a model ma 7 parametrÃ³w, moÅ¼e dostosowaÄ‡ siÄ™ tak, aby przechodziÄ‡ przez wszystkie punkty, co sprawia, Å¼e bÅ‚Ä…d treningowy wynosi 0. Jednak uniemoÅ¼liwia to modelowi zrozumienie wÅ‚aÅ›ciwego wzorca w danych, przez co bÅ‚Ä…d walidacyjny jest bardzo wysoki.

Bardzo waÅ¼ne jest znalezienie odpowiedniej rÃ³wnowagi miÄ™dzy zÅ‚oÅ¼onoÅ›ciÄ… modelu (liczbÄ… parametrÃ³w) a liczbÄ… prÃ³bek treningowych.

## Dlaczego wystÄ™puje nadmierne dopasowanie

  * Zbyt maÅ‚o danych treningowych
  * Zbyt potÄ™Å¼ny model
  * Zbyt duÅ¼o szumu w danych wejÅ›ciowych

## Jak wykryÄ‡ nadmierne dopasowanie

Jak widaÄ‡ na powyÅ¼szym wykresie, nadmierne dopasowanie moÅ¼na wykryÄ‡ po bardzo niskim bÅ‚Ä™dzie treningowym i wysokim bÅ‚Ä™dzie walidacyjnym. Zazwyczaj podczas treningu widzimy, Å¼e zarÃ³wno bÅ‚Ä™dy treningowe, jak i walidacyjne zaczynajÄ… siÄ™ zmniejszaÄ‡, a nastÄ™pnie w pewnym momencie bÅ‚Ä…d walidacyjny moÅ¼e przestaÄ‡ siÄ™ zmniejszaÄ‡ i zaczÄ…Ä‡ rosnÄ…Ä‡. To bÄ™dzie oznaka nadmiernego dopasowania i wskazÃ³wka, Å¼e powinniÅ›my prawdopodobnie zatrzymaÄ‡ trening w tym momencie (lub przynajmniej zrobiÄ‡ migawkÄ™ modelu).

![overfitting](../../../../../translated_images/pl/Overfitting.408ad91cd90b4371.webp)

## Jak zapobiegaÄ‡ nadmiernemu dopasowaniu

JeÅ›li zauwaÅ¼ysz, Å¼e wystÄ™puje nadmierne dopasowanie, moÅ¼esz zrobiÄ‡ jedno z poniÅ¼szych:

 * ZwiÄ™kszyÄ‡ iloÅ›Ä‡ danych treningowych
 * ZmniejszyÄ‡ zÅ‚oÅ¼onoÅ›Ä‡ modelu
 * ZastosowaÄ‡ technikÄ™ [regularizacji](../../4-ComputerVision/08-TransferLearning/TrainingTricks.md), takÄ… jak [Dropout](../../4-ComputerVision/08-TransferLearning/TrainingTricks.md#Dropout), ktÃ³rÄ… omÃ³wimy pÃ³Åºniej.

## Nadmierne dopasowanie a kompromis miÄ™dzy bÅ‚Ä™dem uprzedzenia a bÅ‚Ä™dem wariancji

Nadmierne dopasowanie jest w rzeczywistoÅ›ci przypadkiem bardziej ogÃ³lnego problemu w statystyce, zwanego [kompromisem miÄ™dzy bÅ‚Ä™dem uprzedzenia a bÅ‚Ä™dem wariancji](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff). JeÅ›li rozwaÅ¼ymy moÅ¼liwe ÅºrÃ³dÅ‚a bÅ‚Ä™du w naszym modelu, moÅ¼emy wyrÃ³Å¼niÄ‡ dwa typy bÅ‚Ä™dÃ³w:

* **BÅ‚Ä™dy uprzedzenia** wynikajÄ… z tego, Å¼e nasz algorytm nie jest w stanie poprawnie uchwyciÄ‡ relacji miÄ™dzy danymi treningowymi. MoÅ¼e to wynikaÄ‡ z faktu, Å¼e nasz model nie jest wystarczajÄ…co potÄ™Å¼ny (**niedopasowanie**).
* **BÅ‚Ä™dy wariancji**, ktÃ³re wynikajÄ… z tego, Å¼e model aproksymuje szum w danych wejÅ›ciowych zamiast znaczÄ…cej relacji (**nadmierne dopasowanie**).

Podczas treningu bÅ‚Ä…d uprzedzenia maleje (gdy nasz model uczy siÄ™ aproksymowaÄ‡ dane), a bÅ‚Ä…d wariancji roÅ›nie. WaÅ¼ne jest, aby zatrzymaÄ‡ trening - albo rÄ™cznie (gdy wykryjemy nadmierne dopasowanie), albo automatycznie (poprzez wprowadzenie regularizacji) - aby zapobiec nadmiernemu dopasowaniu.

## Podsumowanie

W tej lekcji dowiedziaÅ‚eÅ› siÄ™ o rÃ³Å¼nicach miÄ™dzy rÃ³Å¼nymi API dla dwÃ³ch najpopularniejszych frameworkÃ³w AI, TensorFlow i PyTorch. Ponadto dowiedziaÅ‚eÅ› siÄ™ o bardzo waÅ¼nym temacie, jakim jest nadmierne dopasowanie.

## ğŸš€ Wyzwanie

W doÅ‚Ä…czonych notatnikach znajdziesz 'zadania' na koÅ„cu; przejdÅº przez notatniki i wykonaj zadania.

## [Quiz po wykÅ‚adzie](https://ff-quizzes.netlify.app/en/ai/quiz/10)

## PrzeglÄ…d i samodzielna nauka

ZrÃ³b badania na nastÄ™pujÄ…ce tematy:

- TensorFlow
- PyTorch
- Nadmierne dopasowanie

Zadaj sobie nastÄ™pujÄ…ce pytania:

- Jaka jest rÃ³Å¼nica miÄ™dzy TensorFlow a PyTorch?
- Jaka jest rÃ³Å¼nica miÄ™dzy nadmiernym dopasowaniem a niedopasowaniem?

## [Zadanie](lab/README.md)

W tym laboratorium masz za zadanie rozwiÄ…zaÄ‡ dwa problemy klasyfikacyjne za pomocÄ… jedno- i wielowarstwowych sieci w peÅ‚ni poÅ‚Ä…czonych, uÅ¼ywajÄ…c PyTorch lub TensorFlow.

* [Instrukcje](lab/README.md)
* [Notatnik](lab/LabFrameworks.ipynb)

---

