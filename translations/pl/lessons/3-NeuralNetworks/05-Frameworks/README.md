<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "2b544f20b796402507fb05a0df893323",
  "translation_date": "2025-08-24T10:41:31+00:00",
  "source_file": "lessons/3-NeuralNetworks/05-Frameworks/README.md",
  "language_code": "pl"
}
-->
# Frameworky Sieci Neuronowych

Jak juÅ¼ siÄ™ nauczyliÅ›my, aby efektywnie trenowaÄ‡ sieci neuronowe, musimy zrobiÄ‡ dwie rzeczy:

* OperowaÄ‡ na tensorach, np. mnoÅ¼yÄ‡, dodawaÄ‡ i obliczaÄ‡ funkcje takie jak sigmoid czy softmax  
* ObliczaÄ‡ gradienty wszystkich wyraÅ¼eÅ„, aby przeprowadzaÄ‡ optymalizacjÄ™ metodÄ… gradientu prostego  

## [Quiz przed wykÅ‚adem](https://ff-quizzes.netlify.app/en/ai/quiz/9)

Podczas gdy biblioteka `numpy` moÅ¼e realizowaÄ‡ pierwszÄ… czÄ™Å›Ä‡, potrzebujemy mechanizmu do obliczania gradientÃ³w. W [naszym frameworku](../../../../../lessons/3-NeuralNetworks/04-OwnFramework/OwnFramework.ipynb), ktÃ³ry opracowaliÅ›my w poprzedniej sekcji, musieliÅ›my rÄ™cznie programowaÄ‡ wszystkie funkcje pochodne w metodzie `backward`, ktÃ³ra realizuje propagacjÄ™ wstecznÄ…. Idealnie byÅ‚oby, gdyby framework umoÅ¼liwiaÅ‚ obliczanie gradientÃ³w *dowolnego wyraÅ¼enia*, ktÃ³re moÅ¼emy zdefiniowaÄ‡.

KolejnÄ… waÅ¼nÄ… rzeczÄ… jest moÅ¼liwoÅ›Ä‡ wykonywania obliczeÅ„ na GPU lub innych wyspecjalizowanych jednostkach obliczeniowych, takich jak [TPU](https://en.wikipedia.org/wiki/Tensor_Processing_Unit). Trenowanie gÅ‚Ä™bokich sieci neuronowych wymaga *ogromnej* liczby obliczeÅ„, a moÅ¼liwoÅ›Ä‡ ich rÃ³wnolegÅ‚ego przetwarzania na GPU jest niezwykle istotna.

> âœ… Termin 'rÃ³wnolegÅ‚oÅ›Ä‡' oznacza rozdzielenie obliczeÅ„ na wiele urzÄ…dzeÅ„.

Obecnie dwa najpopularniejsze frameworki sieci neuronowych to: [TensorFlow](http://TensorFlow.org) i [PyTorch](https://pytorch.org/). Oba oferujÄ… niskopoziomowe API do operowania na tensorach zarÃ³wno na CPU, jak i GPU. Na bazie niskopoziomowego API istniejÄ… rÃ³wnieÅ¼ wyÅ¼szopoziomowe API, takie jak [Keras](https://keras.io/) i [PyTorch Lightning](https://pytorchlightning.ai/).

Low-Level API | [TensorFlow](http://TensorFlow.org) | [PyTorch](https://pytorch.org/)  
--------------|-------------------------------------|--------------------------------  
High-level API| [Keras](https://keras.io/) | [PyTorch Lightning](https://pytorchlightning.ai/)  

**Niskopoziomowe API** w obu frameworkach pozwalajÄ… na budowanie tzw. **grafÃ³w obliczeniowych**. Graf ten definiuje, jak obliczyÄ‡ wynik (zazwyczaj funkcjÄ™ straty) dla danych parametrÃ³w wejÅ›ciowych i moÅ¼e byÄ‡ przesÅ‚any do obliczeÅ„ na GPU, jeÅ›li jest dostÄ™pne. IstniejÄ… funkcje do rÃ³Å¼niczkowania tego grafu obliczeniowego i obliczania gradientÃ³w, ktÃ³re nastÄ™pnie mogÄ… byÄ‡ uÅ¼yte do optymalizacji parametrÃ³w modelu.

**Wysokopoziomowe API** traktujÄ… sieci neuronowe jako **sekwencjÄ™ warstw**, co znacznie uÅ‚atwia budowanie wiÄ™kszoÅ›ci sieci neuronowych. Trenowanie modelu zazwyczaj wymaga przygotowania danych, a nastÄ™pnie wywoÅ‚ania funkcji `fit`, ktÃ³ra wykonuje caÅ‚Ä… pracÄ™.

Wysokopoziomowe API pozwala na szybkie konstruowanie typowych sieci neuronowych bez martwienia siÄ™ o wiele szczegÃ³Å‚Ã³w. JednoczeÅ›nie niskopoziomowe API oferuje znacznie wiÄ™kszÄ… kontrolÄ™ nad procesem trenowania, dlatego jest czÄ™sto uÅ¼ywane w badaniach, gdy pracujemy z nowymi architekturami sieci neuronowych.

Warto rÃ³wnieÅ¼ zrozumieÄ‡, Å¼e moÅ¼na uÅ¼ywaÄ‡ obu API razem, np. moÅ¼na opracowaÄ‡ wÅ‚asnÄ… architekturÄ™ warstwy sieci za pomocÄ… niskopoziomowego API, a nastÄ™pnie uÅ¼yÄ‡ jej w wiÄ™kszej sieci skonstruowanej i trenowanej za pomocÄ… wysokopoziomowego API. MoÅ¼na teÅ¼ zdefiniowaÄ‡ sieÄ‡ za pomocÄ… wysokopoziomowego API jako sekwencjÄ™ warstw, a nastÄ™pnie uÅ¼yÄ‡ wÅ‚asnej pÄ™tli trenowania opartej na niskopoziomowym API do przeprowadzenia optymalizacji. Oba API opierajÄ… siÄ™ na tych samych podstawowych koncepcjach i sÄ… zaprojektowane tak, aby dobrze ze sobÄ… wspÃ³Å‚pracowaÅ‚y.

## Nauka

W tym kursie oferujemy wiÄ™kszoÅ›Ä‡ treÅ›ci zarÃ³wno dla PyTorch, jak i TensorFlow. MoÅ¼esz wybraÄ‡ preferowany framework i przejÅ›Ä‡ tylko przez odpowiednie notatniki. JeÅ›li nie jesteÅ› pewien, ktÃ³ry framework wybraÄ‡, przeczytaj dyskusje w internecie na temat **PyTorch vs. TensorFlow**. MoÅ¼esz rÃ³wnieÅ¼ zapoznaÄ‡ siÄ™ z oboma frameworkami, aby lepiej je zrozumieÄ‡.

Tam, gdzie to moÅ¼liwe, bÄ™dziemy uÅ¼ywaÄ‡ wysokopoziomowych API dla uproszczenia. Jednak uwaÅ¼amy, Å¼e waÅ¼ne jest zrozumienie, jak dziaÅ‚ajÄ… sieci neuronowe od podstaw, dlatego na poczÄ…tku zaczynamy od pracy z niskopoziomowym API i tensorami. JeÅ›li jednak chcesz szybko zaczÄ…Ä‡ i nie chcesz poÅ›wiÄ™caÄ‡ duÅ¼o czasu na naukÄ™ tych szczegÃ³Å‚Ã³w, moÅ¼esz je pominÄ…Ä‡ i od razu przejÅ›Ä‡ do notatnikÃ³w z wysokopoziomowym API.

## âœï¸ Ä†wiczenia: Frameworky

Kontynuuj naukÄ™ w poniÅ¼szych notatnikach:

Low-Level API | [TensorFlow+Keras Notebook](../../../../../lessons/3-NeuralNetworks/05-Frameworks/IntroKerasTF.ipynb) | [PyTorch](../../../../../lessons/3-NeuralNetworks/05-Frameworks/IntroPyTorch.ipynb)  
--------------|-------------------------------------|--------------------------------  
High-level API| [Keras](../../../../../lessons/3-NeuralNetworks/05-Frameworks/IntroKeras.ipynb) | *PyTorch Lightning*  

Po opanowaniu frameworkÃ³w, przypomnijmy sobie pojÄ™cie nadmiernego dopasowania.

# Nadmierne dopasowanie (Overfitting)

Nadmierne dopasowanie to niezwykle waÅ¼ne pojÄ™cie w uczeniu maszynowym i bardzo waÅ¼ne jest, aby je dobrze zrozumieÄ‡!

RozwaÅ¼my nastÄ™pujÄ…cy problem aproksymacji 5 punktÃ³w (reprezentowanych przez `x` na poniÅ¼szych wykresach):

![linear](../../../../../lessons/3-NeuralNetworks/images/overfit1.jpg) | ![overfit](../../../../../lessons/3-NeuralNetworks/images/overfit2.jpg)  
-------------------------|--------------------------  
**Model liniowy, 2 parametry** | **Model nieliniowy, 7 parametrÃ³w**  
BÅ‚Ä…d treningowy = 5.3 | BÅ‚Ä…d treningowy = 0  
BÅ‚Ä…d walidacyjny = 5.1 | BÅ‚Ä…d walidacyjny = 20  

* Po lewej widzimy dobrÄ… aproksymacjÄ™ liniowÄ…. PoniewaÅ¼ liczba parametrÃ³w jest odpowiednia, model poprawnie odczytuje rozkÅ‚ad punktÃ³w.  
* Po prawej model jest zbyt potÄ™Å¼ny. PoniewaÅ¼ mamy tylko 5 punktÃ³w, a model ma 7 parametrÃ³w, moÅ¼e dostosowaÄ‡ siÄ™ tak, aby przechodziÄ‡ przez wszystkie punkty, co sprawia, Å¼e bÅ‚Ä…d treningowy wynosi 0. Jednak uniemoÅ¼liwia to modelowi zrozumienie prawidÅ‚owego wzorca w danych, co skutkuje bardzo wysokim bÅ‚Ä™dem walidacyjnym.  

Bardzo waÅ¼ne jest znalezienie odpowiedniej rÃ³wnowagi miÄ™dzy zÅ‚oÅ¼onoÅ›ciÄ… modelu (liczbÄ… parametrÃ³w) a liczbÄ… prÃ³bek treningowych.

## Dlaczego wystÄ™puje nadmierne dopasowanie

  * Zbyt maÅ‚o danych treningowych  
  * Zbyt potÄ™Å¼ny model  
  * Zbyt duÅ¼o szumu w danych wejÅ›ciowych  

## Jak wykryÄ‡ nadmierne dopasowanie

Jak widaÄ‡ na powyÅ¼szym wykresie, nadmierne dopasowanie moÅ¼na wykryÄ‡ poprzez bardzo niski bÅ‚Ä…d treningowy i wysoki bÅ‚Ä…d walidacyjny. Zazwyczaj podczas treningu widzimy, jak zarÃ³wno bÅ‚Ä™dy treningowe, jak i walidacyjne zaczynajÄ… maleÄ‡, a nastÄ™pnie w pewnym momencie bÅ‚Ä…d walidacyjny moÅ¼e przestaÄ‡ maleÄ‡ i zaczÄ…Ä‡ rosnÄ…Ä‡. To bÄ™dzie oznaka nadmiernego dopasowania i wskazÃ³wka, Å¼e powinniÅ›my prawdopodobnie zatrzymaÄ‡ trening w tym momencie (lub przynajmniej zapisaÄ‡ stan modelu).

![overfitting](../../../../../lessons/3-NeuralNetworks/images/Overfitting.png)

## Jak zapobiegaÄ‡ nadmiernemu dopasowaniu

JeÅ›li zauwaÅ¼ysz, Å¼e wystÄ™puje nadmierne dopasowanie, moÅ¼esz zrobiÄ‡ jedno z poniÅ¼szych:

 * ZwiÄ™kszyÄ‡ iloÅ›Ä‡ danych treningowych  
 * ZmniejszyÄ‡ zÅ‚oÅ¼onoÅ›Ä‡ modelu  
 * UÅ¼yÄ‡ jakiejÅ› [techniki regularyzacji](../../4-ComputerVision/08-TransferLearning/TrainingTricks.md), takiej jak [Dropout](../../4-ComputerVision/08-TransferLearning/TrainingTricks.md#Dropout), ktÃ³rÄ… omÃ³wimy pÃ³Åºniej.  

## Nadmierne dopasowanie a kompromis miÄ™dzy bÅ‚Ä™dem a wariancjÄ…

Nadmierne dopasowanie to w rzeczywistoÅ›ci przypadek bardziej ogÃ³lnego problemu w statystyce, zwanego [kompromisem miÄ™dzy bÅ‚Ä™dem a wariancjÄ…](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff). JeÅ›li rozwaÅ¼ymy moÅ¼liwe ÅºrÃ³dÅ‚a bÅ‚Ä™du w naszym modelu, moÅ¼emy wyrÃ³Å¼niÄ‡ dwa rodzaje bÅ‚Ä™dÃ³w:

* **BÅ‚Ä™dy wynikajÄ…ce z uprzedzeÅ„ (bias errors)** sÄ… spowodowane tym, Å¼e nasz algorytm nie jest w stanie poprawnie uchwyciÄ‡ relacji w danych treningowych. MoÅ¼e to wynikaÄ‡ z faktu, Å¼e nasz model nie jest wystarczajÄ…co potÄ™Å¼ny (**niedopasowanie**).  
* **BÅ‚Ä™dy wynikajÄ…ce z wariancji (variance errors)**, ktÃ³re sÄ… spowodowane tym, Å¼e model aproksymuje szum w danych wejÅ›ciowych zamiast znaczÄ…cych relacji (**nadmierne dopasowanie**).  

Podczas treningu bÅ‚Ä…d wynikajÄ…cy z uprzedzeÅ„ maleje (gdy nasz model uczy siÄ™ aproksymowaÄ‡ dane), a bÅ‚Ä…d wynikajÄ…cy z wariancji roÅ›nie. WaÅ¼ne jest, aby zatrzymaÄ‡ trening - albo rÄ™cznie (gdy wykryjemy nadmierne dopasowanie), albo automatycznie (poprzez wprowadzenie regularyzacji) - aby zapobiec nadmiernemu dopasowaniu.

## Podsumowanie

W tej lekcji dowiedziaÅ‚eÅ› siÄ™ o rÃ³Å¼nicach miÄ™dzy rÃ³Å¼nymi API dla dwÃ³ch najpopularniejszych frameworkÃ³w AI, TensorFlow i PyTorch. Ponadto nauczyÅ‚eÅ› siÄ™ o bardzo waÅ¼nym temacie, jakim jest nadmierne dopasowanie.

## ğŸš€ Wyzwanie

W towarzyszÄ…cych notatnikach znajdziesz 'zadania' na koÅ„cu; przejdÅº przez notatniki i wykonaj zadania.

## [Quiz po wykÅ‚adzie](https://ff-quizzes.netlify.app/en/ai/quiz/10)

## PrzeglÄ…d i samodzielna nauka

ZrÃ³b badania na nastÄ™pujÄ…ce tematy:

- TensorFlow  
- PyTorch  
- Nadmierne dopasowanie  

Zadaj sobie nastÄ™pujÄ…ce pytania:

- Jaka jest rÃ³Å¼nica miÄ™dzy TensorFlow a PyTorch?  
- Jaka jest rÃ³Å¼nica miÄ™dzy nadmiernym dopasowaniem a niedopasowaniem?  

## [Zadanie](lab/README.md)

W tym laboratorium masz za zadanie rozwiÄ…zaÄ‡ dwa problemy klasyfikacyjne, uÅ¼ywajÄ…c jedno- i wielowarstwowych sieci w peÅ‚ni poÅ‚Ä…czonych, korzystajÄ…c z PyTorch lub TensorFlow.

* [Instrukcje](lab/README.md)  
* [Notatnik](../../../../../lessons/3-NeuralNetworks/05-Frameworks/lab/LabFrameworks.ipynb)  

**ZastrzeÅ¼enie**:  
Ten dokument zostaÅ‚ przetÅ‚umaczony za pomocÄ… usÅ‚ugi tÅ‚umaczenia AI [Co-op Translator](https://github.com/Azure/co-op-translator). ChociaÅ¼ staramy siÄ™ zapewniÄ‡ dokÅ‚adnoÅ›Ä‡, prosimy mieÄ‡ na uwadze, Å¼e automatyczne tÅ‚umaczenia mogÄ… zawieraÄ‡ bÅ‚Ä™dy lub nieÅ›cisÅ‚oÅ›ci. Oryginalny dokument w jego rodzimym jÄ™zyku powinien byÄ‡ uznawany za wiarygodne ÅºrÃ³dÅ‚o. W przypadku informacji krytycznych zaleca siÄ™ skorzystanie z profesjonalnego tÅ‚umaczenia przez czÅ‚owieka. Nie ponosimy odpowiedzialnoÅ›ci za jakiekolwiek nieporozumienia lub bÅ‚Ä™dne interpretacje wynikajÄ…ce z uÅ¼ycia tego tÅ‚umaczenia.