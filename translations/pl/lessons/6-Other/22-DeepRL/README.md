# Deep Reinforcement Learning

Uczenie przez wzmacnianie (RL) jest uwa偶ane za jeden z podstawowych paradygmat贸w uczenia maszynowego, obok uczenia nadzorowanego i nienadzorowanego. Podczas gdy w uczeniu nadzorowanym opieramy si na zbiorze danych z okrelonymi wynikami, RL bazuje na **uczeniu si przez dziaanie**. Na przykad, gdy po raz pierwszy widzimy gr komputerow, zaczynamy gra, nawet nie znajc zasad, a wkr贸tce jestemy w stanie poprawi swoje umiejtnoci dziki samemu procesowi grania i dostosowywania swojego zachowania.

## [Quiz przed wykadem](https://ff-quizzes.netlify.app/en/ai/quiz/43)

Aby przeprowadzi RL, potrzebujemy:

* **rodowiska** lub **symulatora**, kt贸ry ustala zasady gry. Powinnimy by w stanie przeprowadza eksperymenty w symulatorze i obserwowa wyniki.
* **Funkcji nagrody**, kt贸ra wskazuje, jak udany by nasz eksperyment. W przypadku nauki gry w gr komputerow, nagrod byby nasz kocowy wynik.

Na podstawie funkcji nagrody powinnimy by w stanie dostosowa swoje zachowanie i poprawi swoje umiejtnoci, aby nastpnym razem gra lepiej. G贸wna r贸偶nica midzy innymi typami uczenia maszynowego a RL polega na tym, 偶e w RL zazwyczaj nie wiemy, czy wygrywamy, czy przegrywamy, dop贸ki nie zakoczymy gry. Dlatego nie mo偶emy stwierdzi, czy dany ruch sam w sobie jest dobry czy nie - nagrod otrzymujemy dopiero na kocu gry.

Podczas RL zazwyczaj przeprowadzamy wiele eksperyment贸w. W ka偶dym eksperymencie musimy balansowa midzy pod偶aniem za optymaln strategi, kt贸r dotychczas opracowalimy (**eksploatacja**), a eksplorowaniem nowych mo偶liwych stan贸w (**eksploracja**).

## OpenAI Gym

wietnym narzdziem do RL jest [OpenAI Gym](https://gym.openai.com/) - **rodowisko symulacyjne**, kt贸re mo偶e symulowa wiele r贸偶nych rodowisk, poczwszy od gier Atari, a偶 po fizyk zwizan z balansowaniem dr偶ka. Jest to jedno z najpopularniejszych rodowisk symulacyjnych do trenowania algorytm贸w uczenia przez wzmacnianie i jest utrzymywane przez [OpenAI](https://openai.com/).

> **Note**: Mo偶esz zobaczy wszystkie dostpne rodowiska OpenAI Gym [tutaj](https://gym.openai.com/envs/#classic_control).

## Balansowanie CartPole

Prawdopodobnie wszyscy widzielicie nowoczesne urzdzenia balansujce, takie jak *Segway* czy *Gyroscooters*. S one w stanie automatycznie balansowa, dostosowujc swoje koa w odpowiedzi na sygna z akcelerometru lub 偶yroskopu. W tej sekcji nauczymy si, jak rozwiza podobny problem - balansowanie dr偶ka. Jest to podobne do sytuacji, gdy artysta cyrkowy musi balansowa dr偶ek na swojej doni - ale to balansowanie dr偶ka odbywa si tylko w 1D.

Uproszczona wersja balansowania jest znana jako problem **CartPole**. W wiecie CartPole mamy poziomy suwak, kt贸ry mo偶e porusza si w lewo lub w prawo, a celem jest balansowanie pionowego dr偶ka na szczycie suwaka podczas jego ruchu.

<img alt="a cartpole" src="../../../../../translated_images/pl/cartpole.f52a67f27e058170.webp" width="200"/>

Aby stworzy i u偶ywa tego rodowiska, potrzebujemy kilku linii kodu w Pythonie:

```python
import gym
env = gym.make("CartPole-v1")

env.reset()
done = False
total_reward = 0
while not done:
   env.render()
   action = env.action_space.sample()
   observaton, reward, done, info = env.step(action)
   total_reward += reward

print(f"Total reward: {total_reward}")
```

Ka偶de rodowisko mo偶na obsugiwa dokadnie w ten sam spos贸b:
* `env.reset` rozpoczyna nowy eksperyment
* `env.step` wykonuje krok symulacji. Otrzymuje **akcj** z **przestrzeni akcji** i zwraca **obserwacj** (z przestrzeni obserwacji), a tak偶e nagrod i flag zakoczenia.

W powy偶szym przykadzie wykonujemy losow akcj na ka偶dym kroku, dlatego 偶ycie eksperymentu jest bardzo kr贸tkie:

![non-balancing cartpole](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-nobalance.gif)

Celem algorytmu RL jest wytrenowanie modelu - tzw. **polityki** &pi; - kt贸ra zwr贸ci akcj w odpowiedzi na dany stan. Mo偶emy r贸wnie偶 rozwa偶y polityk jako probabilistyczn, np. dla ka偶dego stanu *s* i akcji *a* zwr贸ci prawdopodobiestwo &pi;(*a*|*s*), 偶e powinnimy podj *a* w stanie *s*.

## Algorytm Policy Gradients

Najbardziej oczywistym sposobem modelowania polityki jest stworzenie sieci neuronowej, kt贸ra przyjmie stany jako wejcie i zwr贸ci odpowiadajce akcje (a waciwie prawdopodobiestwa wszystkich akcji). W pewnym sensie byoby to podobne do normalnego zadania klasyfikacji, z jedn istotn r贸偶nic - nie wiemy z g贸ry, jakie akcje powinnimy podj na ka偶dym z krok贸w.

Pomys polega na oszacowaniu tych prawdopodobiestw. Budujemy wektor **skumulowanych nagr贸d**, kt贸ry pokazuje nasz cakowit nagrod na ka偶dym kroku eksperymentu. Stosujemy r贸wnie偶 **dyskontowanie nagr贸d**, mno偶c wczeniejsze nagrody przez wsp贸czynnik &gamma;=0.99, aby zmniejszy rol wczeniejszych nagr贸d. Nastpnie wzmacniamy te kroki na cie偶ce eksperymentu, kt贸re przynosz wiksze nagrody.

> Dowiedz si wicej o algorytmie Policy Gradient i zobacz go w akcji w [przykadowym notebooku](CartPole-RL-TF.ipynb).

## Algorytm Actor-Critic

Ulepszona wersja podejcia Policy Gradients nazywa si **Actor-Critic**. G贸wna idea polega na tym, 偶e sie neuronowa bdzie trenowana, aby zwraca dwie rzeczy:

* Polityk, kt贸ra okrela, jak akcj podj. Ta cz nazywa si **aktor**
* Szacowanie cakowitej nagrody, kt贸r mo偶emy oczekiwa w danym stanie - ta cz nazywa si **krytyk**.

W pewnym sensie ta architektura przypomina [GAN](../../4-ComputerVision/10-GANs/README.md), gdzie mamy dwie sieci trenowane przeciwko sobie. W modelu actor-critic aktor proponuje akcj, kt贸r powinnimy podj, a krytyk stara si by krytyczny i oszacowa wynik. Jednak naszym celem jest trenowanie tych sieci w harmonii.

Poniewa偶 znamy zar贸wno rzeczywiste skumulowane nagrody, jak i wyniki zwracane przez krytyka podczas eksperymentu, stosunkowo atwo jest zbudowa funkcj straty, kt贸ra zminimalizuje r贸偶nic midzy nimi. To da nam **strat krytyka**. Mo偶emy obliczy **strat aktora**, u偶ywajc tego samego podejcia co w algorytmie Policy Gradient.

Po uruchomieniu jednego z tych algorytm贸w mo偶emy oczekiwa, 偶e nasz CartPole bdzie zachowywa si tak:

![a balancing cartpole](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-balance.gif)

## 锔 wiczenia: Policy Gradients i Actor-Critic RL

Kontynuuj nauk w poni偶szych notebookach:

* [RL w TensorFlow](CartPole-RL-TF.ipynb)
* [RL w PyTorch](CartPole-RL-PyTorch.ipynb)

## Inne zadania RL

Uczenie przez wzmacnianie jest obecnie szybko rozwijajc si dziedzin bada. Niekt贸re interesujce przykady uczenia przez wzmacnianie to:

* Nauka komputera gry w **gry Atari**. Wyzwanie w tym problemie polega na tym, 偶e nie mamy prostego stanu reprezentowanego jako wektor, lecz raczej zrzut ekranu - i musimy u偶y CNN, aby przeksztaci obraz ekranu w wektor cech lub wyodrbni informacje o nagrodach. Gry Atari s dostpne w Gym.
* Nauka komputera gry w gry planszowe, takie jak szachy i Go. Ostatnio programy na najwy偶szym poziomie, takie jak **Alpha Zero**, byy trenowane od podstaw przez dw贸ch agent贸w grajcych przeciwko sobie i poprawiajcych si na ka偶dym kroku.
* W przemyle RL jest u偶ywane do tworzenia system贸w sterowania na podstawie symulacji. Usuga [Bonsai](https://azure.microsoft.com/services/project-bonsai/?WT.mc_id=academic-77998-cacaste) jest specjalnie zaprojektowana do tego celu.

## Podsumowanie

Nauczylimy si teraz, jak trenowa agent贸w, aby osigali dobre wyniki, zapewniajc im jedynie funkcj nagrody, kt贸ra definiuje po偶dany stan gry, oraz dajc im mo偶liwo inteligentnego eksplorowania przestrzeni wyszukiwania. Z powodzeniem wypr贸bowalimy dwa algorytmy i osignlimy dobry wynik w stosunkowo kr贸tkim czasie. Jednak to dopiero pocztek Twojej podr贸偶y w RL, i zdecydowanie warto rozwa偶y udzia w osobnym kursie, jeli chcesz zgbi temat.

##  Wyzwanie

Zbadaj aplikacje wymienione w sekcji "Inne zadania RL" i spr贸buj zaimplementowa jedn z nich!

## [Quiz po wykadzie](https://ff-quizzes.netlify.app/en/ai/quiz/44)

## Przegld i samodzielna nauka

Dowiedz si wicej o klasycznym uczeniu przez wzmacnianie w naszym [Kursie dla pocztkujcych z uczenia maszynowego](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/README.md).

Obejrzyj [ten wietny film](https://www.youtube.com/watch?v=qv6UVOQ0F44) o tym, jak komputer mo偶e nauczy si gra w Super Mario.

## Zadanie: [Trenuj Mountain Car](lab/README.md)

Twoim celem w tym zadaniu bdzie wytrenowanie innego rodowiska Gym - [Mountain Car](https://www.gymlibrary.ml/environments/classic_control/mountain_car/).

---

