# R√©seaux de Neurones R√©currents

## [Quiz avant le cours](https://ff-quizzes.netlify.app/en/ai/quiz/31)

Dans les sections pr√©c√©dentes, nous avons utilis√© des repr√©sentations s√©mantiques riches du texte et un simple classificateur lin√©aire au-dessus des embeddings. Cette architecture permet de capturer le sens global des mots dans une phrase, mais elle ne prend pas en compte l'**ordre** des mots, car l'op√©ration d'agr√©gation sur les embeddings √©limine cette information du texte original. √âtant donn√© que ces mod√®les ne peuvent pas mod√©liser l'ordre des mots, ils ne peuvent pas r√©soudre des t√¢ches plus complexes ou ambigu√´s comme la g√©n√©ration de texte ou la r√©ponse √† des questions.

Pour capturer le sens d'une s√©quence de texte, nous devons utiliser une autre architecture de r√©seau de neurones, appel√©e **r√©seau de neurones r√©current**, ou RNN. Dans un RNN, nous faisons passer notre phrase √† travers le r√©seau un symbole √† la fois, et le r√©seau produit un certain **√©tat**, que nous transmettons ensuite au r√©seau avec le symbole suivant.

![RNN](../../../../../translated_images/fr/rnn.27f5c29c53d727b5.webp)

> Image par l'auteur

√âtant donn√© la s√©quence d'entr√©e de tokens X<sub>0</sub>,...,X<sub>n</sub>, le RNN cr√©e une s√©quence de blocs de r√©seau de neurones et entra√Æne cette s√©quence de bout en bout en utilisant la r√©tropropagation. Chaque bloc de r√©seau prend une paire (X<sub>i</sub>,S<sub>i</sub>) comme entr√©e et produit S<sub>i+1</sub> comme r√©sultat. L'√©tat final S<sub>n</sub> ou (la sortie Y<sub>n</sub>) est ensuite transmis √† un classificateur lin√©aire pour produire le r√©sultat. Tous les blocs de r√©seau partagent les m√™mes poids et sont entra√Æn√©s de bout en bout en une seule passe de r√©tropropagation.

√âtant donn√© que les vecteurs d'√©tat S<sub>0</sub>,...,S<sub>n</sub> sont transmis √† travers le r√©seau, celui-ci est capable d'apprendre les d√©pendances s√©quentielles entre les mots. Par exemple, lorsque le mot *pas* appara√Æt quelque part dans la s√©quence, il peut apprendre √† n√©gativer certains √©l√©ments dans le vecteur d'√©tat, ce qui entra√Æne une n√©gation.

> ‚úÖ √âtant donn√© que les poids de tous les blocs RNN sur l'image ci-dessus sont partag√©s, la m√™me image peut √™tre repr√©sent√©e comme un seul bloc (√† droite) avec une boucle de r√©troaction r√©currente, qui transmet l'√©tat de sortie du r√©seau √† l'entr√©e.

## Anatomie d'une cellule RNN

Voyons comment une cellule RNN simple est organis√©e. Elle accepte l'√©tat pr√©c√©dent S<sub>i-1</sub> et le symbole actuel X<sub>i</sub> comme entr√©es, et doit produire l'√©tat de sortie S<sub>i</sub> (et, parfois, nous sommes √©galement int√©ress√©s par une autre sortie Y<sub>i</sub>, comme dans le cas des r√©seaux g√©n√©ratifs).

Une cellule RNN simple contient deux matrices de poids : l'une transforme un symbole d'entr√©e (appelons-la W), et l'autre transforme un √©tat d'entr√©e (H). Dans ce cas, la sortie du r√©seau est calcul√©e comme &sigma;(W&times;X<sub>i</sub>+H&times;S<sub>i-1</sub>+b), o√π &sigma; est la fonction d'activation et b est un biais suppl√©mentaire.

<img alt="Anatomie d'une cellule RNN" src="../../../../../translated_images/fr/rnn-anatomy.79ee3f3920b3294b.webp" width="50%"/>

> Image par l'auteur

Dans de nombreux cas, les tokens d'entr√©e sont transmis √† travers une couche d'embedding avant d'entrer dans le RNN pour r√©duire la dimensionnalit√©. Dans ce cas, si la dimension des vecteurs d'entr√©e est *emb_size*, et le vecteur d'√©tat est *hid_size* - la taille de W est *emb_size*&times;*hid_size*, et la taille de H est *hid_size*&times;*hid_size*.

## M√©moire √† Long et Court Terme (LSTM)

L'un des principaux probl√®mes des RNN classiques est le probl√®me des **gradients qui disparaissent**. √âtant donn√© que les RNN sont entra√Æn√©s de bout en bout en une seule passe de r√©tropropagation, il est difficile de propager l'erreur jusqu'aux premi√®res couches du r√©seau, ce qui emp√™che le r√©seau d'apprendre les relations entre des tokens √©loign√©s. Une des fa√ßons d'√©viter ce probl√®me est d'introduire une **gestion explicite de l'√©tat** en utilisant des **portes**. Il existe deux architectures bien connues de ce type : **M√©moire √† Long et Court Terme** (LSTM) et **Unit√© de Relais G√¢t√©e** (GRU).

![Image montrant un exemple de cellule LSTM](../../../../../lessons/5-NLP/16-RNN/images/long-short-term-memory-cell.svg)

> Source de l'image √† d√©terminer

Le r√©seau LSTM est organis√© de mani√®re similaire au RNN, mais il y a deux √©tats qui sont transmis d'une couche √† l'autre : l'√©tat r√©el C, et le vecteur cach√© H. √Ä chaque unit√©, le vecteur cach√© H<sub>i</sub> est concat√©n√© avec l'entr√©e X<sub>i</sub>, et ils contr√¥lent ce qui arrive √† l'√©tat C via des **portes**. Chaque porte est un r√©seau de neurones avec une activation sigmo√Øde (sortie dans la plage [0,1]), qui peut √™tre consid√©r√©e comme un masque binaire lorsqu'elle est multipli√©e par le vecteur d'√©tat. Les portes suivantes existent (de gauche √† droite sur l'image ci-dessus) :

* La **porte d'oubli** prend un vecteur cach√© et d√©termine quelles composantes du vecteur C doivent √™tre oubli√©es et lesquelles doivent √™tre transmises.
* La **porte d'entr√©e** prend certaines informations des vecteurs d'entr√©e et cach√©s et les ins√®re dans l'√©tat.
* La **porte de sortie** transforme l'√©tat via une couche lin√©aire avec activation *tanh*, puis s√©lectionne certaines de ses composantes en utilisant un vecteur cach√© H<sub>i</sub> pour produire un nouvel √©tat C<sub>i+1</sub>.

Les composantes de l'√©tat C peuvent √™tre consid√©r√©es comme des indicateurs qui peuvent √™tre activ√©s ou d√©sactiv√©s. Par exemple, lorsque nous rencontrons un nom *Alice* dans la s√©quence, nous pouvons supposer qu'il fait r√©f√©rence √† un personnage f√©minin, et activer l'indicateur dans l'√©tat indiquant que nous avons un nom f√©minin dans la phrase. Lorsque nous rencontrons ensuite des phrases comme *et Tom*, nous activerons l'indicateur indiquant que nous avons un nom au pluriel. Ainsi, en manipulant l'√©tat, nous pouvons th√©oriquement suivre les propri√©t√©s grammaticales des parties de la phrase.

> ‚úÖ Une excellente ressource pour comprendre les d√©tails des LSTM est cet article [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) de Christopher Olah.

## RNN bidirectionnels et multicouches

Nous avons discut√© des r√©seaux r√©currents qui fonctionnent dans une seule direction, du d√©but d'une s√©quence √† la fin. Cela semble naturel, car cela ressemble √† la fa√ßon dont nous lisons et √©coutons un discours. Cependant, dans de nombreux cas pratiques, nous avons un acc√®s al√©atoire √† la s√©quence d'entr√©e, il peut donc √™tre judicieux d'ex√©cuter le calcul r√©current dans les deux directions. Ces r√©seaux sont appel√©s **RNN bidirectionnels**. Lorsqu'on travaille avec un r√©seau bidirectionnel, nous aurons besoin de deux vecteurs d'√©tat cach√©s, un pour chaque direction.

Un r√©seau r√©current, qu'il soit unidirectionnel ou bidirectionnel, capture certains motifs au sein d'une s√©quence et peut les stocker dans un vecteur d'√©tat ou les transmettre en sortie. Comme pour les r√©seaux convolutionnels, nous pouvons construire une autre couche r√©currente au-dessus de la premi√®re pour capturer des motifs de niveau sup√©rieur et construire √† partir des motifs de bas niveau extraits par la premi√®re couche. Cela nous m√®ne √† la notion de **RNN multicouche**, qui consiste en deux ou plusieurs r√©seaux r√©currents, o√π la sortie de la couche pr√©c√©dente est transmise √† la couche suivante comme entr√©e.

![Image montrant un RNN LSTM multicouche](../../../../../translated_images/fr/multi-layer-lstm.dd975e29bb2a59fe.webp)

*Image tir√©e de [cet excellent article](https://towardsdatascience.com/from-a-lstm-cell-to-a-multilayer-lstm-network-with-pytorch-2899eb5696f3) de Fernando L√≥pez*

## ‚úçÔ∏è Exercices : Embeddings

Poursuivez votre apprentissage dans les notebooks suivants :

* [RNNs avec PyTorch](RNNPyTorch.ipynb)
* [RNNs avec TensorFlow](RNNTF.ipynb)

## Conclusion

Dans cette unit√©, nous avons vu que les RNN peuvent √™tre utilis√©s pour la classification de s√©quences, mais en r√©alit√©, ils peuvent g√©rer de nombreuses autres t√¢ches, telles que la g√©n√©ration de texte, la traduction automatique, et bien plus encore. Nous examinerons ces t√¢ches dans l'unit√© suivante.

## üöÄ D√©fi

Lisez quelques articles sur les LSTM et r√©fl√©chissez √† leurs applications :

- [Grid Long Short-Term Memory](https://arxiv.org/pdf/1507.01526v1.pdf)
- [Show, Attend and Tell: Neural Image Caption
Generation with Visual Attention](https://arxiv.org/pdf/1502.03044v2.pdf)

## [Quiz apr√®s le cours](https://ff-quizzes.netlify.app/en/ai/quiz/32)

## R√©vision & √âtude personnelle

- [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) de Christopher Olah.

## [Devoir : Notebooks](assignment.md)

---

