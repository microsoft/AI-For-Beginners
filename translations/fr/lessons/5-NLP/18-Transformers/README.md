# M√©canismes d'Attention et Transformers

## [Quiz avant le cours](https://ff-quizzes.netlify.app/en/ai/quiz/35)

L'un des probl√®mes les plus importants dans le domaine du NLP est **la traduction automatique**, une t√¢che essentielle qui sous-tend des outils tels que Google Traduction. Dans cette section, nous nous concentrerons sur la traduction automatique, ou, plus g√©n√©ralement, sur toute t√¢che de *s√©quence √† s√©quence* (√©galement appel√©e **transduction de phrase**).

Avec les RNNs, la s√©quence √† s√©quence est mise en ≈ìuvre par deux r√©seaux r√©currents, o√π un r√©seau, l'**encodeur**, condense une s√©quence d'entr√©e en un √©tat cach√©, tandis qu'un autre r√©seau, le **d√©codeur**, d√©ploie cet √©tat cach√© en un r√©sultat traduit. Cette approche pr√©sente quelques probl√®mes :

* L'√©tat final du r√©seau encodeur a du mal √† se souvenir du d√©but d'une phrase, ce qui entra√Æne une qualit√© m√©diocre du mod√®le pour les phrases longues.
* Tous les mots d'une s√©quence ont le m√™me impact sur le r√©sultat. En r√©alit√©, cependant, certains mots sp√©cifiques de la s√©quence d'entr√©e ont souvent plus d'impact sur les sorties s√©quentielles que d'autres.

Les **m√©canismes d'attention** offrent un moyen de pond√©rer l'impact contextuel de chaque vecteur d'entr√©e sur chaque pr√©diction de sortie du RNN. Cela est mis en ≈ìuvre en cr√©ant des raccourcis entre les √©tats interm√©diaires du RNN d'entr√©e et du RNN de sortie. Ainsi, lors de la g√©n√©ration du symbole de sortie y<sub>t</sub>, nous prenons en compte tous les √©tats cach√©s d'entr√©e h<sub>i</sub>, avec diff√©rents coefficients de poids &alpha;<sub>t,i</sub>.

![Image montrant un mod√®le encodeur/d√©codeur avec une couche d'attention additive](../../../../../translated_images/fr/encoder-decoder-attention.7a726296894fb567.webp)

> Le mod√®le encodeur-d√©codeur avec m√©canisme d'attention additive dans [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf), cit√© de [ce blog](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)

La matrice d'attention {&alpha;<sub>i,j</sub>} repr√©sente le degr√© auquel certains mots d'entr√©e jouent un r√¥le dans la g√©n√©ration d'un mot donn√© dans la s√©quence de sortie. Voici un exemple de cette matrice :

![Image montrant un alignement trouv√© par RNNsearch-50, tir√©e de Bahdanau - arviz.org](../../../../../translated_images/fr/bahdanau-fig3.09ba2d37f202a6af.webp)

> Figure tir√©e de [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) (Fig.3)

Les m√©canismes d'attention sont responsables de l'√©tat de l'art actuel ou presque actuel en NLP. Cependant, l'ajout d'attention augmente consid√©rablement le nombre de param√®tres du mod√®le, ce qui a entra√Æn√© des probl√®mes de mise √† l'√©chelle avec les RNNs. Une contrainte cl√© de la mise √† l'√©chelle des RNNs est que la nature r√©currente des mod√®les rend difficile le traitement par lots et la parall√©lisation de l'entra√Ænement. Dans un RNN, chaque √©l√©ment d'une s√©quence doit √™tre trait√© dans un ordre s√©quentiel, ce qui signifie qu'il ne peut pas √™tre facilement parall√©lis√©.

![Encodeur D√©codeur avec Attention](../../../../../lessons/5-NLP/18-Transformers/images/EncDecAttention.gif)

> Figure tir√©e du [blog de Google](https://research.googleblog.com/2016/09/a-neural-network-for-machine.html)

L'adoption des m√©canismes d'attention combin√©e √† cette contrainte a conduit √† la cr√©ation des mod√®les Transformers, d√©sormais √† la pointe de la technologie, que nous connaissons et utilisons aujourd'hui, tels que BERT et Open-GPT3.

## Mod√®les Transformers

L'une des id√©es principales derri√®re les transformers est d'√©viter la nature s√©quentielle des RNNs et de cr√©er un mod√®le parall√©lisable pendant l'entra√Ænement. Cela est r√©alis√© en mettant en ≈ìuvre deux id√©es :

* l'encodage positionnel
* l'utilisation du m√©canisme d'auto-attention pour capturer des motifs au lieu des RNNs (ou CNNs) (c'est pourquoi l'article qui introduit les transformers s'intitule *[Attention is all you need](https://arxiv.org/abs/1706.03762)*).

### Encodage/Embedding Positionnel

L'id√©e de l'encodage positionnel est la suivante :
1. Lors de l'utilisation des RNNs, la position relative des tokens est repr√©sent√©e par le nombre d'√©tapes, et n'a donc pas besoin d'√™tre explicitement repr√©sent√©e.
2. Cependant, une fois que nous passons √† l'attention, nous devons conna√Ætre les positions relatives des tokens dans une s√©quence.
3. Pour obtenir l'encodage positionnel, nous augmentons notre s√©quence de tokens avec une s√©quence de positions des tokens dans la s√©quence (c'est-√†-dire une s√©quence de nombres 0,1, ...).
4. Nous m√©langeons ensuite la position du token avec un vecteur d'embedding du token. Pour transformer la position (entier) en vecteur, nous pouvons utiliser diff√©rentes approches :

* Embedding entra√Ænable, similaire √† l'embedding des tokens. C'est l'approche que nous consid√©rons ici. Nous appliquons des couches d'embedding √† la fois sur les tokens et leurs positions, ce qui donne des vecteurs d'embedding de m√™mes dimensions, que nous additionnons ensuite.
* Fonction d'encodage positionnel fixe, comme propos√© dans l'article original.

<img src="../../../../../translated_images/fr/pos-embedding.e41ce9b6cf6078af.webp" width="50%"/>

> Image par l'auteur

Le r√©sultat obtenu avec l'embedding positionnel int√®gre √† la fois le token original et sa position dans une s√©quence.

### Auto-Attention Multi-T√™te

Ensuite, nous devons capturer certains motifs dans notre s√©quence. Pour ce faire, les transformers utilisent un m√©canisme d'**auto-attention**, qui est essentiellement une attention appliqu√©e √† la m√™me s√©quence en tant qu'entr√©e et sortie. L'application de l'auto-attention nous permet de prendre en compte le **contexte** dans la phrase et de voir quels mots sont interconnect√©s. Par exemple, cela nous permet de voir quels mots sont r√©f√©renc√©s par des cor√©f√©rences, comme *il*, et de prendre √©galement le contexte en compte :

![](../../../../../translated_images/fr/CoreferenceResolution.861924d6d384a7d6.webp)

> Image tir√©e du [blog de Google](https://research.googleblog.com/2017/08/transformer-novel-neural-network.html)

Dans les transformers, nous utilisons l'**attention multi-t√™te** afin de donner au r√©seau la capacit√© de capturer plusieurs types de d√©pendances diff√©rents, par exemple les relations de mots √† long terme vs √† court terme, les cor√©f√©rences vs autre chose, etc.

[Notebook TensorFlow](TransformersTF.ipynb) contient plus de d√©tails sur l'impl√©mentation des couches de transformers.

### Attention Encodeur-D√©codeur

Dans les transformers, l'attention est utilis√©e √† deux endroits :

* Pour capturer des motifs dans le texte d'entr√©e en utilisant l'auto-attention.
* Pour effectuer la traduction de s√©quence - c'est la couche d'attention entre l'encodeur et le d√©codeur.

L'attention encodeur-d√©codeur est tr√®s similaire au m√©canisme d'attention utilis√© dans les RNNs, comme d√©crit au d√©but de cette section. Ce diagramme anim√© explique le r√¥le de l'attention encodeur-d√©codeur.

![GIF anim√© montrant comment les √©valuations sont effectu√©es dans les mod√®les transformers.](../../../../../lessons/5-NLP/18-Transformers/images/transformer-animated-explanation.gif)

√âtant donn√© que chaque position d'entr√©e est mapp√©e ind√©pendamment √† chaque position de sortie, les transformers peuvent mieux parall√©liser que les RNNs, ce qui permet des mod√®les de langage beaucoup plus grands et plus expressifs. Chaque t√™te d'attention peut √™tre utilis√©e pour apprendre diff√©rentes relations entre les mots, ce qui am√©liore les t√¢ches de traitement du langage naturel en aval.

## BERT

**BERT** (Bidirectional Encoder Representations from Transformers) est un r√©seau transformer multi-couches tr√®s large avec 12 couches pour *BERT-base*, et 24 pour *BERT-large*. Le mod√®le est d'abord pr√©-entra√Æn√© sur un large corpus de donn√©es textuelles (WikiPedia + livres) en utilisant un entra√Ænement non supervis√© (pr√©diction des mots masqu√©s dans une phrase). Pendant le pr√©-entra√Ænement, le mod√®le absorbe des niveaux significatifs de compr√©hension du langage qui peuvent ensuite √™tre exploit√©s avec d'autres ensembles de donn√©es via un ajustement fin. Ce processus est appel√© **apprentissage par transfert**.

![image tir√©e de http://jalammar.github.io/illustrated-bert/](../../../../../translated_images/fr/jalammarBERT-language-modeling-masked-lm.34f113ea5fec4362.webp)

> Image [source](http://jalammar.github.io/illustrated-bert/)

## ‚úçÔ∏è Exercices : Transformers

Poursuivez votre apprentissage dans les notebooks suivants :

* [Transformers en PyTorch](TransformersPyTorch.ipynb)
* [Transformers en TensorFlow](TransformersTF.ipynb)

## Conclusion

Dans cette le√ßon, vous avez appris les Transformers et les m√©canismes d'attention, des outils essentiels dans la bo√Æte √† outils du NLP. Il existe de nombreuses variantes des architectures Transformers, notamment BERT, DistilBERT, BigBird, OpenGPT3 et bien d'autres, qui peuvent √™tre ajust√©es. Le [package HuggingFace](https://github.com/huggingface/) fournit un d√©p√¥t pour entra√Æner plusieurs de ces architectures avec PyTorch et TensorFlow.

## üöÄ D√©fi

## [Quiz apr√®s le cours](https://ff-quizzes.netlify.app/en/ai/quiz/36)

## R√©vision & Auto-√©tude

* [Article de blog](https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/), expliquant l'article classique [Attention is all you need](https://arxiv.org/abs/1706.03762) sur les transformers.
* [Une s√©rie d'articles de blog](https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452) sur les transformers, expliquant l'architecture en d√©tail.

## [Devoir](assignment.md)

---

