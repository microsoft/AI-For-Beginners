# Repr√©senter le texte sous forme de tenseurs

## [Quiz avant le cours](https://ff-quizzes.netlify.app/en/ai/quiz/25)

## Classification de texte

Dans la premi√®re partie de cette section, nous allons nous concentrer sur la t√¢che de **classification de texte**. Nous utiliserons le jeu de donn√©es [AG News](https://www.kaggle.com/amananandrai/ag-news-classification-dataset), qui contient des articles de presse comme celui-ci :

* Cat√©gorie : Sci/Tech  
* Titre : Ky. Company Wins Grant to Study Peptides (AP)  
* Corps : AP - Une entreprise fond√©e par un chercheur en chimie de l'Universit√© de Louisville a obtenu une subvention pour d√©velopper...

Notre objectif sera de classer l'article dans l'une des cat√©gories en fonction du texte.

## Repr√©senter le texte

Pour r√©soudre des t√¢ches de traitement du langage naturel (NLP) avec des r√©seaux neuronaux, nous devons trouver un moyen de repr√©senter le texte sous forme de tenseurs. Les ordinateurs repr√©sentent d√©j√† les caract√®res textuels sous forme de nombres qui correspondent aux polices affich√©es sur votre √©cran, en utilisant des encodages tels que ASCII ou UTF-8.

<img alt="Image montrant un diagramme qui associe un caract√®re √† une repr√©sentation ASCII et binaire" src="../../../../../translated_images/fr/ascii-character-map.18ed6aa7f3b0a7ff.webp" width="50%"/>

> [Source de l'image](https://www.seobility.net/en/wiki/ASCII)

En tant qu'humains, nous comprenons ce que chaque lettre **repr√©sente**, et comment tous les caract√®res se combinent pour former les mots d'une phrase. Cependant, les ordinateurs, par eux-m√™mes, n'ont pas cette compr√©hension, et le r√©seau neuronal doit apprendre le sens pendant l'entra√Ænement.

Ainsi, nous pouvons utiliser diff√©rentes approches pour repr√©senter le texte :

* **Repr√©sentation au niveau des caract√®res**, o√π chaque caract√®re est trait√© comme un nombre. √âtant donn√© que nous avons *C* caract√®res diff√©rents dans notre corpus de texte, le mot *Hello* serait repr√©sent√© par un tenseur de 5x*C*. Chaque lettre correspondrait √† une colonne de tenseur en encodage one-hot.  
* **Repr√©sentation au niveau des mots**, o√π nous cr√©ons un **vocabulaire** de tous les mots dans notre texte, puis repr√©sentons les mots en utilisant l'encodage one-hot. Cette approche est meilleure dans une certaine mesure, car chaque lettre seule n'a pas beaucoup de sens. En utilisant des concepts s√©mantiques de niveau sup√©rieur - les mots - nous simplifions la t√¢che pour le r√©seau neuronal. Cependant, √©tant donn√© la taille importante du dictionnaire, nous devons g√©rer des tenseurs creux de haute dimension.

Quelle que soit la repr√©sentation, nous devons d'abord convertir le texte en une s√©quence de **tokens**, un token √©tant soit un caract√®re, un mot, ou parfois m√™me une partie d'un mot. Ensuite, nous convertissons le token en un nombre, g√©n√©ralement en utilisant un **vocabulaire**, et ce nombre peut √™tre introduit dans un r√©seau neuronal via un encodage one-hot.

## N-Grams

Dans le langage naturel, le sens pr√©cis des mots ne peut √™tre d√©termin√© qu'en contexte. Par exemple, les significations de *r√©seau neuronal* et *r√©seau de p√™che* sont compl√®tement diff√©rentes. Une des fa√ßons de prendre cela en compte est de construire notre mod√®le sur des paires de mots, en consid√©rant les paires de mots comme des tokens de vocabulaire distincts. Ainsi, la phrase *J'aime aller p√™cher* sera repr√©sent√©e par la s√©quence de tokens suivante : *J'aime*, *aime aller*, *aller p√™cher*. Le probl√®me avec cette approche est que la taille du dictionnaire augmente consid√©rablement, et des combinaisons comme *aller p√™cher* et *aller faire du shopping* sont repr√©sent√©es par des tokens diff√©rents, qui ne partagent aucune similarit√© s√©mantique malgr√© le m√™me verbe.

Dans certains cas, nous pouvons envisager d'utiliser des tri-grams -- des combinaisons de trois mots -- √©galement. Cette approche est souvent appel√©e **n-grams**. Il est √©galement pertinent d'utiliser des n-grams avec une repr√©sentation au niveau des caract√®res, o√π les n-grams correspondent approximativement √† diff√©rentes syllabes.

## Sac de mots et TF/IDF

Pour r√©soudre des t√¢ches comme la classification de texte, nous devons √™tre capables de repr√©senter le texte par un vecteur de taille fixe, que nous utiliserons comme entr√©e pour le classificateur dense final. Une des fa√ßons les plus simples de le faire est de combiner toutes les repr√©sentations individuelles des mots, par exemple en les additionnant. Si nous additionnons les encodages one-hot de chaque mot, nous obtiendrons un vecteur de fr√©quences, montrant combien de fois chaque mot appara√Æt dans le texte. Une telle repr√©sentation du texte est appel√©e **sac de mots** (BoW).

<img src="../../../../../translated_images/fr/bow.3811869cff59368d.webp" width="90%"/>

> Image par l'auteur

Un BoW repr√©sente essentiellement quels mots apparaissent dans le texte et en quelles quantit√©s, ce qui peut effectivement √™tre une bonne indication de ce dont parle le texte. Par exemple, un article de presse sur la politique est susceptible de contenir des mots tels que *pr√©sident* et *pays*, tandis qu'une publication scientifique pourrait inclure des termes comme *collider*, *d√©couvert*, etc. Ainsi, les fr√©quences des mots peuvent souvent √™tre un bon indicateur du contenu du texte.

Le probl√®me avec BoW est que certains mots courants, tels que *et*, *est*, etc., apparaissent dans la plupart des textes et ont les fr√©quences les plus √©lev√©es, masquant les mots qui sont r√©ellement importants. Nous pouvons r√©duire l'importance de ces mots en tenant compte de la fr√©quence √† laquelle ils apparaissent dans l'ensemble de la collection de documents. C'est l'id√©e principale derri√®re l'approche TF/IDF, qui est expliqu√©e plus en d√©tail dans les notebooks associ√©s √† cette le√ßon.

Cependant, aucune de ces approches ne peut pleinement prendre en compte la **s√©mantique** du texte. Nous avons besoin de mod√®les de r√©seaux neuronaux plus puissants pour cela, que nous aborderons plus tard dans cette section.

## ‚úçÔ∏è Exercices : Repr√©sentation du texte

Poursuivez votre apprentissage dans les notebooks suivants :

* [Repr√©sentation du texte avec PyTorch](TextRepresentationPyTorch.ipynb)  
* [Repr√©sentation du texte avec TensorFlow](TextRepresentationTF.ipynb)  

## Conclusion

Jusqu'√† pr√©sent, nous avons √©tudi√© des techniques qui peuvent ajouter un poids de fr√©quence √† diff√©rents mots. Elles sont cependant incapables de repr√©senter le sens ou l'ordre. Comme l'a dit le c√©l√®bre linguiste J. R. Firth en 1935, "Le sens complet d'un mot est toujours contextuel, et aucune √©tude de sens en dehors du contexte ne peut √™tre prise au s√©rieux." Nous apprendrons plus tard dans le cours comment capturer les informations contextuelles du texte en utilisant la mod√©lisation du langage.

## üöÄ D√©fi

Essayez d'autres exercices en utilisant le sac de mots et diff√©rents mod√®les de donn√©es. Vous pourriez √™tre inspir√© par cette [comp√©tition sur Kaggle](https://www.kaggle.com/competitions/word2vec-nlp-tutorial/overview/part-1-for-beginners-bag-of-words)

## [Quiz apr√®s le cours](https://ff-quizzes.netlify.app/en/ai/quiz/26)

## R√©vision et √©tude autonome

Pratiquez vos comp√©tences avec les techniques d'embedding de texte et de sac de mots sur [Microsoft Learn](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-pytorch/?WT.mc_id=academic-77998-cacaste)

## [Devoir : Notebooks](assignment.md)

---

