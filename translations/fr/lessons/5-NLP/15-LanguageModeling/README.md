# Mod√©lisation du langage

Les embeddings s√©mantiques, tels que Word2Vec et GloVe, constituent en r√©alit√© une premi√®re √©tape vers la **mod√©lisation du langage** - cr√©er des mod√®les qui *comprennent* (ou *repr√©sentent*) d'une certaine mani√®re la nature du langage.

## [Quiz avant le cours](https://ff-quizzes.netlify.app/en/ai/quiz/29)

L'id√©e principale derri√®re la mod√©lisation du langage est de les entra√Æner sur des ensembles de donn√©es non √©tiquet√©es de mani√®re non supervis√©e. Cela est important car nous disposons de grandes quantit√©s de texte non √©tiquet√©, tandis que la quantit√© de texte √©tiquet√© sera toujours limit√©e par l'effort n√©cessaire pour l'√©tiquetage. Le plus souvent, nous pouvons construire des mod√®les de langage capables de **pr√©dire les mots manquants** dans un texte, car il est facile de masquer un mot al√©atoire dans un texte et de l'utiliser comme exemple d'entra√Ænement.

## Entra√Ænement des embeddings

Dans nos exemples pr√©c√©dents, nous avons utilis√© des embeddings s√©mantiques pr√©-entra√Æn√©s, mais il est int√©ressant de voir comment ces embeddings peuvent √™tre entra√Æn√©s. Plusieurs id√©es peuvent √™tre utilis√©es :

* **Mod√©lisation de langage N-Gram**, o√π l'on pr√©dit un token en se basant sur les N tokens pr√©c√©dents (N-gram).
* **Continuous Bag-of-Words** (CBoW), o√π l'on pr√©dit le token central $W_0$ dans une s√©quence de tokens $W_{-N}$, ..., $W_N$.
* **Skip-gram**, o√π l'on pr√©dit un ensemble de tokens voisins {$W_{-N},\dots, W_{-1}, W_1,\dots, W_N$} √† partir du token central $W_0$.

![image tir√©e d'un article sur la conversion des mots en vecteurs](../../../../../translated_images/fr/example-algorithms-for-converting-words-to-vectors.fbe9207a726922f6.webp)

> Image tir√©e de [cet article](https://arxiv.org/pdf/1301.3781.pdf)

## ‚úçÔ∏è Notebooks d'exemple : Entra√Ænement du mod√®le CBoW

Poursuivez votre apprentissage avec les notebooks suivants :

* [Entra√Ænement de CBoW Word2Vec avec TensorFlow](CBoW-TF.ipynb)
* [Entra√Ænement de CBoW Word2Vec avec PyTorch](CBoW-PyTorch.ipynb)

## Conclusion

Dans la le√ßon pr√©c√©dente, nous avons vu que les embeddings de mots fonctionnent comme par magie ! Nous savons maintenant que l'entra√Ænement des embeddings de mots n'est pas une t√¢che tr√®s complexe, et nous devrions √™tre capables d'entra√Æner nos propres embeddings pour des textes sp√©cifiques √† un domaine si n√©cessaire.

## [Quiz apr√®s le cours](https://ff-quizzes.netlify.app/en/ai/quiz/30)

## R√©vision & Auto-apprentissage

* [Tutoriel officiel PyTorch sur la mod√©lisation du langage](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html).
* [Tutoriel officiel TensorFlow sur l'entra√Ænement du mod√®le Word2Vec](https://www.TensorFlow.org/tutorials/text/word2vec).
* Utiliser le framework **gensim** pour entra√Æner les embeddings les plus couramment utilis√©s en quelques lignes de code est d√©crit [dans cette documentation](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html).

## üöÄ [Exercice : Entra√Æner un mod√®le Skip-Gram](lab/README.md)

Dans le laboratoire, nous vous mettons au d√©fi de modifier le code de cette le√ßon pour entra√Æner un mod√®le Skip-Gram √† la place de CBoW. [Lisez les d√©tails](lab/README.md)

---

