<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "d85c8b08f6d1b48fd7f35b99f93c1138",
  "translation_date": "2025-08-24T20:50:42+00:00",
  "source_file": "lessons/4-ComputerVision/11-ObjectDetection/README.md",
  "language_code": "fr"
}
-->
# D√©tection d'Objets

Les mod√®les de classification d'images que nous avons √©tudi√©s jusqu'√† pr√©sent prenaient une image et produisaient un r√©sultat cat√©goriel, comme la classe 'nombre' dans un probl√®me MNIST. Cependant, dans de nombreux cas, nous ne voulons pas seulement savoir qu'une image repr√©sente des objets - nous voulons √©galement pouvoir d√©terminer leur emplacement pr√©cis. C'est pr√©cis√©ment l'objectif de la **d√©tection d'objets**.

## [Quiz avant le cours](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/111)

![D√©tection d'Objets](../../../../../translated_images/Screen_Shot_2016-11-17_at_11.14.54_AM.b4bb3769353287be1b905373ed9c858102c054b16e4595c76ec3f7bba0feb549.fr.png)

> Image tir√©e du [site web YOLO v2](https://pjreddie.com/darknet/yolov2/)

## Une Approche Na√Øve pour la D√©tection d'Objets

Supposons que nous voulions trouver un chat sur une image. Une approche tr√®s na√Øve pour la d√©tection d'objets serait la suivante :

1. Diviser l'image en un certain nombre de tuiles.
2. Appliquer une classification d'images sur chaque tuile.
3. Les tuiles qui produisent une activation suffisamment √©lev√©e peuvent √™tre consid√©r√©es comme contenant l'objet en question.

![D√©tection Na√Øve](../../../../../translated_images/naive-detection.e7f1ba220ccd08c68a2ea8e06a7ed75c3fcc738c2372f9e00b7f4299a8659c01.fr.png)

> *Image tir√©e du [cahier d'exercices](../../../../../lessons/4-ComputerVision/11-ObjectDetection/ObjectDetection-TF.ipynb)*

Cependant, cette approche est loin d'√™tre id√©ale, car elle ne permet √† l'algorithme de localiser la bo√Æte englobante de l'objet que de mani√®re tr√®s impr√©cise. Pour une localisation plus pr√©cise, nous devons utiliser une forme de **r√©gression** pour pr√©dire les coordonn√©es des bo√Ætes englobantes - et pour cela, nous avons besoin de jeux de donn√©es sp√©cifiques.

## R√©gression pour la D√©tection d'Objets

[Cet article de blog](https://towardsdatascience.com/object-detection-with-neural-networks-a4e2c46b4491) offre une excellente introduction √† la d√©tection de formes.

## Jeux de Donn√©es pour la D√©tection d'Objets

Vous pourriez rencontrer les jeux de donn√©es suivants pour cette t√¢che :

* [PASCAL VOC](http://host.robots.ox.ac.uk/pascal/VOC/) - 20 classes
* [COCO](http://cocodataset.org/#home) - Common Objects in Context. 80 classes, bo√Ætes englobantes et masques de segmentation

![COCO](../../../../../translated_images/coco-examples.71bc60380fa6cceb7caad48bd09e35b6028caabd363aa04fee89c414e0870e86.fr.jpg)

## M√©triques pour la D√©tection d'Objets

### Intersection sur Union

Alors que pour la classification d'images, il est facile de mesurer les performances de l'algorithme, pour la d√©tection d'objets, nous devons mesurer √† la fois la justesse de la classe et la pr√©cision de la localisation de la bo√Æte englobante d√©duite. Pour cette derni√®re, nous utilisons ce que l'on appelle l'**Intersection sur Union** (IoU), qui mesure √† quel point deux bo√Ætes (ou deux zones arbitraires) se chevauchent.

![IoU](../../../../../translated_images/iou_equation.9a4751d40fff4e119ecd0a7bcca4e71ab1dc83e0d4f2a0d66ff0859736f593cf.fr.png)

> *Figure 2 tir√©e de [cet excellent article de blog sur l'IoU](https://pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/)*

L'id√©e est simple : nous divisons la zone d'intersection entre deux figures par la zone de leur union. Pour deux zones identiques, l'IoU serait de 1, tandis que pour des zones compl√®tement disjointes, il sera de 0. Sinon, il variera entre 0 et 1. Nous consid√©rons g√©n√©ralement uniquement les bo√Ætes englobantes pour lesquelles l'IoU d√©passe une certaine valeur.

### Pr√©cision Moyenne

Supposons que nous voulions mesurer √† quel point une classe d'objets donn√©e $C$ est bien reconnue. Pour cela, nous utilisons la m√©trique de **Pr√©cision Moyenne**, qui est calcul√©e comme suit :

1. Consid√©rer la courbe Pr√©cision-Rappel qui montre la pr√©cision en fonction d'une valeur seuil de d√©tection (de 0 √† 1).
2. En fonction du seuil, nous d√©tecterons plus ou moins d'objets dans l'image, avec des valeurs diff√©rentes de pr√©cision et de rappel.
3. La courbe ressemblera √† ceci :

<img src="https://github.com/shwars/NeuroWorkshop/raw/master/images/ObjDetectionPrecisionRecall.png"/>

> *Image tir√©e de [NeuroWorkshop](http://github.com/shwars/NeuroWorkshop)*

La Pr√©cision Moyenne pour une classe donn√©e $C$ est l'aire sous cette courbe. Plus pr√©cis√©ment, l'axe du Rappel est g√©n√©ralement divis√© en 10 parties, et la Pr√©cision est moyenn√©e sur tous ces points :

$$
AP = {1\over11}\sum_{i=0}^{10}\mbox{Precision}(\mbox{Recall}={i\over10})
$$

### AP et IoU

Nous ne consid√©rerons que les d√©tections pour lesquelles l'IoU d√©passe une certaine valeur. Par exemple, dans le jeu de donn√©es PASCAL VOC, on suppose g√©n√©ralement que $\mbox{IoU Threshold} = 0.5$, tandis que dans COCO, l'AP est mesur√© pour diff√©rentes valeurs de $\mbox{IoU Threshold}$.

<img src="https://github.com/shwars/NeuroWorkshop/raw/master/images/ObjDetectionPrecisionRecallIoU.png"/>

> *Image tir√©e de [NeuroWorkshop](http://github.com/shwars/NeuroWorkshop)*

### Pr√©cision Moyenne Pond√©r√©e - mAP

La principale m√©trique pour la D√©tection d'Objets est appel√©e **Pr√©cision Moyenne Pond√©r√©e**, ou **mAP**. Il s'agit de la valeur de la Pr√©cision Moyenne, moyenn√©e sur toutes les classes d'objets, et parfois aussi sur $\mbox{IoU Threshold}$. Le processus de calcul de la **mAP** est d√©crit en d√©tail
[dans cet article de blog](https://medium.com/@timothycarlen/understanding-the-map-evaluation-metric-for-object-detection-a07fe6962cf3)), ainsi que [ici avec des exemples de code](https://gist.github.com/tarlen5/008809c3decf19313de216b9208f3734).

## Diff√©rentes Approches pour la D√©tection d'Objets

Il existe deux grandes cat√©gories d'algorithmes de d√©tection d'objets :

* **R√©seaux de Propositions de R√©gions** (R-CNN, Fast R-CNN, Faster R-CNN). L'id√©e principale est de g√©n√©rer des **R√©gions d'Int√©r√™t** (ROI) et d'ex√©cuter un CNN sur celles-ci, en recherchant une activation maximale. Cela ressemble un peu √† l'approche na√Øve, √† la diff√©rence que les ROI sont g√©n√©r√©es de mani√®re plus intelligente. L'un des principaux inconv√©nients de ces m√©thodes est qu'elles sont lentes, car nous avons besoin de nombreux passages du classificateur CNN sur l'image.
* M√©thodes **en un seul passage** (YOLO, SSD, RetinaNet). Dans ces architectures, nous concevons le r√©seau pour pr√©dire √† la fois les classes et les ROI en un seul passage.

### R-CNN : CNN Bas√© sur les R√©gions

[R-CNN](http://islab.ulsan.ac.kr/files/announcement/513/rcnn_pami.pdf) utilise [Selective Search](http://www.huppelen.nl/publications/selectiveSearchDraft.pdf) pour g√©n√©rer une structure hi√©rarchique de r√©gions ROI, qui sont ensuite pass√©es √† travers des extracteurs de caract√©ristiques CNN et des classificateurs SVM pour d√©terminer la classe de l'objet, et une r√©gression lin√©aire pour d√©terminer les coordonn√©es de la *bo√Æte englobante*. [Article Officiel](https://arxiv.org/pdf/1506.01497v1.pdf)

![RCNN](../../../../../translated_images/rcnn1.cae407020dfb1d1fb572656e44f75cd6c512cc220591c116c506652c10e47f26.fr.png)

> *Image tir√©e de van de Sande et al. ICCV‚Äô11*

![RCNN-1](../../../../../translated_images/rcnn2.2d9530bb83516484ec65b250c22dbf37d3d23244f32864ebcb91d98fe7c3112c.fr.png)

> *Images tir√©es de [cet article de blog](https://towardsdatascience.com/r-cnn-fast-r-cnn-faster-r-cnn-yolo-object-detection-algorithms-36d53571365e)*

### F-RCNN - Fast R-CNN

Cette approche est similaire √† R-CNN, mais les r√©gions sont d√©finies apr√®s que les couches de convolution ont √©t√© appliqu√©es.

![FRCNN](../../../../../translated_images/f-rcnn.3cda6d9bb41888754037d2d9763e2298a96de5d9bc2a21db3147357aa5da9b1a.fr.png)

> Image tir√©e de [l'Article Officiel](https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Girshick_Fast_R-CNN_ICCV_2015_paper.pdf), [arXiv](https://arxiv.org/pdf/1504.08083.pdf), 2015

### Faster R-CNN

L'id√©e principale de cette approche est d'utiliser un r√©seau neuronal pour pr√©dire les ROI - le *R√©seau de Propositions de R√©gions*. [Article](https://arxiv.org/pdf/1506.01497.pdf), 2016

![FasterRCNN](../../../../../translated_images/faster-rcnn.8d46c099b87ef30ab2ea26dbc4bdd85b974a57ba8eb526f65dc4cd0a4711de30.fr.png)

> Image tir√©e de [l'article officiel](https://arxiv.org/pdf/1506.01497.pdf)

### R-FCN : R√©seau Compl√®tement Convolutif Bas√© sur les R√©gions

Cet algorithme est encore plus rapide que Faster R-CNN. L'id√©e principale est la suivante :

1. Nous extrayons des caract√©ristiques √† l'aide de ResNet-101.
2. Les caract√©ristiques sont trait√©es par une **Carte de Scores Sensibles √† la Position**. Chaque objet parmi les $C$ classes est divis√© en $k\times k$ r√©gions, et nous entra√Ænons le r√©seau √† pr√©dire des parties d'objets.
3. Pour chaque partie des $k\times k$ r√©gions, tous les r√©seaux votent pour les classes d'objets, et la classe d'objet avec le maximum de votes est s√©lectionn√©e.

![r-fcn image](../../../../../translated_images/r-fcn.13eb88158b99a3da50fa2787a6be5cb310d47f0e9655cc93a1090dc7aab338d1.fr.png)

> Image tir√©e de [l'article officiel](https://arxiv.org/abs/1605.06409)

### YOLO - You Only Look Once

YOLO est un algorithme en temps r√©el en un seul passage. L'id√©e principale est la suivante :

 * L'image est divis√©e en $S\times S$ r√©gions.
 * Pour chaque r√©gion, le **CNN** pr√©dit $n$ objets possibles, les coordonn√©es des *bo√Ætes englobantes* et la *confiance* = *probabilit√©* * IoU.

 ![YOLO](../../../../../translated_images/yolo.a2648ec82ee8bb4ea27537677adb482fd4b733ca1705c561b6a24a85102dced5.fr.png)

> Image tir√©e de [l'article officiel](https://arxiv.org/abs/1506.02640)

### Autres Algorithmes

* RetinaNet : [article officiel](https://arxiv.org/abs/1708.02002)
   - [Impl√©mentation PyTorch dans Torchvision](https://pytorch.org/vision/stable/_modules/torchvision/models/detection/retinanet.html)
   - [Impl√©mentation Keras](https://github.com/fizyr/keras-retinanet)
   - [D√©tection d'Objets avec RetinaNet](https://keras.io/examples/vision/retinanet/) dans les exemples Keras
* SSD (Single Shot Detector) : [article officiel](https://arxiv.org/abs/1512.02325)

## ‚úçÔ∏è Exercices : D√©tection d'Objets

Poursuivez votre apprentissage dans le cahier suivant :

[ObjectDetection.ipynb](../../../../../lessons/4-ComputerVision/11-ObjectDetection/ObjectDetection.ipynb)

## Conclusion

Dans cette le√ßon, vous avez explor√© un aper√ßu des diff√©rentes fa√ßons dont la d√©tection d'objets peut √™tre r√©alis√©e !

## üöÄ D√©fi

Lisez ces articles et cahiers sur YOLO et essayez-les par vous-m√™me :

* [Bon article de blog](https://www.analyticsvidhya.com/blog/2018/12/practical-guide-object-detection-yolo-framewor-python/) d√©crivant YOLO
 * [Site officiel](https://pjreddie.com/darknet/yolo/)
 * YOLO : [Impl√©mentation Keras](https://github.com/experiencor/keras-yolo2), [cahier √©tape par √©tape](https://github.com/experiencor/basic-yolo-keras/blob/master/Yolo%20Step-by-Step.ipynb)
 * YOLO v2 : [Impl√©mentation Keras](https://github.com/experiencor/keras-yolo2), [cahier √©tape par √©tape](https://github.com/experiencor/keras-yolo2/blob/master/Yolo%20Step-by-Step.ipynb)

## [Quiz apr√®s le cours](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/211)

## R√©vision & √âtude Personnelle

* [D√©tection d'Objets](https://tjmachinelearning.com/lectures/1718/obj/) par Nikhil Sardana
* [Une bonne comparaison des algorithmes de d√©tection d'objets](https://lilianweng.github.io/lil-log/2018/12/27/object-detection-part-4.html)
* [Revue des Algorithmes de Deep Learning pour la D√©tection d'Objets](https://medium.com/comet-app/review-of-deep-learning-algorithms-for-object-detection-c1f3d437b852)
* [Introduction √âtape par √âtape aux Algorithmes de D√©tection d'Objets de Base](https://www.analyticsvidhya.com/blog/2018/10/a-step-by-step-introduction-to-the-basic-object-detection-algorithms-part-1/)
* [Impl√©mentation de Faster R-CNN en Python pour la D√©tection d'Objets](https://www.analyticsvidhya.com/blog/2018/11/implementation-faster-r-cnn-python-object-detection/)

## [Devoir : D√©tection d'Objets](lab/README.md)

**Avertissement** :  
Ce document a √©t√© traduit √† l'aide du service de traduction automatique [Co-op Translator](https://github.com/Azure/co-op-translator). Bien que nous nous efforcions d'assurer l'exactitude, veuillez noter que les traductions automatis√©es peuvent contenir des erreurs ou des inexactitudes. Le document original dans sa langue d'origine doit √™tre consid√©r√© comme la source faisant autorit√©. Pour des informations critiques, il est recommand√© de recourir √† une traduction humaine professionnelle. Nous d√©clinons toute responsabilit√© en cas de malentendus ou d'interpr√©tations erron√©es r√©sultant de l'utilisation de cette traduction.