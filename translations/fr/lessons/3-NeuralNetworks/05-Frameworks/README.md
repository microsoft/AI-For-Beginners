# Frameworks de r√©seaux neuronaux

Comme nous l'avons d√©j√† appris, pour entra√Æner efficacement des r√©seaux neuronaux, nous devons faire deux choses :

* Manipuler des tenseurs, par exemple les multiplier, les additionner et calculer certaines fonctions comme sigmoid ou softmax.
* Calculer les gradients de toutes les expressions afin de r√©aliser l'optimisation par descente de gradient.

## [Quiz avant le cours](https://ff-quizzes.netlify.app/en/ai/quiz/9)

Bien que la biblioth√®que `numpy` puisse effectuer la premi√®re t√¢che, nous avons besoin d'un m√©canisme pour calculer les gradients. Dans [notre framework](../04-OwnFramework/OwnFramework.ipynb) que nous avons d√©velopp√© dans la section pr√©c√©dente, nous avons d√ª programmer manuellement toutes les fonctions d√©riv√©es dans la m√©thode `backward`, qui r√©alise la r√©tropropagation. Id√©alement, un framework devrait nous permettre de calculer les gradients de *n'importe quelle expression* que nous pouvons d√©finir.

Un autre aspect important est de pouvoir effectuer des calculs sur GPU, ou sur d'autres unit√©s de calcul sp√©cialis√©es, comme le [TPU](https://en.wikipedia.org/wiki/Tensor_Processing_Unit). L'entra√Ænement des r√©seaux neuronaux profonds n√©cessite *beaucoup* de calculs, et la possibilit√© de parall√©liser ces calculs sur des GPUs est cruciale.

> ‚úÖ Le terme "parall√©liser" signifie r√©partir les calculs sur plusieurs dispositifs.

Actuellement, les deux frameworks de r√©seaux neuronaux les plus populaires sont : [TensorFlow](http://TensorFlow.org) et [PyTorch](https://pytorch.org/). Les deux offrent une API de bas niveau pour manipuler les tenseurs sur CPU et GPU. En plus de l'API de bas niveau, il existe √©galement une API de haut niveau, appel√©e [Keras](https://keras.io/) et [PyTorch Lightning](https://pytorchlightning.ai/) respectivement.

API de bas niveau | [TensorFlow](http://TensorFlow.org) | [PyTorch](https://pytorch.org/)
------------------|-------------------------------------|--------------------------------
API de haut niveau| [Keras](https://keras.io/) | [PyTorch Lightning](https://pytorchlightning.ai/)

**Les APIs de bas niveau** dans les deux frameworks permettent de construire ce qu'on appelle des **graphes computationnels**. Ce graphe d√©finit comment calculer la sortie (g√©n√©ralement la fonction de perte) avec des param√®tres d'entr√©e donn√©s, et peut √™tre envoy√© pour calcul sur GPU, si disponible. Il existe des fonctions pour diff√©rencier ce graphe computationnel et calculer les gradients, qui peuvent ensuite √™tre utilis√©s pour optimiser les param√®tres du mod√®le.

**Les APIs de haut niveau** consid√®rent les r√©seaux neuronaux comme une **s√©quence de couches**, et simplifient consid√©rablement la construction de la plupart des r√©seaux neuronaux. L'entra√Ænement du mod√®le n√©cessite g√©n√©ralement de pr√©parer les donn√©es, puis d'appeler une fonction `fit` pour effectuer le travail.

L'API de haut niveau permet de construire des r√©seaux neuronaux typiques tr√®s rapidement sans se soucier de nombreux d√©tails. En revanche, l'API de bas niveau offre beaucoup plus de contr√¥le sur le processus d'entra√Ænement, et est donc largement utilis√©e en recherche, notamment lorsqu'on travaille sur de nouvelles architectures de r√©seaux neuronaux.

Il est √©galement important de comprendre que vous pouvez utiliser les deux APIs ensemble. Par exemple, vous pouvez d√©velopper votre propre architecture de couche de r√©seau avec l'API de bas niveau, puis l'utiliser dans un r√©seau plus large construit et entra√Æn√© avec l'API de haut niveau. Ou vous pouvez d√©finir un r√©seau avec l'API de haut niveau comme une s√©quence de couches, puis utiliser votre propre boucle d'entra√Ænement de bas niveau pour effectuer l'optimisation. Les deux APIs reposent sur les m√™mes concepts fondamentaux et sont con√ßues pour bien fonctionner ensemble.

## Apprentissage

Dans ce cours, nous proposons la plupart du contenu √† la fois pour PyTorch et TensorFlow. Vous pouvez choisir votre framework pr√©f√©r√© et ne parcourir que les notebooks correspondants. Si vous ne savez pas quel framework choisir, consultez des discussions sur Internet concernant **PyTorch vs. TensorFlow**. Vous pouvez √©galement examiner les deux frameworks pour mieux les comprendre.

Lorsque cela est possible, nous utiliserons les APIs de haut niveau pour simplifier les choses. Cependant, nous pensons qu'il est important de comprendre comment fonctionnent les r√©seaux neuronaux depuis la base. Ainsi, au d√©but, nous travaillons avec l'API de bas niveau et les tenseurs. Cependant, si vous souhaitez avancer rapidement et ne pas passer trop de temps √† apprendre ces d√©tails, vous pouvez les ignorer et passer directement aux notebooks de l'API de haut niveau.

## ‚úçÔ∏è Exercices : Frameworks

Poursuivez votre apprentissage dans les notebooks suivants :

API de bas niveau | [Notebook TensorFlow+Keras](IntroKerasTF.ipynb) | [PyTorch](IntroPyTorch.ipynb)
------------------|-------------------------------------|--------------------------------
API de haut niveau| [Keras](IntroKeras.ipynb) | *PyTorch Lightning*

Apr√®s avoir ma√Ætris√© les frameworks, r√©capitulons la notion de surapprentissage.

# Surapprentissage

Le surapprentissage est un concept extr√™mement important en apprentissage automatique, et il est essentiel de bien le comprendre !

Consid√©rons le probl√®me suivant d'approximation de 5 points (repr√©sent√©s par des `x` sur les graphiques ci-dessous) :

![linear](../../../../../translated_images/fr/overfit1.f24b71c6f652e59e.webp) | ![overfit](../../../../../translated_images/fr/overfit2.131f5800ae10ca5e.webp)
-------------------------|--------------------------
**Mod√®le lin√©aire, 2 param√®tres** | **Mod√®le non lin√©aire, 7 param√®tres**
Erreur d'entra√Ænement = 5.3 | Erreur d'entra√Ænement = 0
Erreur de validation = 5.1 | Erreur de validation = 20

* √Ä gauche, nous voyons une bonne approximation par une ligne droite. Parce que le nombre de param√®tres est ad√©quat, le mod√®le saisit correctement la distribution des points.
* √Ä droite, le mod√®le est trop puissant. Comme nous n'avons que 5 points et que le mod√®le a 7 param√®tres, il peut s'ajuster de mani√®re √† passer par tous les points, rendant l'erreur d'entra√Ænement √©gale √† 0. Cependant, cela emp√™che le mod√®le de comprendre le v√©ritable motif derri√®re les donn√©es, ce qui entra√Æne une erreur de validation tr√®s √©lev√©e.

Il est crucial de trouver un √©quilibre correct entre la richesse du mod√®le (nombre de param√®tres) et le nombre d'√©chantillons d'entra√Ænement.

## Pourquoi le surapprentissage se produit-il ?

  * Pas assez de donn√©es d'entra√Ænement
  * Mod√®le trop puissant
  * Trop de bruit dans les donn√©es d'entr√©e

## Comment d√©tecter le surapprentissage ?

Comme vous pouvez le voir sur le graphique ci-dessus, le surapprentissage peut √™tre d√©tect√© par une erreur d'entra√Ænement tr√®s faible et une erreur de validation √©lev√©e. Normalement, pendant l'entra√Ænement, nous verrons les erreurs d'entra√Ænement et de validation commencer √† diminuer, puis √† un certain moment, l'erreur de validation pourrait arr√™ter de diminuer et commencer √† augmenter. Ce sera un signe de surapprentissage, et un indicateur que nous devrions probablement arr√™ter l'entra√Ænement √† ce moment-l√† (ou au moins faire une sauvegarde du mod√®le).

![surapprentissage](../../../../../translated_images/fr/Overfitting.408ad91cd90b4371.webp)

## Comment pr√©venir le surapprentissage ?

Si vous constatez que le surapprentissage se produit, vous pouvez faire l'une des choses suivantes :

 * Augmenter la quantit√© de donn√©es d'entra√Ænement
 * R√©duire la complexit√© du mod√®le
 * Utiliser une [technique de r√©gularisation](../../4-ComputerVision/08-TransferLearning/TrainingTricks.md), comme le [Dropout](../../4-ComputerVision/08-TransferLearning/TrainingTricks.md#Dropout), que nous examinerons plus tard.

## Surapprentissage et compromis biais-variance

Le surapprentissage est en r√©alit√© un cas d'un probl√®me plus g√©n√©ral en statistique appel√© [compromis biais-variance](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff). Si nous consid√©rons les sources possibles d'erreur dans notre mod√®le, nous pouvons identifier deux types d'erreurs :

* **Les erreurs de biais** sont caus√©es par notre algorithme qui ne parvient pas √† capturer correctement la relation entre les donn√©es d'entra√Ænement. Cela peut r√©sulter du fait que notre mod√®le n'est pas assez puissant (**sous-apprentissage**).
* **Les erreurs de variance**, qui sont caus√©es par le mod√®le qui approxime le bruit dans les donn√©es d'entr√©e au lieu de la relation significative (**surapprentissage**).

Pendant l'entra√Ænement, l'erreur de biais diminue (car notre mod√®le apprend √† approximer les donn√©es), et l'erreur de variance augmente. Il est important d'arr√™ter l'entra√Ænement - soit manuellement (lorsque nous d√©tectons le surapprentissage), soit automatiquement (en introduisant une r√©gularisation) - pour √©viter le surapprentissage.

## Conclusion

Dans cette le√ßon, vous avez appris les diff√©rences entre les diverses APIs des deux frameworks d'IA les plus populaires, TensorFlow et PyTorch. En outre, vous avez d√©couvert un sujet tr√®s important : le surapprentissage.

## üöÄ D√©fi

Dans les notebooks associ√©s, vous trouverez des "t√¢ches" en bas ; parcourez les notebooks et compl√©tez les t√¢ches.

## [Quiz apr√®s le cours](https://ff-quizzes.netlify.app/en/ai/quiz/10)

## R√©vision et auto-apprentissage

Faites des recherches sur les sujets suivants :

- TensorFlow
- PyTorch
- Surapprentissage

Posez-vous les questions suivantes :

- Quelle est la diff√©rence entre TensorFlow et PyTorch ?
- Quelle est la diff√©rence entre surapprentissage et sous-apprentissage ?

## [Devoir](lab/README.md)

Dans ce laboratoire, vous √™tes invit√© √† r√©soudre deux probl√®mes de classification en utilisant des r√©seaux enti√®rement connect√©s √† une ou plusieurs couches avec PyTorch ou TensorFlow.

* [Instructions](lab/README.md)
* [Notebook](lab/LabFrameworks.ipynb)

---

