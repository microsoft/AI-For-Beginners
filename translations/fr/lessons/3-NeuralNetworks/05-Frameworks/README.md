<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "2b544f20b796402507fb05a0df893323",
  "translation_date": "2025-08-24T20:56:06+00:00",
  "source_file": "lessons/3-NeuralNetworks/05-Frameworks/README.md",
  "language_code": "fr"
}
-->
# Frameworks de R√©seaux de Neurones

Comme nous l'avons d√©j√† appris, pour entra√Æner efficacement des r√©seaux de neurones, nous devons faire deux choses :

* Travailler sur des tenseurs, par exemple pour multiplier, additionner et calculer certaines fonctions comme sigmoid ou softmax.
* Calculer les gradients de toutes les expressions, afin d'effectuer l'optimisation par descente de gradient.

## [Quiz avant le cours](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/105)

Bien que la biblioth√®que `numpy` puisse g√©rer la premi√®re partie, nous avons besoin d'un m√©canisme pour calculer les gradients. Dans [notre framework](../../../../../lessons/3-NeuralNetworks/04-OwnFramework/OwnFramework.ipynb) que nous avons d√©velopp√© dans la section pr√©c√©dente, nous avons d√ª programmer manuellement toutes les fonctions d√©riv√©es dans la m√©thode `backward`, qui effectue la r√©tropropagation. Id√©alement, un framework devrait nous permettre de calculer les gradients de *n'importe quelle expression* que nous pouvons d√©finir.

Un autre point important est de pouvoir effectuer des calculs sur GPU, ou sur d'autres unit√©s de calcul sp√©cialis√©es, comme les [TPU](https://en.wikipedia.org/wiki/Tensor_Processing_Unit). L'entra√Ænement des r√©seaux de neurones profonds n√©cessite *beaucoup* de calculs, et il est crucial de pouvoir parall√©liser ces calculs sur des GPUs.

> ‚úÖ Le terme "parall√©liser" signifie distribuer les calculs sur plusieurs dispositifs.

Actuellement, les deux frameworks de r√©seaux de neurones les plus populaires sont : [TensorFlow](http://TensorFlow.org) et [PyTorch](https://pytorch.org/). Les deux offrent une API de bas niveau pour travailler avec des tenseurs sur CPU et GPU. En plus de l'API de bas niveau, il existe √©galement une API de haut niveau, appel√©e [Keras](https://keras.io/) et [PyTorch Lightning](https://pytorchlightning.ai/) respectivement.

API de bas niveau | [TensorFlow](http://TensorFlow.org) | [PyTorch](https://pytorch.org/)
-------------------|-------------------------------------|--------------------------------
API de haut niveau | [Keras](https://keras.io/) | [PyTorch Lightning](https://pytorchlightning.ai/)

Les **APIs de bas niveau** dans les deux frameworks permettent de construire ce qu'on appelle des **graphes computationnels**. Ce graphe d√©finit comment calculer la sortie (g√©n√©ralement la fonction de perte) avec des param√®tres d'entr√©e donn√©s, et peut √™tre envoy√© pour calcul sur GPU, si disponible. Il existe des fonctions pour diff√©rencier ce graphe computationnel et calculer les gradients, qui peuvent ensuite √™tre utilis√©s pour optimiser les param√®tres du mod√®le.

Les **APIs de haut niveau** consid√®rent les r√©seaux de neurones comme une **s√©quence de couches**, ce qui rend la construction de la plupart des r√©seaux de neurones beaucoup plus simple. L'entra√Ænement du mod√®le n√©cessite g√©n√©ralement de pr√©parer les donn√©es, puis d'appeler une fonction `fit` pour effectuer le travail.

L'API de haut niveau permet de construire des r√©seaux de neurones typiques tr√®s rapidement sans se soucier de nombreux d√©tails. En revanche, l'API de bas niveau offre beaucoup plus de contr√¥le sur le processus d'entra√Ænement, et est donc largement utilis√©e en recherche, lorsqu'on travaille sur de nouvelles architectures de r√©seaux de neurones.

Il est √©galement important de comprendre que vous pouvez utiliser les deux APIs ensemble. Par exemple, vous pouvez d√©velopper votre propre architecture de couche de r√©seau avec l'API de bas niveau, puis l'utiliser dans un r√©seau plus large construit et entra√Æn√© avec l'API de haut niveau. Ou vous pouvez d√©finir un r√©seau avec l'API de haut niveau comme une s√©quence de couches, puis utiliser votre propre boucle d'entra√Ænement de bas niveau pour effectuer l'optimisation. Les deux APIs reposent sur les m√™mes concepts fondamentaux et sont con√ßues pour bien fonctionner ensemble.

## Apprentissage

Dans ce cours, nous proposons la plupart des contenus √† la fois pour PyTorch et TensorFlow. Vous pouvez choisir votre framework pr√©f√©r√© et ne parcourir que les notebooks correspondants. Si vous ne savez pas quel framework choisir, lisez quelques discussions sur Internet concernant **PyTorch vs. TensorFlow**. Vous pouvez √©galement examiner les deux frameworks pour mieux les comprendre.

Dans la mesure du possible, nous utiliserons les APIs de haut niveau pour simplifier. Cependant, nous pensons qu'il est important de comprendre comment fonctionnent les r√©seaux de neurones depuis la base. Ainsi, au d√©but, nous travaillons avec l'API de bas niveau et les tenseurs. Cependant, si vous souhaitez avancer rapidement et ne pas passer trop de temps √† apprendre ces d√©tails, vous pouvez les ignorer et passer directement aux notebooks sur les APIs de haut niveau.

## ‚úçÔ∏è Exercices : Frameworks

Poursuivez votre apprentissage dans les notebooks suivants :

API de bas niveau | [Notebook TensorFlow+Keras](../../../../../lessons/3-NeuralNetworks/05-Frameworks/IntroKerasTF.ipynb) | [PyTorch](../../../../../lessons/3-NeuralNetworks/05-Frameworks/IntroPyTorch.ipynb)
-------------------|-------------------------------------|--------------------------------
API de haut niveau | [Keras](../../../../../lessons/3-NeuralNetworks/05-Frameworks/IntroKeras.ipynb) | *PyTorch Lightning*

Apr√®s avoir ma√Ætris√© les frameworks, r√©capitulons la notion de surapprentissage.

# Surapprentissage

Le surapprentissage est un concept extr√™mement important en apprentissage automatique, et il est crucial de bien le comprendre !

Consid√©rons le probl√®me suivant d'approximation de 5 points (repr√©sent√©s par des `x` sur les graphiques ci-dessous) :

![linear](../../../../../translated_images/overfit1.f24b71c6f652e59e6bed7245ffbeaecc3ba320e16e2221f6832b432052c4da43.fr.jpg) | ![overfit](../../../../../translated_images/overfit2.131f5800ae10ca5e41d12a411f5f705d9ee38b1b10916f284b787028dd55cc1c.fr.jpg)
-------------------------|--------------------------
**Mod√®le lin√©aire, 2 param√®tres** | **Mod√®le non lin√©aire, 7 param√®tres**
Erreur d'entra√Ænement = 5.3 | Erreur d'entra√Ænement = 0
Erreur de validation = 5.1 | Erreur de validation = 20

* √Ä gauche, nous voyons une bonne approximation par une ligne droite. Comme le nombre de param√®tres est ad√©quat, le mod√®le comprend bien la distribution des points.
* √Ä droite, le mod√®le est trop puissant. Comme nous n'avons que 5 points et que le mod√®le a 7 param√®tres, il peut s'ajuster de mani√®re √† passer par tous les points, ce qui rend l'erreur d'entra√Ænement √©gale √† 0. Cependant, cela emp√™che le mod√®le de comprendre le bon sch√©ma des donn√©es, ce qui entra√Æne une erreur de validation tr√®s √©lev√©e.

Il est crucial de trouver un √©quilibre correct entre la richesse du mod√®le (nombre de param√®tres) et le nombre d'√©chantillons d'entra√Ænement.

## Pourquoi le surapprentissage se produit-il ?

  * Pas assez de donn√©es d'entra√Ænement
  * Mod√®le trop puissant
  * Trop de bruit dans les donn√©es d'entr√©e

## Comment d√©tecter le surapprentissage ?

Comme vous pouvez le voir sur le graphique ci-dessus, le surapprentissage peut √™tre d√©tect√© par une erreur d'entra√Ænement tr√®s faible et une erreur de validation √©lev√©e. Normalement, pendant l'entra√Ænement, nous voyons √† la fois les erreurs d'entra√Ænement et de validation diminuer, puis √† un moment donn√©, l'erreur de validation peut cesser de diminuer et commencer √† augmenter. Cela sera un signe de surapprentissage, et un indicateur qu'il faut probablement arr√™ter l'entra√Ænement √† ce moment-l√† (ou au moins faire une sauvegarde du mod√®le).

![overfitting](../../../../../translated_images/Overfitting.408ad91cd90b4371d0a81f4287e1409c359751adeb1ae450332af50e84f08c3e.fr.png)

## Comment pr√©venir le surapprentissage ?

Si vous constatez que le surapprentissage se produit, vous pouvez faire l'une des choses suivantes :

 * Augmenter la quantit√© de donn√©es d'entra√Ænement
 * R√©duire la complexit√© du mod√®le
 * Utiliser une [technique de r√©gularisation](../../4-ComputerVision/08-TransferLearning/TrainingTricks.md), comme le [Dropout](../../4-ComputerVision/08-TransferLearning/TrainingTricks.md#Dropout), que nous examinerons plus tard.

## Surapprentissage et compromis Biais-Variance

Le surapprentissage est en fait un cas d'un probl√®me plus g√©n√©ral en statistiques appel√© [compromis Biais-Variance](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff). Si nous consid√©rons les sources possibles d'erreur dans notre mod√®le, nous pouvons distinguer deux types d'erreurs :

* Les **erreurs de biais** sont caus√©es par notre algorithme qui n'est pas capable de capturer correctement la relation entre les donn√©es d'entra√Ænement. Cela peut r√©sulter du fait que notre mod√®le n'est pas assez puissant (**sous-apprentissage**).
* Les **erreurs de variance**, qui sont caus√©es par le mod√®le qui approxime le bruit dans les donn√©es d'entr√©e au lieu de la relation significative (**surapprentissage**).

Pendant l'entra√Ænement, l'erreur de biais diminue (car notre mod√®le apprend √† approximer les donn√©es), et l'erreur de variance augmente. Il est important d'arr√™ter l'entra√Ænement - soit manuellement (lorsque nous d√©tectons le surapprentissage) soit automatiquement (en introduisant une r√©gularisation) - pour √©viter le surapprentissage.

## Conclusion

Dans cette le√ßon, vous avez appris les diff√©rences entre les diverses APIs des deux frameworks d'IA les plus populaires, TensorFlow et PyTorch. De plus, vous avez d√©couvert un sujet tr√®s important : le surapprentissage.

## üöÄ D√©fi

Dans les notebooks associ√©s, vous trouverez des "t√¢ches" en bas ; parcourez les notebooks et compl√©tez les t√¢ches.

## [Quiz apr√®s le cours](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/205)

## R√©vision & Auto-apprentissage

Faites des recherches sur les sujets suivants :

- TensorFlow
- PyTorch
- Surapprentissage

Posez-vous les questions suivantes :

- Quelle est la diff√©rence entre TensorFlow et PyTorch ?
- Quelle est la diff√©rence entre surapprentissage et sous-apprentissage ?

## [Devoir](lab/README.md)

Dans ce laboratoire, vous √™tes invit√© √† r√©soudre deux probl√®mes de classification en utilisant des r√©seaux enti√®rement connect√©s √† une ou plusieurs couches avec PyTorch ou TensorFlow.

* [Instructions](lab/README.md)
* [Notebook](../../../../../lessons/3-NeuralNetworks/05-Frameworks/lab/LabFrameworks.ipynb)

**Avertissement** :  
Ce document a √©t√© traduit √† l'aide du service de traduction automatique [Co-op Translator](https://github.com/Azure/co-op-translator). Bien que nous nous efforcions d'assurer l'exactitude, veuillez noter que les traductions automatis√©es peuvent contenir des erreurs ou des inexactitudes. Le document original dans sa langue d'origine doit √™tre consid√©r√© comme la source faisant autorit√©. Pour des informations critiques, il est recommand√© de recourir √† une traduction professionnelle r√©alis√©e par un humain. Nous d√©clinons toute responsabilit√© en cas de malentendus ou d'interpr√©tations erron√©es r√©sultant de l'utilisation de cette traduction.