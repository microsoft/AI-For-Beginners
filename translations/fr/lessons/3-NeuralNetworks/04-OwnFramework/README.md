# Introduction aux r√©seaux neuronaux. Perceptron multicouche

Dans la section pr√©c√©dente, vous avez d√©couvert le mod√®le de r√©seau neuronal le plus simple : le perceptron √† une couche, un mod√®le de classification lin√©aire √† deux classes.

Dans cette section, nous allons √©tendre ce mod√®le pour en faire un cadre plus flexible, nous permettant de :

* effectuer une **classification multi-classes** en plus de la classification √† deux classes
* r√©soudre des **probl√®mes de r√©gression** en plus de la classification
* s√©parer des classes qui ne sont pas lin√©airement s√©parables

Nous allons √©galement d√©velopper notre propre cadre modulaire en Python, qui nous permettra de construire diff√©rentes architectures de r√©seaux neuronaux.

## [Quiz avant le cours](https://ff-quizzes.netlify.app/en/ai/quiz/7)

## Formalisation de l'apprentissage automatique

Commen√ßons par formaliser le probl√®me de l'apprentissage automatique. Supposons que nous disposons d'un ensemble de donn√©es d'entra√Ænement **X** avec des √©tiquettes **Y**, et que nous devons construire un mod√®le *f* qui fera des pr√©dictions les plus pr√©cises possibles. La qualit√© des pr√©dictions est mesur√©e par une **fonction de perte** &lagran;. Les fonctions de perte suivantes sont souvent utilis√©es :

* Pour un probl√®me de r√©gression, lorsque nous devons pr√©dire un nombre, nous pouvons utiliser **l'erreur absolue** &sum;<sub>i</sub>|f(x<sup>(i)</sup>)-y<sup>(i)</sup>|, ou **l'erreur quadratique** &sum;<sub>i</sub>(f(x<sup>(i)</sup>)-y<sup>(i)</sup>)<sup>2</sup>
* Pour la classification, nous utilisons la **perte 0-1** (qui est essentiellement la m√™me chose que **l'exactitude** du mod√®le), ou la **perte logistique**.

Pour un perceptron √† une couche, la fonction *f* √©tait d√©finie comme une fonction lin√©aire *f(x)=wx+b* (ici *w* est la matrice de poids, *x* est le vecteur des caract√©ristiques d'entr√©e, et *b* est le vecteur de biais). Pour diff√©rentes architectures de r√©seaux neuronaux, cette fonction peut prendre une forme plus complexe.

> Dans le cas de la classification, il est souvent souhaitable d'obtenir des probabilit√©s des classes correspondantes en sortie du r√©seau. Pour convertir des nombres arbitraires en probabilit√©s (par exemple, pour normaliser la sortie), nous utilisons souvent la fonction **softmax** &sigma;, et la fonction *f* devient *f(x)=&sigma;(wx+b)*.

Dans la d√©finition de *f* ci-dessus, *w* et *b* sont appel√©s **param√®tres** &theta;=‚ü®*w,b*‚ü©. √âtant donn√© l'ensemble de donn√©es ‚ü®**X**,**Y**‚ü©, nous pouvons calculer une erreur globale sur l'ensemble des donn√©es en fonction des param√®tres &theta;.

> ‚úÖ **L'objectif de l'entra√Ænement d'un r√©seau neuronal est de minimiser l'erreur en faisant varier les param√®tres &theta;.**

## Optimisation par descente de gradient

Il existe une m√©thode bien connue d'optimisation de fonction appel√©e **descente de gradient**. L'id√©e est que nous pouvons calculer une d√©riv√©e (dans le cas multidimensionnel appel√©e **gradient**) de la fonction de perte par rapport aux param√®tres, et faire varier les param√®tres de mani√®re √† r√©duire l'erreur. Cela peut √™tre formalis√© comme suit :

* Initialiser les param√®tres avec des valeurs al√©atoires w<sup>(0)</sup>, b<sup>(0)</sup>
* R√©p√©ter les √©tapes suivantes plusieurs fois :
    - w<sup>(i+1)</sup> = w<sup>(i)</sup>-&eta;&part;&lagran;/&part;w
    - b<sup>(i+1)</sup> = b<sup>(i)</sup>-&eta;&part;&lagran;/&part;b

Pendant l'entra√Ænement, les √©tapes d'optimisation sont cens√©es √™tre calcul√©es en tenant compte de l'ensemble des donn√©es (rappelez-vous que la perte est calcul√©e comme une somme sur tous les √©chantillons d'entra√Ænement). Cependant, dans la pratique, nous prenons de petites portions de l'ensemble de donn√©es appel√©es **minibatches**, et calculons les gradients sur un sous-ensemble de donn√©es. Comme le sous-ensemble est choisi al√©atoirement √† chaque fois, cette m√©thode est appel√©e **descente de gradient stochastique** (SGD).

## Perceptrons multicouches et r√©tropropagation

Un r√©seau √† une couche, comme nous l'avons vu plus haut, est capable de classer des classes lin√©airement s√©parables. Pour construire un mod√®le plus riche, nous pouvons combiner plusieurs couches du r√©seau. Math√©matiquement, cela signifie que la fonction *f* aurait une forme plus complexe et serait calcul√©e en plusieurs √©tapes :
* z<sub>1</sub>=w<sub>1</sub>x+b<sub>1</sub>
* z<sub>2</sub>=w<sub>2</sub>&alpha;(z<sub>1</sub>)+b<sub>2</sub>
* f = &sigma;(z<sub>2</sub>)

Ici, &alpha; est une **fonction d'activation non lin√©aire**, &sigma; est une fonction softmax, et les param√®tres &theta;=<*w<sub>1</sub>,b<sub>1</sub>,w<sub>2</sub>,b<sub>2</sub>*>.

L'algorithme de descente de gradient reste le m√™me, mais le calcul des gradients devient plus complexe. En utilisant la r√®gle de diff√©renciation en cha√Æne, nous pouvons calculer les d√©riv√©es comme suit :

* &part;&lagran;/&part;w<sub>2</sub> = (&part;&lagran;/&part;&sigma;)(&part;&sigma;/&part;z<sub>2</sub>)(&part;z<sub>2</sub>/&part;w<sub>2</sub>)
* &part;&lagran;/&part;w<sub>1</sub> = (&part;&lagran;/&part;&sigma;)(&part;&sigma;/&part;z<sub>2</sub>)(&part;z<sub>2</sub>/&part;&alpha;)(&part;&alpha;/&part;z<sub>1</sub>)(&part;z<sub>1</sub>/&part;w<sub>1</sub>)

> ‚úÖ La r√®gle de diff√©renciation en cha√Æne est utilis√©e pour calculer les d√©riv√©es de la fonction de perte par rapport aux param√®tres.

Notez que la partie la plus √† gauche de toutes ces expressions est la m√™me, et nous pouvons donc calculer efficacement les d√©riv√©es en commen√ßant par la fonction de perte et en remontant "en arri√®re" √† travers le graphe computationnel. Ainsi, la m√©thode d'entra√Ænement d'un perceptron multicouche est appel√©e **r√©tropropagation**, ou 'backprop'.

<img alt="graphe de calcul" src="../../../../../translated_images/fr/ComputeGraphGrad.4626252c0de03507.webp"/>

> TODO : citation de l'image

> ‚úÖ Nous aborderons la r√©tropropagation en d√©tail dans notre exemple de notebook.  

## Conclusion

Dans cette le√ßon, nous avons construit notre propre biblioth√®que de r√©seaux neuronaux, et nous l'avons utilis√©e pour une t√¢che simple de classification en deux dimensions.

## üöÄ D√©fi

Dans le notebook associ√©, vous allez impl√©menter votre propre cadre pour construire et entra√Æner des perceptrons multicouches. Vous pourrez voir en d√©tail comment fonctionnent les r√©seaux neuronaux modernes.

Passez au notebook [OwnFramework](OwnFramework.ipynb) et travaillez dessus.

## [Quiz apr√®s le cours](https://ff-quizzes.netlify.app/en/ai/quiz/8)

## R√©vision et auto-apprentissage

La r√©tropropagation est un algorithme courant utilis√© en IA et en apprentissage automatique, qui m√©rite d'√™tre √©tudi√© [plus en d√©tail](https://wikipedia.org/wiki/Backpropagation).

## [Devoir](lab/README.md)

Dans ce laboratoire, vous √™tes invit√© √† utiliser le cadre que vous avez construit dans cette le√ßon pour r√©soudre la classification des chiffres manuscrits MNIST.

* [Instructions](lab/README.md)
* [Notebook](lab/MyFW_MNIST.ipynb)

---

