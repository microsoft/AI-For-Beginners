<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "dbacf9b1915612981d76059678e563e5",
  "translation_date": "2025-08-26T07:32:49+00:00",
  "source_file": "lessons/6-Other/22-DeepRL/README.md",
  "language_code": "tr"
}
-->
# Derin PekiÅŸtirmeli Ã–ÄŸrenme

PekiÅŸtirmeli Ã¶ÄŸrenme (RL), denetimli Ã¶ÄŸrenme ve denetimsiz Ã¶ÄŸrenme ile birlikte temel makine Ã¶ÄŸrenimi paradigmalarÄ±ndan biri olarak gÃ¶rÃ¼lÃ¼r. Denetimli Ã¶ÄŸrenmede bilinen sonuÃ§lara sahip bir veri setine dayanÄ±rken, RL **yaparak Ã¶ÄŸrenme** ilkesine dayanÄ±r. Ã–rneÄŸin, bir bilgisayar oyununu ilk kez gÃ¶rdÃ¼ÄŸÃ¼mÃ¼zde, kurallarÄ± bilmeden oynamaya baÅŸlarÄ±z ve sadece oynama ve davranÄ±ÅŸlarÄ±mÄ±zÄ± ayarlama sÃ¼reciyle becerilerimizi geliÅŸtirebiliriz.

## [Ders Ã–ncesi Test](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/122)

RL gerÃ§ekleÅŸtirmek iÃ§in ihtiyacÄ±mÄ±z olanlar:

* Oyunun kurallarÄ±nÄ± belirleyen bir **ortam** veya **simÃ¼latÃ¶r**. Deneyleri simÃ¼latÃ¶rde Ã§alÄ±ÅŸtÄ±rabilmeli ve sonuÃ§larÄ± gÃ¶zlemleyebilmeliyiz.
* Deneyimizin ne kadar baÅŸarÄ±lÄ± olduÄŸunu gÃ¶steren bir **Ã¶dÃ¼l fonksiyonu**. Ã–rneÄŸin, bir bilgisayar oyununu Ã¶ÄŸrenirken Ã¶dÃ¼l, nihai puanÄ±mÄ±z olabilir.

Ã–dÃ¼l fonksiyonuna dayanarak davranÄ±ÅŸlarÄ±mÄ±zÄ± ayarlayabilir ve becerilerimizi geliÅŸtirebiliriz, bÃ¶ylece bir sonraki sefer daha iyi oynarÄ±z. DiÄŸer makine Ã¶ÄŸrenimi tÃ¼rleri ile RL arasÄ±ndaki temel fark, RL'de genellikle oyunu bitirene kadar kazanÄ±p kazanmadÄ±ÄŸÄ±mÄ±zÄ± bilmememizdir. Bu nedenle, belirli bir hamlenin tek baÅŸÄ±na iyi olup olmadÄ±ÄŸÄ±nÄ± sÃ¶yleyemeyiz - Ã¶dÃ¼lÃ¼ yalnÄ±zca oyunun sonunda alÄ±rÄ±z.

RL sÄ±rasÄ±nda genellikle birÃ§ok deney gerÃ§ekleÅŸtiririz. Her deney sÄ±rasÄ±nda, ÅŸimdiye kadar Ã¶ÄŸrendiÄŸimiz en iyi stratejiyi takip etmek (**sÃ¶mÃ¼rÃ¼**) ile yeni olasÄ± durumlarÄ± keÅŸfetmek (**keÅŸif**) arasÄ±nda bir denge kurmamÄ±z gerekir.

## OpenAI Gym

RL iÃ§in harika bir araÃ§ [OpenAI Gym](https://gym.openai.com/) - birÃ§ok farklÄ± ortamÄ± simÃ¼le edebilen bir **simÃ¼lasyon ortamÄ±**. Atari oyunlarÄ±ndan, direk dengeleme fiziÄŸine kadar Ã§eÅŸitli ortamlarÄ± simÃ¼le edebilir. PekiÅŸtirmeli Ã¶ÄŸrenme algoritmalarÄ±nÄ± eÄŸitmek iÃ§in en popÃ¼ler simÃ¼lasyon ortamlarÄ±ndan biridir ve [OpenAI](https://openai.com/) tarafÄ±ndan geliÅŸtirilmiÅŸtir.

> **Note**: OpenAI Gym'deki tÃ¼m mevcut ortamlarÄ± [buradan](https://gym.openai.com/envs/#classic_control) gÃ¶rebilirsiniz.

## CartPole Dengeleme

Muhtemelen hepiniz modern dengeleme cihazlarÄ±nÄ±, Ã¶rneÄŸin *Segway* veya *Gyroscooter* gÃ¶rmÃ¼ÅŸsÃ¼nÃ¼zdÃ¼r. Bu cihazlar, bir ivmeÃ¶lÃ§er veya jiroskoptan gelen sinyale yanÄ±t olarak tekerleklerini ayarlayarak otomatik olarak dengede durabilir. Bu bÃ¶lÃ¼mde, benzer bir problemi - bir direÄŸi dengelemeyi - nasÄ±l Ã§Ã¶zeceÄŸimizi Ã¶ÄŸreneceÄŸiz. Bu, bir sirk sanatÃ§Ä±sÄ±nÄ±n elinde bir direÄŸi dengelemesi gibi bir duruma benzer - ancak bu dengeleme yalnÄ±zca 1D'de gerÃ§ekleÅŸir.

Dengelemenin basitleÅŸtirilmiÅŸ bir versiyonu **CartPole** problemi olarak bilinir. CartPole dÃ¼nyasÄ±nda, sola veya saÄŸa hareket edebilen yatay bir kaydÄ±rÄ±cÄ± vardÄ±r ve hedef, kaydÄ±rÄ±cÄ±nÄ±n hareketi sÄ±rasÄ±nda dikey bir direÄŸi dengede tutmaktÄ±r.

<img alt="bir cartpole" src="images/cartpole.png" width="200"/>

Bu ortamÄ± oluÅŸturmak ve kullanmak iÃ§in birkaÃ§ satÄ±r Python koduna ihtiyacÄ±mÄ±z var:

```python
import gym
env = gym.make("CartPole-v1")

env.reset()
done = False
total_reward = 0
while not done:
   env.render()
   action = env.action_space.sample()
   observaton, reward, done, info = env.step(action)
   total_reward += reward

print(f"Total reward: {total_reward}")
```

Her ortam aynÄ± ÅŸekilde eriÅŸilebilir:
* `env.reset` yeni bir deneyi baÅŸlatÄ±r
* `env.step` bir simÃ¼lasyon adÄ±mÄ± gerÃ§ekleÅŸtirir. **Eylem alanÄ±ndan** bir **eylem** alÄ±r ve bir **gÃ¶zlem** (gÃ¶zlem alanÄ±ndan), Ã¶dÃ¼l ve bir bitiÅŸ bayraÄŸÄ± dÃ¶ndÃ¼rÃ¼r.

YukarÄ±daki Ã¶rnekte her adÄ±mda rastgele bir eylem gerÃ§ekleÅŸtiriyoruz, bu yÃ¼zden deneyin Ã¶mrÃ¼ Ã§ok kÄ±sa:

![dengelemeyen cartpole](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-nobalance.gif)

Bir RL algoritmasÄ±nÄ±n amacÄ±, bir model - **politika** Ï€ - eÄŸitmek ve bu modelin verilen bir duruma yanÄ±t olarak eylemi dÃ¶ndÃ¼rmesini saÄŸlamaktÄ±r. PolitikayÄ± olasÄ±lÄ±ksal olarak da dÃ¼ÅŸÃ¼nebiliriz, Ã¶rneÄŸin herhangi bir durum *s* ve eylem *a* iÃ§in Ï€(*a*|*s*) olasÄ±lÄ±ÄŸÄ±nÄ± dÃ¶ndÃ¼rÃ¼r, yani *s* durumunda *a* eylemini gerÃ§ekleÅŸtirmemiz gerektiÄŸini belirtir.

## Politika GradyanlarÄ± AlgoritmasÄ±

Bir politikayÄ± modellemenin en aÃ§Ä±k yolu, durumlarÄ± giriÅŸ olarak alacak ve karÅŸÄ±lÄ±k gelen eylemleri (veya daha doÄŸrusu tÃ¼m eylemlerin olasÄ±lÄ±klarÄ±nÄ±) dÃ¶ndÃ¼recek bir sinir aÄŸÄ± oluÅŸturmaktÄ±r. Bir anlamda, bu normal bir sÄ±nÄ±flandÄ±rma gÃ¶revine benzer olurdu, ancak bÃ¼yÃ¼k bir farkla - her adÄ±mda hangi eylemleri gerÃ§ekleÅŸtirmemiz gerektiÄŸini Ã¶nceden bilmiyoruz.

Buradaki fikir, bu olasÄ±lÄ±klarÄ± tahmin etmektir. Deneyin her adÄ±mÄ±nda toplam Ã¶dÃ¼lÃ¼mÃ¼zÃ¼ gÃ¶steren bir **kÃ¼mÃ¼latif Ã¶dÃ¼ller** vektÃ¶rÃ¼ oluÅŸtururuz. AyrÄ±ca, Ã¶nceki Ã¶dÃ¼llerin rolÃ¼nÃ¼ azaltmak iÃ§in Ã¶nceki Ã¶dÃ¼lleri Î³=0.99 gibi bir katsayÄ± ile Ã§arparak **Ã¶dÃ¼l indirgeme** uygularÄ±z. Daha sonra, daha bÃ¼yÃ¼k Ã¶dÃ¼ller saÄŸlayan deney yolundaki adÄ±mlarÄ± gÃ¼Ã§lendiririz.

> Politika GradyanlarÄ± algoritmasÄ± hakkÄ±nda daha fazla bilgi edinin ve [Ã¶rnek not defterinde](../../../../../lessons/6-Other/22-DeepRL/CartPole-RL-TF.ipynb) uygulamasÄ±nÄ± gÃ¶rÃ¼n.

## AktÃ¶r-Kritik AlgoritmasÄ±

Politika GradyanlarÄ± yaklaÅŸÄ±mÄ±nÄ±n geliÅŸtirilmiÅŸ bir versiyonu **AktÃ¶r-Kritik** olarak adlandÄ±rÄ±lÄ±r. Bunun arkasÄ±ndaki temel fikir, sinir aÄŸÄ±nÄ±n iki ÅŸeyi dÃ¶ndÃ¼recek ÅŸekilde eÄŸitilmesidir:

* Hangi eylemi gerÃ§ekleÅŸtireceÄŸimizi belirleyen politika. Bu kÄ±sma **aktÃ¶r** denir.
* Bu durumdaki toplam Ã¶dÃ¼l beklentimizi tahmin eden kÄ±sÄ±m - bu kÄ±sma **kritik** denir.

Bir anlamda, bu yapÄ± [GAN](../../4-ComputerVision/10-GANs/README.md)'a benzer, burada iki aÄŸ birbirine karÅŸÄ± eÄŸitilir. AktÃ¶r-kritik modelde, aktÃ¶r gerÃ§ekleÅŸtirmemiz gereken eylemi Ã¶nerir ve kritik, sonucu tahmin etmeye Ã§alÄ±ÅŸÄ±r. Ancak, amacÄ±mÄ±z bu aÄŸlarÄ± uyum iÃ§inde eÄŸitmektir.

Deney sÄ±rasÄ±nda hem gerÃ§ek kÃ¼mÃ¼latif Ã¶dÃ¼lleri hem de kritiÄŸin dÃ¶ndÃ¼rdÃ¼ÄŸÃ¼ sonuÃ§larÄ± bildiÄŸimiz iÃ§in, aralarÄ±ndaki farkÄ± en aza indirecek bir kayÄ±p fonksiyonu oluÅŸturmak nispeten kolaydÄ±r. Bu bize **kritik kaybÄ±** verir. **AktÃ¶r kaybÄ±nÄ±** ise politika gradyanlarÄ± algoritmasÄ±nda olduÄŸu gibi hesaplayabiliriz.

Bu algoritmalardan birini Ã§alÄ±ÅŸtÄ±rdÄ±ktan sonra, CartPole'umuzun ÅŸu ÅŸekilde davranmasÄ±nÄ± bekleyebiliriz:

![dengeleyen cartpole](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-balance.gif)

## âœï¸ AlÄ±ÅŸtÄ±rmalar: Politika GradyanlarÄ± ve AktÃ¶r-Kritik RL

AÅŸaÄŸÄ±daki not defterlerinde Ã¶ÄŸrenmeye devam edin:

* [TensorFlow ile RL](../../../../../lessons/6-Other/22-DeepRL/CartPole-RL-TF.ipynb)
* [PyTorch ile RL](../../../../../lessons/6-Other/22-DeepRL/CartPole-RL-PyTorch.ipynb)

## DiÄŸer RL GÃ¶revleri

PekiÅŸtirmeli Ã¶ÄŸrenme gÃ¼nÃ¼mÃ¼zde hÄ±zla bÃ¼yÃ¼yen bir araÅŸtÄ±rma alanÄ±dÄ±r. PekiÅŸtirmeli Ã¶ÄŸrenmenin bazÄ± ilginÃ§ Ã¶rnekleri ÅŸunlardÄ±r:

* Bilgisayara **Atari OyunlarÄ±** oynatmayÄ± Ã¶ÄŸretmek. Bu problemin zorlu kÄ±smÄ±, basit bir durumun bir vektÃ¶r olarak temsil edilmemesi, bunun yerine bir ekran gÃ¶rÃ¼ntÃ¼sÃ¼ olmasÄ±dÄ±r - ve bu ekran gÃ¶rÃ¼ntÃ¼sÃ¼nÃ¼ bir Ã¶zellik vektÃ¶rÃ¼ne dÃ¶nÃ¼ÅŸtÃ¼rmek veya Ã¶dÃ¼l bilgisi Ã§Ä±karmak iÃ§in CNN kullanmamÄ±z gerekir. Atari oyunlarÄ± Gym'de mevcuttur.
* Bilgisayara satranÃ§ ve Go gibi masa oyunlarÄ± oynamayÄ± Ã¶ÄŸretmek. Son zamanlarda **Alpha Zero** gibi en son teknoloji programlar, iki ajanÄ±n birbirine karÅŸÄ± oynayÄ±p her adÄ±mda geliÅŸmesiyle sÄ±fÄ±rdan eÄŸitildi.
* EndÃ¼stride, RL simÃ¼lasyondan kontrol sistemleri oluÅŸturmak iÃ§in kullanÄ±lÄ±r. [Bonsai](https://azure.microsoft.com/services/project-bonsai/?WT.mc_id=academic-77998-cacaste) adlÄ± bir hizmet Ã¶zellikle bunun iÃ§in tasarlanmÄ±ÅŸtÄ±r.

## SonuÃ§

ArtÄ±k bir Ã¶dÃ¼l fonksiyonu saÄŸlayarak ve arama alanÄ±nÄ± akÄ±llÄ±ca keÅŸfetme fÄ±rsatÄ± vererek, ajanlarÄ± iyi sonuÃ§lar elde etmek iÃ§in nasÄ±l eÄŸiteceÄŸimizi Ã¶ÄŸrendik. Ä°ki algoritmayÄ± baÅŸarÄ±yla denedik ve nispeten kÄ±sa bir sÃ¼rede iyi bir sonuÃ§ elde ettik. Ancak, bu RL yolculuÄŸunuzun sadece baÅŸlangÄ±cÄ±dÄ±r ve daha derinlemesine Ã¶ÄŸrenmek istiyorsanÄ±z ayrÄ± bir kurs almayÄ± kesinlikle dÃ¼ÅŸÃ¼nmelisiniz.

## ğŸš€ Meydan Okuma

'DiÄŸer RL GÃ¶revleri' bÃ¶lÃ¼mÃ¼nde listelenen uygulamalarÄ± keÅŸfedin ve birini uygulamayÄ± deneyin!

## [Ders SonrasÄ± Test](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/222)

## Ä°nceleme ve Kendi Kendine Ã‡alÄ±ÅŸma

[BaÅŸlangÄ±Ã§ Seviyesi iÃ§in Makine Ã–ÄŸrenimi MÃ¼fredatÄ±mÄ±zda](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/README.md) klasik pekiÅŸtirmeli Ã¶ÄŸrenme hakkÄ±nda daha fazla bilgi edinin.

BilgisayarÄ±n Super Mario oynamayÄ± nasÄ±l Ã¶ÄŸrenebileceÄŸini anlatan [bu harika videoyu](https://www.youtube.com/watch?v=qv6UVOQ0F44) izleyin.

## Ã–dev: [Bir DaÄŸ ArabasÄ±nÄ± EÄŸitin](lab/README.md)

Bu Ã¶dev sÄ±rasÄ±nda hedefiniz farklÄ± bir Gym ortamÄ±nÄ± - [Mountain Car](https://www.gymlibrary.ml/environments/classic_control/mountain_car/) - eÄŸitmek olacaktÄ±r.

**Feragatname**:  
Bu belge, AI Ã§eviri hizmeti [Co-op Translator](https://github.com/Azure/co-op-translator) kullanÄ±larak Ã§evrilmiÅŸtir. DoÄŸruluk iÃ§in Ã§aba gÃ¶stersek de, otomatik Ã§evirilerin hata veya yanlÄ±ÅŸlÄ±klar iÃ§erebileceÄŸini lÃ¼tfen unutmayÄ±n. Belgenin orijinal dili, yetkili kaynak olarak kabul edilmelidir. Kritik bilgiler iÃ§in profesyonel insan Ã§evirisi Ã¶nerilir. Bu Ã§evirinin kullanÄ±mÄ±ndan kaynaklanan yanlÄ±ÅŸ anlamalar veya yanlÄ±ÅŸ yorumlamalar iÃ§in sorumluluk kabul etmiyoruz.