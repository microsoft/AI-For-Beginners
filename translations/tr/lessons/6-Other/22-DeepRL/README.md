# Derin PekiÅŸtirmeli Ã–ÄŸrenme

PekiÅŸtirmeli Ã¶ÄŸrenme (RL), denetimli Ã¶ÄŸrenme ve denetimsiz Ã¶ÄŸrenme ile birlikte temel makine Ã¶ÄŸrenimi paradigmalarÄ±ndan biri olarak gÃ¶rÃ¼lÃ¼r. Denetimli Ã¶ÄŸrenmede bilinen sonuÃ§lara sahip bir veri setine dayanÄ±rken, RL **yaparak Ã¶ÄŸrenme** prensibine dayanÄ±r. Ã–rneÄŸin, bir bilgisayar oyununu ilk kez gÃ¶rdÃ¼ÄŸÃ¼mÃ¼zde, kurallarÄ± bilmeden oynamaya baÅŸlarÄ±z ve sadece oynayarak ve davranÄ±ÅŸlarÄ±mÄ±zÄ± ayarlayarak becerilerimizi geliÅŸtirebiliriz.

## [Ders Ã–ncesi Test](https://ff-quizzes.netlify.app/en/ai/quiz/43)

RL gerÃ§ekleÅŸtirmek iÃ§in ihtiyacÄ±mÄ±z olanlar:

* Oyunun kurallarÄ±nÄ± belirleyen bir **ortam** veya **simÃ¼latÃ¶r**. SimÃ¼latÃ¶rde deneyler yapabilmeli ve sonuÃ§larÄ± gÃ¶zlemleyebilmeliyiz.
* Deneyimizin ne kadar baÅŸarÄ±lÄ± olduÄŸunu gÃ¶steren bir **Ã¶dÃ¼l fonksiyonu**. Ã–rneÄŸin, bir bilgisayar oyununu Ã¶ÄŸrenirken Ã¶dÃ¼l, final skorumuz olabilir.

Ã–dÃ¼l fonksiyonuna dayanarak davranÄ±ÅŸlarÄ±mÄ±zÄ± ayarlayabilir ve becerilerimizi geliÅŸtirebiliriz, bÃ¶ylece bir sonraki sefer daha iyi oynarÄ±z. RL ile diÄŸer makine Ã¶ÄŸrenimi tÃ¼rleri arasÄ±ndaki temel fark, RL'de genellikle oyunu bitirene kadar kazanÄ±p kazanmadÄ±ÄŸÄ±mÄ±zÄ± bilmememizdir. Bu nedenle, tek bir hamlenin iyi olup olmadÄ±ÄŸÄ±nÄ± sÃ¶yleyemeyiz - Ã¶dÃ¼lÃ¼ yalnÄ±zca oyunun sonunda alÄ±rÄ±z.

RL sÄ±rasÄ±nda genellikle birÃ§ok deney yaparÄ±z. Her deney sÄ±rasÄ±nda, ÅŸimdiye kadar Ã¶ÄŸrendiÄŸimiz en iyi stratejiyi takip etmek (**kullanÄ±m**) ile yeni olasÄ± durumlarÄ± keÅŸfetmek (**keÅŸif**) arasÄ±nda bir denge kurmamÄ±z gerekir.

## OpenAI Gym

RL iÃ§in harika bir araÃ§ [OpenAI Gym](https://gym.openai.com/) - birÃ§ok farklÄ± ortamÄ± simÃ¼le edebilen bir **simÃ¼lasyon ortamÄ±**. Atari oyunlarÄ±ndan, direk dengeleme fiziÄŸine kadar Ã§eÅŸitli ortamlarÄ± simÃ¼le edebilir. RL algoritmalarÄ±nÄ± eÄŸitmek iÃ§in en popÃ¼ler simÃ¼lasyon ortamlarÄ±ndan biridir ve [OpenAI](https://openai.com/) tarafÄ±ndan geliÅŸtirilmiÅŸtir.

> **Not**: OpenAI Gym'deki tÃ¼m mevcut ortamlarÄ± [buradan](https://gym.openai.com/envs/#classic_control) gÃ¶rebilirsiniz.

## CartPole Dengeleme

Muhtemelen hepiniz modern dengeleme cihazlarÄ±nÄ±, Ã¶rneÄŸin *Segway* veya *Gyroscooter* gÃ¶rmÃ¼ÅŸsÃ¼nÃ¼zdÃ¼r. Bu cihazlar, bir ivmeÃ¶lÃ§er veya jiroskoptan gelen sinyale yanÄ±t olarak tekerleklerini ayarlayarak otomatik olarak denge saÄŸlayabilir. Bu bÃ¶lÃ¼mde, benzer bir problemi - bir direÄŸi dengelemeyi - nasÄ±l Ã§Ã¶zeceÄŸimizi Ã¶ÄŸreneceÄŸiz. Bu, bir sirk sanatÃ§Ä±sÄ±nÄ±n elinde bir direÄŸi dengelemesi gibi bir durumdur - ancak bu dengeleme yalnÄ±zca 1D'de gerÃ§ekleÅŸir.

Dengelemenin basitleÅŸtirilmiÅŸ bir versiyonu **CartPole** problemi olarak bilinir. CartPole dÃ¼nyasÄ±nda, sola veya saÄŸa hareket edebilen yatay bir kaydÄ±rÄ±cÄ±ya sahibiz ve hedef, kaydÄ±rÄ±cÄ±nÄ±n Ã¼zerinde dikey bir direÄŸi dengede tutmaktÄ±r.

<img alt="bir cartpole" src="../../../../../translated_images/tr/cartpole.f52a67f27e058170.webp" width="200"/>

Bu ortamÄ± oluÅŸturmak ve kullanmak iÃ§in birkaÃ§ satÄ±r Python koduna ihtiyacÄ±mÄ±z var:

```python
import gym
env = gym.make("CartPole-v1")

env.reset()
done = False
total_reward = 0
while not done:
   env.render()
   action = env.action_space.sample()
   observaton, reward, done, info = env.step(action)
   total_reward += reward

print(f"Total reward: {total_reward}")
```

Her ortam tam olarak aynÄ± ÅŸekilde eriÅŸilebilir:
* `env.reset` yeni bir deneyi baÅŸlatÄ±r
* `env.step` bir simÃ¼lasyon adÄ±mÄ± gerÃ§ekleÅŸtirir. **Eylem alanÄ±ndan** bir **eylem** alÄ±r ve bir **gÃ¶zlem** (gÃ¶zlem alanÄ±ndan), bir Ã¶dÃ¼l ve bir bitiÅŸ bayraÄŸÄ± dÃ¶ndÃ¼rÃ¼r.

YukarÄ±daki Ã¶rnekte her adÄ±mda rastgele bir eylem gerÃ§ekleÅŸtiriyoruz, bu yÃ¼zden deneyin Ã¶mrÃ¼ Ã§ok kÄ±sa:

![dengelemeyen cartpole](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-nobalance.gif)

Bir RL algoritmasÄ±nÄ±n amacÄ±, bir model - **politika** &pi; - eÄŸitmek ve bu modelin verilen bir duruma yanÄ±t olarak eylemi dÃ¶ndÃ¼rmesini saÄŸlamaktÄ±r. PolitikayÄ± olasÄ±lÄ±ksal olarak da dÃ¼ÅŸÃ¼nebiliriz, Ã¶rneÄŸin herhangi bir durum *s* ve eylem *a* iÃ§in &pi;(*a*|*s*) olasÄ±lÄ±ÄŸÄ±nÄ± dÃ¶ndÃ¼rÃ¼r, yani *s* durumunda *a* eylemini gerÃ§ekleÅŸtirmemiz gerektiÄŸini belirtir.

## Politika GradyanlarÄ± AlgoritmasÄ±

Bir politikayÄ± modellemenin en aÃ§Ä±k yolu, durumlarÄ± girdi olarak alacak ve karÅŸÄ±lÄ±k gelen eylemleri (veya daha doÄŸrusu tÃ¼m eylemlerin olasÄ±lÄ±klarÄ±nÄ±) dÃ¶ndÃ¼recek bir sinir aÄŸÄ± oluÅŸturmaktÄ±r. Bir anlamda, bu normal bir sÄ±nÄ±flandÄ±rma gÃ¶revine benzer, ancak bÃ¼yÃ¼k bir fark vardÄ±r - her adÄ±mda hangi eylemleri gerÃ§ekleÅŸtirmemiz gerektiÄŸini Ã¶nceden bilmiyoruz.

Buradaki fikir, bu olasÄ±lÄ±klarÄ± tahmin etmektir. Deneyin her adÄ±mÄ±nda toplam Ã¶dÃ¼lÃ¼mÃ¼zÃ¼ gÃ¶steren bir **kÃ¼mÃ¼latif Ã¶dÃ¼ller** vektÃ¶rÃ¼ oluÅŸtururuz. AyrÄ±ca, Ã¶nceki Ã¶dÃ¼llerin rolÃ¼nÃ¼ azaltmak iÃ§in Ã¶nceki Ã¶dÃ¼lleri &gamma;=0.99 gibi bir katsayÄ± ile Ã§arparak **Ã¶dÃ¼l indirgeme** uygularÄ±z. Daha sonra, daha bÃ¼yÃ¼k Ã¶dÃ¼ller saÄŸlayan deney yolundaki adÄ±mlarÄ± gÃ¼Ã§lendiririz.

> Politika GradyanlarÄ± algoritmasÄ± hakkÄ±nda daha fazla bilgi edinin ve [Ã¶rnek not defterinde](CartPole-RL-TF.ipynb) uygulamasÄ±nÄ± gÃ¶rÃ¼n.

## AktÃ¶r-Kritik AlgoritmasÄ±

Politika GradyanlarÄ± yaklaÅŸÄ±mÄ±nÄ±n geliÅŸtirilmiÅŸ bir versiyonu **AktÃ¶r-Kritik** olarak adlandÄ±rÄ±lÄ±r. Bunun arkasÄ±ndaki temel fikir, sinir aÄŸÄ±nÄ±n iki ÅŸeyi dÃ¶ndÃ¼recek ÅŸekilde eÄŸitilmesidir:

* Hangi eylemi gerÃ§ekleÅŸtireceÄŸimizi belirleyen politika - bu kÄ±sma **aktÃ¶r** denir.
* Bu durumda almayÄ± bekleyebileceÄŸimiz toplam Ã¶dÃ¼lÃ¼n tahmini - bu kÄ±sma **kritik** denir.

Bir anlamda, bu mimari [GAN](../../4-ComputerVision/10-GANs/README.md) modeline benzer, burada birbirine karÅŸÄ± eÄŸitilen iki aÄŸÄ±mÄ±z vardÄ±r. AktÃ¶r-kritik modelde, aktÃ¶r gerÃ§ekleÅŸtirmemiz gereken eylemi Ã¶nerir ve kritik, sonucu tahmin etmeye Ã§alÄ±ÅŸÄ±r. Ancak, amacÄ±mÄ±z bu aÄŸlarÄ± uyum iÃ§inde eÄŸitmektir.

Deney sÄ±rasÄ±nda hem gerÃ§ek kÃ¼mÃ¼latif Ã¶dÃ¼lleri hem de kritiÄŸin dÃ¶ndÃ¼rdÃ¼ÄŸÃ¼ sonuÃ§larÄ± bildiÄŸimiz iÃ§in, aralarÄ±ndaki farkÄ± en aza indirecek bir kayÄ±p fonksiyonu oluÅŸturmak nispeten kolaydÄ±r. Bu bize **kritik kaybÄ±** verir. **AktÃ¶r kaybÄ±nÄ±** ise politika gradyanlarÄ± algoritmasÄ±nda olduÄŸu gibi hesaplayabiliriz.

Bu algoritmalardan birini Ã§alÄ±ÅŸtÄ±rdÄ±ktan sonra, CartPole'umuzun ÅŸu ÅŸekilde davranmasÄ±nÄ± bekleyebiliriz:

![dengeleyen cartpole](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-balance.gif)

## âœï¸ AlÄ±ÅŸtÄ±rmalar: Politika GradyanlarÄ± ve AktÃ¶r-Kritik RL

AÅŸaÄŸÄ±daki not defterlerinde Ã¶ÄŸrenmeye devam edin:

* [TensorFlow ile RL](CartPole-RL-TF.ipynb)
* [PyTorch ile RL](CartPole-RL-PyTorch.ipynb)

## DiÄŸer RL GÃ¶revleri

PekiÅŸtirmeli Ã–ÄŸrenme gÃ¼nÃ¼mÃ¼zde hÄ±zla bÃ¼yÃ¼yen bir araÅŸtÄ±rma alanÄ±dÄ±r. PekiÅŸtirmeli Ã¶ÄŸrenmenin bazÄ± ilginÃ§ Ã¶rnekleri ÅŸunlardÄ±r:

* Bilgisayara **Atari OyunlarÄ±** oynamayÄ± Ã¶ÄŸretmek. Bu problemin zorlu kÄ±smÄ±, basit bir durumun bir vektÃ¶r olarak temsil edilmemesi, bunun yerine bir ekran gÃ¶rÃ¼ntÃ¼sÃ¼ olmasÄ±dÄ±r - ve bu ekran gÃ¶rÃ¼ntÃ¼sÃ¼nÃ¼ bir Ã¶zellik vektÃ¶rÃ¼ne dÃ¶nÃ¼ÅŸtÃ¼rmek veya Ã¶dÃ¼l bilgisi Ã§Ä±karmak iÃ§in CNN kullanmamÄ±z gerekir. Atari oyunlarÄ± Gym'de mevcuttur.
* Bilgisayara satranÃ§ ve Go gibi masa oyunlarÄ± oynamayÄ± Ã¶ÄŸretmek. Son zamanlarda **Alpha Zero** gibi en son teknoloji programlar, iki ajanÄ±n birbirine karÅŸÄ± oynayarak ve her adÄ±mda geliÅŸerek sÄ±fÄ±rdan eÄŸitildi.
* EndÃ¼stride, simÃ¼lasyondan kontrol sistemleri oluÅŸturmak iÃ§in RL kullanÄ±lÄ±r. [Bonsai](https://azure.microsoft.com/services/project-bonsai/?WT.mc_id=academic-77998-cacaste) adlÄ± bir hizmet Ã¶zellikle bunun iÃ§in tasarlanmÄ±ÅŸtÄ±r.

## SonuÃ§

ArtÄ±k bir Ã¶dÃ¼l fonksiyonu saÄŸlayarak ve arama alanÄ±nÄ± akÄ±llÄ±ca keÅŸfetme fÄ±rsatÄ± vererek ajanlarÄ± iyi sonuÃ§lar elde etmek iÃ§in nasÄ±l eÄŸiteceÄŸimizi Ã¶ÄŸrendik. Ä°ki algoritmayÄ± baÅŸarÄ±yla denedik ve nispeten kÄ±sa bir sÃ¼rede iyi bir sonuÃ§ elde ettik. Ancak, bu RL yolculuÄŸunuzun sadece baÅŸlangÄ±cÄ±dÄ±r ve daha derinlemesine Ã¶ÄŸrenmek istiyorsanÄ±z ayrÄ± bir kurs almayÄ± kesinlikle dÃ¼ÅŸÃ¼nmelisiniz.

## ğŸš€ Meydan Okuma

'DiÄŸer RL GÃ¶revleri' bÃ¶lÃ¼mÃ¼nde listelenen uygulamalarÄ± keÅŸfedin ve birini uygulamayÄ± deneyin!

## [Ders SonrasÄ± Test](https://ff-quizzes.netlify.app/en/ai/quiz/44)

## GÃ¶zden GeÃ§irme ve Kendi Kendine Ã‡alÄ±ÅŸma

Klasik pekiÅŸtirmeli Ã¶ÄŸrenme hakkÄ±nda daha fazla bilgi edinin: [Makine Ã–ÄŸrenimi iÃ§in BaÅŸlangÄ±Ã§ MÃ¼fredatÄ±mÄ±z](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/README.md).

BilgisayarÄ±n Super Mario oynamayÄ± nasÄ±l Ã¶ÄŸrenebileceÄŸini anlatan [bu harika videoyu](https://www.youtube.com/watch?v=qv6UVOQ0F44) izleyin.

## Ã–dev: [Bir DaÄŸ ArabasÄ±nÄ± EÄŸitin](lab/README.md)

Bu Ã¶dev sÄ±rasÄ±nda hedefiniz, farklÄ± bir Gym ortamÄ±nÄ± - [Mountain Car](https://www.gymlibrary.ml/environments/classic_control/mountain_car/) - eÄŸitmek olacaktÄ±r.

---

