# Derin PekiÅŸtirmeli Ã–ÄŸrenme

PekiÅŸtirmeli Ã¶ÄŸrenme (RL), denetimli Ã¶ÄŸrenme ve denetimsiz Ã¶ÄŸrenme ile birlikte temel makine Ã¶ÄŸrenimi paradigmalardan biri olarak gÃ¶rÃ¼lmektedir. Denetimli Ã¶ÄŸrenmede bilinen sonuÃ§lara sahip bir veri setine dayanÄ±rken, RL **yaparak Ã¶ÄŸrenme** temellidir. Ã–rneÄŸin, bir bilgisayar oyunu ile ilk kez karÅŸÄ±laÅŸtÄ±ÄŸÄ±mÄ±zda, kurallarÄ± bilmeden oynamaya baÅŸlarÄ±z ve kÄ±sa sÃ¼re iÃ§inde oynama ve davranÄ±ÅŸÄ±mÄ±zÄ± ayarlama sÃ¼reci sayesinde becerilerimizi geliÅŸtirmeye baÅŸlarÄ±z.

## [Ã–n ders anketi](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/122)

RL gerÃ§ekleÅŸtirmek iÃ§in ÅŸunlara ihtiyacÄ±mÄ±z var:

* Oyun kurallarÄ±nÄ± belirleyen bir **Ã§evre** veya **simÃ¼latÃ¶r**. SimÃ¼latÃ¶rde deneyleri gerÃ§ekleÅŸtirebilmeli ve sonuÃ§larÄ± gÃ¶zlemleyebilmeliyiz.
* Deneyimizin ne kadar baÅŸarÄ±lÄ± olduÄŸunu gÃ¶steren bir **Ã¶dÃ¼l fonksiyonu**. Bir bilgisayar oyunu oynamayÄ± Ã¶ÄŸrenme durumunda, Ã¶dÃ¼l nihai puanÄ±mÄ±z olur.

Ã–dÃ¼l fonksiyonuna dayanarak, davranÄ±ÅŸÄ±mÄ±zÄ± ayarlayÄ±p becerilerimizi geliÅŸtirmeliyiz, bÃ¶ylece bir sonraki oyunumuzda daha iyi oynayabiliriz. DiÄŸer makine Ã¶ÄŸrenimi tÃ¼rleri ile RL arasÄ±ndaki ana fark, RL'de oyunu bitirene kadar kazandÄ±ÄŸÄ±mÄ±zÄ± veya kaybettiÄŸimizi bilmememizdir. Bu nedenle, belirli bir hamlenin iyi mi kÃ¶tÃ¼ mÃ¼ olduÄŸunu sÃ¶yleyemeyiz - yalnÄ±zca oyunun sonunda bir Ã¶dÃ¼l alÄ±rÄ±z.

RL sÄ±rasÄ±nda genellikle birÃ§ok deney gerÃ§ekleÅŸtiririz. Her deneyde, ÅŸimdiye kadar Ã¶ÄŸrendiÄŸimiz optimal stratejiyi takip etme (**sÃ¶mÃ¼rÃ¼**) ve yeni olasÄ± durumlarÄ± keÅŸfetme (**keÅŸif**) arasÄ±nda denge kurmamÄ±z gerekir.

## OpenAI Gym

RL iÃ§in harika bir araÃ§ olan [OpenAI Gym](https://gym.openai.com/) - farklÄ± Ã§evreleri simÃ¼le edebilen bir **simÃ¼lasyon ortamÄ±**. Atari oyunlarÄ±ndan, denge Ã§ubuÄŸunun arkasÄ±ndaki fiziÄŸe kadar birÃ§ok farklÄ± Ã§evreyi simÃ¼le edebilir. PekiÅŸtirmeli Ã¶ÄŸrenme algoritmalarÄ±nÄ± eÄŸitmek iÃ§in en popÃ¼ler simÃ¼lasyon ortamlarÄ±ndan biridir ve [OpenAI](https://openai.com/) tarafÄ±ndan bakÄ±m yapÄ±lmaktadÄ±r.

> **Not**: OpenAI Gym'de mevcut olan tÃ¼m Ã§evreleri [buradan](https://gym.openai.com/envs/#classic_control) gÃ¶rebilirsiniz.

## CartPole Dengeleme

Muhtemelen *Segway* veya *Gyroscooter* gibi modern dengeleme cihazlarÄ±nÄ± gÃ¶rmÃ¼ÅŸsÃ¼nÃ¼zdÃ¼r. Bu cihazlar, bir ivmeÃ¶lÃ§er veya jiroskoptan gelen bir sinyale yanÄ±t olarak tekerleklerini ayarlayarak otomatik olarak denge saÄŸlarlar. Bu bÃ¶lÃ¼mde, benzer bir problemi - bir direÄŸi dengelemeyi - Ã¶ÄŸreneceÄŸiz. Bu, bir sirk sanatÃ§Ä±sÄ±nÄ±n elinde bir direÄŸi dengelemesi durumuna benzer - ancak bu direk dengesi yalnÄ±zca 1D'de gerÃ§ekleÅŸir.

Dengelemenin basitleÅŸtirilmiÅŸ bir versiyonu **CartPole** problemi olarak bilinir. CartPole dÃ¼nyasÄ±nda, sola veya saÄŸa hareket edebilen yatay bir kaydÄ±rÄ±cÄ±ya sahibiz ve amaÃ§, kaydÄ±rÄ±cÄ±nÄ±n Ã¼stÃ¼nde dik bir direÄŸi dengede tutmaktÄ±r.

<img alt="bir cartpole" src="images/cartpole.png" width="200"/>

Bu Ã§evreyi oluÅŸturmak ve kullanmak iÃ§in birkaÃ§ satÄ±r Python koduna ihtiyacÄ±mÄ±z var:

```python
import gym
env = gym.make("CartPole-v1")

env.reset()
done = False
total_reward = 0
while not done:
   env.render()
   action = env.action_space.sample()
   observaton, reward, done, info = env.step(action)
   total_reward += reward

print(f"Total reward: {total_reward}")
```

Her Ã§evre tam olarak aynÄ± ÅŸekilde eriÅŸilebilir:
* `env.reset` starts a new experiment
* `env.step` bir simÃ¼lasyon adÄ±mÄ± gerÃ§ekleÅŸtirir. **Eylem** alanÄ±ndan bir **eylem** alÄ±r ve bir **gÃ¶zlem** (gÃ¶zlem alanÄ±ndan), ayrÄ±ca bir Ã¶dÃ¼l ve bir sonlandÄ±rma bayraÄŸÄ± dÃ¶ndÃ¼rÃ¼r.

YukarÄ±daki Ã¶rnekte, her adÄ±mda rastgele bir eylem gerÃ§ekleÅŸtiriyoruz, bu nedenle deneyin Ã¶mrÃ¼ Ã§ok kÄ±sadÄ±r:

![dengelemeyen cartpole](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-nobalance.gif)

Bir RL algoritmasÄ±nÄ±n amacÄ±, belirli bir duruma yanÄ±t olarak eylemi dÃ¶ndÃ¼ren bir modeli - sÃ¶zde **politika** Ï€ - eÄŸitmektir. PolitikanÄ±n olasÄ±lÄ±ksal olarak da dÃ¼ÅŸÃ¼nÃ¼lebileceÄŸini, Ã¶rneÄŸin, herhangi bir durum *s* ve eylem *a* iÃ§in *s* durumunda *a* eylemini alma olasÄ±lÄ±ÄŸÄ±nÄ± Ï€(*a*|*s*) dÃ¶ndÃ¼rmesi gerektiÄŸini de belirtebiliriz.

## Politika GradyanlarÄ± AlgoritmasÄ±

Bir politikayÄ± modellemenin en belirgin yolu, durumlarÄ± girdi olarak alacak ve karÅŸÄ±lÄ±k gelen eylemleri (veya daha doÄŸrusu tÃ¼m eylemlerin olasÄ±lÄ±klarÄ±nÄ±) dÃ¶ndÃ¼recek bir sinir aÄŸÄ± oluÅŸturmaktÄ±r. Bir bakÄ±ma, bu, her adÄ±mda hangi eylemleri almamÄ±z gerektiÄŸini Ã¶nceden bilmediÄŸimiz iÃ§in normal bir sÄ±nÄ±flandÄ±rma gÃ¶revine benzer.

Buradaki fikir, bu olasÄ±lÄ±klarÄ± tahmin etmektir. Deneyin her adÄ±mÄ±ndaki toplam Ã¶dÃ¼lÃ¼mÃ¼zÃ¼ gÃ¶steren bir **kÃ¼mÃ¼latif Ã¶dÃ¼l** vektÃ¶rÃ¼ oluÅŸturuyoruz. AyrÄ±ca, daha Ã¶nceki Ã¶dÃ¼llerin rolÃ¼nÃ¼ azaltmak iÃ§in daha Ã¶nceki Ã¶dÃ¼lleri bir katsayÄ± Î³=0.99 ile Ã§arparak **Ã¶dÃ¼l indirimleme** uyguluyoruz. ArdÄ±ndan, daha bÃ¼yÃ¼k Ã¶dÃ¼ller getiren deney yolu boyunca bu adÄ±mlarÄ± pekiÅŸtiriyoruz.

> Politika GradyanÄ± algoritmasÄ± hakkÄ±nda daha fazla bilgi edinin ve [Ã¶rnek defterde](../../../../../lessons/6-Other/22-DeepRL/CartPole-RL-TF.ipynb) bunu eylemde gÃ¶rÃ¼n.

## AktÃ¶r-Kritik AlgoritmasÄ±

Politika GradyanlarÄ± yaklaÅŸÄ±mÄ±nÄ±n geliÅŸtirilmiÅŸ bir versiyonu **AktÃ¶r-Kritik** olarak adlandÄ±rÄ±lÄ±r. Bunun arkasÄ±ndaki ana fikir, sinir aÄŸÄ±nÄ±n iki ÅŸeyi dÃ¶ndÃ¼rmek Ã¼zere eÄŸitilmesidir:

* Hangi eylemin alÄ±nacaÄŸÄ±nÄ± belirleyen politika. Bu kÄ±sÄ±m **aktÃ¶r** olarak adlandÄ±rÄ±lÄ±r.
* Bu durumda bekleyebileceÄŸimiz toplam Ã¶dÃ¼lÃ¼n tahmini - bu kÄ±sÄ±m ise **kritik** olarak adlandÄ±rÄ±lÄ±r.

Bir bakÄ±ma, bu mimari, birbirlerine karÅŸÄ± eÄŸitilen iki aÄŸa sahip bir [GAN](../../4-ComputerVision/10-GANs/README.md) ile benzerlik gÃ¶sterir. AktÃ¶r-kritik modelinde, aktÃ¶r almamÄ±z gereken eylemi Ã¶nerir ve kritik, sonucu tahmin etmeye Ã§alÄ±ÅŸÄ±r. Ancak, amacÄ±mÄ±z bu aÄŸlarÄ± birlikte eÄŸitmektir.

Deney sÄ±rasÄ±nda hem gerÃ§ek kÃ¼mÃ¼latif Ã¶dÃ¼lleri hem de kritik tarafÄ±ndan dÃ¶ndÃ¼rÃ¼len sonuÃ§larÄ± bildiÄŸimiz iÃ§in, bunlar arasÄ±ndaki farkÄ± minimize edecek bir kayÄ±p fonksiyonu oluÅŸturmak oldukÃ§a kolaydÄ±r. Bu, bize **kritik kaybÄ±** verir. **AktÃ¶r kaybÄ±nÄ±** ise politika gradyanÄ± algoritmasÄ±ndaki aynÄ± yaklaÅŸÄ±mÄ± kullanarak hesaplayabiliriz.

Bu algoritmalardan birini Ã§alÄ±ÅŸtÄ±rdÄ±ktan sonra, CartPole'umuzun ÅŸÃ¶yle davranmasÄ±nÄ± bekleyebiliriz:

![bir dengeleyen cartpole](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-balance.gif)

## âœï¸ AlÄ±ÅŸtÄ±rmalar: Politika GradyanlarÄ± ve AktÃ¶r-Kritik RL

AÅŸaÄŸÄ±daki defterlerde Ã¶ÄŸreniminize devam edin:

* [TensorFlow'da RL](../../../../../lessons/6-Other/22-DeepRL/CartPole-RL-TF.ipynb)
* [PyTorch'ta RL](../../../../../lessons/6-Other/22-DeepRL/CartPole-RL-PyTorch.ipynb)

## DiÄŸer RL GÃ¶revleri

GÃ¼nÃ¼mÃ¼zde PekiÅŸtirmeli Ã–ÄŸrenme, hÄ±zla bÃ¼yÃ¼yen bir araÅŸtÄ±rma alanÄ±dÄ±r. PekiÅŸtirmeli Ã¶ÄŸrenmenin bazÄ± ilginÃ§ Ã¶rnekleri ÅŸunlardÄ±r:

* Bir bilgisayarÄ± **Atari OyunlarÄ±** oynamayÄ± Ã¶ÄŸretmek. Bu problemin zorluÄŸu, durumun bir vektÃ¶r olarak basit bir ÅŸekilde temsil edilmemesi, daha ziyade bir ekran gÃ¶rÃ¼ntÃ¼sÃ¼ olmasÄ±dÄ±r - ve bu ekran gÃ¶rÃ¼ntÃ¼sÃ¼nÃ¼ bir Ã¶zellik vektÃ¶rÃ¼ne dÃ¶nÃ¼ÅŸtÃ¼rmek veya Ã¶dÃ¼l bilgisini Ã§Ä±karmak iÃ§in CNN kullanmamÄ±z gerekir. Atari oyunlarÄ± Gym'de mevcuttur.
* Bir bilgisayarÄ± satranÃ§ ve go gibi masa oyunlarÄ± oynamayÄ± Ã¶ÄŸretmek. Son zamanlarda **Alpha Zero** gibi son teknoloji programlar, iki ajanÄ±n birbirine karÅŸÄ± oynayarak ve her adÄ±mda geliÅŸerek sÄ±fÄ±rdan eÄŸitilmiÅŸtir.
* EndÃ¼stride, RL simÃ¼lasyondan kontrol sistemleri oluÅŸturmak iÃ§in kullanÄ±lÄ±r. [Bonsai](https://azure.microsoft.com/services/project-bonsai/?WT.mc_id=academic-77998-cacaste) adlÄ± bir hizmet bunun iÃ§in Ã¶zel olarak tasarlanmÄ±ÅŸtÄ±r.

## SonuÃ§

ArtÄ±k, ajanslarÄ± iyi sonuÃ§lar elde etmek iÃ§in sadece onlara oyunun istenen durumunu tanÄ±mlayan bir Ã¶dÃ¼l fonksiyonu saÄŸlayarak ve arama alanÄ±nÄ± akÄ±llÄ±ca keÅŸfetme fÄ±rsatÄ± vererek nasÄ±l eÄŸiteceÄŸimizi Ã¶ÄŸrendik. Ä°ki algoritmayÄ± baÅŸarÄ±yla denedik ve oldukÃ§a kÄ±sa bir sÃ¼re iÃ§inde iyi bir sonuÃ§ elde ettik. Ancak, bu, RL yolculuÄŸunuzun sadece baÅŸlangÄ±cÄ±dÄ±r ve daha derinlemesine incelemek isterseniz ayrÄ± bir kurs almayÄ± kesinlikle dÃ¼ÅŸÃ¼nmelisiniz.

## ğŸš€ Zorluk

'DiÄŸer RL GÃ¶revleri' bÃ¶lÃ¼mÃ¼nde listelenen uygulamalarÄ± keÅŸfedin ve birini uygulamaya Ã§alÄ±ÅŸÄ±n!

## [Ders sonrasÄ± anket](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/222)

## GÃ¶zden GeÃ§irme ve Kendi Kendine Ã‡alÄ±ÅŸma

Klasik pekiÅŸtirmeli Ã¶ÄŸrenme hakkÄ±nda daha fazla bilgi edinin [Makine Ã–ÄŸrenimi iÃ§in BaÅŸlangÄ±Ã§ MÃ¼fredatÄ±](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/README.md) Ã¼zerinden.

Bir bilgisayarÄ±n SÃ¼per Mario oynamayÄ± nasÄ±l Ã¶ÄŸrenebileceÄŸini anlatan [bu harika videoyu](https://www.youtube.com/watch?v=qv6UVOQ0F44) izleyin.

## Ã–dev: [Bir DaÄŸ AracÄ±nÄ± EÄŸit](lab/README.md)

Bu Ã¶dev sÄ±rasÄ±nda amacÄ±nÄ±z farklÄ± bir Gym Ã§evresini - [Mountain Car](https://www.gymlibrary.ml/environments/classic_control/mountain_car/) - eÄŸitmek olacaktÄ±r.

**Sorumluluk Reddi**:  
Bu belge, makine tabanlÄ± yapay zeka Ã§eviri hizmetleri kullanÄ±larak Ã§evrilmiÅŸtir. DoÄŸruluk iÃ§in Ã§aba gÃ¶stersek de, otomatik Ã§evirilerin hatalar veya yanlÄ±ÅŸlÄ±klar iÃ§erebileceÄŸini lÃ¼tfen unutmayÄ±n. Orijinal belge, kendi dilinde otorite kaynaÄŸÄ± olarak deÄŸerlendirilmelidir. Kritik bilgiler iÃ§in profesyonel insan Ã§evirisi Ã¶nerilmektedir. Bu Ã§evirinin kullanÄ±mÄ±ndan kaynaklanan herhangi bir yanlÄ±ÅŸ anlama veya yanlÄ±ÅŸ yorumlama iÃ§in sorumluluk kabul etmiyoruz.