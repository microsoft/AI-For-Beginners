# Tekrarlayan Sinir AÄŸlarÄ±

## [Ders Ã–ncesi Quiz](https://ff-quizzes.netlify.app/en/ai/quiz/31)

Ã–nceki bÃ¶lÃ¼mlerde, metnin zengin anlamsal temsillerini ve gÃ¶mme katmanlarÄ±nÄ±n Ã¼zerine basit bir doÄŸrusal sÄ±nÄ±flandÄ±rÄ±cÄ± kullandÄ±k. Bu mimarinin yaptÄ±ÄŸÄ± ÅŸey, bir cÃ¼mledeki kelimelerin toplu anlamÄ±nÄ± yakalamaktÄ±r, ancak gÃ¶mme katmanlarÄ±nÄ±n Ã¼zerindeki toplama iÅŸlemi, orijinal metindeki **kelime sÄ±rasÄ±nÄ±** dikkate almaz. Bu modeller kelime sÄ±rasÄ±nÄ± modelleyemediÄŸi iÃ§in, metin Ã¼retimi veya soru yanÄ±tlama gibi daha karmaÅŸÄ±k veya belirsiz gÃ¶revleri Ã§Ã¶zemezler.

Metin dizisinin anlamÄ±nÄ± yakalamak iÃ§in, **tekrarlayan sinir aÄŸÄ±** veya RNN adÄ± verilen baÅŸka bir sinir aÄŸÄ± mimarisi kullanmamÄ±z gerekir. RNN'de, cÃ¼mlemizi aÄŸdan bir sembol biriminde geÃ§iririz ve aÄŸ bir **durum** Ã¼retir, bu durumu bir sonraki sembolle birlikte tekrar aÄŸa geÃ§iririz.

![RNN](../../../../../translated_images/tr/rnn.27f5c29c53d727b5.webp)

> GÃ¶rsel yazar tarafÄ±ndan oluÅŸturulmuÅŸtur

X<sub>0</sub>,...,X<sub>n</sub> giriÅŸ dizisi verildiÄŸinde, RNN bir sinir aÄŸÄ± bloklarÄ± dizisi oluÅŸturur ve bu diziyi uÃ§tan uca geri yayÄ±lÄ±m kullanarak eÄŸitir. Her aÄŸ bloÄŸu bir Ã§ift (X<sub>i</sub>,S<sub>i</sub>) alÄ±r ve sonuÃ§ olarak S<sub>i+1</sub> Ã¼retir. Son durum S<sub>n</sub> veya (Ã§Ä±ktÄ± Y<sub>n</sub>) sonucu Ã¼retmek iÃ§in doÄŸrusal bir sÄ±nÄ±flandÄ±rÄ±cÄ±ya gider. TÃ¼m aÄŸ bloklarÄ± aynÄ± aÄŸÄ±rlÄ±klarÄ± paylaÅŸÄ±r ve tek bir geri yayÄ±lÄ±m geÃ§iÅŸiyle uÃ§tan uca eÄŸitilir.

Durum vektÃ¶rleri S<sub>0</sub>,...,S<sub>n</sub> aÄŸdan geÃ§tiÄŸi iÃ§in, aÄŸ kelimeler arasÄ±ndaki sÄ±ralÄ± baÄŸÄ±mlÄ±lÄ±klarÄ± Ã¶ÄŸrenebilir. Ã–rneÄŸin, dizide *deÄŸil* kelimesi bir yerde geÃ§tiÄŸinde, durum vektÃ¶rÃ¼ndeki belirli Ã¶ÄŸeleri olumsuzlamak iÃ§in Ã¶ÄŸrenebilir, bu da olumsuzlama ile sonuÃ§lanÄ±r.

> âœ… YukarÄ±daki resimdeki tÃ¼m RNN bloklarÄ±nÄ±n aÄŸÄ±rlÄ±klarÄ± paylaÅŸÄ±ldÄ±ÄŸÄ±ndan, aynÄ± resim bir geri besleme dÃ¶ngÃ¼sÃ¼ olan tek bir blok (saÄŸda) olarak temsil edilebilir; bu dÃ¶ngÃ¼, aÄŸÄ±n Ã§Ä±ktÄ± durumunu tekrar giriÅŸe geÃ§irir.

## Bir RNN HÃ¼cresinin Anatomisi

Basit bir RNN hÃ¼cresinin nasÄ±l organize edildiÄŸini gÃ¶relim. Ã–nceki durum S<sub>i-1</sub> ve mevcut sembol X<sub>i</sub>'yi giriÅŸ olarak alÄ±r ve Ã§Ä±ktÄ± durumu S<sub>i</sub>'yi Ã¼retmek zorundadÄ±r (ve bazen, Ã¼retici aÄŸlarda olduÄŸu gibi, baÅŸka bir Ã§Ä±ktÄ± Y<sub>i</sub> ile de ilgileniriz).

Basit bir RNN hÃ¼cresinin iÃ§inde iki aÄŸÄ±rlÄ±k matrisi vardÄ±r: biri bir giriÅŸ sembolÃ¼nÃ¼ dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r (buna W diyelim), diÄŸeri ise bir giriÅŸ durumunu dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r (H). Bu durumda aÄŸÄ±n Ã§Ä±ktÄ±sÄ± &sigma;(W&times;X<sub>i</sub>+H&times;S<sub>i-1</sub>+b) olarak hesaplanÄ±r, burada &sigma; aktivasyon fonksiyonu ve b ek bir bias'tÄ±r.

<img alt="RNN HÃ¼cresi Anatomisi" src="../../../../../translated_images/tr/rnn-anatomy.79ee3f3920b3294b.webp" width="50%"/>

> GÃ¶rsel yazar tarafÄ±ndan oluÅŸturulmuÅŸtur

Ã‡oÄŸu durumda, giriÅŸ token'larÄ± RNN'ye girmeden Ã¶nce boyutlarÄ± dÃ¼ÅŸÃ¼rmek iÃ§in gÃ¶mme katmanÄ±ndan geÃ§irilir. Bu durumda, eÄŸer giriÅŸ vektÃ¶rlerinin boyutu *emb_size* ve durum vektÃ¶rÃ¼nÃ¼n boyutu *hid_size* ise, W'nin boyutu *emb_size*&times;*hid_size*, H'nin boyutu ise *hid_size*&times;*hid_size* olur.

## Uzun KÄ±sa SÃ¼reli Bellek (LSTM)

Klasik RNN'lerin ana sorunlarÄ±ndan biri, **kaybolan gradyanlar** problemidir. RNN'ler uÃ§tan uca tek bir geri yayÄ±lÄ±m geÃ§iÅŸiyle eÄŸitildiÄŸinden, hatayÄ± aÄŸÄ±n ilk katmanlarÄ±na iletmekte zorlanÄ±r ve bu nedenle aÄŸ uzak token'lar arasÄ±ndaki iliÅŸkileri Ã¶ÄŸrenemez. Bu sorunu Ã¶nlemenin yollarÄ±ndan biri, **kapÄ±lar** kullanarak **aÃ§Ä±k durum yÃ¶netimi** tanÄ±tmaktÄ±r. Bu tÃ¼r iki iyi bilinen mimari vardÄ±r: **Uzun KÄ±sa SÃ¼reli Bellek** (LSTM) ve **KapÄ±lÄ± RÃ¶le Birimi** (GRU).

![Uzun KÄ±sa SÃ¼reli Bellek hÃ¼cresine bir Ã¶rnek gÃ¶steren gÃ¶rsel](../../../../../lessons/5-NLP/16-RNN/images/long-short-term-memory-cell.svg)

> GÃ¶rsel kaynaÄŸÄ± belirlenecek

LSTM AÄŸÄ±, RNN'ye benzer bir ÅŸekilde organize edilmiÅŸtir, ancak katmandan katmana iki durum aktarÄ±lÄ±r: gerÃ§ek durum C ve gizli vektÃ¶r H. Her birimde, gizli vektÃ¶r H<sub>i</sub> giriÅŸ X<sub>i</sub> ile birleÅŸtirilir ve bunlar **kapÄ±lar** aracÄ±lÄ±ÄŸÄ±yla durum C'de ne olacaÄŸÄ±nÄ± kontrol eder. Her kapÄ±, sigmoid aktivasyonlu (Ã§Ä±ktÄ± aralÄ±ÄŸÄ± [0,1]) bir sinir aÄŸÄ±dÄ±r ve durum vektÃ¶rÃ¼yle Ã§arpÄ±ldÄ±ÄŸÄ±nda bit dÃ¼zeyinde bir maske olarak dÃ¼ÅŸÃ¼nÃ¼lebilir. YukarÄ±daki resimde soldan saÄŸa doÄŸru ÅŸu kapÄ±lar vardÄ±r:

* **Unutma kapÄ±sÄ±**, gizli bir vektÃ¶r alÄ±r ve C vektÃ¶rÃ¼nÃ¼n hangi bileÅŸenlerini unutmamÄ±z gerektiÄŸini ve hangilerini geÃ§irmemiz gerektiÄŸini belirler.
* **GiriÅŸ kapÄ±sÄ±**, giriÅŸ ve gizli vektÃ¶rlerden bazÄ± bilgileri alÄ±r ve duruma ekler.
* **Ã‡Ä±kÄ±ÅŸ kapÄ±sÄ±**, durumu *tanh* aktivasyonlu bir doÄŸrusal katman aracÄ±lÄ±ÄŸÄ±yla dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r, ardÄ±ndan yeni bir durum C<sub>i+1</sub> Ã¼retmek iÃ§in gizli vektÃ¶r H<sub>i</sub>'yi kullanarak bazÄ± bileÅŸenlerini seÃ§er.

Durum C'nin bileÅŸenleri, aÃ§Ä±lÄ±p kapatÄ±labilen bayraklar olarak dÃ¼ÅŸÃ¼nÃ¼lebilir. Ã–rneÄŸin, dizide *Alice* adÄ±nÄ± gÃ¶rdÃ¼ÄŸÃ¼mÃ¼zde, bunun bir kadÄ±n karaktere atÄ±fta bulunduÄŸunu varsayabilir ve cÃ¼mlede bir kadÄ±n isim olduÄŸunu belirten bayraÄŸÄ± kaldÄ±rabiliriz. Daha sonra *ve Tom* ifadelerini gÃ¶rdÃ¼ÄŸÃ¼mÃ¼zde, Ã§oÄŸul bir isim olduÄŸunu belirten bayraÄŸÄ± kaldÄ±rabiliriz. BÃ¶ylece, durumu manipÃ¼le ederek cÃ¼mle parÃ§alarÄ±nÄ±n dilbilgisel Ã¶zelliklerini takip edebiliriz.

> âœ… LSTM'nin iÃ§ iÅŸleyiÅŸini anlamak iÃ§in mÃ¼kemmel bir kaynak, Christopher Olah'Ä±n [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) adlÄ± harika makalesidir.

## Ã‡ift YÃ¶nlÃ¼ ve Ã‡ok KatmanlÄ± RNN'ler

Åimdiye kadar, bir dizinin baÅŸÄ±ndan sonuna doÄŸru tek yÃ¶nlÃ¼ Ã§alÄ±ÅŸan tekrarlayan aÄŸlarÄ± tartÄ±ÅŸtÄ±k. Bu doÄŸal gÃ¶rÃ¼nÃ¼yor, Ã§Ã¼nkÃ¼ okuma ve konuÅŸmayÄ± dinleme ÅŸeklimize benziyor. Ancak, birÃ§ok pratik durumda giriÅŸ dizisine rastgele eriÅŸimimiz olduÄŸundan, tekrarlayan hesaplamayÄ± her iki yÃ¶nde Ã§alÄ±ÅŸtÄ±rmak mantÄ±klÄ± olabilir. Bu tÃ¼r aÄŸlara **Ã§ift yÃ¶nlÃ¼** RNN'ler denir. Ã‡ift yÃ¶nlÃ¼ bir aÄŸla Ã§alÄ±ÅŸÄ±rken, her yÃ¶n iÃ§in birer gizli durum vektÃ¶rÃ¼ne ihtiyacÄ±mÄ±z olacaktÄ±r.

Tek yÃ¶nlÃ¼ veya Ã§ift yÃ¶nlÃ¼ bir tekrarlayan aÄŸ, bir dizideki belirli kalÄ±plarÄ± yakalar ve bunlarÄ± bir durum vektÃ¶rÃ¼ne depolayabilir veya Ã§Ä±ktÄ±ya aktarabilir. KonvolÃ¼syonel aÄŸlarda olduÄŸu gibi, ilk katman tarafÄ±ndan Ã§Ä±karÄ±lan dÃ¼ÅŸÃ¼k seviyeli kalÄ±plardan daha yÃ¼ksek seviyeli kalÄ±plarÄ± yakalamak ve inÅŸa etmek iÃ§in ilk katmanÄ±n Ã¼zerine baÅŸka bir tekrarlayan katman inÅŸa edebiliriz. Bu bizi, Ã¶nceki katmanÄ±n Ã§Ä±ktÄ±sÄ±nÄ±n bir sonraki katmana giriÅŸ olarak geÃ§tiÄŸi iki veya daha fazla tekrarlayan aÄŸdan oluÅŸan bir **Ã§ok katmanlÄ± RNN** kavramÄ±na gÃ¶tÃ¼rÃ¼r.

![Ã‡ok katmanlÄ± uzun kÄ±sa sÃ¼reli bellek RNN'yi gÃ¶steren gÃ¶rsel](../../../../../translated_images/tr/multi-layer-lstm.dd975e29bb2a59fe.webp)

*[Bu harika yazÄ±dan](https://towardsdatascience.com/from-a-lstm-cell-to-a-multilayer-lstm-network-with-pytorch-2899eb5696f3) Fernando LÃ³pez tarafÄ±ndan alÄ±nmÄ±ÅŸtÄ±r.*

## âœï¸ AlÄ±ÅŸtÄ±rmalar: GÃ¶mme KatmanlarÄ±

AÅŸaÄŸÄ±daki not defterlerinde Ã¶ÄŸrenmeye devam edin:

* [PyTorch ile RNN'ler](RNNPyTorch.ipynb)
* [TensorFlow ile RNN'ler](RNNTF.ipynb)

## SonuÃ§

Bu birimde, RNN'lerin dizi sÄ±nÄ±flandÄ±rmasÄ± iÃ§in kullanÄ±labileceÄŸini gÃ¶rdÃ¼k, ancak aslÄ±nda metin Ã¼retimi, makine Ã§evirisi ve daha fazlasÄ± gibi birÃ§ok gÃ¶revi de yerine getirebilirler. Bu gÃ¶revleri bir sonraki birimde ele alacaÄŸÄ±z.

## ğŸš€ Meydan Okuma

LSTM'ler hakkÄ±nda bazÄ± literatÃ¼rleri okuyun ve uygulamalarÄ±nÄ± dÃ¼ÅŸÃ¼nÃ¼n:

- [Grid Long Short-Term Memory](https://arxiv.org/pdf/1507.01526v1.pdf)
- [Show, Attend and Tell: Neural Image Caption
Generation with Visual Attention](https://arxiv.org/pdf/1502.03044v2.pdf)

## [Ders SonrasÄ± Quiz](https://ff-quizzes.netlify.app/en/ai/quiz/32)

## GÃ¶zden GeÃ§irme ve Kendi Kendine Ã‡alÄ±ÅŸma

- Christopher Olah'Ä±n [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) adlÄ± makalesi.

## [Ã–dev: Not Defterleri](assignment.md)

---

