<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "58bf4adb210aab53e8f78c8082040e7c",
  "translation_date": "2025-08-26T07:19:48+00:00",
  "source_file": "lessons/5-NLP/16-RNN/README.md",
  "language_code": "tr"
}
-->
# Tekrarlayan Sinir AÄŸlarÄ±

## [Ders Ã–ncesi Test](https://ff-quizzes.netlify.app/en/ai/quiz/31)

Ã–nceki bÃ¶lÃ¼mlerde, metinlerin zengin anlamsal temsillerini ve gÃ¶mme katmanlarÄ±nÄ±n Ã¼zerine basit bir doÄŸrusal sÄ±nÄ±flandÄ±rÄ±cÄ±yÄ± kullandÄ±k. Bu mimarinin yaptÄ±ÄŸÄ± ÅŸey, bir cÃ¼mledeki kelimelerin toplu anlamÄ±nÄ± yakalamaktÄ±r, ancak kelimelerin **sÄ±rasÄ±nÄ±** dikkate almaz, Ã§Ã¼nkÃ¼ gÃ¶mme katmanlarÄ±nÄ±n Ã¼zerindeki toplama iÅŸlemi bu bilgiyi orijinal metinden kaldÄ±rÄ±r. Bu modeller kelime sÄ±rasÄ±nÄ± modelleyemediÄŸi iÃ§in, metin Ã¼retimi veya soru yanÄ±tlama gibi daha karmaÅŸÄ±k veya belirsiz gÃ¶revleri Ã§Ã¶zemezler.

Metin dizisinin anlamÄ±nÄ± yakalamak iÃ§in, **tekrarlayan sinir aÄŸÄ±** veya RNN adÄ± verilen baÅŸka bir sinir aÄŸÄ± mimarisini kullanmamÄ±z gerekir. RNN'de, cÃ¼mlemizi aÄŸdan bir sembol bir sembol geÃ§iririz ve aÄŸ bir **durum** Ã¼retir, bu durumu bir sonraki sembolle birlikte tekrar aÄŸa iletiriz.

![RNN](../../../../../translated_images/rnn.27f5c29c53d727b546ad3961637a267f0fe9ec5ab01f2a26a853c92fcefbb574.tr.png)

> GÃ¶rsel: Yazar tarafÄ±ndan oluÅŸturulmuÅŸtur

X<sub>0</sub>,...,X<sub>n</sub> giriÅŸ dizi sembolleri verildiÄŸinde, RNN bir sinir aÄŸÄ± bloklarÄ± dizisi oluÅŸturur ve bu diziyi baÅŸtan sona geri yayÄ±lÄ±m kullanarak eÄŸitir. Her aÄŸ bloÄŸu bir Ã§ift (X<sub>i</sub>,S<sub>i</sub>) giriÅŸ olarak alÄ±r ve sonuÃ§ olarak S<sub>i+1</sub> Ã¼retir. Son durum S<sub>n</sub> veya (Ã§Ä±ktÄ± Y<sub>n</sub>), sonucu Ã¼retmek iÃ§in doÄŸrusal bir sÄ±nÄ±flandÄ±rÄ±cÄ±ya gider. TÃ¼m aÄŸ bloklarÄ± aynÄ± aÄŸÄ±rlÄ±klarÄ± paylaÅŸÄ±r ve tek bir geri yayÄ±lÄ±m geÃ§iÅŸiyle baÅŸtan sona eÄŸitilir.

Durum vektÃ¶rleri S<sub>0</sub>,...,S<sub>n</sub> aÄŸdan geÃ§tiÄŸi iÃ§in, kelimeler arasÄ±ndaki sÄ±ralÄ± baÄŸÄ±mlÄ±lÄ±klarÄ± Ã¶ÄŸrenebilir. Ã–rneÄŸin, dizide bir yerde *deÄŸil* kelimesi geÃ§tiÄŸinde, durum vektÃ¶rÃ¼ndeki belirli Ã¶ÄŸeleri olumsuzlamak iÃ§in Ã¶ÄŸrenebilir ve bu da olumsuzlama ile sonuÃ§lanÄ±r.

> âœ… YukarÄ±daki resimdeki tÃ¼m RNN bloklarÄ±nÄ±n aÄŸÄ±rlÄ±klarÄ± paylaÅŸÄ±ldÄ±ÄŸÄ±ndan, aynÄ± resim bir geri besleme dÃ¶ngÃ¼sÃ¼ olan tek bir blok (saÄŸda) olarak temsil edilebilir ve aÄŸÄ±n Ã§Ä±ktÄ± durumunu giriÅŸe geri iletir.

## Bir RNN HÃ¼cresinin Anatomisi

Basit bir RNN hÃ¼cresinin nasÄ±l organize edildiÄŸini gÃ¶relim. Ã–nceki durum S<sub>i-1</sub> ve mevcut sembol X<sub>i</sub>'yi giriÅŸ olarak alÄ±r ve Ã§Ä±ktÄ± durumu S<sub>i</sub>'yi Ã¼retmek zorundadÄ±r (ve bazen, Ã¼retici aÄŸlarda olduÄŸu gibi, baÅŸka bir Ã§Ä±ktÄ± Y<sub>i</sub> ile de ilgileniriz).

Basit bir RNN hÃ¼cresinin iÃ§inde iki aÄŸÄ±rlÄ±k matrisi vardÄ±r: biri bir giriÅŸ sembolÃ¼nÃ¼ dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r (W olarak adlandÄ±ralÄ±m) ve diÄŸeri bir giriÅŸ durumunu dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r (H). Bu durumda aÄŸÄ±n Ã§Ä±ktÄ±sÄ± Ïƒ(WÃ—X<sub>i</sub>+HÃ—S<sub>i-1</sub>+b) olarak hesaplanÄ±r, burada Ïƒ aktivasyon fonksiyonu ve b ek bir yanlÄ±lÄ±ktÄ±r.

<img alt="RNN HÃ¼cresi Anatomisi" src="images/rnn-anatomy.png" width="50%"/>

> GÃ¶rsel: Yazar tarafÄ±ndan oluÅŸturulmuÅŸtur

Ã‡oÄŸu durumda, giriÅŸ sembolleri RNN'ye girmeden Ã¶nce boyutlarÄ± dÃ¼ÅŸÃ¼rmek iÃ§in gÃ¶mme katmanÄ±ndan geÃ§irilir. Bu durumda, eÄŸer giriÅŸ vektÃ¶rlerinin boyutu *emb_size* ve durum vektÃ¶rÃ¼nÃ¼n boyutu *hid_size* ise - W'nin boyutu *emb_size*Ã—*hid_size*, H'nin boyutu ise *hid_size*Ã—*hid_size* olur.

## Uzun KÄ±sa SÃ¼reli Bellek (LSTM)

Klasik RNN'lerin ana problemlerinden biri, **kaybolan gradyanlar** problemidir. RNN'ler tek bir geri yayÄ±lÄ±m geÃ§iÅŸinde baÅŸtan sona eÄŸitildiÄŸi iÃ§in, hatayÄ± aÄŸÄ±n ilk katmanlarÄ±na iletmekte zorlanÄ±r ve bu nedenle aÄŸ, uzak semboller arasÄ±ndaki iliÅŸkileri Ã¶ÄŸrenemez. Bu sorunu Ã¶nlemenin bir yolu, **kapÄ±lar** kullanarak **aÃ§Ä±k durum yÃ¶netimi** tanÄ±tmaktÄ±r. Bu tÃ¼r iki iyi bilinen mimari vardÄ±r: **Uzun KÄ±sa SÃ¼reli Bellek** (LSTM) ve **KapÄ±lÄ± RÃ¶le Birimi** (GRU).

![Bir uzun kÄ±sa sÃ¼reli bellek hÃ¼cresi Ã¶rneÄŸini gÃ¶steren gÃ¶rsel](../../../../../lessons/5-NLP/16-RNN/images/long-short-term-memory-cell.svg)

> GÃ¶rsel kaynaÄŸÄ± belirlenecek

LSTM AÄŸÄ±, RNN'ye benzer bir ÅŸekilde organize edilmiÅŸtir, ancak katmandan katmana geÃ§en iki durum vardÄ±r: gerÃ§ek durum C ve gizli vektÃ¶r H. Her birimde, gizli vektÃ¶r H<sub>i</sub>, giriÅŸ X<sub>i</sub> ile birleÅŸtirilir ve bunlar **kapÄ±lar** aracÄ±lÄ±ÄŸÄ±yla durum C'de ne olacaÄŸÄ±nÄ± kontrol eder. Her kapÄ±, sigmoid aktivasyonlu (Ã§Ä±ktÄ± [0,1] aralÄ±ÄŸÄ±nda) bir sinir aÄŸÄ±dÄ±r ve durum vektÃ¶rÃ¼ ile Ã§arpÄ±ldÄ±ÄŸÄ±nda bit dÃ¼zeyinde bir maske olarak dÃ¼ÅŸÃ¼nÃ¼lebilir. AÅŸaÄŸÄ±daki kapÄ±lar vardÄ±r (yukarÄ±daki resimde soldan saÄŸa):

* **Unutma kapÄ±sÄ±**, bir gizli vektÃ¶r alÄ±r ve C vektÃ¶rÃ¼nÃ¼n hangi bileÅŸenlerini unutmamÄ±z gerektiÄŸini ve hangilerini geÃ§irmemiz gerektiÄŸini belirler.
* **GiriÅŸ kapÄ±sÄ±**, giriÅŸ ve gizli vektÃ¶rlerden bazÄ± bilgileri alÄ±r ve duruma ekler.
* **Ã‡Ä±kÄ±ÅŸ kapÄ±sÄ±**, durumu *tanh* aktivasyonlu bir doÄŸrusal katman aracÄ±lÄ±ÄŸÄ±yla dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r, ardÄ±ndan yeni bir durum C<sub>i+1</sub> Ã¼retmek iÃ§in gizli vektÃ¶r H<sub>i</sub>'yi kullanarak bazÄ± bileÅŸenlerini seÃ§er.

Durum C'nin bileÅŸenleri, aÃ§Ä±lÄ±p kapatÄ±labilen bayraklar olarak dÃ¼ÅŸÃ¼nÃ¼lebilir. Ã–rneÄŸin, dizide *Alice* adÄ±nda bir isimle karÅŸÄ±laÅŸtÄ±ÄŸÄ±mÄ±zda, bunun bir kadÄ±n karaktere atÄ±fta bulunduÄŸunu varsaymak ve cÃ¼mlede bir kadÄ±n isim olduÄŸunu belirten bayraÄŸÄ± kaldÄ±rmak isteyebiliriz. Daha sonra *ve Tom* ifadeleriyle karÅŸÄ±laÅŸtÄ±ÄŸÄ±mÄ±zda, Ã§oÄŸul bir isim olduÄŸunu belirten bayraÄŸÄ± kaldÄ±rabiliriz. BÃ¶ylece, durumu manipÃ¼le ederek cÃ¼mlenin gramer Ã¶zelliklerini takip edebiliriz.

> âœ… LSTM'nin iÃ§ iÅŸleyiÅŸini anlamak iÃ§in mÃ¼kemmel bir kaynak, Christopher Olah'Ä±n [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) adlÄ± harika makalesidir.

## Ã‡ift YÃ¶nlÃ¼ ve Ã‡ok KatmanlÄ± RNN'ler

Bir dizinin baÅŸÄ±ndan sonuna doÄŸru Ã§alÄ±ÅŸan tekrarlayan aÄŸlarÄ± tartÄ±ÅŸtÄ±k. Bu doÄŸal gÃ¶rÃ¼nÃ¼yor, Ã§Ã¼nkÃ¼ okuma ve konuÅŸmayÄ± dinleme ÅŸeklimize benziyor. Ancak, birÃ§ok pratik durumda giriÅŸ dizisine rastgele eriÅŸimimiz olduÄŸu iÃ§in, tekrarlayan hesaplamayÄ± her iki yÃ¶nde de Ã§alÄ±ÅŸtÄ±rmak mantÄ±klÄ± olabilir. Bu tÃ¼r aÄŸlara **Ã§ift yÃ¶nlÃ¼** RNN'ler denir. Ã‡ift yÃ¶nlÃ¼ bir aÄŸla Ã§alÄ±ÅŸÄ±rken, her yÃ¶n iÃ§in birer gizli durum vektÃ¶rÃ¼ne ihtiyacÄ±mÄ±z olacaktÄ±r.

Tek yÃ¶nlÃ¼ veya Ã§ift yÃ¶nlÃ¼ bir tekrarlayan aÄŸ, bir dizideki belirli kalÄ±plarÄ± yakalar ve bunlarÄ± bir durum vektÃ¶rÃ¼ne depolayabilir veya Ã§Ä±ktÄ±ya aktarabilir. KonvolÃ¼syonel aÄŸlarda olduÄŸu gibi, ilk katman tarafÄ±ndan Ã§Ä±karÄ±lan dÃ¼ÅŸÃ¼k seviyeli kalÄ±plardan daha yÃ¼ksek seviyeli kalÄ±plarÄ± yakalamak ve inÅŸa etmek iÃ§in ilk katmanÄ±n Ã¼zerine baÅŸka bir tekrarlayan katman inÅŸa edebiliriz. Bu bizi, Ã¶nceki katmanÄ±n Ã§Ä±ktÄ±sÄ±nÄ±n bir sonraki katmana giriÅŸ olarak geÃ§tiÄŸi iki veya daha fazla tekrarlayan aÄŸdan oluÅŸan bir **Ã§ok katmanlÄ± RNN** kavramÄ±na gÃ¶tÃ¼rÃ¼r.

![Ã‡ok katmanlÄ± uzun kÄ±sa sÃ¼reli bellek RNN'yi gÃ¶steren gÃ¶rsel](../../../../../translated_images/multi-layer-lstm.dd975e29bb2a59fe58b429db833932d734c81f211cad2783797a9608984acb8c.tr.jpg)

*[Bu harika gÃ¶nderiden](https://towardsdatascience.com/from-a-lstm-cell-to-a-multilayer-lstm-network-with-pytorch-2899eb5696f3) alÄ±nan resim, Fernando LÃ³pez tarafÄ±ndan yazÄ±lmÄ±ÅŸtÄ±r.*

## âœï¸ AlÄ±ÅŸtÄ±rmalar: GÃ¶mme KatmanlarÄ±

AÅŸaÄŸÄ±daki defterlerde Ã¶ÄŸrenmeye devam edin:

* [PyTorch ile RNN'ler](../../../../../lessons/5-NLP/16-RNN/RNNPyTorch.ipynb)
* [TensorFlow ile RNN'ler](../../../../../lessons/5-NLP/16-RNN/RNNTF.ipynb)

## SonuÃ§

Bu birimde, RNN'lerin dizi sÄ±nÄ±flandÄ±rmasÄ± iÃ§in kullanÄ±labileceÄŸini gÃ¶rdÃ¼k, ancak aslÄ±nda metin Ã¼retimi, makine Ã§evirisi ve daha fazlasÄ± gibi birÃ§ok gÃ¶revi yerine getirebilirler. Bu gÃ¶revleri bir sonraki birimde ele alacaÄŸÄ±z.

## ğŸš€ Zorluk

LSTM'ler hakkÄ±nda bazÄ± literatÃ¼rleri okuyun ve uygulamalarÄ±nÄ± dÃ¼ÅŸÃ¼nÃ¼n:

- [Grid Long Short-Term Memory](https://arxiv.org/pdf/1507.01526v1.pdf)
- [Show, Attend and Tell: Neural Image Caption
Generation with Visual Attention](https://arxiv.org/pdf/1502.03044v2.pdf)

## [Ders SonrasÄ± Test](https://ff-quizzes.netlify.app/en/ai/quiz/32)

## GÃ¶zden GeÃ§irme ve Kendi Kendine Ã‡alÄ±ÅŸma

- Christopher Olah'Ä±n [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) adlÄ± makalesi.

## [Ã–dev: Defterler](assignment.md)

**Feragatname**:  
Bu belge, AI Ã§eviri hizmeti [Co-op Translator](https://github.com/Azure/co-op-translator) kullanÄ±larak Ã§evrilmiÅŸtir. DoÄŸruluk iÃ§in Ã§aba gÃ¶stersek de, otomatik Ã§evirilerin hata veya yanlÄ±ÅŸlÄ±klar iÃ§erebileceÄŸini lÃ¼tfen unutmayÄ±n. Belgenin orijinal dili, yetkili kaynak olarak kabul edilmelidir. Kritik bilgiler iÃ§in profesyonel insan Ã§evirisi Ã¶nerilir. Bu Ã§evirinin kullanÄ±mÄ±ndan kaynaklanan yanlÄ±ÅŸ anlamalar veya yanlÄ±ÅŸ yorumlamalar iÃ§in sorumluluk kabul etmiyoruz.