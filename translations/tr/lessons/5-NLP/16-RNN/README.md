# Tekrarlayan Sinir AÄŸlarÄ±

## [Ders Ã¶ncesi quiz](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/116)

Ã–nceki bÃ¶lÃ¼mlerde, metnin zengin anlamsal temsillerini ve gÃ¶mme katmanlarÄ±nÄ±n Ã¼stÃ¼nde basit bir doÄŸrusal sÄ±nÄ±flayÄ±cÄ± kullanÄ±yorduk. Bu mimari, bir cÃ¼mledeki kelimelerin toplam anlamÄ±nÄ± yakalamaya Ã§alÄ±ÅŸÄ±r, ancak kelimelerin **sÄ±rasÄ±nÄ±** dikkate almaz; Ã§Ã¼nkÃ¼ gÃ¶mme katmanlarÄ± Ã¼zerindeki toplama iÅŸlemi, bu bilgiyi orijinal metinden Ã§Ä±karmÄ±ÅŸtÄ±r. Bu modeller kelime sÄ±ralamasÄ±nÄ± modelleyemediÄŸinden, metin Ã¼retimi veya soru yanÄ±tlama gibi daha karmaÅŸÄ±k veya belirsiz gÃ¶revleri yerine getiremezler.

Metin dizisinin anlamÄ±nÄ± yakalamak iÃ§in, **tekrarlayan sinir aÄŸÄ±** (RNN) adÄ± verilen baÅŸka bir sinir aÄŸÄ± mimarisi kullanmamÄ±z gerekiyor. RNN'de, cÃ¼mlemizi bir sembolÃ¼ bir seferde geÃ§iriyoruz ve aÄŸ, ardÄ±ndan bir **durum** Ã¼retiyor; bu durumu bir sonraki sembol ile birlikte tekrar aÄŸa iletiyoruz.

![RNN](../../../../../translated_images/rnn.27f5c29c53d727b546ad3961637a267f0fe9ec5ab01f2a26a853c92fcefbb574.tr.png)

> YazarÄ±n resmi

X<sub>0</sub>,...,X<sub>n</sub> token dizisi verildiÄŸinde, RNN bir sinir aÄŸÄ± bloklarÄ± dizisi oluÅŸturur ve bu diziyi geri yayÄ±lÄ±m kullanarak uÃ§tan uca eÄŸitir. Her aÄŸ bloÄŸu, bir Ã§ift (X<sub>i</sub>,S<sub>i</sub>) alÄ±r ve S<sub>i+1</sub> olarak bir sonuÃ§ Ã¼retir. Son durum S<sub>n</sub> veya (Ã§Ä±kÄ±ÅŸ Y<sub>n</sub>) bir doÄŸrusal sÄ±nÄ±flayÄ±cÄ±ya gider ve sonucu Ã¼retir. TÃ¼m aÄŸ bloklarÄ± aynÄ± aÄŸÄ±rlÄ±klarÄ± paylaÅŸÄ±r ve tek bir geri yayÄ±lÄ±m geÃ§iÅŸi kullanÄ±larak uÃ§tan uca eÄŸitilir.

Durum vektÃ¶rleri S<sub>0</sub>,...,S<sub>n</sub> aÄŸdan geÃ§irilerek, kelimeler arasÄ±ndaki sÄ±ralÄ± baÄŸÄ±mlÄ±lÄ±klarÄ± Ã¶ÄŸrenebilir. Ã–rneÄŸin, dizide *not* kelimesi geÃ§tiÄŸinde, belirli Ã¶ÄŸeleri durum vektÃ¶rÃ¼nde olumsuzlamak iÃ§in Ã¶ÄŸrenebiliriz, bu da olumsuzluk yaratÄ±r.

> âœ… YukarÄ±daki resimdeki tÃ¼m RNN bloklarÄ±nÄ±n aÄŸÄ±rlÄ±klarÄ± paylaÅŸÄ±ldÄ±ÄŸÄ±ndan, aynÄ± resim, aÄŸÄ±n Ã§Ä±kÄ±ÅŸ durumunu girdi olarak geri ileten bir tekrarlayan geri besleme dÃ¶ngÃ¼sÃ¼ ile bir blok (saÄŸda) olarak temsil edilebilir.

## RNN HÃ¼cresinin Anatomisi

Basit bir RNN hÃ¼cresinin nasÄ±l organize edildiÄŸine bakalÄ±m. Ã–nceki durum S<sub>i-1</sub> ve mevcut sembol X<sub>i</sub> olarak girdileri kabul eder ve Ã§Ä±kÄ±ÅŸ durumu S<sub>i</sub> Ã¼retmelidir (ve bazen, Ã¼retken aÄŸlarla olduÄŸu gibi, baÅŸka bir Ã§Ä±kÄ±ÅŸ Y<sub>i</sub> ile de ilgileniyoruz).

Basit bir RNN hÃ¼cresinin iÃ§inde iki aÄŸÄ±rlÄ±k matris vardÄ±r: biri bir giriÅŸ sembolÃ¼nÃ¼ dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r (buna W diyelim), diÄŸeri ise bir giriÅŸ durumunu dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r (H). Bu durumda, aÄŸÄ±n Ã§Ä±ktÄ±sÄ± Ïƒ(WÃ—X<sub>i</sub>+HÃ—S<sub>i-1</sub>+b) olarak hesaplanÄ±r; burada Ïƒ aktivasyon fonksiyonu ve b ek bir Ã¶nyargÄ±dÄ±r.

<img alt="RNN HÃ¼cre Anatomisi" src="images/rnn-anatomy.png" width="50%"/>

> YazarÄ±n resmi

BirÃ§ok durumda, giriÅŸ tokenlarÄ± RNN'ye girmeden Ã¶nce gÃ¶mme katmanÄ±ndan geÃ§irilir, bu da boyutu azaltÄ±r. Bu durumda, giriÅŸ vektÃ¶rlerinin boyutu *emb_size* ve durum vektÃ¶rÃ¼ *hid_size* ise, W'nin boyutu *emb_size*Ã—*hid_size* ve H'nin boyutu *hid_size*Ã—*hid_size* olur.

## Uzun KÄ±sa SÃ¼reli Bellek (LSTM)

Klasik RNN'lerin en bÃ¼yÃ¼k problemlerinden biri, sÃ¶zde **kaybolan gradyanlar** problemidir. RNN'ler, tek bir geri yayÄ±lÄ±m geÃ§iÅŸinde uÃ§tan uca eÄŸitildiÄŸinden, hatayÄ± aÄŸÄ±n ilk katmanlarÄ±na iletmekte zorluk Ã§eker ve dolayÄ±sÄ±yla aÄŸ, uzak tokenlar arasÄ±ndaki iliÅŸkileri Ã¶ÄŸrenemez. Bu problemi aÅŸmanÄ±n yollarÄ±ndan biri, **aÃ§Ä±k durum yÃ¶netimi** saÄŸlamak iÃ§in sÃ¶zde **kapÄ±lar** kullanmaktÄ±r. Bu tÃ¼rde iki iyi bilinen mimari vardÄ±r: **Uzun KÄ±sa SÃ¼reli Bellek** (LSTM) ve **KapalÄ± RÃ¶le Birimi** (GRU).

![Uzun KÄ±sa SÃ¼reli Bellek hÃ¼cresini gÃ¶steren bir resim](../../../../../lessons/5-NLP/16-RNN/images/long-short-term-memory-cell.svg)

> Resim kaynaÄŸÄ± TBD

LSTM AÄŸÄ±, RNN'e benzer bir ÅŸekilde organize edilmiÅŸtir, ancak katmanlar arasÄ±nda geÃ§irilen iki durum vardÄ±r: gerÃ§ek durum C ve gizli vektÃ¶r H. Her birimde, gizli vektÃ¶r H<sub>i</sub>, giriÅŸ X<sub>i</sub> ile birleÅŸtirilir ve durum C Ã¼zerindeki etkilerini **kapÄ±lar** aracÄ±lÄ±ÄŸÄ±yla kontrol ederler. Her kapÄ±, sigmoid aktivasyona sahip bir sinir aÄŸÄ±dÄ±r (Ã§Ä±kÄ±ÅŸ [0,1] aralÄ±ÄŸÄ±nda), bu da durum vektÃ¶rÃ¼ ile Ã§arpÄ±ldÄ±ÄŸÄ±nda bit dÃ¼zeyinde bir maske olarak dÃ¼ÅŸÃ¼nÃ¼lebilir. YukarÄ±daki resimde soldan saÄŸa doÄŸru ÅŸu kapÄ±lar vardÄ±r:

* **Unutma kapÄ±sÄ±**, bir gizli vektÃ¶r alÄ±r ve vektÃ¶r C'nin hangi bileÅŸenlerini unutmamÄ±z gerektiÄŸini ve hangilerini geÃ§irmemiz gerektiÄŸini belirler.
* **GiriÅŸ kapÄ±sÄ±**, giriÅŸ ve gizli vektÃ¶rlerden bazÄ± bilgileri alÄ±r ve bunu duruma ekler.
* **Ã‡Ä±kÄ±ÅŸ kapÄ±sÄ±**, durumu *tanh* aktivasyonuna sahip bir doÄŸrusal katman aracÄ±lÄ±ÄŸÄ±yla dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r, ardÄ±ndan yeni bir durum C<sub>i+1</sub> Ã¼retmek iÃ§in gizli vektÃ¶r H<sub>i</sub> kullanarak bazÄ± bileÅŸenlerini seÃ§er.

Durum C'nin bileÅŸenleri, aÃ§Ä±lÄ±p kapanabilen bazÄ± bayraklar olarak dÃ¼ÅŸÃ¼nÃ¼lebilir. Ã–rneÄŸin, dizide *Alice* adÄ±nÄ± bulduÄŸumuzda, bunun bir kadÄ±n karakteri ifade ettiÄŸini varsayabiliriz ve cÃ¼mlede bir kadÄ±n isim olduÄŸu bayraÄŸÄ±nÄ± aÃ§arÄ±z. Daha sonra *and Tom* ifadeleri ile karÅŸÄ±laÅŸtÄ±ÄŸÄ±mÄ±zda, Ã§oÄŸul bir isim olduÄŸuna dair bayraÄŸÄ± aÃ§arÄ±z. BÃ¶ylece durumu manipÃ¼le ederek cÃ¼mle parÃ§alarÄ±nÄ±n dilbilgisel Ã¶zelliklerini takip edebiliriz.

> âœ… LSTM'nin iÃ§ iÅŸleyiÅŸini anlamak iÃ§in harika bir kaynak, Christopher Olah tarafÄ±ndan yazÄ±lmÄ±ÅŸ [LSTM AÄŸlarÄ±nÄ± Anlamak](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) adlÄ± makaledir.

## Ä°ki YÃ¶nlÃ¼ ve Ã‡ok KatmanlÄ± RNN'ler

Bir dizinin baÅŸlangÄ±cÄ±ndan sonuna doÄŸru tek yÃ¶nde Ã§alÄ±ÅŸan tekrarlayan aÄŸlarÄ± tartÄ±ÅŸtÄ±k. Bu doÄŸal gÃ¶rÃ¼nmektedir, Ã§Ã¼nkÃ¼ okumaya ve konuÅŸmayÄ± dinlemeye benzer. Ancak, birÃ§ok pratik durumda girdi dizisine rastgele eriÅŸimimiz olduÄŸundan, tekrarlayan hesaplamayÄ± her iki yÃ¶nde de Ã§alÄ±ÅŸtÄ±rmak mantÄ±klÄ± olabilir. Bu tÃ¼r aÄŸlara **iki yÃ¶nlÃ¼** RNN'ler denir. Ä°ki yÃ¶nlÃ¼ bir aÄŸla Ã§alÄ±ÅŸÄ±rken, her yÃ¶n iÃ§in iki gizli durum vektÃ¶rÃ¼ne ihtiyacÄ±mÄ±z olacaktÄ±r.

Bir Tekrarlayan aÄŸ, ister tek yÃ¶nlÃ¼ ister iki yÃ¶nlÃ¼ olsun, bir dizideki belirli kalÄ±plarÄ± yakalar ve bunlarÄ± bir durum vektÃ¶rÃ¼nde depolayabilir veya Ã§Ä±kÄ±ÅŸa iletebilir. KonvolÃ¼syonel aÄŸlarla olduÄŸu gibi, ilk katmanÄ±n Ã§Ä±kÄ±ÅŸÄ±ndan daha yÃ¼ksek dÃ¼zeyde kalÄ±plarÄ± yakalamak ve ilk katmanÄ±n Ã§Ä±kardÄ±ÄŸÄ± dÃ¼ÅŸÃ¼k dÃ¼zey kalÄ±plardan inÅŸa etmek iÃ§in Ã¼stte baÅŸka bir tekrarlayan katman oluÅŸturabiliriz. Bu, **Ã§ok katmanlÄ± RNN** kavramÄ±na gÃ¶tÃ¼rÃ¼r; bu, bir Ã¶nceki katmanÄ±n Ã§Ä±ktÄ±sÄ±nÄ±n bir sonraki katmana girdi olarak iletildiÄŸi iki veya daha fazla tekrarlayan aÄŸdan oluÅŸur.

![Ã‡ok KatmanlÄ± uzun-kÄ±sa sÃ¼reli bellek RNN'yi gÃ¶steren bir resim](../../../../../translated_images/multi-layer-lstm.dd975e29bb2a59fe58b429db833932d734c81f211cad2783797a9608984acb8c.tr.jpg)

*Resim [bu harika yazÄ±dan](https://towardsdatascience.com/from-a-lstm-cell-to-a-multilayer-lstm-network-with-pytorch-2899eb5696f3) Fernando LÃ³pez tarafÄ±ndan alÄ±nmÄ±ÅŸtÄ±r.*

## âœï¸ AlÄ±ÅŸtÄ±rmalar: GÃ¶mme

AÅŸaÄŸÄ±daki defterlerde Ã¶ÄŸrenmeye devam edin:

* [PyTorch ile RNN'ler](../../../../../lessons/5-NLP/16-RNN/RNNPyTorch.ipynb)
* [TensorFlow ile RNN'ler](../../../../../lessons/5-NLP/16-RNN/RNNTF.ipynb)

## SonuÃ§

Bu birimde, RNN'lerin dizi sÄ±nÄ±flandÄ±rmasÄ± iÃ§in kullanÄ±labileceÄŸini gÃ¶rdÃ¼k, ancak aslÄ±nda metin Ã¼retimi, makine Ã§evirisi ve daha fazlasÄ± gibi birÃ§ok baÅŸka gÃ¶revi de yerine getirebilirler. Bu gÃ¶revleri bir sonraki birimde ele alacaÄŸÄ±z.

## ğŸš€ Zorluk

LSTM'ler hakkÄ±nda bazÄ± literatÃ¼rleri okuyun ve uygulamalarÄ±nÄ± dÃ¼ÅŸÃ¼nÃ¼n:

- [Grid Uzun KÄ±sa SÃ¼reli Bellek](https://arxiv.org/pdf/1507.01526v1.pdf)
- [GÃ¶ster, KatÄ±l ve SÃ¶yle: GÃ¶rsel Dikkat ile Sinirsel GÃ¶rÃ¼ntÃ¼ BaÅŸlÄ±ÄŸÄ± Ãœretimi](https://arxiv.org/pdf/1502.03044v2.pdf)

## [Ders sonrasÄ± quiz](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/216)

## Ä°nceleme & Kendi Kendine Ã‡alÄ±ÅŸma

- [LSTM AÄŸlarÄ±nÄ± Anlamak](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) Christopher Olah tarafÄ±ndan.

## [Ã–dev: Defterler](assignment.md)

**AÃ§Ä±klama**:  
Bu belge, makine tabanlÄ± yapay zeka Ã§eviri hizmetleri kullanÄ±larak Ã§evrilmiÅŸtir. DoÄŸruluk iÃ§in Ã§aba gÃ¶stersek de, otomatik Ã§evirilerin hatalar veya yanlÄ±ÅŸlÄ±klar iÃ§erebileceÄŸini lÃ¼tfen unutmayÄ±n. Orijinal belge, kendi dilinde yetkili kaynak olarak kabul edilmelidir. Kritik bilgiler iÃ§in profesyonel insan Ã§evirisi Ã¶nerilmektedir. Bu Ã§evirinin kullanÄ±lmasÄ± sonucu ortaya Ã§Ä±kan herhangi bir yanlÄ±ÅŸ anlama veya yanlÄ±ÅŸ yorumlama iÃ§in sorumluluk kabul etmiyoruz.