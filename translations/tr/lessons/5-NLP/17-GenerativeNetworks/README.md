<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "d9de7847385eeeda67cfdcce1640ab72",
  "translation_date": "2025-08-26T07:21:08+00:00",
  "source_file": "lessons/5-NLP/17-GenerativeNetworks/README.md",
  "language_code": "tr"
}
-->
# Ãœretken AÄŸlar

## [Ders Ã–ncesi Test](https://ff-quizzes.netlify.app/en/ai/quiz/33)

Tekrarlayan Sinir AÄŸlarÄ± (RNN'ler) ve Uzun KÄ±sa SÃ¼reli Bellek HÃ¼creleri (LSTM'ler) ile Gated Recurrent Units (GRU'ler) gibi kapÄ±lÄ± hÃ¼cre varyantlarÄ±, dil modelleme iÃ§in bir mekanizma saÄŸlar. Bu mekanizma, kelime sÄ±ralamasÄ±nÄ± Ã¶ÄŸrenebilir ve bir dizideki bir sonraki kelime iÃ§in tahminler sunabilir. Bu, RNN'lerin **Ã¼retken gÃ¶revler** iÃ§in kullanÄ±lmasÄ±nÄ± saÄŸlar; Ã¶rneÄŸin, sÄ±radan metin Ã¼retimi, makine Ã§evirisi ve hatta gÃ¶rÃ¼ntÃ¼ aÃ§Ä±klamasÄ±.

> âœ… Yazarken metin tamamlama gibi Ã¼retken gÃ¶revlerden faydalandÄ±ÄŸÄ±nÄ±z tÃ¼m zamanlarÄ± dÃ¼ÅŸÃ¼nÃ¼n. Favori uygulamalarÄ±nÄ±zÄ±n RNN'leri kullanÄ±p kullanmadÄ±ÄŸÄ±nÄ± gÃ¶rmek iÃ§in biraz araÅŸtÄ±rma yapÄ±n.

Ã–nceki birimde tartÄ±ÅŸtÄ±ÄŸÄ±mÄ±z RNN mimarisinde, her RNN birimi bir sonraki gizli durumu Ã§Ä±ktÄ± olarak Ã¼retir. Ancak, her tekrarlayan birime baÅŸka bir Ã§Ä±ktÄ± ekleyebiliriz, bu da bir **dizi** (orijinal dizinin uzunluÄŸuna eÅŸit) Ã¼retmemizi saÄŸlar. AyrÄ±ca, her adÄ±mda bir giriÅŸ kabul etmeyen ve sadece bir baÅŸlangÄ±Ã§ durum vektÃ¶rÃ¼ alarak bir Ã§Ä±ktÄ± dizisi Ã¼reten RNN birimleri kullanabiliriz.

Bu, aÅŸaÄŸÄ±daki resimde gÃ¶sterilen farklÄ± sinir mimarilerini mÃ¼mkÃ¼n kÄ±lar:

![YaygÄ±n tekrarlayan sinir aÄŸÄ± desenlerini gÃ¶steren resim.](../../../../../translated_images/unreasonable-effectiveness-of-rnn.541ead816778f42dce6c42d8a56c184729aa2378d059b851be4ce12b993033df.tr.jpg)

> Resim, [Andrej Karpaty](http://karpathy.github.io/) tarafÄ±ndan yazÄ±lan [Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) blog yazÄ±sÄ±ndan alÄ±nmÄ±ÅŸtÄ±r.

* **Bire bir**: Bir giriÅŸ ve bir Ã§Ä±kÄ±ÅŸ ile geleneksel bir sinir aÄŸÄ±dÄ±r.
* **Bire Ã§ok**: Bir giriÅŸ deÄŸeri kabul eden ve bir Ã§Ä±ktÄ± deÄŸerleri dizisi Ã¼reten Ã¼retken bir mimaridir. Ã–rneÄŸin, bir **gÃ¶rÃ¼ntÃ¼ aÃ§Ä±klama** aÄŸÄ± eÄŸitmek istiyorsak, bir resmi giriÅŸ olarak alabilir, bir CNN'den geÃ§irerek gizli durumunu elde edebilir ve ardÄ±ndan bir tekrarlayan zincir aÃ§Ä±klamayÄ± kelime kelime Ã¼retebiliriz.
* **Ã‡oktan bire**: Ã–nceki birimde tanÄ±mladÄ±ÄŸÄ±mÄ±z RNN mimarilerine karÅŸÄ±lÄ±k gelir, Ã¶rneÄŸin metin sÄ±nÄ±flandÄ±rma.
* **Ã‡oktan Ã§oÄŸa** veya **diziden diziye**: **Makine Ã§evirisi** gibi gÃ¶revlere karÅŸÄ±lÄ±k gelir. Burada, ilk RNN giriÅŸ dizisinden tÃ¼m bilgiyi gizli duruma toplar ve baÅŸka bir RNN zinciri bu durumu Ã§Ä±ktÄ± dizisine aÃ§ar.

Bu birimde, metin Ã¼retmemize yardÄ±mcÄ± olan basit Ã¼retken modeller Ã¼zerine odaklanacaÄŸÄ±z. Basitlik iÃ§in karakter dÃ¼zeyinde tokenizasyon kullanacaÄŸÄ±z.

Bu RNN'yi adÄ±m adÄ±m metin Ã¼retmek iÃ§in eÄŸiteceÄŸiz. Her adÄ±mda, `nchars` uzunluÄŸunda bir karakter dizisi alacaÄŸÄ±z ve aÄŸdan her giriÅŸ karakteri iÃ§in bir sonraki Ã§Ä±ktÄ± karakterini Ã¼retmesini isteyeceÄŸiz:

![RNN'nin 'HELLO' kelimesini Ã¼retme Ã¶rneÄŸini gÃ¶steren resim.](../../../../../translated_images/rnn-generate.56c54afb52f9781d63a7c16ea9c1b86cb70e6e1eae6a742b56b7b37468576b17.tr.png)

Metin Ã¼retirken (Ã§Ä±karsama sÄ±rasÄ±nda), bazÄ± **baÅŸlangÄ±Ã§ noktasÄ±** ile baÅŸlarÄ±z. Bu baÅŸlangÄ±Ã§ noktasÄ± RNN hÃ¼crelerinden geÃ§irilerek ara durumu oluÅŸturur ve ardÄ±ndan bu durumdan Ã¼retim baÅŸlar. Her seferinde bir karakter Ã¼retiriz ve durumu ve Ã¼retilen karakteri bir sonraki karakteri Ã¼retmek iÃ§in baÅŸka bir RNN hÃ¼cresine geÃ§iririz. Bu iÅŸlem yeterli sayÄ±da karakter Ã¼retilene kadar devam eder.

<img src="images/rnn-generate-inf.png" width="60%"/>

> Resim, yazar tarafÄ±ndan oluÅŸturulmuÅŸtur.

## âœï¸ AlÄ±ÅŸtÄ±rmalar: Ãœretken AÄŸlar

AÅŸaÄŸÄ±daki not defterlerinde Ã¶ÄŸrenmeye devam edin:

* [PyTorch ile Ãœretken AÄŸlar](../../../../../lessons/5-NLP/17-GenerativeNetworks/GenerativePyTorch.ipynb)
* [TensorFlow ile Ãœretken AÄŸlar](../../../../../lessons/5-NLP/17-GenerativeNetworks/GenerativeTF.ipynb)

## YumuÅŸak Metin Ãœretimi ve SÄ±caklÄ±k

Her RNN hÃ¼cresinin Ã§Ä±ktÄ±sÄ± bir karakter olasÄ±lÄ±k daÄŸÄ±lÄ±mÄ±dÄ±r. EÄŸer Ã¼retilen metindeki bir sonraki karakter olarak her zaman en yÃ¼ksek olasÄ±lÄ±ÄŸa sahip karakteri seÃ§ersek, metin genellikle aynÄ± karakter dizileri arasÄ±nda "dÃ¶nmeye" baÅŸlayabilir, aÅŸaÄŸÄ±daki Ã¶rnekte olduÄŸu gibi:

```
today of the second the company and a second the company ...
```

Ancak, bir sonraki karakter iÃ§in olasÄ±lÄ±k daÄŸÄ±lÄ±mÄ±na bakarsak, en yÃ¼ksek olasÄ±lÄ±klardan birkaÃ§Ä±nÄ±n arasÄ±ndaki farkÄ±n bÃ¼yÃ¼k olmadÄ±ÄŸÄ±nÄ± gÃ¶rebiliriz. Ã–rneÄŸin, bir karakterin olasÄ±lÄ±ÄŸÄ± 0.2, diÄŸerinin ise 0.19 olabilir. Ã–rneÄŸin, '*play*' dizisindeki bir sonraki karakter, boÅŸluk veya **e** (kelime *player*'daki gibi) olabilir.

Bu, her zaman en yÃ¼ksek olasÄ±lÄ±ÄŸa sahip karakteri seÃ§menin "adil" olmadÄ±ÄŸÄ±nÄ± gÃ¶sterir, Ã§Ã¼nkÃ¼ ikinci en yÃ¼ksek olasÄ±lÄ±ÄŸa sahip karakteri seÃ§mek de anlamlÄ± bir metin oluÅŸturabilir. Daha akÄ±llÄ±ca bir yaklaÅŸÄ±m, aÄŸ Ã§Ä±ktÄ±sÄ±nÄ±n verdiÄŸi olasÄ±lÄ±k daÄŸÄ±lÄ±mÄ±ndan karakterleri **Ã¶rneklemek** olacaktÄ±r. AyrÄ±ca, olasÄ±lÄ±k daÄŸÄ±lÄ±mÄ±nÄ± dÃ¼zleÅŸtirmek veya daha dik hale getirmek iÃ§in **sÄ±caklÄ±k** adlÄ± bir parametre kullanabiliriz. Bu, daha fazla rastgelelik eklemek veya en yÃ¼ksek olasÄ±lÄ±klÄ± karakterlere daha fazla baÄŸlÄ± kalmak istediÄŸimizde iÅŸe yarar.

Bu yumuÅŸak metin Ã¼retiminin yukarÄ±daki not defterlerinde nasÄ±l uygulandÄ±ÄŸÄ±nÄ± keÅŸfedin.

## SonuÃ§

Metin Ã¼retimi kendi baÅŸÄ±na faydalÄ± olabilir, ancak asÄ±l avantajlar, RNN'ler kullanÄ±larak bir baÅŸlangÄ±Ã§ Ã¶zellik vektÃ¶rÃ¼nden metin Ã¼retme yeteneÄŸinden gelir. Ã–rneÄŸin, metin Ã¼retimi makine Ã§evirisinin bir parÃ§asÄ± olarak kullanÄ±lÄ±r (diziden diziye, bu durumda *kodlayÄ±cÄ±*dan gelen durum vektÃ¶rÃ¼ Ã§evrilen mesajÄ± Ã¼retmek veya *Ã§Ã¶zmek* iÃ§in kullanÄ±lÄ±r) veya bir gÃ¶rÃ¼ntÃ¼nÃ¼n metinsel aÃ§Ä±klamasÄ±nÄ± Ã¼retmek iÃ§in kullanÄ±lÄ±r (bu durumda Ã¶zellik vektÃ¶rÃ¼ CNN Ã§Ä±karÄ±cÄ±sÄ±ndan gelir).

## ğŸš€ Meydan Okuma

Bu konuyla ilgili Microsoft Learn'de bazÄ± dersler alÄ±n:

* [PyTorch](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-pytorch/6-generative-networks/?WT.mc_id=academic-77998-cacaste)/[TensorFlow](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-tensorflow/5-generative-networks/?WT.mc_id=academic-77998-cacaste) ile Metin Ãœretimi

## [Ders SonrasÄ± Test](https://ff-quizzes.netlify.app/en/ai/quiz/34)

## Ä°nceleme ve Kendi Kendine Ã‡alÄ±ÅŸma

Bilginizi geniÅŸletmek iÃ§in bazÄ± makaleler:

* Markov Zinciri, LSTM ve GPT-2 ile metin Ã¼retimine farklÄ± yaklaÅŸÄ±mlar: [blog yazÄ±sÄ±](https://towardsdatascience.com/text-generation-gpt-2-lstm-markov-chain-9ea371820e1e)
* [Keras dokÃ¼mantasyonu](https://keras.io/examples/generative/lstm_character_level_text_generation/) iÃ§inde metin Ã¼retim Ã¶rneÄŸi

## [Ã–dev](lab/README.md)

Metni karakter karakter nasÄ±l Ã¼reteceÄŸimizi gÃ¶rdÃ¼k. Laboratuvarda, kelime dÃ¼zeyinde metin Ã¼retimini keÅŸfedeceksiniz.

**Feragatname**:  
Bu belge, AI Ã§eviri hizmeti [Co-op Translator](https://github.com/Azure/co-op-translator) kullanÄ±larak Ã§evrilmiÅŸtir. DoÄŸruluk iÃ§in Ã§aba gÃ¶stersek de, otomatik Ã§evirilerin hata veya yanlÄ±ÅŸlÄ±k iÃ§erebileceÄŸini lÃ¼tfen unutmayÄ±n. Belgenin orijinal dilindeki hali, yetkili kaynak olarak kabul edilmelidir. Kritik bilgiler iÃ§in profesyonel insan Ã§evirisi Ã¶nerilir. Bu Ã§evirinin kullanÄ±mÄ±ndan kaynaklanan yanlÄ±ÅŸ anlamalar veya yanlÄ±ÅŸ yorumlamalar iÃ§in sorumluluk kabul etmiyoruz.