# Ãœretken aÄŸlar

## [Ders Ã¶ncesi quiz](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/117)

Tekrarlayan Sinir AÄŸlarÄ± (RNN'ler) ve Long Short Term Memory Cells (LSTM'ler) ile Gated Recurrent Units (GRU'lar) gibi kapalÄ± hÃ¼cre varyantlarÄ±, dil modellemesi iÃ§in bir mekanizma saÄŸladÄ±; Ã§Ã¼nkÃ¼ kelime sÄ±ralamasÄ±nÄ± Ã¶ÄŸrenebilirler ve bir dizideki bir sonraki kelime iÃ§in tahminlerde bulunabilirler. Bu, RNN'leri **Ã¼retken gÃ¶revler** iÃ§in kullanmamÄ±za olanak tanÄ±r; Ã¶rneÄŸin sÄ±radan metin oluÅŸturma, makine Ã§evirisi ve hatta gÃ¶rÃ¼ntÃ¼ baÅŸlÄ±ÄŸÄ± oluÅŸturma.

> âœ… Yazarken metin tamamlama gibi Ã¼retken gÃ¶revlerden faydalandÄ±ÄŸÄ±nÄ±z tÃ¼m zamanlarÄ± dÃ¼ÅŸÃ¼nÃ¼n. Favori uygulamalarÄ±nÄ±zda RNN'lerin kullanÄ±lÄ±p kullanÄ±lmadÄ±ÄŸÄ±nÄ± gÃ¶rmek iÃ§in biraz araÅŸtÄ±rma yapÄ±n.

Ã–nceki biriminde tartÄ±ÅŸtÄ±ÄŸÄ±mÄ±z RNN mimarisinde, her RNN birimi bir sonraki gizli durumu bir Ã§Ä±ktÄ± olarak Ã¼retmiÅŸtir. Ancak, her tekrarlayan birime bir baÅŸka Ã§Ä±ktÄ± ekleyebiliriz; bu da bize **dizi** (orijinal dizinin uzunluÄŸuna eÅŸit) Ã§Ä±ktÄ±sÄ± vermemizi saÄŸlar. AyrÄ±ca, her adÄ±mda bir giriÅŸ kabul etmeyen RNN birimleri kullanabiliriz; sadece bazÄ± baÅŸlangÄ±Ã§ durumu vektÃ¶rlerini alÄ±r ve ardÄ±ndan bir Ã§Ä±ktÄ± dizisi Ã¼retebiliriz.

Bu, aÅŸaÄŸÄ±daki resimde gÃ¶sterilen farklÄ± sinir mimarilerine olanak tanÄ±r:

![YaygÄ±n tekrarlayan sinir aÄŸÄ± desenlerini gÃ¶steren bir resim.](../../../../../translated_images/unreasonable-effectiveness-of-rnn.541ead816778f42dce6c42d8a56c184729aa2378d059b851be4ce12b993033df.tr.jpg)

> Resim [Andrej Karpaty](http://karpathy.github.io/) tarafÄ±ndan [Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) baÅŸlÄ±klÄ± blog yazÄ±sÄ±ndan alÄ±nmÄ±ÅŸtÄ±r.

* **Bir-bir** bir giriÅŸ ve bir Ã§Ä±kÄ±ÅŸÄ± olan geleneksel bir sinir aÄŸÄ±dÄ±r.
* **Bir-Ã§ok** bir giriÅŸ deÄŸeri kabul eden ve bir Ã§Ä±ktÄ± deÄŸeri dizisi Ã¼reten Ã¼retken bir mimaridir. Ã–rneÄŸin, bir resmi metin olarak tanÄ±mlayan bir **gÃ¶rÃ¼ntÃ¼ baÅŸlÄ±ÄŸÄ± oluÅŸturma** aÄŸÄ± eÄŸitmek istiyorsak, resmi giriÅŸ olarak alabiliriz, bir CNN'den geÃ§irerek gizli durumunu elde edebiliriz ve ardÄ±ndan tekrarlayan bir zincir ile baÅŸlÄ±k kelimelerini kelime kelime Ã¼retebiliriz.
* **Ã‡ok-bir** Ã¶nceki birimde tanÄ±mladÄ±ÄŸÄ±mÄ±z RNN mimarilerine karÅŸÄ±lÄ±k gelir, Ã¶rneÄŸin metin sÄ±nÄ±flandÄ±rma.
* **Ã‡ok-Ã§ok**, veya **diziye-dizi** ise **makine Ã§evirisi** gibi gÃ¶revlerle ilgilidir; burada ilk RNN, giriÅŸ dizisinden tÃ¼m bilgileri gizli duruma toplar ve baÅŸka bir RNN zinciri bu durumu Ã§Ä±ktÄ± dizisine aÃ§ar.

Bu birimde, metin oluÅŸturmaya yardÄ±mcÄ± olan basit Ã¼retken modellere odaklanacaÄŸÄ±z. KolaylÄ±k olmasÄ± aÃ§Ä±sÄ±ndan, karakter dÃ¼zeyinde tokenizasyon kullanacaÄŸÄ±z.

Bu RNN'yi adÄ±m adÄ±m metin Ã¼retmesi iÃ§in eÄŸiteceÄŸiz. Her adÄ±mda, `nchars` uzunluÄŸunda bir karakter dizisi alacaÄŸÄ±z ve aÄŸa her giriÅŸ karakteri iÃ§in bir sonraki Ã§Ä±ktÄ± karakterini Ã¼retmesini isteyeceÄŸiz:

!['HELLO' kelimesinin RNN ile Ã¼retilmesini gÃ¶steren bir Ã¶rnek resim.](../../../../../translated_images/rnn-generate.56c54afb52f9781d63a7c16ea9c1b86cb70e6e1eae6a742b56b7b37468576b17.tr.png)

Metin oluÅŸtururken (Ã§Ä±karÄ±m sÄ±rasÄ±nda), bazÄ± **uyarÄ±cÄ±larla** baÅŸlarÄ±z; bu, RNN hÃ¼crelerinden geÃ§irilerek ara durumunu Ã¼retir ve ardÄ±ndan bu durumdan Ã¼retim baÅŸlar. Bir seferde bir karakter Ã¼retiriz ve durumu ve Ã¼retilen karakteri bir sonraki RNN hÃ¼cresine geÃ§irerek bir sonraki karakteri Ã¼retiriz; yeterli karakter Ã¼rettiÄŸimizdeye kadar devam ederiz.

<img src="images/rnn-generate-inf.png" width="60%"/>

> Resim yazar tarafÄ±ndan

## âœï¸ AlÄ±ÅŸtÄ±rmalar: Ãœretken AÄŸlar

AÅŸaÄŸÄ±daki not defterlerinde Ã¶ÄŸrenmeye devam edin:

* [PyTorch ile Ãœretken AÄŸlar](../../../../../lessons/5-NLP/17-GenerativeNetworks/GenerativePyTorch.ipynb)
* [TensorFlow ile Ãœretken AÄŸlar](../../../../../lessons/5-NLP/17-GenerativeNetworks/GenerativeTF.ipynb)

## YumuÅŸak metin oluÅŸturma ve sÄ±caklÄ±k

Her RNN hÃ¼cresinin Ã§Ä±ktÄ±sÄ±, karakterlerin bir olasÄ±lÄ±k daÄŸÄ±lÄ±mÄ±dÄ±r. EÄŸer her zaman en yÃ¼ksek olasÄ±lÄ±ÄŸa sahip karakteri Ã¼retilen metindeki bir sonraki karakter olarak alÄ±rsak, metin genellikle aynÄ± karakter dizileri arasÄ±nda "dÃ¶nmeye" baÅŸlayabilir; bu Ã¶rnekte olduÄŸu gibi:

```
today of the second the company and a second the company ...
```

Ancak, bir sonraki karakterin olasÄ±lÄ±k daÄŸÄ±lÄ±mÄ±na baktÄ±ÄŸÄ±mÄ±zda, en yÃ¼ksek birkaÃ§ olasÄ±lÄ±k arasÄ±ndaki farkÄ±n Ã§ok bÃ¼yÃ¼k olmayabileceÄŸi durumlar vardÄ±r; Ã¶rneÄŸin, bir karakterin olasÄ±lÄ±ÄŸÄ± 0.2, diÄŸerinin 0.19 olabilir. Ã–rneÄŸin, '*play*' dizisinde bir sonraki karakter, ya boÅŸluk ya da **e** (Ã¶rneÄŸin *player* kelimesinde olduÄŸu gibi) olabilir.

Bu, daha yÃ¼ksek olasÄ±lÄ±ÄŸa sahip karakteri seÃ§menin her zaman "adil" olmadÄ±ÄŸÄ±nÄ± sonucuna gÃ¶tÃ¼rÃ¼r; Ã§Ã¼nkÃ¼ ikinci en yÃ¼ksek olanÄ± seÃ§mek de anlamlÄ± bir metne yol aÃ§abilir. **OlasÄ±lÄ±k daÄŸÄ±lÄ±mÄ±ndan** karakterler **Ã¶rneklemek** daha akÄ±llÄ±cadÄ±r. AyrÄ±ca, daha fazla rastgelelik eklemek istiyorsak olasÄ±lÄ±k daÄŸÄ±lÄ±mÄ±nÄ± dÃ¼zleÅŸtirecek bir parametre, **sÄ±caklÄ±k** kullanabiliriz; ya da en yÃ¼ksek olasÄ±lÄ±ÄŸa sahip karakterlere daha fazla baÄŸlÄ± kalmak istiyorsak daha dik bir hale getirebiliriz.

Bu yumuÅŸak metin oluÅŸturmanÄ±n yukarÄ±da baÄŸlantÄ±lÄ± not defterlerinde nasÄ±l uygulandÄ±ÄŸÄ±nÄ± keÅŸfedin.

## SonuÃ§

Metin oluÅŸturma kendi baÅŸÄ±na yararlÄ± olsa da, asÄ±l faydalar, RNN'ler kullanarak bazÄ± baÅŸlangÄ±Ã§ Ã¶zellik vektÃ¶rlerinden metin Ã¼retebilme yeteneÄŸinden gelir. Ã–rneÄŸin, metin oluÅŸturma, makine Ã§evirisinin bir parÃ§asÄ± olarak kullanÄ±lÄ±r (bu durumda *encoder*'dan gelen durum vektÃ¶rÃ¼, Ã§evrilen mesajÄ± Ã¼retmek veya *decode* etmek iÃ§in kullanÄ±lÄ±r) veya bir gÃ¶rÃ¼ntÃ¼nÃ¼n metinsel tanÄ±mÄ±nÄ± oluÅŸturma (bu durumda Ã¶zellik vektÃ¶rÃ¼ CNN Ã§Ä±karÄ±cÄ±sÄ±ndan gelir).

## ğŸš€ Zorluk

Bu konuda Microsoft Learn'da bazÄ± dersler alÄ±n

* Metin OluÅŸturma ile [PyTorch](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-pytorch/6-generative-networks/?WT.mc_id=academic-77998-cacaste)/[TensorFlow](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-tensorflow/5-generative-networks/?WT.mc_id=academic-77998-cacaste)

## [Ders sonrasÄ± quiz](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/217)

## GÃ¶zden GeÃ§irme & Kendi Kendine Ã‡alÄ±ÅŸma

Bilgilerinizi geniÅŸletmek iÃ§in bazÄ± makaleler

* Markov Zinciri, LSTM ve GPT-2 ile metin oluÅŸturmanÄ±n farklÄ± yaklaÅŸÄ±mlarÄ±: [blog yazÄ±sÄ±](https://towardsdatascience.com/text-generation-gpt-2-lstm-markov-chain-9ea371820e1e)
* [Keras belgelerinde](https://keras.io/examples/generative/lstm_character_level_text_generation/) metin oluÅŸturma Ã¶rneÄŸi

## [GÃ¶rev](lab/README.md)

Karakter karakter metin oluÅŸturmanÄ±n nasÄ±l yapÄ±ldÄ±ÄŸÄ±nÄ± gÃ¶rdÃ¼k. Laboratuvar ortamÄ±nda, kelime dÃ¼zeyinde metin oluÅŸturmayÄ± keÅŸfedeceksiniz.

**AÃ§Ä±klama**:  
Bu belge, makine tabanlÄ± AI Ã§eviri hizmetleri kullanÄ±larak Ã§evrilmiÅŸtir. DoÄŸruluk konusunda Ã§aba gÃ¶stersek de, otomatik Ã§evirilerin hatalar veya yanlÄ±ÅŸ anlamalar iÃ§erebileceÄŸini lÃ¼tfen unutmayÄ±n. Orijinal belge, kendi dilinde yetkili kaynak olarak kabul edilmelidir. Kritik bilgiler iÃ§in profesyonel insan Ã§evirisi Ã¶nerilmektedir. Bu Ã§evirinin kullanÄ±mÄ± sonucunda ortaya Ã§Ä±kan yanlÄ±ÅŸ anlamalar veya yanlÄ±ÅŸ yorumlamalardan sorumlu deÄŸiliz.