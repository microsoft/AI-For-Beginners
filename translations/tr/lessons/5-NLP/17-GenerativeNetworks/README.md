# Ãœretici AÄŸlar

## [Ders Ã–ncesi Test](https://ff-quizzes.netlify.app/en/ai/quiz/33)

Tekrarlayan Sinir AÄŸlarÄ± (RNN'ler) ve Uzun KÄ±sa SÃ¼reli Bellek HÃ¼creleri (LSTM'ler) ile Gated Recurrent Units (GRU'lar) gibi kapÄ±lÄ± hÃ¼cre varyantlarÄ±, dil modelleme iÃ§in bir mekanizma saÄŸlar. Bu aÄŸlar, kelime sÄ±ralamasÄ±nÄ± Ã¶ÄŸrenebilir ve bir dizideki bir sonraki kelime iÃ§in tahminlerde bulunabilir. Bu, RNN'lerin **Ã¼retici gÃ¶revler** iÃ§in kullanÄ±lmasÄ±na olanak tanÄ±r; Ã¶rneÄŸin, sÄ±radan metin Ã¼retimi, makine Ã§evirisi ve hatta gÃ¶rÃ¼ntÃ¼ aÃ§Ä±klamasÄ±.

> âœ… Yazarken metin tamamlama gibi Ã¼retici gÃ¶revlerden faydalandÄ±ÄŸÄ±nÄ±z tÃ¼m zamanlarÄ± dÃ¼ÅŸÃ¼nÃ¼n. SevdiÄŸiniz uygulamalarÄ±n RNN'leri kullanÄ±p kullanmadÄ±ÄŸÄ±nÄ± araÅŸtÄ±rÄ±n.

Ã–nceki birimde tartÄ±ÅŸtÄ±ÄŸÄ±mÄ±z RNN mimarisinde, her RNN birimi bir sonraki gizli durumu Ã§Ä±ktÄ± olarak Ã¼retir. Ancak, her tekrarlayan birime baÅŸka bir Ã§Ä±ktÄ± ekleyebiliriz, bu da bir **dizi** (orijinal dizinin uzunluÄŸuna eÅŸit) Ã§Ä±karmamÄ±za olanak tanÄ±r. AyrÄ±ca, her adÄ±mda bir giriÅŸ kabul etmeyen ve sadece bir baÅŸlangÄ±Ã§ durum vektÃ¶rÃ¼ alÄ±p ardÄ±ndan bir Ã§Ä±ktÄ± dizisi Ã¼reten RNN birimleri kullanabiliriz.

Bu, aÅŸaÄŸÄ±daki resimde gÃ¶sterilen farklÄ± sinir aÄŸÄ± mimarilerini mÃ¼mkÃ¼n kÄ±lar:

![YaygÄ±n tekrarlayan sinir aÄŸÄ± desenlerini gÃ¶steren bir resim.](../../../../../translated_images/tr/unreasonable-effectiveness-of-rnn.541ead816778f42d.webp)

> Resim, [Andrej Karpaty](http://karpathy.github.io/) tarafÄ±ndan yazÄ±lan [Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) blog yazÄ±sÄ±ndan alÄ±nmÄ±ÅŸtÄ±r.

* **Bire bir**: Bir giriÅŸ ve bir Ã§Ä±kÄ±ÅŸa sahip geleneksel bir sinir aÄŸÄ±dÄ±r.
* **Bire Ã§ok**: Bir giriÅŸ deÄŸeri kabul eden ve bir Ã§Ä±ktÄ± deÄŸerleri dizisi Ã¼reten Ã¼retici bir mimaridir. Ã–rneÄŸin, bir **gÃ¶rÃ¼ntÃ¼ aÃ§Ä±klama** aÄŸÄ± eÄŸitmek istiyorsak, bir resmi giriÅŸ olarak alabilir, bir CNN'den geÃ§irerek gizli durumunu elde edebilir ve ardÄ±ndan bir tekrarlayan zincirle aÃ§Ä±klamayÄ± kelime kelime Ã¼retebiliriz.
* **Ã‡oktan bire**: Ã–nceki birimde tanÄ±mladÄ±ÄŸÄ±mÄ±z, metin sÄ±nÄ±flandÄ±rma gibi RNN mimarilerine karÅŸÄ±lÄ±k gelir.
* **Ã‡oktan Ã§oÄŸa** veya **diziden diziye**: **Makine Ã§evirisi** gibi gÃ¶revlere karÅŸÄ±lÄ±k gelir. Ä°lk RNN, giriÅŸ dizisinden tÃ¼m bilgiyi gizli duruma toplar ve baÅŸka bir RNN zinciri bu durumu Ã§Ä±ktÄ± dizisine aÃ§ar.

Bu birimde, metin Ã¼retmemize yardÄ±mcÄ± olan basit Ã¼retici modeller Ã¼zerine odaklanacaÄŸÄ±z. Basitlik iÃ§in karakter dÃ¼zeyinde tokenizasyon kullanacaÄŸÄ±z.

Bu RNN'yi adÄ±m adÄ±m metin Ã¼retmek iÃ§in eÄŸiteceÄŸiz. Her adÄ±mda, `nchars` uzunluÄŸunda bir karakter dizisi alacaÄŸÄ±z ve aÄŸdan her giriÅŸ karakteri iÃ§in bir sonraki Ã§Ä±ktÄ± karakterini Ã¼retmesini isteyeceÄŸiz:

![RNN'nin 'HELLO' kelimesini Ã¼retme Ã¶rneÄŸini gÃ¶steren bir resim.](../../../../../translated_images/tr/rnn-generate.56c54afb52f9781d.webp)

Metin Ã¼retirken (Ã§Ä±karsama sÄ±rasÄ±nda), bazÄ± **baÅŸlangÄ±Ã§** verileriyle baÅŸlarÄ±z. Bu veri RNN hÃ¼crelerinden geÃ§irilerek ara durumu oluÅŸturur ve ardÄ±ndan Ã¼retim baÅŸlar. Her seferinde bir karakter Ã¼retiriz ve durumu ve Ã¼retilen karakteri bir sonraki karakteri Ã¼retmek iÃ§in baÅŸka bir RNN hÃ¼cresine geÃ§iririz. Bu iÅŸlem yeterli sayÄ±da karakter Ã¼retilene kadar devam eder.

<img src="../../../../../translated_images/tr/rnn-generate-inf.5168dc65e0370eea.webp" width="60%"/>

> Resim, yazar tarafÄ±ndan oluÅŸturulmuÅŸtur.

## âœï¸ AlÄ±ÅŸtÄ±rmalar: Ãœretici AÄŸlar

AÅŸaÄŸÄ±daki not defterlerinde Ã¶ÄŸrenmeye devam edin:

* [PyTorch ile Ãœretici AÄŸlar](GenerativePyTorch.ipynb)
* [TensorFlow ile Ãœretici AÄŸlar](GenerativeTF.ipynb)

## YumuÅŸak Metin Ãœretimi ve SÄ±caklÄ±k

Her RNN hÃ¼cresinin Ã§Ä±ktÄ±sÄ± bir karakter olasÄ±lÄ±k daÄŸÄ±lÄ±mÄ±dÄ±r. EÄŸer her zaman en yÃ¼ksek olasÄ±lÄ±ÄŸa sahip karakteri bir sonraki karakter olarak seÃ§ersek, metin genellikle aynÄ± karakter dizileri arasÄ±nda "dÃ¶nmeye" baÅŸlayabilir. Ã–rneÄŸin:

```
today of the second the company and a second the company ...
```

Ancak, bir sonraki karakter iÃ§in olasÄ±lÄ±k daÄŸÄ±lÄ±mÄ±na bakarsak, en yÃ¼ksek olasÄ±lÄ±klar arasÄ±ndaki farkÄ±n bÃ¼yÃ¼k olmayabileceÄŸini gÃ¶rebiliriz. Ã–rneÄŸin, bir karakterin olasÄ±lÄ±ÄŸÄ± 0.2, diÄŸerinin ise 0.19 olabilir. Ã–rneÄŸin, '*play*' dizisindeki bir sonraki karakter, eÅŸit derecede boÅŸluk veya **e** (Ã¶rneÄŸin *player* kelimesinde olduÄŸu gibi) olabilir.

Bu, her zaman en yÃ¼ksek olasÄ±lÄ±ÄŸa sahip karakteri seÃ§menin "adil" olmayabileceÄŸi sonucuna gÃ¶tÃ¼rÃ¼r. Ä°kinci en yÃ¼ksek olasÄ±lÄ±ÄŸÄ± seÃ§mek de anlamlÄ± bir metne yol aÃ§abilir. Daha akÄ±llÄ±ca bir yaklaÅŸÄ±m, aÄŸÄ±n Ã§Ä±ktÄ±sÄ±nÄ±n verdiÄŸi olasÄ±lÄ±k daÄŸÄ±lÄ±mÄ±ndan karakterleri **Ã¶rneklemek** olacaktÄ±r. AyrÄ±ca, daha fazla rastgelelik eklemek veya en yÃ¼ksek olasÄ±lÄ±klÄ± karakterlere daha fazla baÄŸlÄ± kalmak istediÄŸimizde olasÄ±lÄ±k daÄŸÄ±lÄ±mÄ±nÄ± dÃ¼zleÅŸtirecek veya dikleÅŸtirecek bir **sÄ±caklÄ±k** parametresi kullanabiliriz.

Bu yumuÅŸak metin Ã¼retiminin nasÄ±l uygulandÄ±ÄŸÄ±nÄ± yukarÄ±daki not defterlerinde keÅŸfedin.

## SonuÃ§

Metin Ã¼retimi kendi baÅŸÄ±na faydalÄ± olabilirken, asÄ±l avantajlar RNN'ler kullanÄ±larak bir baÅŸlangÄ±Ã§ Ã¶zellik vektÃ¶rÃ¼nden metin Ã¼retme yeteneÄŸinden gelir. Ã–rneÄŸin, metin Ã¼retimi makine Ã§evirisinin bir parÃ§asÄ± olarak kullanÄ±lÄ±r (diziden diziye, bu durumda *encoder*'dan gelen durum vektÃ¶rÃ¼ Ã§evrilen mesajÄ± Ã¼retmek veya *decode* etmek iÃ§in kullanÄ±lÄ±r) veya bir gÃ¶rÃ¼ntÃ¼nÃ¼n metinsel aÃ§Ä±klamasÄ±nÄ± Ã¼retmek iÃ§in kullanÄ±lÄ±r (bu durumda Ã¶zellik vektÃ¶rÃ¼ CNN Ã§Ä±karÄ±cÄ±dan gelir).

## ğŸš€ Meydan Okuma

Bu konuyla ilgili Microsoft Learn'de bazÄ± dersler alÄ±n:

* [PyTorch](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-pytorch/6-generative-networks/?WT.mc_id=academic-77998-cacaste)/[TensorFlow](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-tensorflow/5-generative-networks/?WT.mc_id=academic-77998-cacaste) ile Metin Ãœretimi

## [Ders SonrasÄ± Test](https://ff-quizzes.netlify.app/en/ai/quiz/34)

## Ä°nceleme ve Kendi Kendine Ã‡alÄ±ÅŸma

Bilginizi geniÅŸletmek iÃ§in bazÄ± makaleler:

* Markov Zinciri, LSTM ve GPT-2 ile metin Ã¼retimine farklÄ± yaklaÅŸÄ±mlar: [blog yazÄ±sÄ±](https://towardsdatascience.com/text-generation-gpt-2-lstm-markov-chain-9ea371820e1e)
* [Keras dokÃ¼mantasyonunda](https://keras.io/examples/generative/lstm_character_level_text_generation/) metin Ã¼retim Ã¶rneÄŸi

## [Ã–dev](lab/README.md)

Metni karakter karakter nasÄ±l Ã¼reteceÄŸimizi gÃ¶rdÃ¼k. Laboratuvarda, kelime dÃ¼zeyinde metin Ã¼retimini keÅŸfedeceksiniz.

---

