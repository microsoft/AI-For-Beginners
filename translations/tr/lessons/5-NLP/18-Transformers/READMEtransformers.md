# Dikkat MekanizmalarÄ± ve Transformerlar

## [Ders Ã¶ncesi quiz](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/118)

NLP alanÄ±ndaki en Ã¶nemli problemlerden biri **makine Ã§evirisi**dir; bu, Google Translate gibi araÃ§larÄ±n temelini oluÅŸturan Ã¶nemli bir gÃ¶revdir. Bu bÃ¶lÃ¼mde, makine Ã§evirisine, daha genel olarak ise herhangi bir *dizi-dizi* gÃ¶revine (bu gÃ¶reve **cÃ¼mle dÃ¶nÃ¼ÅŸtÃ¼rme** de denir) odaklanacaÄŸÄ±z.

RNN'lerle, dizi-dizi iÅŸlemi, bir giriÅŸ dizisini gizli bir duruma dÃ¶nÃ¼ÅŸtÃ¼ren **kodlayÄ±cÄ±** adÄ±ndaki bir aÄŸ ile bu gizli durumu Ã§evrilmiÅŸ bir sonuca aÃ§an **Ã§Ã¶zÃ¼cÃ¼** adÄ±ndaki baÅŸka bir aÄŸdan oluÅŸan iki tekrarlayan aÄŸ ile gerÃ§ekleÅŸtirilir. Bu yaklaÅŸÄ±mda birkaÃ§ problem bulunmaktadÄ±r:

* KodlayÄ±cÄ± aÄŸÄ±n son durumu, bir cÃ¼mlenin baÅŸÄ±nÄ± hatÄ±rlamakta zorlanÄ±r, bu da uzun cÃ¼mleler iÃ§in modelin kalitesini dÃ¼ÅŸÃ¼rÃ¼r.
* Bir dizideki tÃ¼m kelimelerin sonuca olan etkisi aynÄ±dÄ±r. Ancak gerÃ§ekte, giriÅŸ dizisindeki belirli kelimeler, sÄ±ralÄ± Ã§Ä±ktÄ±lar Ã¼zerinde diÄŸerlerinden daha fazla etkiye sahiptir.

**Dikkat MekanizmalarÄ±**, RNN'nin her bir Ã§Ä±ktÄ± tahmini Ã¼zerindeki her giriÅŸ vektÃ¶rÃ¼nÃ¼n baÄŸlamsal etkisini aÄŸÄ±rlÄ±klandÄ±rmanÄ±n bir yolunu sunar. Bu, giriÅŸ RNN'sinin ara durumlarÄ± ile Ã§Ä±ktÄ± RNN'si arasÄ±nda kÄ±sayollar oluÅŸturarak uygulanÄ±r. Bu ÅŸekilde, Ã§Ä±ktÄ± sembolÃ¼ y<sub>t</sub> Ã¼retilirken, farklÄ± aÄŸÄ±rlÄ±k katsayÄ±larÄ± Î±<sub>t,i</sub> ile tÃ¼m giriÅŸ gizli durumlarÄ± h<sub>i</sub> dikkate alÄ±nacaktÄ±r.

![Eklemeli dikkat katmanÄ±na sahip bir kodlayÄ±cÄ±/Ã§Ã¶zÃ¼cÃ¼ modelini gÃ¶steren resim](../../../../../translated_images/encoder-decoder-attention.7a726296894fb567aa2898c94b17b3289087f6705c11907df8301df9e5eeb3de.tr.png)

> Eklemeli dikkat mekanizmasÄ±na sahip kodlayÄ±cÄ±-Ã§Ã¶zÃ¼cÃ¼ modeli [Bahdanau ve diÄŸerleri, 2015](https://arxiv.org/pdf/1409.0473.pdf) tarafÄ±ndan, [bu blog yazÄ±sÄ±ndan](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html) alÄ±ntÄ±dÄ±r.

Dikkat matris {Î±<sub>i,j</sub>} belirli giriÅŸ kelimelerinin bir Ã§Ä±ktÄ± dizisindeki belirli bir kelimenin Ã¼retilmesindeki rolÃ¼nÃ¼ temsil eder. AÅŸaÄŸÄ±da bÃ¶yle bir matrisin Ã¶rneÄŸi verilmiÅŸtir:

![Bahdanau - arviz.org'dan alÄ±nan RNNsearch-50 tarafÄ±ndan bulunan Ã¶rnek hizalamayÄ± gÃ¶steren resim](../../../../../translated_images/bahdanau-fig3.09ba2d37f202a6af11de6c82d2d197830ba5f4528d9ea430eb65fd3a75065973.tr.png)

> [Bahdanau ve diÄŸerleri, 2015](https://arxiv.org/pdf/1409.0473.pdf) (Åekil 3)

Dikkat mekanizmalarÄ±, gÃ¼nÃ¼mÃ¼z veya yakÄ±n dÃ¶nemdeki NLP'deki en son teknolojinin bÃ¼yÃ¼k bir kÄ±smÄ±ndan sorumludur. Ancak dikkatin eklenmesi, model parametrelerinin sayÄ±sÄ±nÄ± bÃ¼yÃ¼k Ã¶lÃ§Ã¼de artÄ±rÄ±r ve bu da RNN'lerde Ã¶lÃ§eklenebilirlik sorunlarÄ±na yol aÃ§ar. RNN'lerin Ã¶lÃ§eklenebilirliÄŸinde temel bir kÄ±sÄ±tlama, modellerin tekrarlayan doÄŸasÄ±nÄ±n, eÄŸitimi toplu ve paralel hale getirmeyi zorlaÅŸtÄ±rmasÄ±dÄ±r. Bir RNN'de bir dizinin her bir Ã¶ÄŸesi, ardÄ±ÅŸÄ±k sÄ±rayla iÅŸlenmelidir; bu da kolayca paralel hale getirilemeyeceÄŸi anlamÄ±na gelir.

![Dikkat ile KodlayÄ±cÄ± Ã‡Ã¶zÃ¼cÃ¼](../../../../../lessons/5-NLP/18-Transformers/images/EncDecAttention.gif)

> [Google Blogu](https://research.googleblog.com/2016/09/a-neural-network-for-machine.html) kaynaklÄ± ÅŸekil

Dikkat mekanizmalarÄ±nÄ±n benimsenmesi ve bu kÄ±sÄ±tlamanÄ±n birleÅŸimi, gÃ¼nÃ¼mÃ¼zde bildiÄŸimiz ve kullandÄ±ÄŸÄ±mÄ±z, BERT'ten Open-GPT3'e kadar uzanan en son teknoloji Transformer Modellerinin yaratÄ±lmasÄ±na yol aÃ§tÄ±.

## Transformer Modelleri

TransformerlarÄ±n arkasÄ±ndaki ana fikirlerden biri, RNN'lerin ardÄ±ÅŸÄ±k doÄŸasÄ±ndan kaÃ§Ä±nmak ve eÄŸitimi sÄ±rasÄ±nda paralel hale getirilebilen bir model oluÅŸturmaktÄ±r. Bu, iki fikrin uygulanmasÄ±yla gerÃ§ekleÅŸtirilir:

* konumsal kodlama
* RNN'ler (veya CNN'ler) yerine desenleri yakalamak iÃ§in kendine dikkat mekanizmasÄ±nÄ± kullanmak (bu nedenle transformerlarÄ± tanÄ±tan makalenin adÄ± *[Dikkat, ihtiyacÄ±nÄ±z olan her ÅŸey](https://arxiv.org/abs/1706.03762)*dir)

### Konumsal Kodlama/GÃ¶mme

Konumsal kodlamanÄ±n temel fikri ÅŸudur:
1. RNN'ler kullanÄ±rken, tokenlarÄ±n gÃ¶receli konumu adÄ±m sayÄ±sÄ± ile temsil edilir ve bu nedenle aÃ§Ä±kÃ§a temsil edilmesine gerek yoktur.
2. Ancak, dikkate geÃ§tiÄŸimizde, bir dizideki tokenlarÄ±n gÃ¶receli konumlarÄ±nÄ± bilmemiz gerekir.
3. Konumsal kodlama almak iÃ§in, token dizimizi dizideki token konumlarÄ± dizisi ile artÄ±rÄ±rÄ±z (yani, 0,1, ... sayÄ±lar dizisi).
4. Daha sonra token konumunu bir token gÃ¶mme vektÃ¶rÃ¼ ile karÄ±ÅŸtÄ±rÄ±rÄ±z. Konumu (tam sayÄ±) bir vektÃ¶re dÃ¶nÃ¼ÅŸtÃ¼rmek iÃ§in farklÄ± yaklaÅŸÄ±mlar kullanabiliriz:

* Token gÃ¶mme ile benzer ÅŸekilde eÄŸitilebilir gÃ¶mme. Burada dikkate aldÄ±ÄŸÄ±mÄ±z yaklaÅŸÄ±m budur. Hem tokenlar hem de konumlarÄ± Ã¼zerinde gÃ¶mme katmanlarÄ± uygularÄ±z ve sonuÃ§ta aynÄ± boyutlarda gÃ¶mme vektÃ¶rleri elde ederiz, bunlarÄ± toplarÄ±z.
* Orijinal makalede Ã¶nerilen sabit konum kodlama fonksiyonu.

<img src="images/pos-embedding.png" width="50%"/>

> YazarÄ±n resmi

Konumsal gÃ¶mme ile elde ettiÄŸimiz sonuÃ§, hem orijinal tokenÄ± hem de dizideki konumunu gÃ¶mer.

### Ã‡oklu BaÅŸlÄ±k Kendine Dikkat

Sonraki adÄ±m, dizimizdeki bazÄ± desenleri yakalamaktÄ±r. Bunu yapmak iÃ§in, transformerlar **kendine dikkat** mekanizmasÄ±nÄ± kullanÄ±r; bu, esasen aynÄ± diziyi girdi ve Ã§Ä±ktÄ± olarak uygulanan dikkattir. Kendine dikkati uygulamak, cÃ¼mle iÃ§indeki **baÄŸlamÄ±** dikkate almamÄ±zÄ± saÄŸlar ve hangi kelimelerin birbiriyle iliÅŸkili olduÄŸunu gÃ¶rmemize olanak tanÄ±r. Ã–rneÄŸin, hangi kelimelerin *o* gibi referanslarla ifade edildiÄŸini gÃ¶rmemizi saÄŸlar ve ayrÄ±ca baÄŸlamÄ± dikkate alÄ±r:

![](../../../../../translated_images/CoreferenceResolution.861924d6d384a7d68d8d0039d06a71a151f18a796b8b1330239d3590bd4947eb.tr.png)

> Resim [Google Blogu](https://research.googleblog.com/2017/08/transformer-novel-neural-network.html) kaynaklÄ±

Transformerlarda, aÄŸÄ±n farklÄ± baÄŸÄ±mlÄ±lÄ±k tÃ¼rlerini yakalama gÃ¼cÃ¼nÃ¼ artÄ±rmak iÃ§in **Ã‡oklu BaÅŸlÄ±k Dikkati** kullanÄ±lÄ±r; Ã¶rneÄŸin, uzun vadeli ve kÄ±sa vadeli kelime iliÅŸkileri, karÅŸÄ±lÄ±klÄ± referanslar vs.

[TensorFlow Not Defteri](../../../../../lessons/5-NLP/18-Transformers/TransformersTF.ipynb) transformer katmanlarÄ±nÄ±n uygulanmasÄ± hakkÄ±nda daha fazla detay iÃ§erir.

### KodlayÄ±cÄ±-Ã‡Ã¶zÃ¼cÃ¼ Dikkati

Transformerlarda, dikkat iki yerde kullanÄ±lÄ±r:

* Kendine dikkat kullanarak giriÅŸ metnindeki desenleri yakalamak iÃ§in
* Dizi Ã§evirisi gerÃ§ekleÅŸtirmek iÃ§in - bu, kodlayÄ±cÄ± ve Ã§Ã¶zÃ¼cÃ¼ arasÄ±ndaki dikkat katmanÄ±dÄ±r.

KodlayÄ±cÄ±-Ã§Ã¶zÃ¼cÃ¼ dikkati, bu bÃ¶lÃ¼mÃ¼n baÅŸÄ±nda aÃ§Ä±klandÄ±ÄŸÄ± gibi RNN'lerde kullanÄ±lan dikkat mekanizmasÄ±na Ã§ok benzer. Bu animasyonlu diyagram, kodlayÄ±cÄ±-Ã§Ã¶zÃ¼cÃ¼ dikkatin rolÃ¼nÃ¼ aÃ§Ä±klar.

![Transformer modellerinde deÄŸerlendirmelerin nasÄ±l yapÄ±ldÄ±ÄŸÄ±nÄ± gÃ¶steren animasyonlu GIF.](../../../../../lessons/5-NLP/18-Transformers/images/transformer-animated-explanation.gif)

Her bir giriÅŸ konumu baÄŸÄ±msÄ±z olarak her bir Ã§Ä±kÄ±ÅŸ konumuna eÅŸlendiÄŸinden, transformerlar RNN'lerden daha iyi paralelleÅŸebilir; bu da Ã§ok daha bÃ¼yÃ¼k ve daha etkili dil modellerinin oluÅŸmasÄ±nÄ± saÄŸlar. Her bir dikkat baÅŸlÄ±ÄŸÄ±, kelimeler arasÄ±ndaki farklÄ± iliÅŸkileri Ã¶ÄŸrenmek iÃ§in kullanÄ±labilir ve bu da doÄŸal dil iÅŸleme gÃ¶revlerini geliÅŸtirir.

## BERT

**BERT** (Transformerlardan Ä°kili KodlayÄ±cÄ± Temsilleri), *BERT-base* iÃ§in 12 katman ve *BERT-large* iÃ§in 24 katman iÃ§eren Ã§ok bÃ¼yÃ¼k bir Ã§ok katmanlÄ± transformer aÄŸÄ±dÄ±r. Model, Ã¶ncelikle geniÅŸ bir metin veri kÃ¼mesi (WikiPedia + kitaplar) Ã¼zerinde denetimsiz eÄŸitim (bir cÃ¼mlede maskelenmiÅŸ kelimeleri tahmin etme) kullanÄ±larak Ã¶nceden eÄŸitilir. Ã–n eÄŸitim sÄ±rasÄ±nda model, daha sonra ince ayar ile diÄŸer veri kÃ¼meleri ile kullanÄ±labilecek Ã¶nemli dÃ¼zeyde dil anlayÄ±ÅŸÄ± kazanÄ±r. Bu sÃ¼reÃ§ **aktarÄ±m Ã¶ÄŸrenimi** olarak adlandÄ±rÄ±lÄ±r.

![http://jalammar.github.io/illustrated-bert/ adresinden bir resim](../../../../../translated_images/jalammarBERT-language-modeling-masked-lm.34f113ea5fec4362e39ee4381aab7cad06b5465a0b5f053a0f2aa05fbe14e746.tr.png)

> Resim [kaynak](http://jalammar.github.io/illustrated-bert/)

## âœï¸ AlÄ±ÅŸtÄ±rmalar: Transformerlar

AÅŸaÄŸÄ±daki not defterlerinde Ã¶ÄŸreniminize devam edin:

* [PyTorch'da Transformerlar](../../../../../lessons/5-NLP/18-Transformers/TransformersPyTorch.ipynb)
* [TensorFlow'da Transformerlar](../../../../../lessons/5-NLP/18-Transformers/TransformersTF.ipynb)

## SonuÃ§

Bu derste, NLP araÃ§ kutusundaki tÃ¼m temel araÃ§lar olan Transformerlar ve Dikkat MekanizmalarÄ± hakkÄ±nda bilgi edindiniz. BERT, DistilBERT, BigBird, OpenGPT3 ve daha fazlasÄ± dahil olmak Ã¼zere birÃ§ok Transformer mimarisi varyasyonu bulunmaktadÄ±r ve bunlar ince ayar yapÄ±labilir. [HuggingFace paketi](https://github.com/huggingface/) bu mimarilerin Ã§oÄŸunu hem PyTorch hem de TensorFlow ile eÄŸitmek iÃ§in bir depo saÄŸlar.

## ğŸš€ Zorluk

## [Ders sonrasÄ± quiz](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/218)

## GÃ¶zden GeÃ§irme & Kendi Kendine Ã‡alÄ±ÅŸma

* [Blog yazÄ±sÄ±](https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/), klasik [Dikkat, ihtiyacÄ±nÄ±z olan her ÅŸey](https://arxiv.org/abs/1706.03762) makalesini aÃ§Ä±klamaktadÄ±r.
* Transformerlar hakkÄ±nda detaylÄ± mimariyi aÃ§Ä±klayan [bir dizi blog yazÄ±sÄ±](https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452).

## [GÃ¶rev](assignment.md)

**AÃ§Ä±klama**:  
Bu belge, makine tabanlÄ± AI Ã§eviri hizmetleri kullanÄ±larak Ã§evrilmiÅŸtir. DoÄŸruluk iÃ§in Ã§aba gÃ¶stersek de, otomatik Ã§evirilerin hatalar veya yanlÄ±ÅŸ anlamalar iÃ§erebileceÄŸini lÃ¼tfen unutmayÄ±n. Orijinal belge, kendi dilinde yetkili kaynak olarak kabul edilmelidir. Kritik bilgiler iÃ§in profesyonel insan Ã§evirisi Ã¶nerilmektedir. Bu Ã§evirinin kullanÄ±mÄ±ndan kaynaklanan yanlÄ±ÅŸ anlamalar veya yanlÄ±ÅŸ yorumlamalardan dolayÄ± sorumluluk kabul etmiyoruz.