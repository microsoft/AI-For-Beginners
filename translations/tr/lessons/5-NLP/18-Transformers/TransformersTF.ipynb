{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dikkat Mekanizmaları ve Transformerlar\n",
    "\n",
    "Tekrarlayan ağların (recurrent networks) en büyük dezavantajlarından biri, bir dizideki tüm kelimelerin sonuca aynı etkiyi yapmasıdır. Bu durum, Adlandırılmış Varlık Tanıma (Named Entity Recognition) ve Makine Çevirisi gibi dizi-dizi görevlerinde standart LSTM kodlayıcı-çözücü modellerinin performansını düşürür. Gerçekte, giriş dizisindeki belirli kelimeler, sıralı çıktılar üzerinde diğerlerinden daha fazla etkiye sahiptir.\n",
    "\n",
    "Makine çevirisi gibi bir dizi-dizi modelini düşünelim. Bu model, iki tekrarlayan ağ tarafından uygulanır: biri (**kodlayıcı**) giriş dizisini gizli bir duruma sıkıştırır, diğeri (**çözücü**) bu gizli durumu çevrilmiş bir sonuca açar. Bu yaklaşımın sorunu, ağın son durumunun bir cümlenin başını hatırlamakta zorlanmasıdır, bu da uzun cümlelerde modelin kalitesinin düşmesine neden olur.\n",
    "\n",
    "**Dikkat Mekanizmaları**, her bir giriş vektörünün RNN'nin her bir çıktı tahmini üzerindeki bağlamsal etkisini ağırlıklandırmanın bir yolunu sunar. Bu, giriş RNN'sinin ara durumları ile çıkış RNN'si arasında kısayollar oluşturarak uygulanır. Bu şekilde, $y_t$ çıktı sembolünü üretirken, farklı ağırlık katsayıları $\\alpha_{t,i}$ ile tüm giriş gizli durumlarını $h_i$ dikkate alırız.\n",
    "\n",
    "![Eklendiği bir dikkat katmanına sahip kodlayıcı/çözücü modelini gösteren görsel](../../../../../translated_images/tr/encoder-decoder-attention.7a726296894fb567.webp)\n",
    "*[Bahdanau ve diğerleri, 2015](https://arxiv.org/pdf/1409.0473.pdf)'teki toplamsal dikkat mekanizmasına sahip kodlayıcı-çözücü modeli, [bu blog yazısından](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html) alıntılanmıştır.*\n",
    "\n",
    "Dikkat matrisi $\\{\\alpha_{i,j}\\}$, belirli giriş kelimelerinin çıktı dizisindeki bir kelimenin oluşturulmasında ne derece etkili olduğunu temsil eder. Aşağıda böyle bir matrisin örneği verilmiştir:\n",
    "\n",
    "![RNNsearch-50 tarafından bulunan bir hizalamayı gösteren görsel, Bahdanau - arviz.org'dan alınmıştır](../../../../../translated_images/tr/bahdanau-fig3.09ba2d37f202a6af.webp)\n",
    "\n",
    "*[Bahdanau ve diğerleri, 2015](https://arxiv.org/pdf/1409.0473.pdf)'ten alınan şekil (Şekil 3)*\n",
    "\n",
    "Dikkat mekanizmaları, Doğal Dil İşleme'deki mevcut veya mevcut duruma yakın en iyi performansın büyük bir kısmından sorumludur. Ancak dikkat eklemek, model parametrelerinin sayısını büyük ölçüde artırır ve bu da RNN'lerde ölçekleme sorunlarına yol açar. RNN'lerin ölçeklenmesindeki temel bir kısıtlama, modellerin tekrarlayan doğasının eğitimi toplu işleme ve paralelleştirme açısından zorlaştırmasıdır. Bir RNN'de, bir dizinin her bir öğesi sıralı bir şekilde işlenmelidir, bu da kolayca paralelleştirilemeyeceği anlamına gelir.\n",
    "\n",
    "Dikkat mekanizmalarının benimsenmesi ve bu kısıtlama, bugün BERT'ten OpenGPT3'e kadar bildiğimiz ve kullandığımız en iyi performansa sahip Transformer Modellerinin oluşturulmasına yol açmıştır.\n",
    "\n",
    "## Transformer Modelleri\n",
    "\n",
    "Her bir önceki tahminin bağlamını bir sonraki değerlendirme adımına iletmek yerine, **transformer modelleri**, verilen bir metin penceresi içinde bir girişin bağlamını yakalamak için **konumsal kodlamalar** ve **dikkat** kullanır. Aşağıdaki görsel, konumsal kodlamaların dikkatle birlikte bir pencere içinde bağlamı nasıl yakalayabildiğini göstermektedir.\n",
    "\n",
    "![Transformer modellerinde değerlendirmelerin nasıl yapıldığını gösteren animasyonlu GIF.](../../../../../lessons/5-NLP/18-Transformers/images/transformer-animated-explanation.gif)\n",
    "\n",
    "Her bir giriş pozisyonu bağımsız olarak her bir çıkış pozisyonuna eşlendiğinden, transformerlar RNN'lere göre daha iyi paralelleştirilebilir. Bu da çok daha büyük ve daha ifade gücü yüksek dil modellerini mümkün kılar. Her bir dikkat başlığı, kelimeler arasındaki farklı ilişkileri öğrenmek için kullanılabilir ve bu da Doğal Dil İşleme görevlerini iyileştirir.\n",
    "\n",
    "## Basit Bir Transformer Modeli Oluşturma\n",
    "\n",
    "Keras, yerleşik bir Transformer katmanı içermez, ancak kendi modelimizi oluşturabiliriz. Daha önce olduğu gibi, AG News veri kümesinin metin sınıflandırmasına odaklanacağız, ancak Transformer modellerinin daha zor NLP görevlerinde en iyi sonuçları verdiğini belirtmekte fayda var.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "\n",
    "ds_train, ds_test = tfds.load('ag_news_subset').values()\n",
    "\n",
    "def extract_text(x):\n",
    "    return x['title']+' '+x['description']\n",
    "\n",
    "def tupelize(x):\n",
    "    return (extract_text(x),x['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras'ta yeni katmanlar `Layer` sınıfını alt sınıf olarak almalı ve `call` metodunu uygulamalıdır. **Konumsal Gömme** katmanı ile başlayalım. [Resmi Keras dokümantasyonundan bazı kodları](https://keras.io/examples/nlp/text_classification_with_transformer/) kullanacağız. Tüm giriş dizilerini `maxlen` uzunluğuna kadar doldurduğumuzu varsayacağız.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(keras.layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = keras.layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "        self.maxlen = maxlen\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = self.maxlen\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x+positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bu katman, iki `Embedding` katmanından oluşur: biri tokenleri gömmek için (daha önce tartıştığımız şekilde), diğeri ise token pozisyonlarını gömmek için. Token pozisyonları, 0'dan `maxlen` değerine kadar doğal sayıların bir dizisi olarak `tf.range` kullanılarak oluşturulur ve ardından gömme katmanından geçirilir. Elde edilen iki gömme vektörü toplanarak, girişin `maxlen`$\\times$`embed_dim` şeklinde pozisyonel olarak gömülü bir temsili elde edilir.\n",
    "\n",
    "Şimdi, transformer bloğunu uygulayalım. Bu, daha önce tanımlanan gömme katmanının çıktısını alacaktır:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim, name='attn')\n",
    "        self.ffn = keras.Sequential(\n",
    "            [keras.layers.Dense(ff_dim, activation=\"relu\"), keras.layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = keras.layers.Dropout(rate)\n",
    "        self.dropout2 = keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Artık, tam bir transformer modelini tanımlamaya hazırız:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "text_vectorization (TextVect (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "token_and_position_embedding (None, 256, 32)           648192    \n",
      "_________________________________________________________________\n",
      "transformer_block (Transform (None, 256, 32)           10656     \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 20)                660       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 84        \n",
      "=================================================================\n",
      "Total params: 659,592\n",
      "Trainable params: 659,592\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 32  # Embedding size for each token\n",
    "num_heads = 2  # Number of attention heads\n",
    "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
    "maxlen = 256\n",
    "vocab_size = 20000\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.experimental.preprocessing.TextVectorization(max_tokens=vocab_size,output_sequence_length=maxlen, input_shape=(1,)),\n",
    "    TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim),\n",
    "    TransformerBlock(embed_dim, num_heads, ff_dim),\n",
    "    keras.layers.GlobalAveragePooling1D(),\n",
    "    keras.layers.Dropout(0.1),\n",
    "    keras.layers.Dense(20, activation=\"relu\"),\n",
    "    keras.layers.Dropout(0.1),\n",
    "    keras.layers.Dense(4, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokenizer\n",
      "938/938 [==============================] - 45s 39ms/step - loss: 0.4978 - acc: 0.8068 - val_loss: 0.2808 - val_acc: 0.9124\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9c2427a0d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Training tokenizer')\n",
    "model.layers[0].adapt(ds_train.map(extract_text))\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'], optimizer='adam')\n",
    "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT Transformer Modelleri\n",
    "\n",
    "**BERT** (Bidirectional Encoder Representations from Transformers), *BERT-base* için 12 katman ve *BERT-large* için 24 katman içeren çok büyük bir çok katmanlı transformer ağıdır. Model, büyük bir metin veri kümesi (Vikipedi + kitaplar) üzerinde denetimsiz eğitim (bir cümledeki maskelenmiş kelimeleri tahmin etme) kullanılarak önce önceden eğitilir. Ön eğitim sırasında model, önemli bir dil anlama seviyesini öğrenir ve bu bilgi, diğer veri kümeleriyle ince ayar yapılarak kullanılabilir. Bu sürece **transfer öğrenimi** denir.\n",
    "\n",
    "![http://jalammar.github.io/illustrated-bert/ adresinden alınan görsel](../../../../../translated_images/tr/jalammarBERT-language-modeling-masked-lm.34f113ea5fec4362.webp)\n",
    "\n",
    "BERT, DistilBERT, BigBird, OpenGPT3 ve daha fazlası gibi ince ayar yapılabilen birçok Transformer mimarisi varyasyonu bulunmaktadır.\n",
    "\n",
    "Şimdi, önceden eğitilmiş bir BERT modelini kullanarak geleneksel sıralama sınıflandırma problemimizi nasıl çözebileceğimize bakalım. [Resmi dokümantasyon](https://www.tensorflow.org/text/tutorials/classify_text_with_bert)'dan fikir ve bazı kodlar ödünç alacağız.\n",
    "\n",
    "Önceden eğitilmiş modelleri yüklemek için **Tensorflow hub** kullanacağız. İlk olarak, BERT'e özgü vektörleştiriciyi yükleyelim:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_41180/4216669875.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow_text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow_hub\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mhub\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhub\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mKerasLayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_text'"
     ]
    }
   ],
   "source": [
    "import tensorflow_text \n",
    "import tensorflow_hub as hub\n",
    "vectorizer = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_type_ids': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       " array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "       dtype=int32)>,\n",
       " 'input_word_ids': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       " array([[  101,  1045,  2293, 19081,   102,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]], dtype=int32)>,\n",
       " 'input_mask': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       " array([[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "       dtype=int32)>}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer(['I love transformers'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Orijinal ağın eğitildiği vektörleştiriciyi kullanmanız önemlidir. Ayrıca, BERT vektörleştirici üç bileşen döndürür:\n",
    "* `input_word_ids`, giriş cümlesi için bir dizi token numarasıdır\n",
    "* `input_mask`, dizinin hangi kısmının gerçek girdi içerdiğini ve hangisinin dolgu olduğunu gösterir. Bu, `Masking` katmanı tarafından üretilen maskeye benzer\n",
    "* `input_type_ids`, dil modelleme görevlerinde kullanılır ve bir dizide iki giriş cümlesi belirtmeye olanak tanır.\n",
    "\n",
    "Sonrasında, BERT özellik çıkarıcısını oluşturabiliriz:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = hub.KerasLayer('https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pooled_output -> (1, 128)\n",
      "encoder_outputs -> 4\n",
      "sequence_output -> (1, 128, 128)\n",
      "default -> (1, 128)\n"
     ]
    }
   ],
   "source": [
    "z = bert(vectorizer(['I love transformers']))\n",
    "for i,x in z.items():\n",
    "    print(f\"{i} -> { len(x) if isinstance(x, list) else x.shape }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yani, BERT katmanı bir dizi faydalı sonuç döndürür:\n",
    "* `pooled_output`, dizideki tüm tokenların ortalamasının alınmasıyla elde edilen bir sonuçtur. Bunu tüm ağın akıllı bir anlamsal gömüsü olarak düşünebilirsiniz. Önceki modelimizdeki `GlobalAveragePooling1D` katmanının çıktısına eşdeğerdir.\n",
    "* `sequence_output`, son transformer katmanının çıktısıdır (yukarıdaki modelimizdeki `TransformerBlock` çıktısına karşılık gelir).\n",
    "* `encoder_outputs`, tüm transformer katmanlarının çıktılarıdır. 4 katmanlı bir BERT modeli yüklediğimiz için (muhtemelen adından tahmin edebileceğiniz gibi, içinde `4_H` içeriyor), 4 tensöre sahiptir. Sonuncusu `sequence_output` ile aynıdır.\n",
    "\n",
    "Şimdi uçtan uca sınıflandırma modelini tanımlayacağız. Model girişini tanımladığımız ve ardından çıktısını hesaplamak için bir dizi ifade sağladığımız *fonksiyonel model tanımını* kullanacağız. Ayrıca, BERT model ağırlıklarını eğitilemez hale getirecek ve yalnızca son sınıflandırıcıyı eğiteceğiz:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer (KerasLayer)        {'input_type_ids': ( 0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer_1 (KerasLayer)      {'pooled_output': (N 4782465     keras_layer[0][0]                \n",
      "                                                                 keras_layer[0][1]                \n",
      "                                                                 keras_layer[0][2]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 128)          0           keras_layer_1[0][5]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 4)            516         dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 4,782,981\n",
      "Trainable params: 516\n",
      "Non-trainable params: 4,782,465\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inp = keras.Input(shape=(),dtype=tf.string)\n",
    "x = vectorizer(inp)\n",
    "x = bert(x)\n",
    "x = keras.layers.Dropout(0.1)(x['pooled_output'])\n",
    "out = keras.layers.Dense(4,activation='softmax')(x)\n",
    "model = keras.models.Model(inp,out)\n",
    "bert.trainable = False\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 528s 559ms/step - loss: 0.8056 - acc: 0.6983 - val_loss: 0.5953 - val_acc: 0.7888\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9bb1e36d00>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'], optimizer='adam')\n",
    "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT özellik çıkarıcısı hesaplama açısından oldukça ağır olduğu için eğitilebilir parametre sayısı az olmasına rağmen süreç oldukça yavaş ilerliyor. Görünüşe göre, ya yeterli eğitim yapılmadığından ya da model parametreleri yetersiz olduğundan makul bir doğruluk elde edemedik.\n",
    "\n",
    "Şimdi BERT ağırlıklarını serbest bırakmayı ve onu da eğitmeyi deneyelim. Bu, çok küçük bir öğrenme oranı gerektirir ve ayrıca **warmup** ile daha dikkatli bir eğitim stratejisi, **AdamW** optimizasyon algoritmasını kullanmayı gerektirir. Optimizatörü oluşturmak için `tf-models-official` paketini kullanacağız:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer (KerasLayer)        {'input_type_ids': ( 0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer_1 (KerasLayer)      {'pooled_output': (N 4782465     keras_layer[0][0]                \n",
      "                                                                 keras_layer[0][1]                \n",
      "                                                                 keras_layer[0][2]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 128)          0           keras_layer_1[0][5]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 4)            516         dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 4,782,981\n",
      "Trainable params: 4,782,980\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n",
      "938/938 [==============================] - 629s 664ms/step - loss: 0.6344 - acc: 0.7658 - val_loss: 0.4876 - val_acc: 0.8247\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9bb0bd0070>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from official.nlp import optimization \n",
    "bert.trainable=True\n",
    "model.summary()\n",
    "epochs = 3\n",
    "opt = optimization.create_optimizer(\n",
    "    init_lr=3e-5,\n",
    "    num_train_steps=epochs*len(ds_train),\n",
    "    num_warmup_steps=0.1*epochs*len(ds_train),\n",
    "    optimizer_type='adamw')\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'], optimizer=opt)\n",
    "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eğitim oldukça yavaş ilerliyor - ancak birkaç epoch (5-10) boyunca modeli eğiterek, daha önce kullandığımız yaklaşımlarla karşılaştırıldığında en iyi sonucu alıp alamayacağınızı denemek isteyebilirsiniz.\n",
    "\n",
    "## Huggingface Transformers Kütüphanesi\n",
    "\n",
    "Transformer modellerini kullanmanın başka bir çok yaygın (ve biraz daha basit) yolu [HuggingFace paketi](https://github.com/huggingface/)dir. Bu paket, farklı NLP görevleri için basit yapı taşları sağlar. Hem Tensorflow hem de başka bir çok popüler sinir ağı çerçevesi olan PyTorch için kullanılabilir.\n",
    "\n",
    "> **Not**: Eğer Transformers kütüphanesinin nasıl çalıştığını görmekle ilgilenmiyorsanız, bu not defterinin sonuna geçebilirsiniz. Çünkü yukarıda yaptıklarımızdan önemli ölçüde farklı bir şey görmeyeceksiniz. Daha büyük bir model ve farklı bir kütüphane kullanarak BERT modelini eğitme adımlarını tekrarlayacağız. Bu nedenle süreç oldukça uzun bir eğitim içeriyor, sadece kodu gözden geçirmek isteyebilirsiniz.\n",
    "\n",
    "Hadi, [Huggingface Transformers](http://huggingface.co) kullanarak problemimizin nasıl çözülebileceğine bir bakalım.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "İlk olarak, kullanacağımız modeli seçmemiz gerekiyor. Dahili modellerin yanı sıra, Huggingface'in [çevrimiçi model deposu](https://huggingface.co/models) bulunmaktadır. Bu depoda topluluk tarafından oluşturulmuş birçok önceden eğitilmiş model bulabilirsiniz. Bu modellerin tamamı, yalnızca model adını sağlayarak yüklenip kullanılabilir. Model için gerekli olan tüm ikili dosyalar otomatik olarak indirilecektir.\n",
    "\n",
    "Bazı durumlarda kendi modellerinizi yüklemeniz gerekebilir. Bu durumda, tokenizer parametreleri, model parametrelerini içeren `config.json` dosyası, ikili ağırlıklar gibi ilgili tüm dosyaları içeren dizini belirtebilirsiniz.\n",
    "\n",
    "Model adından, hem modeli hem de tokenizer'ı oluşturabiliriz. Hadi bir tokenizer ile başlayalım:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "# To load the model from Internet repository using model name. \n",
    "# Use this if you are running from your own copy of the notebooks\n",
    "bert_model = 'bert-base-uncased' \n",
    "\n",
    "# To load the model from the directory on disk. Use this for Microsoft Learn module, because we have\n",
    "# prepared all required files for you.\n",
    "#bert_model = './bert'\n",
    "\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(bert_model)\n",
    "\n",
    "MAX_SEQ_LEN = 128\n",
    "PAD_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "UNK_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tokenizer` nesnesi, metni doğrudan kodlamak için kullanılabilecek `encode` işlevini içerir:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 23435, 12314, 2003, 1037, 2307, 7705, 2005, 17953, 2361, 102]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('Tensorflow is a great framework for NLP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bir diziyi modele uygun bir şekilde kodlamak için tokenizer'ı da kullanabiliriz, yani `token_ids`, `input_mask` alanları vb. içerecek şekilde. Ayrıca `return_tensors='tf'` argümanını sağlayarak Tensorflow tensörleri istediğimizi belirtebiliriz:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[ 101, 7592, 1010, 2045,  102]], dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[0, 0, 0, 0, 0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[1, 1, 1, 1, 1]], dtype=int32)>}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(['Hello, there'],return_tensors='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bu durumda, önceden eğitilmiş `bert-base-uncased` adlı BERT modelini kullanacağız. *Uncased* modelin büyük/küçük harf duyarsız olduğunu belirtir.\n",
    "\n",
    "Modeli eğitirken, giriş olarak ayrıştırılmış bir dizi sağlamamız gerekir ve bu nedenle veri işleme hattını tasarlayacağız. `tokenizer.encode` bir Python fonksiyonu olduğundan, önceki birimde olduğu gibi `py_function` kullanarak çağırma yaklaşımını kullanacağız:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(x):\n",
    "    return tokenizer.encode(x.numpy().decode('utf-8'),return_tensors='tf',padding='max_length',max_length=MAX_SEQ_LEN,truncation=True)[0]\n",
    "\n",
    "def process_fn(x):\n",
    "    s = x['title']+' '+x['description']\n",
    "    e = tf.py_function(process,inp=[s],Tout=(tf.int32))\n",
    "    e.set_shape(MAX_SEQ_LEN)\n",
    "    return e,x['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Artık `BertForSequenceClassification` paketini kullanarak gerçek modeli yükleyebiliriz. Bu, modelimizin sınıflandırma için gerekli olan mimariye, son sınıflandırıcı dahil, zaten sahip olmasını sağlar. Son sınıflandırıcının ağırlıklarının başlatılmadığını ve modelin ön eğitim gerektireceğini belirten bir uyarı mesajı göreceksiniz - bu tamamen normaldir, çünkü tam olarak yapmayı planladığımız şey budur!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = transformers.TFBertForSequenceClassification.from_pretrained(bert_model,num_labels=4,output_attentions=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  109482240 \n",
      "_________________________________________________________________\n",
      "dropout_75 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  3076      \n",
      "=================================================================\n",
      "Total params: 109,485,316\n",
      "Trainable params: 109,485,316\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`summary()`'den görebileceğiniz gibi, model neredeyse 110 milyon parametre içeriyor! Muhtemelen, nispeten küçük bir veri kümesinde basit bir sınıflandırma görevi yapmak istiyorsak, BERT temel katmanını eğitmek istemeyiz:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  109482240 \n",
      "_________________________________________________________________\n",
      "dropout_75 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  3076      \n",
      "=================================================================\n",
      "Total params: 109,485,316\n",
      "Trainable params: 3,076\n",
      "Non-trainable params: 109,482,240\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.layers[0].trainable = False\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Şimdi eğitime başlamaya hazırız!\n",
    "\n",
    "> **Not**: Tam ölçekli bir BERT modelini eğitmek oldukça zaman alıcı olabilir! Bu yüzden sadece ilk 32 batch için eğitim yapacağız. Bu, model eğitiminin nasıl kurulduğunu göstermek içindir. Eğer tam ölçekli eğitimi denemek isterseniz, `steps_per_epoch` ve `validation_steps` parametrelerini kaldırın ve beklemeye hazırlanın!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 142s 4s/step - loss: 1.3896 - acc: 0.2500 - val_loss: 1.3863 - val_acc: 0.2480\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f1d40a4b6a0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile('adam','sparse_categorical_crossentropy',['acc'])\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "model.fit(ds_train.map(process_fn).batch(32),validation_data=ds_test.map(process_fn).batch(32),steps_per_epoch=32,validation_steps=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eğer iterasyon sayısını artırır, yeterince beklerseniz ve birkaç epoch boyunca eğitim yaparsanız, BERT sınıflandırmasının en iyi doğruluğu sağladığını görebilirsiniz! Bunun nedeni, BERT'in dilin yapısını zaten oldukça iyi anlaması ve bizim sadece son sınıflandırıcıyı ince ayar yapmamızın gerekmesidir. Ancak, BERT büyük bir model olduğu için tüm eğitim süreci uzun sürer ve ciddi bir hesaplama gücü gerektirir! (GPU ve tercihen birden fazla).\n",
    "\n",
    "> **Note:** Örneğimizde, en küçük önceden eğitilmiş BERT modellerinden birini kullanıyoruz. Daha büyük modeller, muhtemelen daha iyi sonuçlar verecektir.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Özet\n",
    "\n",
    "Bu bölümde, **transformer** tabanlı çok yeni model mimarilerini inceledik. Bunları metin sınıflandırma görevimizde uyguladık, ancak benzer şekilde BERT modelleri varlık çıkarımı, soru yanıtlama ve diğer NLP görevlerinde de kullanılabilir.\n",
    "\n",
    "Transformer modelleri, NLP alanında güncel en iyi performansı temsil eder ve çoğu durumda özel NLP çözümleri uygularken denemeye başlamanız gereken ilk çözüm olmalıdır. Ancak, bu modülde tartışılan tekrarlayan sinir ağlarının temel prensiplerini anlamak, gelişmiş sinir modelleri oluşturmak istiyorsanız son derece önemlidir.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Feragatname**:  \nBu belge, AI çeviri hizmeti [Co-op Translator](https://github.com/Azure/co-op-translator) kullanılarak çevrilmiştir. Doğruluk için çaba göstersek de, otomatik çevirilerin hata veya yanlışlıklar içerebileceğini lütfen unutmayın. Belgenin orijinal dili, yetkili kaynak olarak kabul edilmelidir. Kritik bilgiler için profesyonel insan çevirisi önerilir. Bu çevirinin kullanımından kaynaklanan yanlış anlamalar veya yanlış yorumlamalar için sorumluluk kabul etmiyoruz.\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0cb620c6d4b9f7a635928804c26cf22403d89d98d79684e4529119355ee6d5a5"
  },
  "kernelspec": {
   "display_name": "py38_tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "coopTranslator": {
   "original_hash": "ab59c532409774988ab875f2260e8e53",
   "translation_date": "2025-08-28T14:16:39+00:00",
   "source_file": "lessons/5-NLP/18-Transformers/TransformersTF.ipynb",
   "language_code": "tr"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}