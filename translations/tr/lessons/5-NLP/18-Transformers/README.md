<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "7e617f0b8de85a43957a853aba09bfeb",
  "translation_date": "2025-08-26T08:39:56+00:00",
  "source_file": "lessons/5-NLP/18-Transformers/README.md",
  "language_code": "tr"
}
-->
# Dikkat MekanizmalarÄ± ve Transformerlar

## [Ders Ã–ncesi Testi](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/118)

NLP alanÄ±ndaki en Ã¶nemli problemlerden biri, Google Translate gibi araÃ§larÄ±n temelini oluÅŸturan **makine Ã§evirisi**dir. Bu bÃ¶lÃ¼mde, makine Ã§evirisine ya da daha genel olarak herhangi bir *diziden-diziye* (sequence-to-sequence) gÃ¶reve (bu aynÄ± zamanda **cÃ¼mle dÃ¶nÃ¼ÅŸÃ¼mÃ¼** olarak da adlandÄ±rÄ±lÄ±r) odaklanacaÄŸÄ±z.

RNN'lerle, diziden-diziye iÅŸlemi iki tekrarlayan aÄŸ ile gerÃ§ekleÅŸtirilir. Bu aÄŸlardan biri olan **encoder**, bir giriÅŸ dizisini gizli bir duruma sÄ±kÄ±ÅŸtÄ±rÄ±rken, diÄŸer aÄŸ olan **decoder**, bu gizli durumu Ã§evrilmiÅŸ bir sonuca aÃ§ar. Ancak bu yaklaÅŸÄ±mda birkaÃ§ sorun vardÄ±r:

* Encoder aÄŸÄ±nÄ±n son durumu, bir cÃ¼mlenin baÅŸlangÄ±cÄ±nÄ± hatÄ±rlamakta zorlanÄ±r, bu da uzun cÃ¼mleler iÃ§in modelin kalitesinin dÃ¼ÅŸmesine neden olur.
* Bir dizideki tÃ¼m kelimeler sonuca aynÄ± etkiyi yapar. Ancak gerÃ§ekte, giriÅŸ dizisindeki belirli kelimeler genellikle ardÄ±ÅŸÄ±k Ã§Ä±ktÄ±lar Ã¼zerinde diÄŸerlerinden daha fazla etkiye sahiptir.

**Dikkat MekanizmalarÄ±**, her bir giriÅŸ vektÃ¶rÃ¼nÃ¼n RNN'nin her bir Ã§Ä±ktÄ± tahmini Ã¼zerindeki baÄŸlamsal etkisini aÄŸÄ±rlÄ±klandÄ±rmanÄ±n bir yolunu saÄŸlar. Bu, giriÅŸ RNN'sinin ara durumlarÄ± ile Ã§Ä±kÄ±ÅŸ RNN'si arasÄ±nda kÄ±sayollar oluÅŸturarak uygulanÄ±r. Bu ÅŸekilde, Ã§Ä±ktÄ± sembolÃ¼ y<sub>t</sub>'yi oluÅŸtururken, farklÄ± aÄŸÄ±rlÄ±k katsayÄ±larÄ± Î±<sub>t,i</sub> ile tÃ¼m giriÅŸ gizli durumlarÄ±nÄ± h<sub>i</sub> dikkate alÄ±rÄ±z.

![Bir ekleyici dikkat katmanÄ±na sahip encoder/decoder modelini gÃ¶steren gÃ¶rsel](../../../../../translated_images/encoder-decoder-attention.7a726296894fb567aa2898c94b17b3289087f6705c11907df8301df9e5eeb3de.tr.png)

> [Bahdanau ve diÄŸerleri, 2015](https://arxiv.org/pdf/1409.0473.pdf)'teki ekleyici dikkat mekanizmasÄ±na sahip encoder-decoder modeli, [bu blog yazÄ±sÄ±ndan](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html) alÄ±ntÄ±lanmÄ±ÅŸtÄ±r.

Dikkat matrisi {Î±<sub>i,j</sub>}, belirli giriÅŸ kelimelerinin Ã§Ä±ktÄ± dizisindeki bir kelimenin oluÅŸturulmasÄ±nda ne derece rol oynadÄ±ÄŸÄ±nÄ± temsil eder. AÅŸaÄŸÄ±da bÃ¶yle bir matrisin bir Ã¶rneÄŸi verilmiÅŸtir:

![Bahdanau - arviz.org'dan alÄ±nan, RNNsearch-50 tarafÄ±ndan bulunan bir Ã¶rnek hizalamayÄ± gÃ¶steren gÃ¶rsel](../../../../../translated_images/bahdanau-fig3.09ba2d37f202a6af11de6c82d2d197830ba5f4528d9ea430eb65fd3a75065973.tr.png)

> Åekil [Bahdanau ve diÄŸerleri, 2015](https://arxiv.org/pdf/1409.0473.pdf)'ten alÄ±nmÄ±ÅŸtÄ±r (Åekil 3)

Dikkat mekanizmalarÄ±, NLP'deki mevcut veya mevcut duruma yakÄ±n en iyi performansÄ±n bÃ¼yÃ¼k bir kÄ±smÄ±ndan sorumludur. Ancak dikkat eklemek, model parametrelerinin sayÄ±sÄ±nÄ± bÃ¼yÃ¼k Ã¶lÃ§Ã¼de artÄ±rÄ±r ve bu da RNN'lerle Ã¶lÃ§ekleme sorunlarÄ±na yol aÃ§ar. RNN'lerin Ã¶lÃ§eklenmesindeki temel kÄ±sÄ±tlama, modellerin tekrarlayÄ±cÄ± doÄŸasÄ±nÄ±n, eÄŸitimi toplu iÅŸleme ve paralelleÅŸtirme aÃ§Ä±sÄ±ndan zorlaÅŸtÄ±rmasÄ±dÄ±r. Bir RNN'de bir dizinin her bir Ã¶ÄŸesi sÄ±ralÄ± bir ÅŸekilde iÅŸlenmelidir, bu da kolayca paralelleÅŸtirilemeyeceÄŸi anlamÄ±na gelir.

![Dikkatli Encoder Decoder](../../../../../lessons/5-NLP/18-Transformers/images/EncDecAttention.gif)

> Åekil [Google'Ä±n Blogu](https://research.googleblog.com/2016/09/a-neural-network-for-machine.html)'ndan alÄ±nmÄ±ÅŸtÄ±r.

Dikkat mekanizmalarÄ±nÄ±n benimsenmesi ve bu kÄ±sÄ±tlama, bugÃ¼n bildiÄŸimiz ve kullandÄ±ÄŸÄ±mÄ±z BERT ve Open-GPT3 gibi en iyi performans gÃ¶steren Transformer Modellerinin oluÅŸturulmasÄ±na yol aÃ§mÄ±ÅŸtÄ±r.

## Transformer Modelleri

TransformerlarÄ±n arkasÄ±ndaki temel fikirlerden biri, RNN'lerin sÄ±ralÄ± doÄŸasÄ±ndan kaÃ§Ä±nmak ve eÄŸitim sÄ±rasÄ±nda paralelleÅŸtirilebilir bir model oluÅŸturmaktÄ±r. Bu, iki fikirle gerÃ§ekleÅŸtirilir:

* pozisyonel kodlama
* RNN'ler (veya CNN'ler) yerine desenleri yakalamak iÃ§in kendine dikkat mekanizmasÄ±nÄ±n kullanÄ±lmasÄ± (bu nedenle transformerlarÄ± tanÄ±tan makale *[Attention is all you need](https://arxiv.org/abs/1706.03762)* olarak adlandÄ±rÄ±lmÄ±ÅŸtÄ±r)

### Pozisyonel Kodlama/GÃ¶mme

Pozisyonel kodlama fikri ÅŸu ÅŸekildedir:
1. RNN'ler kullanÄ±ldÄ±ÄŸÄ±nda, tokenlarÄ±n gÃ¶receli pozisyonu adÄ±m sayÄ±sÄ±yla temsil edilir ve bu nedenle aÃ§Ä±kÃ§a temsil edilmesine gerek yoktur.
2. Ancak, dikkate geÃ§tiÄŸimizde, bir dizideki tokenlarÄ±n gÃ¶receli pozisyonlarÄ±nÄ± bilmemiz gerekir.
3. Pozisyonel kodlama elde etmek iÃ§in, token dizimizi dizideki token pozisyonlarÄ±nÄ±n bir dizisiyle (Ã¶rneÄŸin, 0,1, ...) geniÅŸletiriz.
4. Daha sonra token pozisyonunu bir token gÃ¶mme vektÃ¶rÃ¼yle karÄ±ÅŸtÄ±rÄ±rÄ±z. Pozisyonu (tam sayÄ±) bir vektÃ¶re dÃ¶nÃ¼ÅŸtÃ¼rmek iÃ§in farklÄ± yaklaÅŸÄ±mlar kullanabiliriz:

* Token gÃ¶mmeye benzer ÅŸekilde eÄŸitilebilir gÃ¶mme. Burada bu yaklaÅŸÄ±mÄ± ele alÄ±yoruz. Hem tokenlar hem de pozisyonlarÄ± Ã¼zerinde gÃ¶mme katmanlarÄ± uygularÄ±z, aynÄ± boyutlarda gÃ¶mme vektÃ¶rleri elde ederiz ve bunlarÄ± toplarÄ±z.
* Orijinal makalede Ã¶nerildiÄŸi gibi sabit pozisyon kodlama fonksiyonu.

<img src="images/pos-embedding.png" width="50%"/>

> GÃ¶rsel yazar tarafÄ±ndan oluÅŸturulmuÅŸtur.

Pozisyonel gÃ¶mme ile elde ettiÄŸimiz sonuÃ§, hem orijinal tokenÄ± hem de dizideki pozisyonunu gÃ¶mmektir.

### Ã‡oklu BaÅŸlÄ± Kendine Dikkat

Sonraki adÄ±mda, dizimizdeki bazÄ± desenleri yakalamamÄ±z gerekir. Bunu yapmak iÃ§in transformerlar, giriÅŸ ve Ã§Ä±kÄ±ÅŸ olarak aynÄ± diziye uygulanan dikkat mekanizmasÄ± olan **kendine dikkat** mekanizmasÄ±nÄ± kullanÄ±r. Kendine dikkat uygulamak, cÃ¼mle iÃ§indeki **baÄŸlamÄ±** dikkate almamÄ±zÄ± ve hangi kelimelerin birbiriyle iliÅŸkili olduÄŸunu gÃ¶rmemizi saÄŸlar. Ã–rneÄŸin, *it* gibi zamirlerin hangi kelimelere atÄ±fta bulunduÄŸunu gÃ¶rmemizi ve baÄŸlamÄ± dikkate almamÄ±zÄ± saÄŸlar:

![](../../../../../translated_images/CoreferenceResolution.861924d6d384a7d68d8d0039d06a71a151f18a796b8b1330239d3590bd4947eb.tr.png)

> GÃ¶rsel [Google Blogu](https://research.googleblog.com/2017/08/transformer-novel-neural-network.html)'ndan alÄ±nmÄ±ÅŸtÄ±r.

Transformerlarda, aÄŸÄ±n uzun vadeli ve kÄ±sa vadeli kelime iliÅŸkileri, eÅŸ referanslar gibi farklÄ± baÄŸÄ±mlÄ±lÄ±k tÃ¼rlerini yakalama gÃ¼cÃ¼nÃ¼ artÄ±rmak iÃ§in **Ã‡oklu BaÅŸlÄ± Dikkat** kullanÄ±lÄ±r.

[TensorFlow Notebook](../../../../../lessons/5-NLP/18-Transformers/TransformersTF.ipynb), transformer katmanlarÄ±nÄ±n uygulanmasÄ± hakkÄ±nda daha fazla ayrÄ±ntÄ± iÃ§erir.

### Encoder-Decoder Dikkati

Transformerlarda dikkat iki yerde kullanÄ±lÄ±r:

* GiriÅŸ metni iÃ§indeki desenleri yakalamak iÃ§in kendine dikkat
* Dizi Ã§evirisi yapmak iÃ§in - bu, encoder ve decoder arasÄ±ndaki dikkat katmanÄ±dÄ±r.

Encoder-decoder dikkati, bu bÃ¶lÃ¼mÃ¼n baÅŸÄ±nda RNN'lerde kullanÄ±lan dikkat mekanizmasÄ±na Ã§ok benzerdir. Bu animasyonlu diyagram, encoder-decoder dikkatinin rolÃ¼nÃ¼ aÃ§Ä±klar.

![Transformer modellerinde deÄŸerlendirmelerin nasÄ±l yapÄ±ldÄ±ÄŸÄ±nÄ± gÃ¶steren animasyonlu GIF.](../../../../../lessons/5-NLP/18-Transformers/images/transformer-animated-explanation.gif)

Her giriÅŸ pozisyonu baÄŸÄ±msÄ±z olarak her Ã§Ä±kÄ±ÅŸ pozisyonuna eÅŸlendiÄŸinden, transformerlar RNN'lere gÃ¶re daha iyi paralelleÅŸtirilebilir, bu da Ã§ok daha bÃ¼yÃ¼k ve daha ifade gÃ¼cÃ¼ yÃ¼ksek dil modellerini mÃ¼mkÃ¼n kÄ±lar. Her dikkat baÅŸlÄ±ÄŸÄ±, kelimeler arasÄ±ndaki farklÄ± iliÅŸkileri Ã¶ÄŸrenmek iÃ§in kullanÄ±labilir ve bu da DoÄŸal Dil Ä°ÅŸleme gÃ¶revlerini geliÅŸtirir.

## BERT

**BERT** (Bidirectional Encoder Representations from Transformers), *BERT-base* iÃ§in 12 katmanlÄ± ve *BERT-large* iÃ§in 24 katmanlÄ± Ã§ok bÃ¼yÃ¼k bir transformer aÄŸÄ±dÄ±r. Model, bÃ¼yÃ¼k bir metin veri kÃ¼mesi (WikiPedia + kitaplar) Ã¼zerinde denetimsiz eÄŸitim (bir cÃ¼mledeki maskelenmiÅŸ kelimeleri tahmin etme) kullanÄ±larak Ã¶nceden eÄŸitilir. Ã–n eÄŸitim sÄ±rasÄ±nda model, dil anlayÄ±ÅŸÄ±nÄ±n Ã¶nemli seviyelerini Ã¶ÄŸrenir ve bu, diÄŸer veri kÃ¼meleriyle ince ayar yapÄ±larak kullanÄ±labilir. Bu iÅŸleme **transfer Ã¶ÄŸrenme** denir.

![http://jalammar.github.io/illustrated-bert/ adresinden alÄ±nan gÃ¶rsel](../../../../../translated_images/jalammarBERT-language-modeling-masked-lm.34f113ea5fec4362e39ee4381aab7cad06b5465a0b5f053a0f2aa05fbe14e746.tr.png)

> GÃ¶rsel [kaynaÄŸÄ±](http://jalammar.github.io/illustrated-bert/)

## âœï¸ AlÄ±ÅŸtÄ±rmalar: Transformerlar

AÅŸaÄŸÄ±daki defterlerde Ã¶ÄŸrenmeye devam edin:

* [PyTorch ile Transformerlar](../../../../../lessons/5-NLP/18-Transformers/TransformersPyTorch.ipynb)
* [TensorFlow ile Transformerlar](../../../../../lessons/5-NLP/18-Transformers/TransformersTF.ipynb)

## SonuÃ§

Bu derste Transformerlar ve Dikkat MekanizmalarÄ± hakkÄ±nda bilgi edindiniz; bunlar NLP araÃ§ kutusundaki temel araÃ§lardÄ±r. BERT, DistilBERT, BigBird, OpenGPT3 ve daha fazlasÄ± gibi birÃ§ok Transformer mimarisi varyasyonu vardÄ±r ve bunlar ince ayar yapÄ±labilir. [HuggingFace paketi](https://github.com/huggingface/), bu mimarilerin birÃ§oÄŸunu hem PyTorch hem de TensorFlow ile eÄŸitmek iÃ§in bir depo saÄŸlar.

## ğŸš€ Meydan Okuma

## [Ders SonrasÄ± Testi](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/218)

## Ä°nceleme ve Kendi Kendine Ã‡alÄ±ÅŸma

* TransformerlarÄ± aÃ§Ä±klayan klasik [Attention is all you need](https://arxiv.org/abs/1706.03762) makalesini aÃ§Ä±klayan bir [blog yazÄ±sÄ±](https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/).
* TransformerlarÄ± detaylÄ± bir ÅŸekilde aÃ§Ä±klayan [bir dizi blog yazÄ±sÄ±](https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452).

## [Ã–dev](assignment.md)

**Feragatname**:  
Bu belge, [Co-op Translator](https://github.com/Azure/co-op-translator) adlÄ± bir yapay zeka Ã§eviri hizmeti kullanÄ±larak Ã§evrilmiÅŸtir. DoÄŸruluk iÃ§in Ã§aba gÃ¶stersek de, otomatik Ã§evirilerin hata veya yanlÄ±ÅŸlÄ±klar iÃ§erebileceÄŸini lÃ¼tfen unutmayÄ±n. Belgenin orijinal dili, yetkili kaynak olarak kabul edilmelidir. Kritik bilgiler iÃ§in profesyonel bir insan Ã§evirisi Ã¶nerilir. Bu Ã§evirinin kullanÄ±mÄ±ndan kaynaklanan yanlÄ±ÅŸ anlamalar veya yanlÄ±ÅŸ yorumlamalar iÃ§in sorumluluk kabul edilmez.