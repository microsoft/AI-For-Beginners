# Dikkat MekanizmalarÄ± ve Transformerlar

## [Ders Ã–ncesi Test](https://ff-quizzes.netlify.app/en/ai/quiz/35)

NLP alanÄ±ndaki en Ã¶nemli problemlerden biri **makine Ã§evirisi**dir; bu, Google Translate gibi araÃ§larÄ±n temelini oluÅŸturan Ã¶nemli bir gÃ¶revdir. Bu bÃ¶lÃ¼mde, makine Ã§evirisine, daha genel olarak ise herhangi bir *diziden-diziye* (sequence-to-sequence) gÃ¶reve odaklanacaÄŸÄ±z (bu aynÄ± zamanda **cÃ¼mle dÃ¶nÃ¼ÅŸÃ¼mÃ¼** olarak da adlandÄ±rÄ±lÄ±r).

RNN'lerle diziden-diziye yaklaÅŸÄ±mÄ±, iki tekrarlayan aÄŸ tarafÄ±ndan uygulanÄ±r. Bu aÄŸlardan biri olan **encoder**, bir giriÅŸ dizisini gizli bir duruma sÄ±kÄ±ÅŸtÄ±rÄ±rken, diÄŸer aÄŸ olan **decoder**, bu gizli durumu Ã§Ã¶zerek Ã§evrilmiÅŸ bir sonuÃ§ Ã¼retir. Ancak bu yaklaÅŸÄ±mda birkaÃ§ sorun vardÄ±r:

* Encoder aÄŸÄ±nÄ±n son durumu, bir cÃ¼mlenin baÅŸlangÄ±cÄ±nÄ± hatÄ±rlamakta zorlanÄ±r, bu da uzun cÃ¼mleler iÃ§in modelin kalitesinin dÃ¼ÅŸmesine neden olur.
* Bir dizideki tÃ¼m kelimeler sonuca aynÄ± etkiyi yapar. Ancak gerÃ§ekte, giriÅŸ dizisindeki belirli kelimeler genellikle ardÄ±ÅŸÄ±k Ã§Ä±ktÄ±lar Ã¼zerinde diÄŸerlerinden daha fazla etkiye sahiptir.

**Dikkat MekanizmalarÄ±**, RNN'nin her bir Ã§Ä±ktÄ± tahmininde her bir giriÅŸ vektÃ¶rÃ¼nÃ¼n baÄŸlamsal etkisini aÄŸÄ±rlÄ±klandÄ±rmanÄ±n bir yolunu saÄŸlar. Bu, giriÅŸ RNN'nin ara durumlarÄ± ile Ã§Ä±kÄ±ÅŸ RNN arasÄ±nda kÄ±sayollar oluÅŸturarak uygulanÄ±r. Bu ÅŸekilde, Ã§Ä±ktÄ± sembolÃ¼ y<sub>t</sub>'yi oluÅŸtururken, farklÄ± aÄŸÄ±rlÄ±k katsayÄ±larÄ± &alpha;<sub>t,i</sub> ile tÃ¼m giriÅŸ gizli durumlarÄ±nÄ± h<sub>i</sub> dikkate alÄ±rÄ±z.

![Eklenecek bir dikkat katmanÄ± ile encoder/decoder modelini gÃ¶steren gÃ¶rsel](../../../../../translated_images/tr/encoder-decoder-attention.7a726296894fb567.webp)

> [Bahdanau ve diÄŸerleri, 2015](https://arxiv.org/pdf/1409.0473.pdf) tarafÄ±ndan Ã¶nerilen eklemeli dikkat mekanizmasÄ± ile encoder-decoder modeli, [bu blog yazÄ±sÄ±ndan](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html) alÄ±ntÄ±lanmÄ±ÅŸtÄ±r.

Dikkat matrisi {&alpha;<sub>i,j</sub>} belirli giriÅŸ kelimelerinin Ã§Ä±ktÄ± dizisindeki bir kelimenin oluÅŸturulmasÄ±nda oynadÄ±ÄŸÄ± rolÃ¼ temsil eder. AÅŸaÄŸÄ±da bÃ¶yle bir matrisin Ã¶rneÄŸi verilmiÅŸtir:

![Bahdanau - arviz.org'dan alÄ±nan Ã¶rnek hizalamayÄ± gÃ¶steren gÃ¶rsel](../../../../../translated_images/tr/bahdanau-fig3.09ba2d37f202a6af.webp)

> [Bahdanau ve diÄŸerleri, 2015](https://arxiv.org/pdf/1409.0473.pdf) (Åekil 3) tarafÄ±ndan Ã¶nerilen ÅŸekil.

Dikkat mekanizmalarÄ±, NLP'deki mevcut veya yakÄ±n zamanda mevcut olan en iyi performansÄ±n bÃ¼yÃ¼k bir kÄ±smÄ±ndan sorumludur. Ancak dikkat eklemek, model parametrelerinin sayÄ±sÄ±nÄ± bÃ¼yÃ¼k Ã¶lÃ§Ã¼de artÄ±rÄ±r ve bu da RNN'lerde Ã¶lÃ§ekleme sorunlarÄ±na yol aÃ§ar. RNN'lerin Ã¶lÃ§eklenmesindeki temel bir kÄ±sÄ±tlama, modellerin tekrarlayan doÄŸasÄ±nÄ±n eÄŸitim sÄ±rasÄ±nda toplu iÅŸlem ve paralelleÅŸtirmeyi zorlaÅŸtÄ±rmasÄ±dÄ±r. Bir RNN'de bir dizinin her bir Ã¶ÄŸesi sÄ±ralÄ± olarak iÅŸlenmelidir, bu da kolayca paralelleÅŸtirilemeyeceÄŸi anlamÄ±na gelir.

![Dikkat ile Encoder Decoder](../../../../../lessons/5-NLP/18-Transformers/images/EncDecAttention.gif)

> [Google'Ä±n Blogundan](https://research.googleblog.com/2016/09/a-neural-network-for-machine.html) alÄ±nan ÅŸekil.

Dikkat mekanizmalarÄ±nÄ±n benimsenmesi ve bu kÄ±sÄ±tlama, bugÃ¼n bildiÄŸimiz ve kullandÄ±ÄŸÄ±mÄ±z BERT'ten Open-GPT3'e kadar olan en iyi performanslÄ± Transformer Modellerinin oluÅŸturulmasÄ±na yol aÃ§tÄ±.

## Transformer Modelleri

TransformerlarÄ±n arkasÄ±ndaki temel fikirlerden biri, RNN'lerin sÄ±ralÄ± doÄŸasÄ±ndan kaÃ§Ä±nmak ve eÄŸitim sÄ±rasÄ±nda paralelleÅŸtirilebilir bir model oluÅŸturmaktÄ±r. Bu, iki fikirle gerÃ§ekleÅŸtirilir:

* pozisyonel kodlama
* RNN'ler (veya CNN'ler) yerine desenleri yakalamak iÃ§in kendine dikkat mekanizmasÄ±nÄ±n kullanÄ±lmasÄ± (bu nedenle transformerlarÄ± tanÄ±tan makale *[Attention is all you need](https://arxiv.org/abs/1706.03762)* olarak adlandÄ±rÄ±lmÄ±ÅŸtÄ±r)

### Pozisyonel Kodlama/GÃ¶mme

Pozisyonel kodlama fikri ÅŸu ÅŸekildedir:
1. RNN'ler kullanÄ±ldÄ±ÄŸÄ±nda, tokenlarÄ±n gÃ¶receli pozisyonu adÄ±m sayÄ±sÄ± ile temsil edilir ve bu nedenle aÃ§Ä±kÃ§a temsil edilmesine gerek yoktur.
2. Ancak dikkat mekanizmasÄ±na geÃ§tiÄŸimizde, bir dizideki tokenlarÄ±n gÃ¶receli pozisyonlarÄ±nÄ± bilmemiz gerekir.
3. Pozisyonel kodlama elde etmek iÃ§in, token dizimizi dizideki token pozisyonlarÄ±nÄ±n bir dizisiyle (Ã¶rneÄŸin, 0,1, ...) geniÅŸletiriz.
4. Daha sonra token pozisyonunu bir token gÃ¶mme vektÃ¶rÃ¼yle karÄ±ÅŸtÄ±rÄ±rÄ±z. Pozisyonu (tam sayÄ±) bir vektÃ¶re dÃ¶nÃ¼ÅŸtÃ¼rmek iÃ§in farklÄ± yaklaÅŸÄ±mlar kullanabiliriz:

* Token gÃ¶mmeye benzer ÅŸekilde eÄŸitilebilir gÃ¶mme. Burada bu yaklaÅŸÄ±mÄ± ele alÄ±yoruz. Hem tokenlar hem de pozisyonlarÄ± Ã¼zerinde gÃ¶mme katmanlarÄ± uygularÄ±z, aynÄ± boyutlarda gÃ¶mme vektÃ¶rleri elde ederiz ve bunlarÄ± toplarÄ±z.
* Orijinal makalede Ã¶nerildiÄŸi gibi sabit pozisyon kodlama fonksiyonu.

<img src="../../../../../translated_images/tr/pos-embedding.e41ce9b6cf6078af.webp" width="50%"/>

> GÃ¶rsel yazar tarafÄ±ndan oluÅŸturulmuÅŸtur.

Pozisyonel gÃ¶mme ile elde ettiÄŸimiz sonuÃ§, hem orijinal tokenÄ± hem de dizideki pozisyonunu gÃ¶mer.

### Ã‡oklu BaÅŸlÄ± Kendine Dikkat

Sonraki adÄ±mda, dizimizdeki bazÄ± desenleri yakalamamÄ±z gerekir. Bunu yapmak iÃ§in transformerlar **kendine dikkat** mekanizmasÄ±nÄ± kullanÄ±r; bu, giriÅŸ ve Ã§Ä±kÄ±ÅŸ olarak aynÄ± diziye uygulanan dikkattir. Kendine dikkat uygulamak, cÃ¼mle iÃ§indeki **baÄŸlamÄ±** dikkate almamÄ±zÄ± ve hangi kelimelerin birbirleriyle iliÅŸkili olduÄŸunu gÃ¶rmemizi saÄŸlar. Ã–rneÄŸin, *it* gibi zamirlerin hangi kelimelere atÄ±fta bulunduÄŸunu gÃ¶rmemizi ve baÄŸlamÄ± dikkate almamÄ±zÄ± saÄŸlar:

![](../../../../../translated_images/tr/CoreferenceResolution.861924d6d384a7d6.webp)

> [Google Blogundan](https://research.googleblog.com/2017/08/transformer-novel-neural-network.html) alÄ±nan gÃ¶rsel.

Transformerlarda, aÄŸÄ±n uzun vadeli ve kÄ±sa vadeli kelime iliÅŸkileri, zamir referanslarÄ± gibi farklÄ± baÄŸÄ±mlÄ±lÄ±k tÃ¼rlerini yakalama gÃ¼cÃ¼nÃ¼ vermek iÃ§in **Ã‡oklu BaÅŸlÄ± Dikkat** kullanÄ±rÄ±z.

[TensorFlow Notebook](TransformersTF.ipynb) transformer katmanlarÄ±nÄ±n uygulanmasÄ± hakkÄ±nda daha fazla ayrÄ±ntÄ± iÃ§erir.

### Encoder-Decoder Dikkati

Transformerlarda dikkat iki yerde kullanÄ±lÄ±r:

* GiriÅŸ metni iÃ§indeki desenleri kendine dikkat ile yakalamak iÃ§in
* Dizi Ã§evirisi yapmak iÃ§in - bu, encoder ve decoder arasÄ±ndaki dikkat katmanÄ±dÄ±r.

Encoder-decoder dikkati, bu bÃ¶lÃ¼mÃ¼n baÅŸÄ±nda RNN'lerde kullanÄ±lan dikkat mekanizmasÄ±na Ã§ok benzerdir. Bu animasyonlu diyagram, encoder-decoder dikkatinin rolÃ¼nÃ¼ aÃ§Ä±klar.

![Transformer modellerinde deÄŸerlendirmelerin nasÄ±l yapÄ±ldÄ±ÄŸÄ±nÄ± gÃ¶steren animasyonlu GIF.](../../../../../lessons/5-NLP/18-Transformers/images/transformer-animated-explanation.gif)

Her giriÅŸ pozisyonu baÄŸÄ±msÄ±z olarak her Ã§Ä±kÄ±ÅŸ pozisyonuna eÅŸlendiÄŸinden, transformerlar RNN'lere gÃ¶re daha iyi paralelleÅŸtirilebilir, bu da Ã§ok daha bÃ¼yÃ¼k ve daha ifade gÃ¼cÃ¼ yÃ¼ksek dil modellerini mÃ¼mkÃ¼n kÄ±lar. Her dikkat baÅŸlÄ±ÄŸÄ±, kelimeler arasÄ±ndaki farklÄ± iliÅŸkileri Ã¶ÄŸrenmek iÃ§in kullanÄ±labilir ve bu da doÄŸal dil iÅŸleme gÃ¶revlerini iyileÅŸtirir.

## BERT

**BERT** (Bidirectional Encoder Representations from Transformers), *BERT-base* iÃ§in 12 katman ve *BERT-large* iÃ§in 24 katman iÃ§eren Ã§ok bÃ¼yÃ¼k bir Ã§ok katmanlÄ± transformer aÄŸÄ±dÄ±r. Model, bÃ¼yÃ¼k bir metin veri kÃ¼mesi (WikiPedia + kitaplar) Ã¼zerinde denetimsiz eÄŸitim (bir cÃ¼mledeki maskelenmiÅŸ kelimeleri tahmin etme) kullanÄ±larak Ã¶nceden eÄŸitilir. Ã–n eÄŸitim sÄ±rasÄ±nda model, dil anlayÄ±ÅŸÄ±nÄ±n Ã¶nemli seviyelerini emer ve bu daha sonra diÄŸer veri kÃ¼meleriyle ince ayar yapÄ±larak kullanÄ±labilir. Bu sÃ¼rece **transfer Ã¶ÄŸrenme** denir.

![http://jalammar.github.io/illustrated-bert/ adresinden alÄ±nan gÃ¶rsel](../../../../../translated_images/tr/jalammarBERT-language-modeling-masked-lm.34f113ea5fec4362.webp)

> GÃ¶rsel [kaynaÄŸÄ±](http://jalammar.github.io/illustrated-bert/)

## âœï¸ Egzersizler: Transformerlar

AÅŸaÄŸÄ±daki notebooklarda Ã¶ÄŸreniminize devam edin:

* [PyTorch ile Transformerlar](TransformersPyTorch.ipynb)
* [TensorFlow ile Transformerlar](TransformersTF.ipynb)

## SonuÃ§

Bu derste, NLP araÃ§ kutusundaki temel araÃ§lar olan Transformerlar ve Dikkat MekanizmalarÄ± hakkÄ±nda bilgi edindiniz. BERT, DistilBERT, BigBird, OpenGPT3 ve daha fazlasÄ± gibi birÃ§ok Transformer mimarisi varyasyonu vardÄ±r ve bunlar ince ayar yapÄ±labilir. [HuggingFace paketi](https://github.com/huggingface/) hem PyTorch hem de TensorFlow ile bu mimarilerin Ã§oÄŸunu eÄŸitmek iÃ§in bir depo saÄŸlar.

## ğŸš€ Meydan Okuma

## [Ders SonrasÄ± Test](https://ff-quizzes.netlify.app/en/ai/quiz/36)

## GÃ¶zden GeÃ§irme ve Kendi Kendine Ã‡alÄ±ÅŸma

* TransformerlarÄ± aÃ§Ä±klayan klasik [Attention is all you need](https://arxiv.org/abs/1706.03762) makalesini aÃ§Ä±klayan [blog yazÄ±sÄ±](https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/).
* TransformerlarÄ± detaylÄ± bir ÅŸekilde aÃ§Ä±klayan [bir dizi blog yazÄ±sÄ±](https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452).

## [Ã–dev](assignment.md)

---

