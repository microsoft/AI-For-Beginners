<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "31b46ba1f3aa78578134d4829f88be53",
  "translation_date": "2025-08-26T07:22:25+00:00",
  "source_file": "lessons/5-NLP/15-LanguageModeling/README.md",
  "language_code": "tr"
}
-->
# Dil Modellemesi

Word2Vec ve GloVe gibi anlamsal gÃ¶mme yÃ¶ntemleri aslÄ±nda **dil modellemesine** doÄŸru atÄ±lmÄ±ÅŸ ilk adÄ±mdÄ±r - dilin doÄŸasÄ±nÄ± bir ÅŸekilde *anlayan* (veya *temsil eden*) modeller oluÅŸturmak.

## [Ders Ã–ncesi Test](https://ff-quizzes.netlify.app/en/ai/quiz/29)

Dil modellemesinin temel fikri, modelleri etiketlenmemiÅŸ veri kÃ¼meleri Ã¼zerinde gÃ¶zetimsiz bir ÅŸekilde eÄŸitmektir. Bu Ã¶nemlidir Ã§Ã¼nkÃ¼ elimizde bÃ¼yÃ¼k miktarda etiketlenmemiÅŸ metin bulunurken, etiketlenmiÅŸ metin miktarÄ± her zaman etiketleme iÃ§in harcayabileceÄŸimiz Ã§abayla sÄ±nÄ±rlÄ± olacaktÄ±r. Ã‡oÄŸu zaman, metindeki **eksik kelimeleri tahmin edebilen** dil modelleri oluÅŸturabiliriz, Ã§Ã¼nkÃ¼ metindeki rastgele bir kelimeyi maskelemek ve bunu bir eÄŸitim Ã¶rneÄŸi olarak kullanmak oldukÃ§a kolaydÄ±r.

## GÃ¶mme YÃ¶ntemlerinin EÄŸitimi

Ã–nceki Ã¶rneklerimizde, Ã¶nceden eÄŸitilmiÅŸ anlamsal gÃ¶mme yÃ¶ntemlerini kullandÄ±k, ancak bu gÃ¶mme yÃ¶ntemlerinin nasÄ±l eÄŸitilebileceÄŸini gÃ¶rmek ilginÃ§tir. KullanÄ±labilecek birkaÃ§ farklÄ± fikir vardÄ±r:

* **N-Gram** dil modellemesi: Bir belirteci, Ã¶nceki N belirtece bakarak tahmin ettiÄŸimiz yÃ¶ntem (N-gram).
* **SÃ¼rekli Kelime Ã‡antasÄ±** (CBoW): Bir belirteÃ§ dizisindeki $W_{-N}$, ..., $W_N$ belirteÃ§leri arasÄ±nda ortadaki belirteÃ§ $W_0$'Ä± tahmin ettiÄŸimiz yÃ¶ntem.
* **Skip-gram**: Ortadaki belirteÃ§ $W_0$'dan, komÅŸu belirteÃ§ler kÃ¼mesini {$W_{-N},\dots, W_{-1}, W_1,\dots, W_N$} tahmin ettiÄŸimiz yÃ¶ntem.

![Kelimeleri vektÃ¶rlere dÃ¶nÃ¼ÅŸtÃ¼rme algoritmalarÄ±na dair bir makaleden gÃ¶rsel](../../../../../translated_images/example-algorithms-for-converting-words-to-vectors.fbe9207a726922f6f0f5de66427e8a6eda63809356114e28fb1fa5f4a83ebda7.tr.png)

> GÃ¶rsel [bu makaleden](https://arxiv.org/pdf/1301.3781.pdf) alÄ±nmÄ±ÅŸtÄ±r.

## âœï¸ Ã–rnek Defterler: CBoW Modeli EÄŸitimi

AÅŸaÄŸÄ±daki defterlerde Ã¶ÄŸreniminize devam edebilirsiniz:

* [TensorFlow ile CBoW Word2Vec EÄŸitimi](../../../../../lessons/5-NLP/15-LanguageModeling/CBoW-TF.ipynb)
* [PyTorch ile CBoW Word2Vec EÄŸitimi](../../../../../lessons/5-NLP/15-LanguageModeling/CBoW-PyTorch.ipynb)

## SonuÃ§

Ã–nceki derste, kelime gÃ¶mmelerinin adeta bir sihir gibi Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ± gÃ¶rmÃ¼ÅŸtÃ¼k! Åimdi ise kelime gÃ¶mmelerini eÄŸitmenin Ã§ok karmaÅŸÄ±k bir iÅŸ olmadÄ±ÄŸÄ±nÄ± biliyoruz ve gerekirse alanÄ±na Ã¶zgÃ¼ metinler iÃ§in kendi kelime gÃ¶mmelerimizi eÄŸitebileceÄŸimizi Ã¶ÄŸrenmiÅŸ olduk.

## [Ders SonrasÄ± Test](https://ff-quizzes.netlify.app/en/ai/quiz/30)

## GÃ¶zden GeÃ§irme ve Kendi Kendine Ã‡alÄ±ÅŸma

* [PyTorch'un Resmi Dil Modelleme EÄŸitimi](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html).
* [TensorFlow'un Word2Vec Modeli EÄŸitimi iÃ§in Resmi EÄŸitimi](https://www.TensorFlow.org/tutorials/text/word2vec).
* **gensim** Ã§erÃ§evesini kullanarak en yaygÄ±n kullanÄ±lan gÃ¶mme yÃ¶ntemlerini birkaÃ§ satÄ±r kodla eÄŸitme yÃ¶ntemi [bu belgede](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html) aÃ§Ä±klanmÄ±ÅŸtÄ±r.

## ğŸš€ [GÃ¶rev: Skip-Gram Modeli EÄŸitin](lab/README.md)

Laboratuvarda, bu dersteki kodu deÄŸiÅŸtirerek CBoW yerine skip-gram modeli eÄŸitmenizi istiyoruz. [DetaylarÄ± okuyun](lab/README.md).

**Feragatname**:  
Bu belge, AI Ã§eviri hizmeti [Co-op Translator](https://github.com/Azure/co-op-translator) kullanÄ±larak Ã§evrilmiÅŸtir. DoÄŸruluk iÃ§in Ã§aba gÃ¶stersek de, otomatik Ã§evirilerin hata veya yanlÄ±ÅŸlÄ±klar iÃ§erebileceÄŸini lÃ¼tfen unutmayÄ±n. Belgenin orijinal dili, yetkili kaynak olarak kabul edilmelidir. Kritik bilgiler iÃ§in profesyonel insan Ã§evirisi Ã¶nerilir. Bu Ã§evirinin kullanÄ±mÄ±ndan kaynaklanan yanlÄ±ÅŸ anlamalar veya yanlÄ±ÅŸ yorumlamalar iÃ§in sorumluluk kabul etmiyoruz.