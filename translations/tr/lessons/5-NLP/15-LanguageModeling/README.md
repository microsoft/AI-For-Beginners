# Dil Modellemesi

Word2Vec ve GloVe gibi anlamsal gÃ¶mmeler aslÄ±nda **dil modellemesi**ne doÄŸru atÄ±lmÄ±ÅŸ ilk adÄ±mlardÄ±r - dilin doÄŸasÄ±nÄ± bir ÅŸekilde *anlayan* (veya *temsil eden*) modeller oluÅŸturmak.

## [Ders Ã–ncesi Testi](https://ff-quizzes.netlify.app/en/ai/quiz/29)

Dil modellemesinin temel fikri, modelleri etiketlenmemiÅŸ veri kÃ¼meleri Ã¼zerinde gÃ¶zetimsiz bir ÅŸekilde eÄŸitmektir. Bu Ã¶nemlidir Ã§Ã¼nkÃ¼ elimizde Ã§ok bÃ¼yÃ¼k miktarda etiketlenmemiÅŸ metin bulunurken, etiketlenmiÅŸ metin miktarÄ± her zaman etiketleme iÃ§in harcayabileceÄŸimiz Ã§aba ile sÄ±nÄ±rlÄ± olacaktÄ±r. Ã‡oÄŸu zaman, metindeki **eksik kelimeleri tahmin edebilen** dil modelleri oluÅŸturabiliriz, Ã§Ã¼nkÃ¼ metindeki rastgele bir kelimeyi gizlemek ve bunu bir eÄŸitim Ã¶rneÄŸi olarak kullanmak oldukÃ§a kolaydÄ±r.

## GÃ¶mmeleri EÄŸitmek

Ã–nceki Ã¶rneklerimizde Ã¶nceden eÄŸitilmiÅŸ anlamsal gÃ¶mmeler kullandÄ±k, ancak bu gÃ¶mmelerin nasÄ±l eÄŸitilebileceÄŸini gÃ¶rmek ilginÃ§tir. KullanÄ±labilecek birkaÃ§ fikir vardÄ±r:

* **N-Gram** dil modellemesi, burada bir tokeni Ã¶nceki N tokena bakarak tahmin ederiz (N-gram).
* **Continuous Bag-of-Words** (CBoW), burada bir token dizisindeki $W_{-N}$, ..., $W_N$ arasÄ±nda ortadaki token $W_0$'Ä± tahmin ederiz.
* **Skip-gram**, burada ortadaki token $W_0$'dan komÅŸu tokenlerin bir setini {$W_{-N},\dots, W_{-1}, W_1,\dots, W_N$} tahmin ederiz.

![Kelimeyi vektÃ¶re dÃ¶nÃ¼ÅŸtÃ¼rme algoritmalarÄ±na dair makaleden gÃ¶rsel](../../../../../translated_images/tr/example-algorithms-for-converting-words-to-vectors.fbe9207a726922f6.webp)

> GÃ¶rsel [bu makaleden](https://arxiv.org/pdf/1301.3781.pdf)

## âœï¸ Ã–rnek Not Defterleri: CBoW Modeli EÄŸitimi

AÅŸaÄŸÄ±daki not defterlerinde Ã¶ÄŸreniminize devam edin:

* [TensorFlow ile CBoW Word2Vec EÄŸitimi](CBoW-TF.ipynb)
* [PyTorch ile CBoW Word2Vec EÄŸitimi](CBoW-PyTorch.ipynb)

## SonuÃ§

Ã–nceki derste kelime gÃ¶mmelerinin adeta sihir gibi Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ± gÃ¶rdÃ¼k! Åimdi kelime gÃ¶mmeleri eÄŸitmenin Ã§ok karmaÅŸÄ±k bir iÅŸ olmadÄ±ÄŸÄ±nÄ± biliyoruz ve gerekirse alan spesifik metinler iÃ§in kendi kelime gÃ¶mmelerimizi eÄŸitebiliriz.

## [Ders SonrasÄ± Testi](https://ff-quizzes.netlify.app/en/ai/quiz/30)

## GÃ¶zden GeÃ§irme ve Kendi Kendine Ã‡alÄ±ÅŸma

* [PyTorch'un Resmi Dil Modelleme EÄŸitimi](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html).
* [TensorFlow'un Resmi Word2Vec Modeli EÄŸitimi](https://www.TensorFlow.org/tutorials/text/word2vec).
* **gensim** Ã§erÃ§evesini kullanarak en yaygÄ±n kullanÄ±lan gÃ¶mmeleri birkaÃ§ satÄ±r kodla eÄŸitmek [bu belgede](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html) aÃ§Ä±klanmÄ±ÅŸtÄ±r.

## ğŸš€ [GÃ¶rev: Skip-Gram Modeli EÄŸitimi](lab/README.md)

Laboratuvarda, bu dersteki kodu deÄŸiÅŸtirerek CBoW yerine skip-gram modeli eÄŸitmenizi istiyoruz. [DetaylarÄ± okuyun](lab/README.md)

---

