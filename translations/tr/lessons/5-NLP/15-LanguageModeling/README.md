# Dil Modelleme

Anlam gÃ¶mme yÃ¶ntemleri, Word2Vec ve GloVe gibi, aslÄ±nda **dil modelleme** iÃ§in bir ilk adÄ±mdÄ±r - dilin doÄŸasÄ±nÄ± bir ÅŸekilde *anlayan* (veya *temsil eden*) modeller oluÅŸturmak.

## [Ders Ã¶ncesi quiz](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/115)

Dil modellemenin arkasÄ±ndaki temel fikir, etiketlenmemiÅŸ veri setleri Ã¼zerinde denetimsiz bir ÅŸekilde eÄŸitim yapmaktÄ±r. Bu Ã¶nemlidir Ã§Ã¼nkÃ¼ elimizde bÃ¼yÃ¼k miktarda etiketlenmemiÅŸ metin bulunmaktadÄ±r, oysa etiketlenmiÅŸ metin miktarÄ± her zaman etiketleme iÃ§in harcayabileceÄŸimiz Ã§aba ile sÄ±nÄ±rlÄ± olacaktÄ±r. Genellikle, metindeki **eksik kelimeleri tahmin edebilen** dil modelleri oluÅŸturabiliriz, Ã§Ã¼nkÃ¼ metinde rastgele bir kelimeyi maskeleyip bunu bir eÄŸitim Ã¶rneÄŸi olarak kullanmak kolaydÄ±r.

## GÃ¶mme EÄŸitimleri

Ã–nceki Ã¶rneklerimizde, Ã¶nceden eÄŸitilmiÅŸ anlam gÃ¶mmelerini kullandÄ±k, ancak bu gÃ¶mmelerin nasÄ±l eÄŸitilebileceÄŸini gÃ¶rmek ilginÃ§tir. KullanÄ±labilecek birkaÃ§ olasÄ± fikir vardÄ±r:

* **N-Gram** dil modelleme, N Ã¶nceki token'a bakarak bir token'Ä± tahmin ettiÄŸimiz durumdur (N-gram)
* **SÃ¼rekli Kelime TorbasÄ±** (CBoW), bir token dizisi $W_{-N}$, ..., $W_N$ iÃ§indeki ortadaki token $W_0$'Ä± tahmin ettiÄŸimiz durumdur.
* **Skip-gram**, ortadaki token $W_0$'dan komÅŸu token'larÄ±n {$W_{-N},\dots, W_{-1}, W_1,\dots, W_N$} bir kÃ¼mesini tahmin ettiÄŸimiz durumdur.

![kelimeleri vektÃ¶rlere dÃ¶nÃ¼ÅŸtÃ¼rme Ã¼zerine makaleden bir resim](../../../../../translated_images/example-algorithms-for-converting-words-to-vectors.fbe9207a726922f6f0f5de66427e8a6eda63809356114e28fb1fa5f4a83ebda7.tr.png)

> [bu makaleden](https://arxiv.org/pdf/1301.3781.pdf) alÄ±nmÄ±ÅŸtÄ±r.

## âœï¸ Ã–rnek Not Defterleri: CBoW modelini eÄŸitme

AÅŸaÄŸÄ±daki not defterlerinde Ã¶ÄŸrenmeye devam edin:

* [TensorFlow ile CBoW Word2Vec EÄŸitimi](../../../../../lessons/5-NLP/15-LanguageModeling/CBoW-TF.ipynb)
* [PyTorch ile CBoW Word2Vec EÄŸitimi](../../../../../lessons/5-NLP/15-LanguageModeling/CBoW-PyTorch.ipynb)

## SonuÃ§

Ã–nceki derste, kelime gÃ¶mmelerinin sihir gibi Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ± gÃ¶rdÃ¼k! ArtÄ±k kelime gÃ¶mmelerini eÄŸitmenin Ã§ok karmaÅŸÄ±k bir gÃ¶rev olmadÄ±ÄŸÄ±nÄ± biliyoruz ve gerekirse alan spesifik metinler iÃ§in kendi kelime gÃ¶mmelerimizi eÄŸitebilmeliyiz.

## [Ders sonrasÄ± quiz](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/215)

## GÃ¶zden GeÃ§irme & Kendi Kendine Ã‡alÄ±ÅŸma

* [Dil Modelleme Ã¼zerine resmi PyTorch eÄŸitimi](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html).
* [Word2Vec modelini eÄŸitme Ã¼zerine resmi TensorFlow eÄŸitimi](https://www.TensorFlow.org/tutorials/text/word2vec).
* En yaygÄ±n kullanÄ±lan gÃ¶mmeleri birkaÃ§ satÄ±r kodla eÄŸitmek iÃ§in **gensim** Ã§erÃ§evesinin kullanÄ±mÄ± [bu belgede](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html) aÃ§Ä±klanmÄ±ÅŸtÄ±r.

## ğŸš€ [GÃ¶rev: Skip-Gram Modelini EÄŸit](lab/README.md)

Laboratuvar Ã§alÄ±ÅŸmasÄ±nda, bu dersten aldÄ±ÄŸÄ±nÄ±z kodu CBoW yerine skip-gram modelini eÄŸitmek iÃ§in deÄŸiÅŸtirmenizi istiyoruz. [DetaylarÄ± okuyun](lab/README.md)

**AÃ§Ä±klama**:  
Bu belge, makine tabanlÄ± yapay zeka Ã§eviri hizmetleri kullanÄ±larak Ã§evrilmiÅŸtir. DoÄŸruluÄŸu saÄŸlamak iÃ§in Ã§aba gÃ¶stersek de, otomatik Ã§evirilerin hatalar veya yanlÄ±ÅŸlÄ±klar iÃ§erebileceÄŸini lÃ¼tfen dikkate alÄ±nÄ±z. Orijinal belge, kendi dilinde yetkili kaynak olarak kabul edilmelidir. Kritik bilgiler iÃ§in profesyonel insan Ã§evirisi Ã¶nerilmektedir. Bu Ã§evirinin kullanÄ±mÄ±ndan kaynaklanan yanlÄ±ÅŸ anlamalar veya yanlÄ±ÅŸ yorumlamalardan sorumlu deÄŸiliz.