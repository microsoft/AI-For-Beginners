# GÃ¶mÃ¼lÃ¼ Temsiller

## [Ders Ã–ncesi Test](https://ff-quizzes.netlify.app/en/ai/quiz/27)

BoW veya TF/IDF tabanlÄ± sÄ±nÄ±flandÄ±rÄ±cÄ±lar eÄŸitirken, `vocab_size` uzunluÄŸunda yÃ¼ksek boyutlu kelime torbasÄ± vektÃ¶rleri Ã¼zerinde Ã§alÄ±ÅŸtÄ±k ve dÃ¼ÅŸÃ¼k boyutlu konumsal temsil vektÃ¶rlerinden seyrek tekil temsil vektÃ¶rlerine aÃ§Ä±kÃ§a dÃ¶nÃ¼ÅŸtÃ¼rme yapÄ±yorduk. Ancak, bu tekil temsil bellek aÃ§Ä±sÄ±ndan verimli deÄŸildir. AyrÄ±ca, her kelime birbirinden baÄŸÄ±msÄ±z olarak ele alÄ±nÄ±r, yani tekil kodlanmÄ±ÅŸ vektÃ¶rler kelimeler arasÄ±ndaki herhangi bir anlamsal benzerliÄŸi ifade etmez.

**GÃ¶mÃ¼lÃ¼ temsil** fikri, kelimeleri bir ÅŸekilde kelimenin anlamsal anlamÄ±nÄ± yansÄ±tan daha dÃ¼ÅŸÃ¼k boyutlu yoÄŸun vektÃ¶rlerle temsil etmektir. Daha sonra anlamlÄ± kelime gÃ¶mÃ¼lÃ¼ temsillerinin nasÄ±l oluÅŸturulacaÄŸÄ±nÄ± tartÄ±ÅŸacaÄŸÄ±z, ancak ÅŸimdilik gÃ¶mÃ¼lÃ¼ temsilleri bir kelime vektÃ¶rÃ¼nÃ¼n boyutunu dÃ¼ÅŸÃ¼rmenin bir yolu olarak dÃ¼ÅŸÃ¼nelim.

Bu nedenle, gÃ¶mÃ¼lÃ¼ temsil katmanÄ± bir kelimeyi giriÅŸ olarak alÄ±r ve belirli bir `embedding_size` boyutunda bir Ã§Ä±ktÄ± vektÃ¶rÃ¼ Ã¼retir. Bir anlamda, bir `Linear` katmana Ã§ok benzer, ancak tekil kodlanmÄ±ÅŸ bir vektÃ¶r almak yerine, bir kelime numarasÄ±nÄ± giriÅŸ olarak alabilir, bÃ¶ylece bÃ¼yÃ¼k tekil kodlanmÄ±ÅŸ vektÃ¶rler oluÅŸturmaktan kaÃ§Ä±nabiliriz.

SÄ±nÄ±flandÄ±rÄ±cÄ± aÄŸÄ±mÄ±zda ilk katman olarak bir gÃ¶mÃ¼lÃ¼ temsil katmanÄ± kullanarak, kelime torbasÄ±ndan **gÃ¶mÃ¼lÃ¼ torba** modeline geÃ§ebiliriz. Bu modelde, Ã¶nce metnimizdeki her kelimeyi ilgili gÃ¶mÃ¼lÃ¼ temsile dÃ¶nÃ¼ÅŸtÃ¼rÃ¼rÃ¼z ve ardÄ±ndan bu gÃ¶mÃ¼lÃ¼ temsillerin tÃ¼mÃ¼ Ã¼zerinde `sum`, `average` veya `max` gibi bir toplama fonksiyonu hesaplarÄ±z.

![BeÅŸ kelimelik bir dizinin gÃ¶mÃ¼lÃ¼ temsil sÄ±nÄ±flandÄ±rÄ±cÄ±sÄ±nÄ± gÃ¶steren gÃ¶rsel.](../../../../../translated_images/tr/embedding-classifier-example.b77f021a7ee67eee.webp)

> GÃ¶rsel yazar tarafÄ±ndan oluÅŸturulmuÅŸtur

## âœï¸ AlÄ±ÅŸtÄ±rmalar: GÃ¶mÃ¼lÃ¼ Temsiller

AÅŸaÄŸÄ±daki defterlerde Ã¶ÄŸrenmeye devam edin:
* [PyTorch ile GÃ¶mÃ¼lÃ¼ Temsiller](EmbeddingsPyTorch.ipynb)
* [TensorFlow ile GÃ¶mÃ¼lÃ¼ Temsiller](EmbeddingsTF.ipynb)

## Anlamsal GÃ¶mÃ¼lÃ¼ Temsiller: Word2Vec

GÃ¶mÃ¼lÃ¼ temsil katmanÄ± kelimeleri vektÃ¶r temsiline eÅŸlemeyi Ã¶ÄŸrenmiÅŸ olsa da, bu temsil mutlaka Ã§ok fazla anlamsal anlam taÅŸÄ±mÄ±yor olabilir. Kelimelerin vektÃ¶r temsillerini Ã¶yle bir ÅŸekilde Ã¶ÄŸrenmek gÃ¼zel olurdu ki, benzer kelimeler veya eÅŸ anlamlÄ±lar, bazÄ± vektÃ¶r mesafelerine (Ã¶rneÄŸin Ã–klid mesafesi) gÃ¶re birbirine yakÄ±n olan vektÃ¶rlere karÅŸÄ±lÄ±k gelsin.

Bunu yapmak iÃ§in, gÃ¶mÃ¼lÃ¼ temsil modelimizi bÃ¼yÃ¼k bir metin koleksiyonu Ã¼zerinde belirli bir ÅŸekilde Ã¶nceden eÄŸitmemiz gerekir. Anlamsal gÃ¶mÃ¼lÃ¼ temsilleri eÄŸitmenin bir yolu [Word2Vec](https://en.wikipedia.org/wiki/Word2vec) olarak adlandÄ±rÄ±lÄ±r. Bu yÃ¶ntem, kelimelerin daÄŸÄ±tÄ±lmÄ±ÅŸ temsillerini Ã¼retmek iÃ§in kullanÄ±lan iki ana mimariye dayanÄ±r:

 - **SÃ¼rekli kelime torbasÄ±** (CBoW) â€” Bu mimaride, model Ã§evresindeki baÄŸlamdan bir kelimeyi tahmin etmek iÃ§in eÄŸitilir. $(W_{-2},W_{-1},W_0,W_1,W_2)$ ngrami verildiÄŸinde, modelin amacÄ± $(W_{-2},W_{-1},W_1,W_2)$'den $W_0$'Ä± tahmin etmektir.
 - **SÃ¼rekli atlama-gram** CBoW'un tersidir. Model, baÄŸlam kelimelerinin Ã§evresindeki pencereyi kullanarak mevcut kelimeyi tahmin eder.

CBoW daha hÄ±zlÄ±dÄ±r, ancak atlama-gram daha yavaÅŸ olmasÄ±na raÄŸmen nadir kelimeleri temsil etmede daha iyi bir iÅŸ Ã§Ä±karÄ±r.

![Kelimeyi vektÃ¶re dÃ¶nÃ¼ÅŸtÃ¼rmek iÃ§in kullanÄ±lan CBoW ve Skip-Gram algoritmalarÄ±nÄ± gÃ¶steren gÃ¶rsel.](../../../../../translated_images/tr/example-algorithms-for-converting-words-to-vectors.fbe9207a726922f6.webp)

> GÃ¶rsel [bu makaleden](https://arxiv.org/pdf/1301.3781.pdf) alÄ±nmÄ±ÅŸtÄ±r

Word2Vec Ã¶nceden eÄŸitilmiÅŸ gÃ¶mÃ¼lÃ¼ temsiller (GloVe gibi diÄŸer benzer modellerle birlikte) sinir aÄŸlarÄ±nda gÃ¶mÃ¼lÃ¼ temsil katmanÄ± yerine kullanÄ±labilir. Ancak, kelime daÄŸarcÄ±klarÄ±yla baÅŸa Ã§Ä±kmamÄ±z gerekir, Ã§Ã¼nkÃ¼ Word2Vec/GloVe ile Ã¶nceden eÄŸitilmiÅŸ kelime daÄŸarcÄ±ÄŸÄ±, metin corpusumuzdaki kelime daÄŸarcÄ±ÄŸÄ±ndan farklÄ± olabilir. YukarÄ±daki defterlere gÃ¶z atarak bu sorunun nasÄ±l Ã§Ã¶zÃ¼lebileceÄŸini gÃ¶rebilirsiniz.

## BaÄŸlamsal GÃ¶mÃ¼lÃ¼ Temsiller

Word2Vec gibi geleneksel Ã¶nceden eÄŸitilmiÅŸ gÃ¶mÃ¼lÃ¼ temsil modellerinin temel sÄ±nÄ±rlamalarÄ±ndan biri kelime anlamÄ± ayrÄ±mÄ±nÄ±n problemidir. Ã–nceden eÄŸitilmiÅŸ gÃ¶mÃ¼lÃ¼ temsiller kelimelerin baÄŸlamdaki anlamlarÄ±nÄ±n bir kÄ±smÄ±nÄ± yakalayabilse de, bir kelimenin her olasÄ± anlamÄ± aynÄ± gÃ¶mÃ¼lÃ¼ temsile kodlanÄ±r. Bu, 'play' gibi birÃ§ok kelimenin kullanÄ±ldÄ±klarÄ± baÄŸlama baÄŸlÄ± olarak farklÄ± anlamlara sahip olmasÄ± nedeniyle aÅŸaÄŸÄ± akÄ±ÅŸ modellerinde sorunlara yol aÃ§abilir.

Ã–rneÄŸin, 'play' kelimesi ÅŸu iki farklÄ± cÃ¼mlede oldukÃ§a farklÄ± anlamlara sahiptir:

- Tiyatroda bir **oyun** izledim.
- John arkadaÅŸlarÄ±yla **oynamak** istiyor.

YukarÄ±daki Ã¶nceden eÄŸitilmiÅŸ gÃ¶mÃ¼lÃ¼ temsiller, 'play' kelimesinin her iki anlamÄ±nÄ± da aynÄ± gÃ¶mÃ¼lÃ¼ temsilde temsil eder. Bu sÄ±nÄ±rlamayÄ± aÅŸmak iÃ§in, bÃ¼yÃ¼k bir metin corpusunda eÄŸitilmiÅŸ ve kelimelerin farklÄ± baÄŸlamlarda nasÄ±l bir araya gelebileceÄŸini *bilen* bir **dil modeli** temelinde gÃ¶mÃ¼lÃ¼ temsiller oluÅŸturmamÄ±z gerekir. BaÄŸlamsal gÃ¶mÃ¼lÃ¼ temsilleri tartÄ±ÅŸmak bu dersin kapsamÄ± dÄ±ÅŸÄ±nda, ancak kursun ilerleyen bÃ¶lÃ¼mlerinde dil modellerini ele alÄ±rken bu konuya geri dÃ¶neceÄŸiz.

## SonuÃ§

Bu derste, kelimelerin anlamsal anlamlarÄ±nÄ± daha iyi yansÄ±tmak iÃ§in TensorFlow ve PyTorch'ta gÃ¶mÃ¼lÃ¼ temsil katmanlarÄ± oluÅŸturmayÄ± ve kullanmayÄ± Ã¶ÄŸrendiniz.

## ğŸš€ Meydan Okuma

Word2Vec, ÅŸarkÄ± sÃ¶zleri ve ÅŸiir oluÅŸturma gibi ilginÃ§ uygulamalarda kullanÄ±lmÄ±ÅŸtÄ±r. [Bu makaleye](https://www.politetype.com/blog/word2vec-color-poems) gÃ¶z atarak yazarÄ±n Word2Vec'i ÅŸiir oluÅŸturmak iÃ§in nasÄ±l kullandÄ±ÄŸÄ±nÄ± Ã¶ÄŸrenin. AyrÄ±ca [Dan Shiffmann'Ä±n bu videosunu](https://www.youtube.com/watch?v=LSS_bos_TPI&ab_channel=TheCodingTrain) izleyerek bu tekniÄŸin farklÄ± bir aÃ§Ä±klamasÄ±nÄ± keÅŸfedin. ArdÄ±ndan, bu teknikleri Kaggle'dan alÄ±nmÄ±ÅŸ bir metin corpusunda kendi metinlerinize uygulamayÄ± deneyin.

## [Ders SonrasÄ± Test](https://ff-quizzes.netlify.app/en/ai/quiz/28)

## GÃ¶zden GeÃ§irme ve Kendi Kendine Ã‡alÄ±ÅŸma

Word2Vec ile ilgili bu makaleyi okuyun: [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf)

## [Ã–dev: Defterler](assignment.md)

---

