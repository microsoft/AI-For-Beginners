<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "e40b47ac3fd48f71304ede1474e66293",
  "translation_date": "2025-08-26T07:20:38+00:00",
  "source_file": "lessons/5-NLP/14-Embeddings/README.md",
  "language_code": "tr"
}
-->
# GÃ¶mÃ¼lÃ¼ Temsiller

## [Ders Ã–ncesi Test](https://ff-quizzes.netlify.app/en/ai/quiz/27)

BoW veya TF/IDF tabanlÄ± sÄ±nÄ±flandÄ±rÄ±cÄ±lar eÄŸitirken, `vocab_size` uzunluÄŸunda yÃ¼ksek boyutlu kelime torbasÄ± vektÃ¶rleri Ã¼zerinde Ã§alÄ±ÅŸÄ±yorduk ve dÃ¼ÅŸÃ¼k boyutlu konumsal temsil vektÃ¶rlerini seyrek tekil temsil vektÃ¶rlerine aÃ§Ä±kÃ§a dÃ¶nÃ¼ÅŸtÃ¼rÃ¼yorduk. Ancak, bu tekil temsil bellek aÃ§Ä±sÄ±ndan verimli deÄŸildir. AyrÄ±ca, her kelime birbirinden baÄŸÄ±msÄ±z olarak ele alÄ±nÄ±r, yani tekil kodlanmÄ±ÅŸ vektÃ¶rler kelimeler arasÄ±ndaki herhangi bir anlamsal benzerliÄŸi ifade etmez.

**GÃ¶mÃ¼lÃ¼ temsil** fikri, kelimeleri bir ÅŸekilde bir kelimenin anlamsal anlamÄ±nÄ± yansÄ±tan daha dÃ¼ÅŸÃ¼k boyutlu yoÄŸun vektÃ¶rlerle temsil etmektir. Daha sonra anlamlÄ± kelime gÃ¶mÃ¼lÃ¼ temsillerinin nasÄ±l oluÅŸturulacaÄŸÄ±nÄ± tartÄ±ÅŸacaÄŸÄ±z, ancak ÅŸimdilik gÃ¶mÃ¼lÃ¼ temsilleri bir kelime vektÃ¶rÃ¼nÃ¼n boyutunu dÃ¼ÅŸÃ¼rmenin bir yolu olarak dÃ¼ÅŸÃ¼nebiliriz.

Bu nedenle, gÃ¶mÃ¼lÃ¼ katman bir kelimeyi giriÅŸ olarak alÄ±r ve belirtilen `embedding_size` boyutunda bir Ã§Ä±ktÄ± vektÃ¶rÃ¼ Ã¼retir. Bir anlamda, bu bir `Linear` katmana Ã§ok benzer, ancak tekil kodlanmÄ±ÅŸ bir vektÃ¶r almak yerine, bir kelime numarasÄ±nÄ± giriÅŸ olarak alabilir, bÃ¶ylece bÃ¼yÃ¼k tekil kodlanmÄ±ÅŸ vektÃ¶rler oluÅŸturmaktan kaÃ§Ä±nmamÄ±zÄ± saÄŸlar.

SÄ±nÄ±flandÄ±rÄ±cÄ± aÄŸÄ±mÄ±zda ilk katman olarak bir gÃ¶mÃ¼lÃ¼ katman kullanarak, kelime torbasÄ±ndan **gÃ¶mÃ¼lÃ¼ torba** modeline geÃ§ebiliriz. Bu modelde, Ã¶nce metnimizdeki her kelimeyi ilgili gÃ¶mÃ¼lÃ¼ temsile dÃ¶nÃ¼ÅŸtÃ¼rÃ¼rÃ¼z ve ardÄ±ndan bu gÃ¶mÃ¼lÃ¼ temsillerin tÃ¼mÃ¼ Ã¼zerinde `sum`, `average` veya `max` gibi bir toplama fonksiyonu hesaplarÄ±z.  

![BeÅŸ kelimelik bir dizi iÃ§in bir gÃ¶mÃ¼lÃ¼ sÄ±nÄ±flandÄ±rÄ±cÄ±yÄ± gÃ¶steren gÃ¶rsel.](../../../../../translated_images/embedding-classifier-example.b77f021a7ee67eeec8e68bfe11636c5b97d6eaa067515a129bfb1d0034b1ac5b.tr.png)

> GÃ¶rsel yazar tarafÄ±ndan oluÅŸturulmuÅŸtur

## âœï¸ AlÄ±ÅŸtÄ±rmalar: GÃ¶mÃ¼lÃ¼ Temsiller

AÅŸaÄŸÄ±daki defterlerde Ã¶ÄŸrenmeye devam edin:
* [PyTorch ile GÃ¶mÃ¼lÃ¼ Temsiller](../../../../../lessons/5-NLP/14-Embeddings/EmbeddingsPyTorch.ipynb)
* [TensorFlow ile GÃ¶mÃ¼lÃ¼ Temsiller](../../../../../lessons/5-NLP/14-Embeddings/EmbeddingsTF.ipynb)

## Anlamsal GÃ¶mÃ¼lÃ¼ Temsiller: Word2Vec

GÃ¶mÃ¼lÃ¼ katman kelimeleri vektÃ¶r temsiline eÅŸlemeyi Ã¶ÄŸrenirken, bu temsilin mutlaka Ã§ok fazla anlamsal anlamÄ± olmayabilir. Benzer kelimelerin veya eÅŸanlamlÄ±larÄ±n, bazÄ± vektÃ¶r mesafeleri (Ã¶r. Ã–klid mesafesi) aÃ§Ä±sÄ±ndan birbirine yakÄ±n olan vektÃ¶rlere karÅŸÄ±lÄ±k geldiÄŸi bir vektÃ¶r temsili Ã¶ÄŸrenmek gÃ¼zel olurdu.

Bunu yapmak iÃ§in, gÃ¶mÃ¼lÃ¼ modelimizi bÃ¼yÃ¼k bir metin koleksiyonu Ã¼zerinde belirli bir ÅŸekilde Ã¶nceden eÄŸitmemiz gerekir. Anlamsal gÃ¶mÃ¼lÃ¼ temsilleri eÄŸitmenin bir yolu [Word2Vec](https://en.wikipedia.org/wiki/Word2vec) olarak adlandÄ±rÄ±lÄ±r. Bu yÃ¶ntem, kelimelerin daÄŸÄ±tÄ±lmÄ±ÅŸ bir temsilini Ã¼retmek iÃ§in kullanÄ±lan iki ana mimariye dayanÄ±r:

 - **SÃ¼rekli kelime torbasÄ±** (CBoW) â€” Bu mimaride, model Ã§evredeki baÄŸlamdan bir kelime tahmin etmek iÃ§in eÄŸitilir. $$(W_{-2},W_{-1},W_0,W_1,W_2)$$ ngram'Ä± verildiÄŸinde, modelin amacÄ± $W_0$'Ä± $$(W_{-2},W_{-1},W_1,W_2)$$'den tahmin etmektir.
 - **SÃ¼rekli atlama-gram** CBoW'un tersidir. Model, baÄŸlam kelimelerinin Ã§evresindeki pencereyi kullanarak mevcut kelimeyi tahmin eder.

CBoW daha hÄ±zlÄ±dÄ±r, ancak atlama-gram daha yavaÅŸtÄ±r ve nadir kelimeleri temsil etmede daha iyidir.

![Kelimeleri vektÃ¶rlere dÃ¶nÃ¼ÅŸtÃ¼rmek iÃ§in hem CBoW hem de Skip-Gram algoritmalarÄ±nÄ± gÃ¶steren gÃ¶rsel.](../../../../../translated_images/example-algorithms-for-converting-words-to-vectors.fbe9207a726922f6f0f5de66427e8a6eda63809356114e28fb1fa5f4a83ebda7.tr.png)

> GÃ¶rsel [bu makaleden](https://arxiv.org/pdf/1301.3781.pdf) alÄ±nmÄ±ÅŸtÄ±r

Word2Vec Ã¶nceden eÄŸitilmiÅŸ gÃ¶mÃ¼lÃ¼ temsilleri (GloVe gibi diÄŸer benzer modellerle birlikte) sinir aÄŸlarÄ±ndaki gÃ¶mÃ¼lÃ¼ katman yerine de kullanÄ±labilir. Ancak, kelime daÄŸarcÄ±klarÄ±yla baÅŸa Ã§Ä±kmamÄ±z gerekir, Ã§Ã¼nkÃ¼ Word2Vec/GloVe'yi Ã¶nceden eÄŸitmek iÃ§in kullanÄ±lan kelime daÄŸarcÄ±ÄŸÄ±, metin korpusumuzdaki kelime daÄŸarcÄ±ÄŸÄ±ndan farklÄ± olabilir. Bu sorunun nasÄ±l Ã§Ã¶zÃ¼lebileceÄŸini gÃ¶rmek iÃ§in yukarÄ±daki defterlere gÃ¶z atÄ±n.

## BaÄŸlama DayalÄ± GÃ¶mÃ¼lÃ¼ Temsiller

Word2Vec gibi geleneksel Ã¶nceden eÄŸitilmiÅŸ gÃ¶mÃ¼lÃ¼ temsil yÃ¶ntemlerinin temel sÄ±nÄ±rlamalarÄ±ndan biri, kelime anlamÄ± ayrÄ±mÄ± problemidir. Ã–nceden eÄŸitilmiÅŸ gÃ¶mÃ¼lÃ¼ temsiller, kelimelerin baÄŸlamdaki anlamlarÄ±nÄ±n bir kÄ±smÄ±nÄ± yakalayabilse de, bir kelimenin her olasÄ± anlamÄ± aynÄ± gÃ¶mÃ¼lÃ¼ temsile kodlanÄ±r. Bu, 'play' gibi birÃ§ok kelimenin kullanÄ±ldÄ±klarÄ± baÄŸlama baÄŸlÄ± olarak farklÄ± anlamlara sahip olmasÄ± nedeniyle, aÅŸaÄŸÄ± akÄ±ÅŸ modellerinde sorunlara neden olabilir.

Ã–rneÄŸin, 'play' kelimesi ÅŸu iki farklÄ± cÃ¼mlede oldukÃ§a farklÄ± anlamlara sahiptir:

- Tiyatroda bir **oyun** izledim.
- John arkadaÅŸlarÄ±yla **oynamak** istiyor.

YukarÄ±daki Ã¶nceden eÄŸitilmiÅŸ gÃ¶mÃ¼lÃ¼ temsiller, 'play' kelimesinin her iki anlamÄ±nÄ± da aynÄ± gÃ¶mÃ¼lÃ¼ temsilde ifade eder. Bu sÄ±nÄ±rlamayÄ± aÅŸmak iÃ§in, bÃ¼yÃ¼k bir metin korpusu Ã¼zerinde eÄŸitilmiÅŸ ve kelimelerin farklÄ± baÄŸlamlarda nasÄ±l bir araya getirilebileceÄŸini *bilen* bir **dil modeli** temelinde gÃ¶mÃ¼lÃ¼ temsiller oluÅŸturmamÄ±z gerekir. BaÄŸlama dayalÄ± gÃ¶mÃ¼lÃ¼ temsilleri tartÄ±ÅŸmak bu dersin kapsamÄ± dÄ±ÅŸÄ±ndadÄ±r, ancak kursun ilerleyen bÃ¶lÃ¼mlerinde dil modellerini ele alÄ±rken bu konuya geri dÃ¶neceÄŸiz.

## SonuÃ§

Bu derste, kelimelerin anlamsal anlamlarÄ±nÄ± daha iyi yansÄ±tmak iÃ§in TensorFlow ve PyTorch'ta gÃ¶mÃ¼lÃ¼ katmanlarÄ±n nasÄ±l oluÅŸturulacaÄŸÄ±nÄ± ve kullanÄ±lacaÄŸÄ±nÄ± keÅŸfettiniz.

## ğŸš€ Meydan Okuma

Word2Vec, ÅŸarkÄ± sÃ¶zleri ve ÅŸiir oluÅŸturma gibi ilginÃ§ uygulamalar iÃ§in kullanÄ±lmÄ±ÅŸtÄ±r. YazarÄ±n Word2Vec'i kullanarak ÅŸiir oluÅŸturma sÃ¼recini anlattÄ±ÄŸÄ± [bu makaleye](https://www.politetype.com/blog/word2vec-color-poems) gÃ¶z atÄ±n. AyrÄ±ca, bu tekniÄŸin farklÄ± bir aÃ§Ä±klamasÄ±nÄ± keÅŸfetmek iÃ§in Dan Shiffmann'Ä±n [bu videosunu](https://www.youtube.com/watch?v=LSS_bos_TPI&ab_channel=TheCodingTrain) izleyin. ArdÄ±ndan, bu teknikleri kendi metin korpusunuza uygulamayÄ± deneyin, belki Kaggle'dan bir kaynak kullanabilirsiniz.

## [Ders SonrasÄ± Test](https://ff-quizzes.netlify.app/en/ai/quiz/28)

## GÃ¶zden GeÃ§irme ve Kendi Kendine Ã‡alÄ±ÅŸma

Word2Vec ile ilgili bu makaleyi okuyun: [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf)

## [Ã–dev: Defterler](assignment.md)

**Feragatname**:  
Bu belge, AI Ã§eviri hizmeti [Co-op Translator](https://github.com/Azure/co-op-translator) kullanÄ±larak Ã§evrilmiÅŸtir. DoÄŸruluk iÃ§in Ã§aba gÃ¶stersek de, otomatik Ã§evirilerin hata veya yanlÄ±ÅŸlÄ±k iÃ§erebileceÄŸini lÃ¼tfen unutmayÄ±n. Belgenin orijinal dili, yetkili kaynak olarak kabul edilmelidir. Kritik bilgiler iÃ§in profesyonel insan Ã§evirisi Ã¶nerilir. Bu Ã§evirinin kullanÄ±mÄ±ndan kaynaklanan yanlÄ±ÅŸ anlamalar veya yanlÄ±ÅŸ yorumlamalar iÃ§in sorumluluk kabul etmiyoruz.