# GÃ¶mme (Embeddings)

## [Ders Ã¶ncesi quiz](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/114)

BoW veya TF/IDF tabanlÄ± sÄ±nÄ±flandÄ±rÄ±cÄ±larÄ± eÄŸitirken, uzunluÄŸu `vocab_size` olan yÃ¼ksek boyutlu kelime torbasÄ± (bag-of-words) vektÃ¶rleri Ã¼zerinde Ã§alÄ±ÅŸtÄ±k ve dÃ¼ÅŸÃ¼k boyutlu konumsal temsil vektÃ¶rlerini seyrek bir one-hot temsiline aÃ§Ä±kÃ§a dÃ¶nÃ¼ÅŸtÃ¼rdÃ¼k. Ancak bu one-hot temsili, bellek aÃ§Ä±sÄ±ndan verimli deÄŸildir. AyrÄ±ca, her kelime birbirinden baÄŸÄ±msÄ±z olarak ele alÄ±nÄ±r; yani one-hot kodlanmÄ±ÅŸ vektÃ¶rler kelimeler arasÄ±nda herhangi bir anlamsal benzerlik ifade etmez.

**GÃ¶mme** fikri, kelimeleri daha dÃ¼ÅŸÃ¼k boyutlu yoÄŸun vektÃ¶rler ile temsil etmektir ve bu vektÃ¶rler bir kelimenin anlamsal anlamÄ±nÄ± bir ÅŸekilde yansÄ±tÄ±r. AnlamlÄ± kelime gÃ¶mmeleri nasÄ±l oluÅŸturulacaÄŸÄ±nÄ± daha sonra tartÄ±ÅŸacaÄŸÄ±z, ancak ÅŸimdilik gÃ¶mmeleri bir kelime vektÃ¶rÃ¼nÃ¼n boyutunu dÃ¼ÅŸÃ¼rmenin bir yolu olarak dÃ¼ÅŸÃ¼nelim.

Bu nedenle, gÃ¶mme katmanÄ± bir kelimeyi girdi olarak alÄ±r ve belirli bir `embedding_size` uzunluÄŸunda bir Ã§Ä±ktÄ± vektÃ¶rÃ¼ Ã¼retir. Bir anlamda, bu, `Linear` katmanÄ±na Ã§ok benzer, ancak bir one-hot kodlanmÄ±ÅŸ vektÃ¶r almak yerine, bir kelime numarasÄ±nÄ± girdi olarak alabilir, bu da bÃ¼yÃ¼k one-hot kodlanmÄ±ÅŸ vektÃ¶rler oluÅŸturmaktan kaÃ§Ä±nmamÄ±zÄ± saÄŸlar.

SÄ±nÄ±flandÄ±rÄ±cÄ± aÄŸÄ±mÄ±zda gÃ¶mme katmanÄ±nÄ± ilk katman olarak kullanarak, kelime torbasÄ±ndan **gÃ¶mme torbasÄ±** modeline geÃ§iÅŸ yapabiliriz; burada metnimizdeki her kelimeyi karÅŸÄ±lÄ±k gelen gÃ¶mme ile dÃ¶nÃ¼ÅŸtÃ¼rÃ¼yor ve ardÄ±ndan `sum`, `average` veya `max` gibi tÃ¼m bu gÃ¶mmeler Ã¼zerinde bazÄ± toplama fonksiyonlarÄ± hesaplÄ±yoruz.

![BeÅŸ sÄ±ralÄ± kelime iÃ§in bir gÃ¶mme sÄ±nÄ±flandÄ±rÄ±cÄ±sÄ±nÄ± gÃ¶steren bir resim.](../../../../../translated_images/embedding-classifier-example.b77f021a7ee67eeec8e68bfe11636c5b97d6eaa067515a129bfb1d0034b1ac5b.tr.png)

> Resim yazar tarafÄ±ndan hazÄ±rlanmÄ±ÅŸtÄ±r.

## âœï¸ AlÄ±ÅŸtÄ±rmalar: GÃ¶mme

AÅŸaÄŸÄ±daki not defterlerinde Ã¶ÄŸrenmeye devam edin:
* [PyTorch ile GÃ¶mme](../../../../../lessons/5-NLP/14-Embeddings/EmbeddingsPyTorch.ipynb)
* [TensorFlow ile GÃ¶mme](../../../../../lessons/5-NLP/14-Embeddings/EmbeddingsTF.ipynb)

## Anlamsal GÃ¶mme: Word2Vec

GÃ¶mme katmanÄ± kelimeleri vektÃ¶r temsilcisine haritalamayÄ± Ã¶ÄŸrenirken, bu temsilin mutlaka Ã§ok fazla anlamsal anlamÄ± yoktu. Benzer kelimelerin veya eÅŸanlamlÄ±larÄ±n, bazÄ± vektÃ¶r mesafesi (Ã¶rneÄŸin, Ã–klid mesafesi) aÃ§Ä±sÄ±ndan birbirine yakÄ±n vektÃ¶rlere karÅŸÄ±lÄ±k geldiÄŸi bir vektÃ¶r temsilini Ã¶ÄŸrenmek gÃ¼zel olurdu.

Bunu yapmak iÃ§in, gÃ¶mme modelimizi belirli bir ÅŸekilde bÃ¼yÃ¼k bir metin koleksiyonu Ã¼zerinde Ã¶nceden eÄŸitmemiz gerekiyor. Anlamsal gÃ¶mmeleri eÄŸitmenin bir yolu [Word2Vec](https://en.wikipedia.org/wiki/Word2vec) olarak adlandÄ±rÄ±lÄ±r. Bu, kelimelerin daÄŸÄ±tÄ±lmÄ±ÅŸ bir temsilini Ã¼retmek iÃ§in kullanÄ±lan iki ana mimariye dayanÄ±r:

 - **SÃ¼rekli kelime torbasÄ±** (CBoW) â€” bu mimaride, modeli Ã§evresindeki baÄŸlamdan bir kelimeyi tahmin etmek iÃ§in eÄŸitiyoruz. ngram $(W_{-2},W_{-1},W_0,W_1,W_2)$ verildiÄŸinde, modelin amacÄ± $(W_{-2},W_{-1},W_1,W_2)$'den $W_0$'Ä± tahmin etmektir.
 - **SÃ¼rekli skip-gram**, CBoW'nun tersidir. Model, mevcut kelimeyi tahmin etmek iÃ§in Ã§evresindeki baÄŸlam kelimelerinin penceresini kullanÄ±r.

CBoW daha hÄ±zlÄ±dÄ±r, oysa skip-gram daha yavaÅŸtÄ±r, ancak nadir kelimeleri temsil etme konusunda daha iyi bir iÅŸ Ã§Ä±karÄ±r.

![Kelimeleri vektÃ¶rlere dÃ¶nÃ¼ÅŸtÃ¼rmek iÃ§in hem CBoW hem de Skip-Gram algoritmalarÄ±nÄ± gÃ¶steren bir resim.](../../../../../translated_images/example-algorithms-for-converting-words-to-vectors.fbe9207a726922f6f0f5de66427e8a6eda63809356114e28fb1fa5f4a83ebda7.tr.png)

> Resim [bu makaleden](https://arxiv.org/pdf/1301.3781.pdf) alÄ±nmÄ±ÅŸtÄ±r.

Word2Vec Ã¶nceden eÄŸitilmiÅŸ gÃ¶mmeleri (ve GloVe gibi diÄŸer benzer modeller) sinir aÄŸlarÄ±ndaki gÃ¶mme katmanÄ±nÄ±n yerinde de kullanÄ±labilir. Ancak, kelime daÄŸarcÄ±klarÄ± ile ilgilenmemiz gerekiyor, Ã§Ã¼nkÃ¼ Word2Vec/GloVe'yi Ã¶nceden eÄŸitmek iÃ§in kullanÄ±lan kelime daÄŸarcÄ±ÄŸÄ±, metin koleksiyonumuzdaki kelime daÄŸarcÄ±ÄŸÄ±ndan farklÄ± olabilir. Bu sorunun nasÄ±l Ã§Ã¶zÃ¼lebileceÄŸini gÃ¶rmek iÃ§in yukarÄ±daki not defterlerine gÃ¶z atÄ±n.

## BaÄŸlamsal GÃ¶mme

Word2Vec gibi geleneksel Ã¶nceden eÄŸitilmiÅŸ gÃ¶mme temsillerinin bir ana sÄ±nÄ±rlamasÄ±, kelime anlamÄ± ayrÄ±mÄ±nÄ± yapma problemidir. Ã–nceden eÄŸitilmiÅŸ gÃ¶mmeler, kelimelerin baÄŸlam iÃ§indeki anlamlarÄ±nÄ±n bir kÄ±smÄ±nÄ± yakalayabilse de, bir kelimenin her olasÄ± anlamÄ± aynÄ± gÃ¶mme iÃ§inde kodlanÄ±r. Bu, birÃ§ok kelimenin, Ã¶rneÄŸin 'play' kelimesinin, kullanÄ±ldÄ±ÄŸÄ± baÄŸlama baÄŸlÄ± olarak farklÄ± anlamlarÄ± olduÄŸu iÃ§in, aÅŸaÄŸÄ± akÄ±ÅŸ modellerinde sorunlara neden olabilir.

Ã–rneÄŸin, 'play' kelimesi bu iki farklÄ± cÃ¼mlede oldukÃ§a farklÄ± anlamlar taÅŸÄ±r:

- Tiyatroda bir **oyun** izlemeye gittim.
- John arkadaÅŸlarÄ±yla **oynamak** istiyor.

YukarÄ±daki Ã¶nceden eÄŸitilmiÅŸ gÃ¶mmeler, 'play' kelimesinin bu iki anlamÄ±nÄ± da aynÄ± gÃ¶mme iÃ§inde temsil eder. Bu sÄ±nÄ±rlamanÄ±n Ã¼stesinden gelmek iÃ§in, bÃ¼yÃ¼k bir metin koleksiyonu Ã¼zerinde eÄŸitilen ve kelimelerin farklÄ± baÄŸlamlarda nasÄ±l bir araya getirileceÄŸini *bilen* **dil modeli** tabanlÄ± gÃ¶mmeler oluÅŸturmamÄ±z gerekiyor. BaÄŸlamsal gÃ¶mmeleri tartÄ±ÅŸmak bu eÄŸitimin kapsamÄ± dÄ±ÅŸÄ±nda, ancak daha sonra dil modellerinden bahsederken onlara geri dÃ¶neceÄŸiz.

## SonuÃ§

Bu derste, TensorFlow ve Pytorch'ta gÃ¶mme katmanlarÄ± oluÅŸturmayÄ± ve kullanmayÄ± Ã¶ÄŸrendiniz; bu sayede kelimelerin anlamsal anlamlarÄ±nÄ± daha iyi yansÄ±tabilirsiniz.

## ğŸš€ Zorluk

Word2Vec, ÅŸarkÄ± sÃ¶zleri ve ÅŸiir Ã¼retimi gibi bazÄ± ilginÃ§ uygulamalar iÃ§in kullanÄ±lmÄ±ÅŸtÄ±r. YazarÄ±n Word2Vec kullanarak ÅŸiir nasÄ±l Ã¼rettiÄŸini anlatan [bu makaleye](https://www.politetype.com/blog/word2vec-color-poems) gÃ¶z atÄ±n. AyrÄ±ca, bu tekniÄŸin farklÄ± bir aÃ§Ä±klamasÄ±nÄ± keÅŸfetmek iÃ§in [Dan Shiffmann'Ä±n bu videosunu](https://www.youtube.com/watch?v=LSS_bos_TPI&ab_channel=TheCodingTrain) izleyin. ArdÄ±ndan, bu teknikleri kendi metin koleksiyonunuza uygulamayÄ± deneyin; belki Kaggle'dan elde ettiÄŸiniz verilerle.

## [Ders sonrasÄ± quiz](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/214)

## GÃ¶zden GeÃ§irme & Kendi Kendine Ã‡alÄ±ÅŸma

Word2Vec hakkÄ±nda [VektÃ¶r UzayÄ±nda Kelime Temsillerinin Verimli Tahmini](https://arxiv.org/pdf/1301.3781.pdf) baÅŸlÄ±klÄ± bu makaleyi okuyun.

## [Ã–dev: Not Defterleri](assignment.md)

**AÃ§Ä±klama**:  
Bu belge, makine tabanlÄ± AI Ã§eviri hizmetleri kullanÄ±larak Ã§evrilmiÅŸtir. DoÄŸruluÄŸa Ã¶zen gÃ¶stersek de, otomatik Ã§evirilerin hatalar veya yanlÄ±ÅŸlÄ±klar iÃ§erebileceÄŸini lÃ¼tfen unutmayÄ±n. Orijinal belge, kendi dilinde otorite kaynaÄŸÄ± olarak kabul edilmelidir. Kritik bilgiler iÃ§in profesyonel insan Ã§evirisi Ã¶nerilmektedir. Bu Ã§evirinin kullanÄ±mÄ±ndan kaynaklanan herhangi bir yanlÄ±ÅŸ anlama veya yanlÄ±ÅŸ yorumlama iÃ§in sorumluluk kabul etmiyoruz.