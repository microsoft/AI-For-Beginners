# Metinleri TensÃ¶rler Olarak Temsil Etme

## [Ders Ã¶ncesi quiz](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/113)

## Metin SÄ±nÄ±flandÄ±rmasÄ±

Bu bÃ¶lÃ¼mÃ¼n ilk kÄ±smÄ±nda, **metin sÄ±nÄ±flandÄ±rmasÄ±** gÃ¶revine odaklanacaÄŸÄ±z. [AG News](https://www.kaggle.com/amananandrai/ag-news-classification-dataset) veri setini kullanacaÄŸÄ±z; bu veri seti aÅŸaÄŸÄ±daki gibi haber makalelerini iÃ§ermektedir:

* Kategori: Bilim/Teknoloji
* BaÅŸlÄ±k: Ky. Åirketi Peptitleri Ä°ncelemek Ä°Ã§in Hibe KazandÄ± (AP)
* Metin: AP - Louisville Ãœniversitesi'nde bir kimya araÅŸtÄ±rmacÄ±sÄ± tarafÄ±ndan kurulan bir ÅŸirket, geliÅŸtirmek iÃ§in bir hibe kazandÄ±...

AmacÄ±mÄ±z, haber maddesini metne dayalÄ± olarak bir kategoriye sÄ±nÄ±flandÄ±rmaktÄ±r.

## Metni Temsil Etme

DoÄŸal Dil Ä°ÅŸleme (NLP) gÃ¶revlerini sinir aÄŸlarÄ± ile Ã§Ã¶zmek istiyorsak, metni tensÃ¶rler olarak temsil etmenin bir yoluna ihtiyacÄ±mÄ±z var. Bilgisayarlar, metin karakterlerini, ekranÄ±nÄ±zdaki yazÄ± tiplerine karÅŸÄ±lÄ±k gelen sayÄ±lar olarak temsil eder; bu, ASCII veya UTF-8 gibi kodlamalar kullanÄ±larak yapÄ±lÄ±r.

<img alt="Bir karakteri ASCII ve ikili temsile eÅŸleyen diyagramÄ± gÃ¶steren bir gÃ¶rÃ¼ntÃ¼" src="images/ascii-character-map.png" width="50%"/>

> [GÃ¶rÃ¼ntÃ¼ kaynaÄŸÄ±](https://www.seobility.net/en/wiki/ASCII)

Ä°nsanlar olarak, her harfin neyi **temsil ettiÄŸini** anlarÄ±z ve tÃ¼m karakterlerin bir araya gelerek bir cÃ¼mledeki kelimeleri nasÄ±l oluÅŸturduÄŸunu biliriz. Ancak, bilgisayarlar kendi baÅŸlarÄ±na bÃ¶yle bir anlayÄ±ÅŸa sahip deÄŸildir ve sinir aÄŸÄ±, eÄŸitimi sÄ±rasÄ±nda anlamÄ± Ã¶ÄŸrenmek zorundadÄ±r.

Bu nedenle, metni temsil ederken farklÄ± yaklaÅŸÄ±mlar kullanabiliriz:

* **Karakter dÃ¼zeyinde temsil**, metni her karakteri bir sayÄ± olarak ele alarak temsil ettiÄŸimiz durumdur. Metin koleksiyonumuzda *C* farklÄ± karakter bulunduÄŸunu varsayarsak, *Hello* kelimesi 5x*C* tensÃ¶r ile temsil edilir. Her harf, bir sÄ±cak kodlama (one-hot encoding) ile bir tensÃ¶r sÃ¼tununa karÅŸÄ±lÄ±k gelir.
* **Kelime dÃ¼zeyinde temsil**, metindeki tÃ¼m kelimelerin bir **kelime hazinesi** oluÅŸturularak temsil edildiÄŸi durumdur; ardÄ±ndan kelimeleri sÄ±cak kodlama ile temsil ederiz. Bu yaklaÅŸÄ±m, her harfin kendisiyle Ã§ok fazla anlam ifade etmediÄŸi iÃ§in daha iyidir ve bu nedenle daha yÃ¼ksek dÃ¼zeydeki anlamsal kavramlarÄ± - kelimeleri - kullanarak sinir aÄŸÄ± iÃ§in gÃ¶revi basitleÅŸtiririz. Ancak, bÃ¼yÃ¼k sÃ¶zlÃ¼k boyutu nedeniyle yÃ¼ksek boyutlu seyrek tensÃ¶rlerle baÅŸa Ã§Ä±kmamÄ±z gerekir.

Temsilden baÄŸÄ±msÄ±z olarak, Ã¶nce metni bir **token** dizisine dÃ¶nÃ¼ÅŸtÃ¼rmemiz gerekir; bir token, ya bir karakter, bir kelime ya da bazen bir kelimenin bir parÃ§asÄ± olabilir. ArdÄ±ndan, token'Ä± bir sayÄ±ya dÃ¶nÃ¼ÅŸtÃ¼rÃ¼rÃ¼z; genellikle **kelime hazinesi** kullanarak, bu sayÄ± bir sinir aÄŸÄ±na sÄ±cak kodlama ile beslenebilir.

## N-Gramlar

DoÄŸal dilde, kelimelerin kesin anlamÄ± yalnÄ±zca baÄŸlamda belirlenebilir. Ã–rneÄŸin, *sinir aÄŸÄ±* ve *balÄ±k avlama aÄŸÄ±* ifadelerinin anlamlarÄ± tamamen farklÄ±dÄ±r. Bu durumu dikkate almanÄ±n yollarÄ±ndan biri, modelimizi kelime Ã§iftleri Ã¼zerinde inÅŸa etmek ve kelime Ã§iftlerini ayrÄ± kelime hazinesi tokenleri olarak ele almaktÄ±r. Bu ÅŸekilde, *I like to go fishing* cÃ¼mlesi aÅŸaÄŸÄ±daki token dizisi ile temsil edilecektir: *I like*, *like to*, *to go*, *go fishing*. Bu yaklaÅŸÄ±mÄ±n sorunu, sÃ¶zlÃ¼k boyutunun Ã¶nemli Ã¶lÃ§Ã¼de bÃ¼yÃ¼mesidir ve *go fishing* ve *go shopping* gibi kombinasyonlar, aynÄ± fiil olmasÄ±na raÄŸmen hiÃ§bir anlamsal benzerlik paylaÅŸmayan farklÄ± tokenler tarafÄ±ndan temsil edilir.

BazÄ± durumlarda, Ã¼Ã§ kelimeden oluÅŸan kombinasyonlar - tri-gramlar - kullanmayÄ± da dÃ¼ÅŸÃ¼nebiliriz. Bu nedenle, bu yaklaÅŸÄ±m genellikle **n-gramlar** olarak adlandÄ±rÄ±lÄ±r. AyrÄ±ca, karakter dÃ¼zeyinde temsil ile n-gramlarÄ± kullanmak mantÄ±klÄ±dÄ±r; bu durumda n-gramlar kabaca farklÄ± hecelere karÅŸÄ±lÄ±k gelecektir.

## Kelime TorbasÄ± ve TF/IDF

Metin sÄ±nÄ±flandÄ±rmasÄ± gibi gÃ¶revleri Ã§Ã¶zerken, metni tek bir sabit boyutlu vektÃ¶rle temsil edebilmemiz gerekir; bu vektÃ¶rÃ¼ son yoÄŸun sÄ±nÄ±flandÄ±rÄ±cÄ±ya girdi olarak kullanacaÄŸÄ±z. Bunu yapmanÄ±n en basit yollarÄ±ndan biri, tÃ¼m bireysel kelime temsillerini bir araya getirmektir; Ã¶rneÄŸin, bunlarÄ± toplamak. Her kelimenin sÄ±cak kodlamalarÄ±nÄ± toplarsak, metin iÃ§inde her kelimenin ne kadar sÄ±klÄ±kla geÃ§tiÄŸini gÃ¶steren bir frekans vektÃ¶rÃ¼ elde ederiz. Bu metin temsiline **kelime torbasÄ±** (BoW) denir.

<img src="images/bow.png" width="90%"/>

> YazarÄ±n gÃ¶rÃ¼ntÃ¼sÃ¼

BoW, metinde hangi kelimelerin ve hangi miktarlarda gÃ¶rÃ¼ndÃ¼ÄŸÃ¼nÃ¼ temsil eder; bu da metnin ne hakkÄ±nda olduÄŸu konusunda iyi bir gÃ¶sterge olabilir. Ã–rneÄŸin, bir politikayla ilgili haber makalesi, muhtemelen *baÅŸkan* ve *Ã¼lke* gibi kelimeleri iÃ§erecektir; bilimsel bir yayÄ±nda ise *Ã§arpÄ±ÅŸtÄ±rÄ±cÄ±*, *bulundu* gibi kelimeler olacaktÄ±r. Bu nedenle, kelime sÄ±klÄ±klarÄ± birÃ§ok durumda metin iÃ§eriÄŸinin iyi bir gÃ¶stergesi olabilir.

BoW ile ilgili sorun, *ve*, *dir* gibi belirli yaygÄ±n kelimelerin Ã§oÄŸu metinde yer almasÄ± ve en yÃ¼ksek sÄ±klÄ±klara sahip olmalarÄ±dÄ±r; bu durum, gerÃ§ekten Ã¶nemli olan kelimeleri gÃ¶lgede bÄ±rakmaktadÄ±r. Bu kelimelerin Ã¶nemini, kelimelerin tÃ¼m belge koleksiyonunda ne sÄ±klÄ±kla geÃ§tiÄŸini dikkate alarak azaltabiliriz. Bu, TF/IDF yaklaÅŸÄ±mÄ±nÄ±n temel fikridir ve bu konu, bu derse ekli not defterlerinde daha ayrÄ±ntÄ±lÄ± olarak ele alÄ±nmaktadÄ±r.

Ancak, bu yaklaÅŸÄ±mlarÄ±n hiÃ§biri metnin **anlamÄ±nÄ±** tam olarak dikkate alamaz. Bunu yapmak iÃ§in daha gÃ¼Ã§lÃ¼ sinir aÄŸÄ± modellerine ihtiyacÄ±mÄ±z var; bunu bu bÃ¶lÃ¼mÃ¼n ilerleyen kÄ±sÄ±mlarÄ±nda tartÄ±ÅŸacaÄŸÄ±z.

## âœï¸ AlÄ±ÅŸtÄ±rmalar: Metin Temsili

AÅŸaÄŸÄ±daki not defterlerinde Ã¶ÄŸreniminize devam edin:

* [PyTorch ile Metin Temsili](../../../../../lessons/5-NLP/13-TextRep/TextRepresentationPyTorch.ipynb)
* [TensorFlow ile Metin Temsili](../../../../../lessons/5-NLP/13-TextRep/TextRepresentationTF.ipynb)

## SonuÃ§

Åu ana kadar, farklÄ± kelimelere frekans aÄŸÄ±rlÄ±ÄŸÄ± ekleyebilen teknikleri inceledik. Ancak, bu teknikler anlamÄ± veya sÄ±ralamayÄ± temsil edemez. ÃœnlÃ¼ dilbilimci J. R. Firth'Ã¼n 1935 yÄ±lÄ±nda sÃ¶ylediÄŸi gibi, "Bir kelimenin tam anlamÄ± her zaman baÄŸlamsaldÄ±r ve baÄŸlamdan ayrÄ± bir anlam Ã§alÄ±ÅŸmasÄ± ciddiye alÄ±namaz." Daha sonra kursta, metinden baÄŸlamsal bilgileri nasÄ±l yakalayacaÄŸÄ±mÄ±zÄ± dil modelleme ile Ã¶ÄŸreneceÄŸiz.

## ğŸš€ Zorluk

Kelime torbasÄ± ve farklÄ± veri modelleri kullanarak bazÄ± diÄŸer alÄ±ÅŸtÄ±rmalarÄ± deneyin. Bu [Kaggle yarÄ±ÅŸmasÄ±ndan](https://www.kaggle.com/competitions/word2vec-nlp-tutorial/overview/part-1-for-beginners-bag-of-words) ilham alabilirsiniz.

## [Ders sonrasÄ± quiz](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/213)

## GÃ¶zden GeÃ§irme & Kendi Kendine Ã‡alÄ±ÅŸma

Metin gÃ¶mme ve kelime torbasÄ± teknikleri ile becerilerinizi [Microsoft Learn](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-pytorch/?WT.mc_id=academic-77998-cacaste) Ã¼zerinde pratik yaparak geliÅŸtirin.

## [Ã–dev: Not Defterleri](assignment.md)

**AÃ§Ä±klama**:  
Bu belge, makine tabanlÄ± yapay zeka Ã§eviri hizmetleri kullanÄ±larak Ã§evrilmiÅŸtir. DoÄŸruluk iÃ§in Ã§aba gÃ¶stersek de, otomatik Ã§evirilerin hatalar veya yanlÄ±ÅŸ anlamalar iÃ§erebileceÄŸini lÃ¼tfen unutmayÄ±n. Orijinal belge, kendi dilinde yetkili kaynak olarak deÄŸerlendirilmelidir. Kritik bilgiler iÃ§in profesyonel insan Ã§evirisi Ã¶nerilmektedir. Bu Ã§evirinin kullanÄ±mÄ±ndan kaynaklanan yanlÄ±ÅŸ anlamalar veya yanlÄ±ÅŸ yorumlamalar iÃ§in sorumluluk kabul etmiyoruz.