<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "4522e22e150be0845e03aa41209a39d5",
  "translation_date": "2025-08-26T07:21:50+00:00",
  "source_file": "lessons/5-NLP/13-TextRep/README.md",
  "language_code": "tr"
}
-->
# Metni TensÃ¶rler Olarak Temsil Etme

## [Ders Ã–ncesi Test](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/113)

## Metin SÄ±nÄ±flandÄ±rma

Bu bÃ¶lÃ¼mÃ¼n ilk kÄ±smÄ±nda **metin sÄ±nÄ±flandÄ±rma** gÃ¶revine odaklanacaÄŸÄ±z. [AG News](https://www.kaggle.com/amananandrai/ag-news-classification-dataset) veri setini kullanacaÄŸÄ±z. Bu veri seti aÅŸaÄŸÄ±daki gibi haber makalelerini iÃ§erir:

* Kategori: Bilim/Teknoloji
* BaÅŸlÄ±k: Ky. Åirketi Peptitleri AraÅŸtÄ±rmak Ä°Ã§in Hibe KazandÄ± (AP)
* GÃ¶vde: AP - Louisville Ãœniversitesi'nde bir kimya araÅŸtÄ±rmacÄ±sÄ± tarafÄ±ndan kurulan bir ÅŸirket, geliÅŸtirme iÃ§in bir hibe kazandÄ±...

AmacÄ±mÄ±z, haber Ã¶ÄŸesini metne dayanarak kategorilerden birine sÄ±nÄ±flandÄ±rmak olacak.

## Metni Temsil Etme

DoÄŸal Dil Ä°ÅŸleme (NLP) gÃ¶revlerini sinir aÄŸlarÄ±yla Ã§Ã¶zmek istiyorsak, metni tensÃ¶rler olarak temsil etmenin bir yoluna ihtiyacÄ±mÄ±z var. Bilgisayarlar zaten metinsel karakterleri, ekranÄ±nÄ±zdaki yazÄ± tiplerine eÅŸlenen sayÄ±lar olarak ASCII veya UTF-8 gibi kodlamalar kullanarak temsil eder.

<img alt="Bir karakteri ASCII ve ikili temsil ile eÅŸleyen diyagramÄ± gÃ¶steren gÃ¶rÃ¼ntÃ¼" src="images/ascii-character-map.png" width="50%"/>

> [GÃ¶rsel kaynaÄŸÄ±](https://www.seobility.net/en/wiki/ASCII)

Ä°nsanlar olarak, her harfin **ne ifade ettiÄŸini** ve tÃ¼m karakterlerin bir cÃ¼mlenin kelimelerini oluÅŸturmak iÃ§in nasÄ±l bir araya geldiÄŸini anlÄ±yoruz. Ancak, bilgisayarlar bu anlayÄ±ÅŸa sahip deÄŸildir ve sinir aÄŸÄ±nÄ±n anlamÄ± eÄŸitim sÄ±rasÄ±nda Ã¶ÄŸrenmesi gerekir.

Bu nedenle, metni temsil ederken farklÄ± yaklaÅŸÄ±mlar kullanabiliriz:

* **Karakter dÃ¼zeyinde temsil**, metni her karakteri bir sayÄ± olarak ele alarak temsil ettiÄŸimizde. Metin korpusumuzda *C* farklÄ± karakter olduÄŸunu varsayarsak, *Merhaba* kelimesi 5x*C* tensÃ¶rle temsil edilir. Her harf, tek-sÄ±cak kodlama ile bir tensÃ¶r sÃ¼tununa karÅŸÄ±lÄ±k gelir.
* **Kelime dÃ¼zeyinde temsil**, metnimizdeki tÃ¼m kelimelerin bir **kelime daÄŸarcÄ±ÄŸÄ±** oluÅŸturduÄŸumuz ve ardÄ±ndan kelimeleri tek-sÄ±cak kodlama ile temsil ettiÄŸimiz yÃ¶ntemdir. Bu yaklaÅŸÄ±m bir ÅŸekilde daha iyidir, Ã§Ã¼nkÃ¼ tek baÅŸÄ±na her harf Ã§ok fazla anlam taÅŸÄ±maz ve bu nedenle daha yÃ¼ksek dÃ¼zeydeki anlamsal kavramlar - kelimeler - kullanarak sinir aÄŸÄ± iÃ§in gÃ¶revi basitleÅŸtiririz. Ancak, bÃ¼yÃ¼k bir sÃ¶zlÃ¼k boyutuna sahip olduÄŸumuzda, yÃ¼ksek boyutlu seyrek tensÃ¶rlerle baÅŸa Ã§Ä±kmamÄ±z gerekir.

Temsil yÃ¶nteminden baÄŸÄ±msÄ±z olarak, Ã¶nce metni bir **token** dizisine dÃ¶nÃ¼ÅŸtÃ¼rmemiz gerekir. Bir token, bir karakter, bir kelime veya bazen bir kelimenin bir parÃ§asÄ± olabilir. Daha sonra, token'Ä± genellikle bir **kelime daÄŸarcÄ±ÄŸÄ±** kullanarak bir sayÄ±ya dÃ¶nÃ¼ÅŸtÃ¼rÃ¼rÃ¼z ve bu sayÄ± tek-sÄ±cak kodlama ile bir sinir aÄŸÄ±na beslenebilir.

## N-Gramlar

DoÄŸal dilde, kelimelerin kesin anlamÄ± yalnÄ±zca baÄŸlam iÃ§inde belirlenebilir. Ã–rneÄŸin, *sinir aÄŸÄ±* ve *balÄ±k aÄŸÄ±* ifadelerinin anlamlarÄ± tamamen farklÄ±dÄ±r. Bunu dikkate almanÄ±n yollarÄ±ndan biri, modelimizi kelime Ã§iftleri Ã¼zerine inÅŸa etmek ve kelime Ã§iftlerini ayrÄ± kelime daÄŸarcÄ±ÄŸÄ± tokenlarÄ± olarak ele almaktÄ±r. Bu ÅŸekilde, *BalÄ±k tutmaya gitmeyi seviyorum* cÃ¼mlesi ÅŸu token dizisiyle temsil edilir: *BalÄ±k tutmayÄ±*, *tutmayÄ± gitmeyi*, *gitmeyi seviyorum*. Bu yaklaÅŸÄ±mÄ±n sorunu, sÃ¶zlÃ¼k boyutunun Ã¶nemli Ã¶lÃ§Ã¼de bÃ¼yÃ¼mesidir ve *gitmeyi seviyorum* ve *gitmeyi alÄ±ÅŸveriÅŸe* gibi kombinasyonlar farklÄ± tokenlarla temsil edilir, bu da aynÄ± fiile raÄŸmen herhangi bir anlamsal benzerlik paylaÅŸmaz.

BazÄ± durumlarda, Ã¼Ã§ kelimeden oluÅŸan tri-gramlar kullanmayÄ± dÃ¼ÅŸÃ¼nebiliriz. Bu nedenle, bu yaklaÅŸÄ±m genellikle **n-gramlar** olarak adlandÄ±rÄ±lÄ±r. AyrÄ±ca, karakter dÃ¼zeyinde temsil ile n-gramlar kullanmak mantÄ±klÄ±dÄ±r, bu durumda n-gramlar kabaca farklÄ± hecelere karÅŸÄ±lÄ±k gelir.

## Kelime TorbasÄ± ve TF/IDF

Metin sÄ±nÄ±flandÄ±rma gibi gÃ¶revleri Ã§Ã¶zerken, metni sabit boyutlu bir vektÃ¶rle temsil edebilmemiz gerekir. Bu vektÃ¶rÃ¼, son yoÄŸun sÄ±nÄ±flandÄ±rÄ±cÄ±ya giriÅŸ olarak kullanacaÄŸÄ±z. Bunu yapmanÄ±n en basit yollarÄ±ndan biri, tÃ¼m bireysel kelime temsillerini birleÅŸtirmek, Ã¶rneÄŸin onlarÄ± toplayarak. Her kelimenin tek-sÄ±cak kodlamalarÄ±nÄ± toplarsak, metin iÃ§inde her kelimenin kaÃ§ kez gÃ¶rÃ¼ndÃ¼ÄŸÃ¼nÃ¼ gÃ¶steren bir frekans vektÃ¶rÃ¼ elde ederiz. Bu tÃ¼r bir metin temsili **kelime torbasÄ±** (BoW) olarak adlandÄ±rÄ±lÄ±r.

<img src="images/bow.png" width="90%"/>

> GÃ¶rsel yazar tarafÄ±ndan oluÅŸturulmuÅŸtur

Bir BoW, metinde hangi kelimelerin gÃ¶rÃ¼ndÃ¼ÄŸÃ¼nÃ¼ ve hangi miktarlarda olduÄŸunu temsil eder, bu da metnin ne hakkÄ±nda olduÄŸuna dair iyi bir gÃ¶sterge olabilir. Ã–rneÄŸin, politika Ã¼zerine bir haber makalesi muhtemelen *baÅŸkan* ve *Ã¼lke* gibi kelimeler iÃ§erirken, bilimsel bir yayÄ±n *Ã§arpÄ±ÅŸtÄ±rÄ±cÄ±*, *keÅŸfedildi* gibi kelimeler iÃ§erebilir. Bu nedenle, kelime frekanslarÄ± birÃ§ok durumda metin iÃ§eriÄŸinin iyi bir gÃ¶stergesi olabilir.

BoW'un sorunu, *ve*, *bu*, *bir* gibi yaygÄ±n kelimelerin Ã§oÄŸu metinde gÃ¶rÃ¼nmesi ve en yÃ¼ksek frekanslara sahip olmasÄ±dÄ±r, bu da gerÃ§ekten Ã¶nemli olan kelimeleri gÃ¶lgede bÄ±rakÄ±r. Bu kelimelerin Ã¶nemini azaltmak iÃ§in, kelimelerin tÃ¼m belge koleksiyonunda ne sÄ±klÄ±kta gÃ¶rÃ¼ndÃ¼ÄŸÃ¼nÃ¼ dikkate alabiliriz. Bu, TF/IDF yaklaÅŸÄ±mÄ±nÄ±n temel fikridir ve bu dersle iliÅŸkilendirilmiÅŸ not defterlerinde daha ayrÄ±ntÄ±lÄ± olarak ele alÄ±nmÄ±ÅŸtÄ±r.

Ancak, bu yaklaÅŸÄ±mlarÄ±n hiÃ§biri metnin **anlamÄ±nÄ±** tam olarak dikkate alamaz. Bunu yapmak iÃ§in daha gÃ¼Ã§lÃ¼ sinir aÄŸÄ± modellerine ihtiyacÄ±mÄ±z var ve bu modelleri bu bÃ¶lÃ¼mde daha sonra tartÄ±ÅŸacaÄŸÄ±z.

## âœï¸ AlÄ±ÅŸtÄ±rmalar: Metin Temsili

AÅŸaÄŸÄ±daki not defterlerinde Ã¶ÄŸrenmeye devam edin:

* [PyTorch ile Metin Temsili](../../../../../lessons/5-NLP/13-TextRep/TextRepresentationPyTorch.ipynb)
* [TensorFlow ile Metin Temsili](../../../../../lessons/5-NLP/13-TextRep/TextRepresentationTF.ipynb)

## SonuÃ§

Åimdiye kadar, farklÄ± kelimelere frekans aÄŸÄ±rlÄ±ÄŸÄ± ekleyebilen teknikleri inceledik. Ancak, bu teknikler anlamÄ± veya sÄ±ralamayÄ± temsil edemez. ÃœnlÃ¼ dilbilimci J. R. Firth'in 1935'te sÃ¶ylediÄŸi gibi, "Bir kelimenin tam anlamÄ± her zaman baÄŸlamsaldÄ±r ve baÄŸlamdan baÄŸÄ±msÄ±z bir anlam Ã§alÄ±ÅŸmasÄ± ciddiye alÄ±namaz." Bu kursta daha sonra, dil modelleme kullanarak metinden baÄŸlamsal bilgiyi nasÄ±l yakalayacaÄŸÄ±mÄ±zÄ± Ã¶ÄŸreneceÄŸiz.

## ğŸš€ Meydan Okuma

Kelime torbasÄ± ve farklÄ± veri modelleri kullanarak baÅŸka alÄ±ÅŸtÄ±rmalar yapmayÄ± deneyin. Bu [Kaggle yarÄ±ÅŸmasÄ±](https://www.kaggle.com/competitions/word2vec-nlp-tutorial/overview/part-1-for-beginners-bag-of-words) size ilham verebilir.

## [Ders SonrasÄ± Test](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/213)

## GÃ¶zden GeÃ§irme ve Kendi Kendine Ã‡alÄ±ÅŸma

[Microsoft Learn](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-pytorch/?WT.mc_id=academic-77998-cacaste) Ã¼zerinde metin gÃ¶mme ve kelime torbasÄ± teknikleriyle becerilerinizi geliÅŸtirin.

## [Ã–dev: Not Defterleri](assignment.md)

**Feragatname**:  
Bu belge, [Co-op Translator](https://github.com/Azure/co-op-translator) adlÄ± yapay zeka Ã§eviri hizmeti kullanÄ±larak Ã§evrilmiÅŸtir. DoÄŸruluk iÃ§in Ã§aba gÃ¶stersek de, otomatik Ã§evirilerin hata veya yanlÄ±ÅŸlÄ±klar iÃ§erebileceÄŸini lÃ¼tfen unutmayÄ±n. Belgenin orijinal dili, yetkili kaynak olarak kabul edilmelidir. Kritik bilgiler iÃ§in profesyonel insan Ã§evirisi Ã¶nerilir. Bu Ã§evirinin kullanÄ±mÄ±ndan kaynaklanan herhangi bir yanlÄ±ÅŸ anlama veya yanlÄ±ÅŸ yorumlama durumunda sorumluluk kabul edilmez.