# Metni TensÃ¶rler Olarak Temsil Etmek

## [Ders Ã–ncesi Testi](https://ff-quizzes.netlify.app/en/ai/quiz/25)

## Metin SÄ±nÄ±flandÄ±rma

Bu bÃ¶lÃ¼mÃ¼n ilk kÄ±smÄ±nda **metin sÄ±nÄ±flandÄ±rma** gÃ¶revine odaklanacaÄŸÄ±z. [AG News](https://www.kaggle.com/amananandrai/ag-news-classification-dataset) veri setini kullanacaÄŸÄ±z. Bu veri seti, aÅŸaÄŸÄ±daki gibi haber makalelerini iÃ§erir:

* Kategori: Bilim/Teknoloji
* BaÅŸlÄ±k: Ky. Åirketi Peptitleri AraÅŸtÄ±rmak Ä°Ã§in Hibe KazandÄ± (AP)
* GÃ¶vde: AP - Louisville Ãœniversitesi'nde bir kimya araÅŸtÄ±rmacÄ±sÄ± tarafÄ±ndan kurulan bir ÅŸirket, geliÅŸtirme iÃ§in hibe kazandÄ±...

AmacÄ±mÄ±z, metne dayanarak haber Ã¶ÄŸesini kategorilerden birine sÄ±nÄ±flandÄ±rmak olacak.

## Metni Temsil Etmek

DoÄŸal Dil Ä°ÅŸleme (NLP) gÃ¶revlerini sinir aÄŸlarÄ±yla Ã§Ã¶zmek istiyorsak, metni tensÃ¶rler olarak temsil etmenin bir yoluna ihtiyacÄ±mÄ±z var. Bilgisayarlar zaten metinsel karakterleri, ekranÄ±nÄ±zdaki yazÄ± tiplerine eÅŸleyen ASCII veya UTF-8 gibi kodlamalar kullanarak sayÄ±larla temsil eder.

<img alt="Bir karakteri ASCII ve ikili temsil ile eÅŸleyen diyagramÄ± gÃ¶steren gÃ¶rÃ¼ntÃ¼" src="../../../../../translated_images/tr/ascii-character-map.18ed6aa7f3b0a7ff.webp" width="50%"/>

> [GÃ¶rsel kaynaÄŸÄ±](https://www.seobility.net/en/wiki/ASCII)

Ä°nsanlar olarak, her harfin **neyi temsil ettiÄŸini** ve tÃ¼m karakterlerin bir cÃ¼mlenin kelimelerini oluÅŸturmak iÃ§in nasÄ±l bir araya geldiÄŸini anlÄ±yoruz. Ancak, bilgisayarlar bu anlayÄ±ÅŸa sahip deÄŸildir ve sinir aÄŸÄ±nÄ±n anlamÄ± eÄŸitim sÄ±rasÄ±nda Ã¶ÄŸrenmesi gerekir.

Bu nedenle, metni temsil ederken farklÄ± yaklaÅŸÄ±mlar kullanabiliriz:

* **Karakter dÃ¼zeyinde temsil**, metni her karakteri bir sayÄ± olarak ele alarak temsil ettiÄŸimizde. Metin korpusumuzda *C* farklÄ± karakter olduÄŸunu varsayarsak, *Hello* kelimesi 5x*C* tensÃ¶r olarak temsil edilir. Her harf, tek sÄ±cak kodlama ile bir tensÃ¶r sÃ¼tununa karÅŸÄ±lÄ±k gelir.
* **Kelime dÃ¼zeyinde temsil**, tÃ¼m kelimelerin bir **kelime daÄŸarcÄ±ÄŸÄ±** oluÅŸturduÄŸumuz ve ardÄ±ndan kelimeleri tek sÄ±cak kodlama ile temsil ettiÄŸimiz yÃ¶ntem. Bu yaklaÅŸÄ±m bir ÅŸekilde daha iyidir, Ã§Ã¼nkÃ¼ her harf tek baÅŸÄ±na Ã§ok fazla anlam ifade etmez ve bu nedenle daha yÃ¼ksek dÃ¼zeyde anlamsal kavramlar - kelimeler - kullanarak sinir aÄŸÄ± iÃ§in gÃ¶revi basitleÅŸtiririz. Ancak, bÃ¼yÃ¼k bir sÃ¶zlÃ¼k boyutuna sahip olduÄŸumuzda, yÃ¼ksek boyutlu seyrek tensÃ¶rlerle baÅŸa Ã§Ä±kmamÄ±z gerekir.

Temsil yÃ¶nteminden baÄŸÄ±msÄ±z olarak, Ã¶nce metni bir **token** dizisine dÃ¶nÃ¼ÅŸtÃ¼rmemiz gerekir. Bir token, bir karakter, bir kelime veya bazen bir kelimenin bir parÃ§asÄ± olabilir. Daha sonra, token'Ä± genellikle **kelime daÄŸarcÄ±ÄŸÄ±** kullanarak bir sayÄ±ya dÃ¶nÃ¼ÅŸtÃ¼rÃ¼rÃ¼z ve bu sayÄ± tek sÄ±cak kodlama ile bir sinir aÄŸÄ±na beslenebilir.

## N-Gramlar

DoÄŸal dilde, kelimelerin kesin anlamÄ± yalnÄ±zca baÄŸlam iÃ§inde belirlenebilir. Ã–rneÄŸin, *sinir aÄŸÄ±* ve *balÄ±k aÄŸÄ±* ifadelerinin anlamlarÄ± tamamen farklÄ±dÄ±r. Bunu dikkate almanÄ±n yollarÄ±ndan biri, modelimizi kelime Ã§iftleri Ã¼zerine inÅŸa etmek ve kelime Ã§iftlerini ayrÄ± kelime daÄŸarcÄ±ÄŸÄ± tokenlarÄ± olarak ele almaktÄ±r. Bu ÅŸekilde, *BalÄ±k tutmaya gitmeyi seviyorum* cÃ¼mlesi ÅŸu token dizisiyle temsil edilir: *BalÄ±k tutmayÄ±*, *tutmayÄ± gitmeyi*, *gitmeyi seviyorum*. Bu yaklaÅŸÄ±mÄ±n sorunu, sÃ¶zlÃ¼k boyutunun Ã¶nemli Ã¶lÃ§Ã¼de bÃ¼yÃ¼mesidir ve *gitmeyi seviyorum* ve *gitmeyi alÄ±ÅŸveriÅŸe* gibi kombinasyonlar, aynÄ± fiile raÄŸmen hiÃ§bir anlamsal benzerlik paylaÅŸmayan farklÄ± tokenlarla temsil edilir.

BazÄ± durumlarda, Ã¼Ã§ kelimeden oluÅŸan tri-gramlar kullanmayÄ± dÃ¼ÅŸÃ¼nebiliriz. Bu nedenle, bu yaklaÅŸÄ±m genellikle **n-gramlar** olarak adlandÄ±rÄ±lÄ±r. AyrÄ±ca, karakter dÃ¼zeyinde temsil ile n-gramlar kullanmak mantÄ±klÄ±dÄ±r, bu durumda n-gramlar kabaca farklÄ± hecelere karÅŸÄ±lÄ±k gelir.

## Kelime TorbasÄ± ve TF/IDF

Metin sÄ±nÄ±flandÄ±rma gibi gÃ¶revleri Ã§Ã¶zerken, metni sabit boyutlu bir vektÃ¶rle temsil edebilmemiz gerekir. Bu vektÃ¶rÃ¼, son yoÄŸun sÄ±nÄ±flandÄ±rÄ±cÄ±ya giriÅŸ olarak kullanacaÄŸÄ±z. Bunu yapmanÄ±n en basit yollarÄ±ndan biri, tÃ¼m bireysel kelime temsillerini birleÅŸtirmek, Ã¶rneÄŸin onlarÄ± toplayarak. Her kelimenin tek sÄ±cak kodlamalarÄ±nÄ± toplarsak, metin iÃ§inde her kelimenin kaÃ§ kez gÃ¶rÃ¼ndÃ¼ÄŸÃ¼nÃ¼ gÃ¶steren bir frekans vektÃ¶rÃ¼ elde ederiz. Bu tÃ¼r bir metin temsili **kelime torbasÄ±** (BoW) olarak adlandÄ±rÄ±lÄ±r.

<img src="../../../../../translated_images/tr/bow.3811869cff59368d.webp" width="90%"/>

> GÃ¶rsel yazar tarafÄ±ndan oluÅŸturulmuÅŸtur

Bir BoW, metinde hangi kelimelerin gÃ¶rÃ¼ndÃ¼ÄŸÃ¼nÃ¼ ve hangi miktarlarda olduÄŸunu temsil eder, bu da metnin ne hakkÄ±nda olduÄŸunu anlamak iÃ§in iyi bir gÃ¶sterge olabilir. Ã–rneÄŸin, politika Ã¼zerine bir haber makalesi muhtemelen *baÅŸkan* ve *Ã¼lke* gibi kelimeler iÃ§erirken, bilimsel bir yayÄ±n *Ã§arpÄ±ÅŸtÄ±rÄ±cÄ±*, *keÅŸfedildi* gibi kelimeler iÃ§erebilir. Bu nedenle, kelime frekanslarÄ± birÃ§ok durumda metin iÃ§eriÄŸinin iyi bir gÃ¶stergesi olabilir.

BoW'un sorunu, *ve*, *bu*, vb. gibi yaygÄ±n kelimelerin Ã§oÄŸu metinde gÃ¶rÃ¼nmesi ve en yÃ¼ksek frekanslara sahip olmasÄ±dÄ±r. Bu durum, gerÃ§ekten Ã¶nemli olan kelimeleri gÃ¶lgede bÄ±rakÄ±r. Bu kelimelerin Ã¶nemini azaltmak iÃ§in, kelimelerin tÃ¼m belge koleksiyonunda ne sÄ±klÄ±kta gÃ¶rÃ¼ndÃ¼ÄŸÃ¼nÃ¼ dikkate alabiliriz. Bu, TF/IDF yaklaÅŸÄ±mÄ±nÄ±n arkasÄ±ndaki ana fikirdir ve bu dersin ekli not defterlerinde daha ayrÄ±ntÄ±lÄ± olarak ele alÄ±nmÄ±ÅŸtÄ±r.

Ancak, bu yaklaÅŸÄ±mlarÄ±n hiÃ§biri metnin **anlamÄ±nÄ±** tam olarak dikkate alamaz. Bunu yapmak iÃ§in daha gÃ¼Ã§lÃ¼ sinir aÄŸÄ± modellerine ihtiyacÄ±mÄ±z var ve bu modelleri bu bÃ¶lÃ¼mde daha sonra tartÄ±ÅŸacaÄŸÄ±z.

## âœï¸ AlÄ±ÅŸtÄ±rmalar: Metin Temsili

AÅŸaÄŸÄ±daki not defterlerinde Ã¶ÄŸrenmeye devam edin:

* [PyTorch ile Metin Temsili](TextRepresentationPyTorch.ipynb)
* [TensorFlow ile Metin Temsili](TextRepresentationTF.ipynb)

## SonuÃ§

Åimdiye kadar, farklÄ± kelimelere frekans aÄŸÄ±rlÄ±ÄŸÄ± ekleyebilen teknikleri inceledik. Ancak, bu teknikler anlamÄ± veya sÄ±ralamayÄ± temsil edemez. ÃœnlÃ¼ dilbilimci J. R. Firth'in 1935'te sÃ¶ylediÄŸi gibi, "Bir kelimenin tam anlamÄ± her zaman baÄŸlamsaldÄ±r ve baÄŸlamdan baÄŸÄ±msÄ±z bir anlam Ã§alÄ±ÅŸmasÄ± ciddiye alÄ±namaz." Bu kursta daha sonra, dil modelleme kullanarak metinden baÄŸlamsal bilgiyi nasÄ±l yakalayacaÄŸÄ±mÄ±zÄ± Ã¶ÄŸreneceÄŸiz.

## ğŸš€ Meydan Okuma

Kelime torbasÄ± ve farklÄ± veri modelleri kullanarak baÅŸka alÄ±ÅŸtÄ±rmalar yapmayÄ± deneyin. Bu [Kaggle yarÄ±ÅŸmasÄ±ndan](https://www.kaggle.com/competitions/word2vec-nlp-tutorial/overview/part-1-for-beginners-bag-of-words) ilham alabilirsiniz.

## [Ders SonrasÄ± Testi](https://ff-quizzes.netlify.app/en/ai/quiz/26)

## GÃ¶zden GeÃ§irme ve Kendi Kendine Ã‡alÄ±ÅŸma

Microsoft Learn'de [text embeddings ve kelime torbasÄ± teknikleri](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-pytorch/?WT.mc_id=academic-77998-cacaste) ile becerilerinizi geliÅŸtirin.

## [Ã–dev: Not Defterleri](assignment.md)

---

