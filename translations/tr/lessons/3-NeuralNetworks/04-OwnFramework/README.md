<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "186bf7eeab776b36f557357ea56d4751",
  "translation_date": "2025-08-26T07:34:19+00:00",
  "source_file": "lessons/3-NeuralNetworks/04-OwnFramework/README.md",
  "language_code": "tr"
}
-->
# Sinir AÄŸlarÄ±na GiriÅŸ. Ã‡ok KatmanlÄ± AlgÄ±layÄ±cÄ±

Ã–nceki bÃ¶lÃ¼mde, en basit sinir aÄŸÄ± modeli olan tek katmanlÄ± algÄ±layÄ±cÄ±yÄ±, iki sÄ±nÄ±flÄ± doÄŸrusal sÄ±nÄ±flandÄ±rma modelini Ã¶ÄŸrendiniz.

Bu bÃ¶lÃ¼mde, bu modeli daha esnek bir Ã§erÃ§eveye geniÅŸleteceÄŸiz ve bÃ¶ylece:

* **Ã‡ok sÄ±nÄ±flÄ± sÄ±nÄ±flandÄ±rma** yapabilmenin yanÄ± sÄ±ra iki sÄ±nÄ±flÄ± sÄ±nÄ±flandÄ±rma yapabiliriz
* SÄ±nÄ±flandÄ±rmanÄ±n yanÄ± sÄ±ra **regresyon problemlerini** Ã§Ã¶zebiliriz
* DoÄŸrusal olarak ayrÄ±labilir olmayan sÄ±nÄ±flarÄ± ayÄ±rabiliriz

AyrÄ±ca, farklÄ± sinir aÄŸÄ± mimarileri oluÅŸturabilmemizi saÄŸlayacak kendi modÃ¼ler Python Ã§erÃ§evemizi geliÅŸtireceÄŸiz.

## [Ders Ã–ncesi Testi](https://ff-quizzes.netlify.app/en/ai/quiz/7)

## Makine Ã–ÄŸreniminin Formalizasyonu

Makine Ã–ÄŸrenimi problemini formalize ederek baÅŸlayalÄ±m. Diyelim ki **X** eÄŸitim veri setimiz ve **Y** etiketlerimiz var ve en doÄŸru tahminleri yapacak bir model *f* oluÅŸturmalÄ±yÄ±z. Tahminlerin kalitesi **KayÄ±p Fonksiyonu** â„’ ile Ã¶lÃ§Ã¼lÃ¼r. SÄ±kÃ§a kullanÄ±lan kayÄ±p fonksiyonlarÄ± ÅŸunlardÄ±r:

* Regresyon problemleri iÃ§in, bir sayÄ± tahmin etmemiz gerektiÄŸinde, **mutlak hata** âˆ‘<sub>i</sub>|f(x<sup>(i)</sup>)-y<sup>(i)</sup>| veya **kare hata** âˆ‘<sub>i</sub>(f(x<sup>(i)</sup>)-y<sup>(i)</sup>)<sup>2</sup> kullanabiliriz.
* SÄ±nÄ±flandÄ±rma iÃ§in, **0-1 kaybÄ±** (temelde modelin **doÄŸruluÄŸu** ile aynÄ±dÄ±r) veya **lojistik kayÄ±p** kullanÄ±rÄ±z.

Tek katmanlÄ± algÄ±layÄ±cÄ± iÃ§in, *f* fonksiyonu doÄŸrusal bir fonksiyon olarak tanÄ±mlanmÄ±ÅŸtÄ±: *f(x)=wx+b* (burada *w* aÄŸÄ±rlÄ±k matrisi, *x* giriÅŸ Ã¶zelliklerinin vektÃ¶rÃ¼ ve *b* Ã¶nyargÄ± vektÃ¶rÃ¼dÃ¼r). FarklÄ± sinir aÄŸÄ± mimarileri iÃ§in, bu fonksiyon daha karmaÅŸÄ±k bir form alabilir.

> SÄ±nÄ±flandÄ±rma durumunda, aÄŸ Ã§Ä±ktÄ±sÄ± olarak ilgili sÄ±nÄ±flarÄ±n olasÄ±lÄ±klarÄ±nÄ± elde etmek genellikle arzu edilir. Rastgele sayÄ±larÄ± olasÄ±lÄ±klara dÃ¶nÃ¼ÅŸtÃ¼rmek (Ã¶rneÄŸin Ã§Ä±ktÄ±yÄ± normalleÅŸtirmek) iÃ§in genellikle **softmax** fonksiyonu Ïƒ kullanÄ±lÄ±r ve *f* fonksiyonu *f(x)=Ïƒ(wx+b)* olur.

YukarÄ±daki *f* tanÄ±mÄ±nda, *w* ve *b* **parametreler** olarak adlandÄ±rÄ±lÄ±r: Î¸=âŸ¨*w,b*âŸ©. Veri seti âŸ¨**X**,**Y**âŸ© verildiÄŸinde, tÃ¼m veri setindeki genel hatayÄ± parametreler Î¸'nin bir fonksiyonu olarak hesaplayabiliriz.

> âœ… **Sinir aÄŸÄ± eÄŸitiminin amacÄ±, parametreler Î¸'yi deÄŸiÅŸtirerek hatayÄ± minimize etmektir.**

## Gradyan Ä°niÅŸi Optimizasyonu

**Gradyan iniÅŸi** adÄ± verilen iyi bilinen bir fonksiyon optimizasyon yÃ¶ntemi vardÄ±r. Fikir, kayÄ±p fonksiyonunun parametrelere gÃ¶re tÃ¼revini (Ã§ok boyutlu durumda **gradyan** olarak adlandÄ±rÄ±lÄ±r) hesaplayabileceÄŸimiz ve parametreleri hatayÄ± azaltacak ÅŸekilde deÄŸiÅŸtirebileceÄŸimizdir. Bu ÅŸu ÅŸekilde formalize edilebilir:

* Parametreleri rastgele deÄŸerlerle baÅŸlatÄ±n: w<sup>(0)</sup>, b<sup>(0)</sup>
* Åu adÄ±mÄ± birÃ§ok kez tekrarlayÄ±n:
    - w<sup>(i+1)</sup> = w<sup>(i)</sup>-Î·âˆ‚â„’/âˆ‚w
    - b<sup>(i+1)</sup> = b<sup>(i)</sup>-Î·âˆ‚â„’/âˆ‚b

EÄŸitim sÄ±rasÄ±nda, optimizasyon adÄ±mlarÄ± tÃ¼m veri seti dikkate alÄ±narak hesaplanmalÄ±dÄ±r (unutmayÄ±n ki kayÄ±p, tÃ¼m eÄŸitim Ã¶rnekleri Ã¼zerinden bir toplam olarak hesaplanÄ±r). Ancak, gerÃ§ek hayatta veri setinin kÃ¼Ã§Ã¼k bÃ¶lÃ¼mleri olan **minibatch'ler** alÄ±rÄ±z ve gradyanlarÄ± bir alt veri kÃ¼mesine dayanarak hesaplarÄ±z. Alt kÃ¼me her seferinde rastgele alÄ±ndÄ±ÄŸÄ± iÃ§in, bu yÃ¶nteme **stokastik gradyan iniÅŸi** (SGD) denir.

## Ã‡ok KatmanlÄ± AlgÄ±layÄ±cÄ±lar ve Geri YayÄ±lÄ±m

Tek katmanlÄ± aÄŸ, yukarÄ±da gÃ¶rdÃ¼ÄŸÃ¼mÃ¼z gibi, doÄŸrusal olarak ayrÄ±labilir sÄ±nÄ±flarÄ± sÄ±nÄ±flandÄ±rabilir. Daha zengin bir model oluÅŸturmak iÃ§in, aÄŸÄ±n birkaÃ§ katmanÄ±nÄ± birleÅŸtirebiliriz. Matematiksel olarak bu, *f* fonksiyonunun daha karmaÅŸÄ±k bir form alacaÄŸÄ± ve birkaÃ§ adÄ±mda hesaplanacaÄŸÄ± anlamÄ±na gelir:
* z<sub>1</sub>=w<sub>1</sub>x+b<sub>1</sub>
* z<sub>2</sub>=w<sub>2</sub>Î±(z<sub>1</sub>)+b<sub>2</sub>
* f = Ïƒ(z<sub>2</sub>)

Burada, Î± bir **doÄŸrusal olmayan aktivasyon fonksiyonu**, Ïƒ bir softmax fonksiyonu ve parametreler Î¸=<*w<sub>1</sub>,b<sub>1</sub>,w<sub>2</sub>,b<sub>2</sub>*>'dir.

Gradyan iniÅŸi algoritmasÄ± aynÄ± kalÄ±r, ancak gradyanlarÄ± hesaplamak daha zor olur. Zincir tÃ¼revleme kuralÄ± sayesinde, tÃ¼revleri ÅŸu ÅŸekilde hesaplayabiliriz:

* âˆ‚â„’/âˆ‚w<sub>2</sub> = (âˆ‚â„’/âˆ‚Ïƒ)(âˆ‚Ïƒ/âˆ‚z<sub>2</sub>)(âˆ‚z<sub>2</sub>/âˆ‚w<sub>2</sub>)
* âˆ‚â„’/âˆ‚w<sub>1</sub> = (âˆ‚â„’/âˆ‚Ïƒ)(âˆ‚Ïƒ/âˆ‚z<sub>2</sub>)(âˆ‚z<sub>2</sub>/âˆ‚Î±)(âˆ‚Î±/âˆ‚z<sub>1</sub>)(âˆ‚z<sub>1</sub>/âˆ‚w<sub>1</sub>)

> âœ… Zincir tÃ¼revleme kuralÄ±, kayÄ±p fonksiyonunun parametrelere gÃ¶re tÃ¼revlerini hesaplamak iÃ§in kullanÄ±lÄ±r.

Bu ifadelerin en sol kÄ±smÄ± aynÄ± olduÄŸundan, tÃ¼revleri kayÄ±p fonksiyonundan baÅŸlayarak ve hesaplama grafiÄŸi boyunca "geri giderek" etkili bir ÅŸekilde hesaplayabiliriz. Bu nedenle, Ã§ok katmanlÄ± bir algÄ±layÄ±cÄ±yÄ± eÄŸitme yÃ¶ntemi **geri yayÄ±lÄ±m** veya 'backprop' olarak adlandÄ±rÄ±lÄ±r.

<img alt="hesaplama grafiÄŸi" src="images/ComputeGraphGrad.png"/>

> TODO: gÃ¶rsel kaynaÄŸÄ±

> âœ… Geri yayÄ±lÄ±mÄ±, notebook Ã¶rneÄŸimizde Ã§ok daha ayrÄ±ntÄ±lÄ± bir ÅŸekilde ele alacaÄŸÄ±z.  

## SonuÃ§

Bu derste, kendi sinir aÄŸÄ± kÃ¼tÃ¼phanemizi oluÅŸturduk ve bunu basit bir iki boyutlu sÄ±nÄ±flandÄ±rma gÃ¶revi iÃ§in kullandÄ±k.

## ğŸš€ Meydan Okuma

EÅŸlik eden notebook'ta, Ã§ok katmanlÄ± algÄ±layÄ±cÄ±lar oluÅŸturmak ve eÄŸitmek iÃ§in kendi Ã§erÃ§evenizi uygulayacaksÄ±nÄ±z. Modern sinir aÄŸlarÄ±nÄ±n nasÄ±l Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ± ayrÄ±ntÄ±lÄ± bir ÅŸekilde gÃ¶rebileceksiniz.

[OwnFramework](../../../../../lessons/3-NeuralNetworks/04-OwnFramework/OwnFramework.ipynb) notebook'una geÃ§in ve Ã¼zerinde Ã§alÄ±ÅŸÄ±n.

## [Ders SonrasÄ± Testi](https://ff-quizzes.netlify.app/en/ai/quiz/8)

## Ä°nceleme ve Kendi Kendine Ã‡alÄ±ÅŸma

Geri yayÄ±lÄ±m, AI ve ML'de yaygÄ±n olarak kullanÄ±lan bir algoritmadÄ±r ve [daha ayrÄ±ntÄ±lÄ±](https://wikipedia.org/wiki/Backpropagation) Ã§alÄ±ÅŸmaya deÄŸerdir.

## [Ã–dev](lab/README.md)

Bu laboratuvarda, bu derste oluÅŸturduÄŸunuz Ã§erÃ§eveyi kullanarak MNIST el yazÄ±sÄ± rakam sÄ±nÄ±flandÄ±rmasÄ±nÄ± Ã§Ã¶zmeniz isteniyor.

* [Talimatlar](lab/README.md)
* [Notebook](../../../../../lessons/3-NeuralNetworks/04-OwnFramework/lab/MyFW_MNIST.ipynb)

**Feragatname**:  
Bu belge, AI Ã§eviri hizmeti [Co-op Translator](https://github.com/Azure/co-op-translator) kullanÄ±larak Ã§evrilmiÅŸtir. DoÄŸruluk iÃ§in Ã§aba gÃ¶stersek de, otomatik Ã§evirilerin hata veya yanlÄ±ÅŸlÄ±klar iÃ§erebileceÄŸini lÃ¼tfen unutmayÄ±n. Belgenin orijinal dili, yetkili kaynak olarak kabul edilmelidir. Kritik bilgiler iÃ§in profesyonel insan Ã§evirisi Ã¶nerilir. Bu Ã§evirinin kullanÄ±mÄ±ndan kaynaklanan yanlÄ±ÅŸ anlamalar veya yanlÄ±ÅŸ yorumlamalar iÃ§in sorumluluk kabul etmiyoruz.