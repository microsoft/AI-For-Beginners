<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "2b544f20b796402507fb05a0df893323",
  "translation_date": "2025-08-26T07:34:59+00:00",
  "source_file": "lessons/3-NeuralNetworks/05-Frameworks/README.md",
  "language_code": "tr"
}
-->
# Sinir AÄŸÄ± Ã‡erÃ§eveleri

Daha Ã¶nce Ã¶ÄŸrendiÄŸimiz gibi, sinir aÄŸlarÄ±nÄ± verimli bir ÅŸekilde eÄŸitebilmek iÃ§in iki ÅŸeyi yapmamÄ±z gerekiyor:

* TensÃ¶rler Ã¼zerinde iÅŸlem yapmak, Ã¶rneÄŸin Ã§arpma, toplama ve sigmoid veya softmax gibi bazÄ± fonksiyonlarÄ± hesaplamak
* TÃ¼m ifadelerin gradyanlarÄ±nÄ± hesaplamak, bÃ¶ylece gradyan iniÅŸi optimizasyonunu gerÃ§ekleÅŸtirebilmek

## [Ders Ã–ncesi Test](https://ff-quizzes.netlify.app/en/ai/quiz/9)

`numpy` kÃ¼tÃ¼phanesi ilk kÄ±smÄ± yapabilse de, gradyanlarÄ± hesaplayacak bir mekanizmaya ihtiyacÄ±mÄ±z var. Ã–nceki bÃ¶lÃ¼mde geliÅŸtirdiÄŸimiz [kendi Ã§erÃ§evemizde](../../../../../lessons/3-NeuralNetworks/04-OwnFramework/OwnFramework.ipynb), tÃ¼m tÃ¼rev fonksiyonlarÄ±nÄ± `backward` metodunun iÃ§ine manuel olarak programlamamÄ±z gerekiyordu. Bu yÃ¶ntem geri yayÄ±lÄ±mÄ± gerÃ§ekleÅŸtirir. Ä°deal olarak, bir Ã§erÃ§eve bize tanÄ±mlayabileceÄŸimiz *herhangi bir ifadenin* gradyanlarÄ±nÄ± hesaplama fÄ±rsatÄ± sunmalÄ±dÄ±r.

Bir diÄŸer Ã¶nemli ÅŸey, GPU veya [TPU](https://en.wikipedia.org/wiki/Tensor_Processing_Unit) gibi diÄŸer Ã¶zel iÅŸlem birimlerinde hesaplama yapabilmektir. Derin sinir aÄŸÄ± eÄŸitimi *Ã§ok fazla* hesaplama gerektirir ve bu hesaplamalarÄ± GPU'lar Ã¼zerinde paralelleÅŸtirebilmek oldukÃ§a Ã¶nemlidir.

> âœ… 'ParalelleÅŸtirme' terimi, hesaplamalarÄ±n birden fazla cihaz arasÄ±nda daÄŸÄ±tÄ±lmasÄ± anlamÄ±na gelir.

Åu anda en popÃ¼ler iki sinir aÄŸÄ± Ã§erÃ§evesi: [TensorFlow](http://TensorFlow.org) ve [PyTorch](https://pytorch.org/). Her ikisi de CPU ve GPU Ã¼zerinde tensÃ¶rlerle Ã§alÄ±ÅŸmak iÃ§in dÃ¼ÅŸÃ¼k seviyeli bir API saÄŸlar. DÃ¼ÅŸÃ¼k seviyeli API'nin yanÄ± sÄ±ra, sÄ±rasÄ±yla [Keras](https://keras.io/) ve [PyTorch Lightning](https://pytorchlightning.ai/) adÄ± verilen yÃ¼ksek seviyeli bir API de mevcuttur.

DÃ¼ÅŸÃ¼k Seviyeli API | [TensorFlow](http://TensorFlow.org) | [PyTorch](https://pytorch.org/)
--------------------|-------------------------------------|--------------------------------
YÃ¼ksek Seviyeli API | [Keras](https://keras.io/) | [PyTorch Lightning](https://pytorchlightning.ai/)

**DÃ¼ÅŸÃ¼k seviyeli API'ler**, her iki Ã§erÃ§evede de **hesaplama grafikleri** oluÅŸturmanÄ±za olanak tanÄ±r. Bu grafik, verilen giriÅŸ parametreleriyle Ã§Ä±ktÄ±nÄ±n (genellikle kayÄ±p fonksiyonu) nasÄ±l hesaplanacaÄŸÄ±nÄ± tanÄ±mlar ve GPU'da hesaplama iÃ§in gÃ¶nderilebilir. Bu hesaplama grafiÄŸini tÃ¼retmek ve gradyanlarÄ± hesaplamak iÃ§in fonksiyonlar vardÄ±r; bu gradyanlar daha sonra model parametrelerini optimize etmek iÃ§in kullanÄ±labilir.

**YÃ¼ksek seviyeli API'ler**, sinir aÄŸlarÄ±nÄ± genellikle bir **katmanlar dizisi** olarak ele alÄ±r ve Ã§oÄŸu sinir aÄŸÄ±nÄ± oluÅŸturmayÄ± Ã§ok daha kolay hale getirir. Modeli eÄŸitmek genellikle verileri hazÄ±rlamayÄ± ve ardÄ±ndan iÅŸi yapmak iÃ§in bir `fit` fonksiyonu Ã§aÄŸÄ±rmayÄ± gerektirir.

YÃ¼ksek seviyeli API, tipik sinir aÄŸlarÄ±nÄ± Ã§ok hÄ±zlÄ± bir ÅŸekilde oluÅŸturmanÄ±za olanak tanÄ±r ve birÃ§ok ayrÄ±ntÄ±yla uÄŸraÅŸmanÄ±za gerek kalmaz. AynÄ± zamanda, dÃ¼ÅŸÃ¼k seviyeli API, eÄŸitim sÃ¼reci Ã¼zerinde Ã§ok daha fazla kontrol saÄŸlar ve bu nedenle yeni sinir aÄŸÄ± mimarileriyle Ã§alÄ±ÅŸÄ±rken araÅŸtÄ±rmalarda sÄ±kÃ§a kullanÄ±lÄ±r.

AyrÄ±ca, her iki API'yi birlikte kullanabileceÄŸinizi anlamak Ã¶nemlidir. Ã–rneÄŸin, dÃ¼ÅŸÃ¼k seviyeli API kullanarak kendi aÄŸ katmanÄ± mimarinizi geliÅŸtirebilir ve ardÄ±ndan bunu yÃ¼ksek seviyeli API ile oluÅŸturulan ve eÄŸitilen daha bÃ¼yÃ¼k bir aÄŸÄ±n iÃ§inde kullanabilirsiniz. Ya da katmanlar dizisi olarak yÃ¼ksek seviyeli API kullanarak bir aÄŸ tanÄ±mlayabilir ve ardÄ±ndan kendi dÃ¼ÅŸÃ¼k seviyeli eÄŸitim dÃ¶ngÃ¼nÃ¼zÃ¼ kullanarak optimizasyon yapabilirsiniz. Her iki API de aynÄ± temel kavramlarÄ± kullanÄ±r ve birlikte iyi Ã§alÄ±ÅŸacak ÅŸekilde tasarlanmÄ±ÅŸtÄ±r.

## Ã–ÄŸrenme

Bu kursta, iÃ§eriÄŸin Ã§oÄŸunu hem PyTorch hem de TensorFlow iÃ§in sunuyoruz. Tercih ettiÄŸiniz Ã§erÃ§eveyi seÃ§ebilir ve yalnÄ±zca ilgili not defterlerini inceleyebilirsiniz. Hangi Ã§erÃ§eveyi seÃ§eceÄŸinizden emin deÄŸilseniz, **PyTorch vs. TensorFlow** hakkÄ±nda internetteki bazÄ± tartÄ±ÅŸmalarÄ± okuyabilirsiniz. AyrÄ±ca, her iki Ã§erÃ§eveye de gÃ¶z atarak daha iyi bir anlayÄ±ÅŸ kazanabilirsiniz.

MÃ¼mkÃ¼n olduÄŸunda, basitlik iÃ§in YÃ¼ksek Seviyeli API'leri kullanacaÄŸÄ±z. Ancak, sinir aÄŸlarÄ±nÄ±n temelden nasÄ±l Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ± anlamanÄ±n Ã¶nemli olduÄŸuna inanÄ±yoruz, bu nedenle baÅŸlangÄ±Ã§ta dÃ¼ÅŸÃ¼k seviyeli API ve tensÃ¶rlerle Ã§alÄ±ÅŸmaya baÅŸlÄ±yoruz. Ancak, hÄ±zlÄ± bir ÅŸekilde baÅŸlamak ve bu ayrÄ±ntÄ±larÄ± Ã¶ÄŸrenmek iÃ§in fazla zaman harcamak istemiyorsanÄ±z, bunlarÄ± atlayabilir ve doÄŸrudan yÃ¼ksek seviyeli API not defterlerine geÃ§ebilirsiniz.

## âœï¸ AlÄ±ÅŸtÄ±rmalar: Ã‡erÃ§eveler

Ã–ÄŸreniminize aÅŸaÄŸÄ±daki not defterlerinde devam edin:

DÃ¼ÅŸÃ¼k Seviyeli API | [TensorFlow+Keras Not Defteri](../../../../../lessons/3-NeuralNetworks/05-Frameworks/IntroKerasTF.ipynb) | [PyTorch](../../../../../lessons/3-NeuralNetworks/05-Frameworks/IntroPyTorch.ipynb)
--------------------|-------------------------------------|--------------------------------
YÃ¼ksek Seviyeli API | [Keras](../../../../../lessons/3-NeuralNetworks/05-Frameworks/IntroKeras.ipynb) | *PyTorch Lightning*

Ã‡erÃ§eveleri Ã¶ÄŸrendikten sonra, aÅŸÄ±rÄ± Ã¶ÄŸrenme (overfitting) kavramÄ±nÄ± tekrar gÃ¶zden geÃ§irelim.

# AÅŸÄ±rÄ± Ã–ÄŸrenme

AÅŸÄ±rÄ± Ã¶ÄŸrenme, makine Ã¶ÄŸreniminde son derece Ã¶nemli bir kavramdÄ±r ve doÄŸru bir ÅŸekilde anlaÅŸÄ±lmasÄ± Ã§ok Ã¶nemlidir!

AÅŸaÄŸÄ±daki 5 noktayÄ± (grafiklerde `x` ile gÃ¶sterilen) yaklaÅŸÄ±k olarak tahmin etme problemini dÃ¼ÅŸÃ¼nÃ¼n:

![linear](../../../../../translated_images/overfit1.f24b71c6f652e59e6bed7245ffbeaecc3ba320e16e2221f6832b432052c4da43.tr.jpg) | ![overfit](../../../../../translated_images/overfit2.131f5800ae10ca5e41d12a411f5f705d9ee38b1b10916f284b787028dd55cc1c.tr.jpg)
-------------------------|--------------------------
**DoÄŸrusal model, 2 parametre** | **DoÄŸrusal olmayan model, 7 parametre**
EÄŸitim hatasÄ± = 5.3 | EÄŸitim hatasÄ± = 0
DoÄŸrulama hatasÄ± = 5.1 | DoÄŸrulama hatasÄ± = 20

* Solda, iyi bir doÄŸru Ã§izgisi yaklaÅŸÄ±mÄ± gÃ¶rÃ¼yoruz. Parametre sayÄ±sÄ± yeterli olduÄŸu iÃ§in model, nokta daÄŸÄ±lÄ±mÄ±nÄ±n arkasÄ±ndaki fikri doÄŸru bir ÅŸekilde kavrÄ±yor.
* SaÄŸda, model Ã§ok gÃ¼Ã§lÃ¼. Sadece 5 noktamÄ±z olduÄŸu ve modelin 7 parametresi olduÄŸu iÃ§in, tÃ¼m noktalardan geÃ§ecek ÅŸekilde ayarlanabiliyor ve bu da eÄŸitim hatasÄ±nÄ± 0 yapÄ±yor. Ancak, bu durum modelin verilerin arkasÄ±ndaki doÄŸru deseni anlamasÄ±nÄ± engelliyor ve bu nedenle doÄŸrulama hatasÄ± Ã§ok yÃ¼ksek oluyor.

Modelin zenginliÄŸi (parametre sayÄ±sÄ±) ile eÄŸitim Ã¶rneklerinin sayÄ±sÄ± arasÄ±nda doÄŸru bir denge kurmak Ã§ok Ã¶nemlidir.

## AÅŸÄ±rÄ± Ã–ÄŸrenme Neden OluÅŸur?

  * Yeterli eÄŸitim verisinin olmamasÄ±
  * Ã‡ok gÃ¼Ã§lÃ¼ bir model
  * GiriÅŸ verilerinde Ã§ok fazla gÃ¼rÃ¼ltÃ¼

## AÅŸÄ±rÄ± Ã–ÄŸrenme NasÄ±l Tespit Edilir?

YukarÄ±daki grafikten de gÃ¶rebileceÄŸiniz gibi, aÅŸÄ±rÄ± Ã¶ÄŸrenme Ã§ok dÃ¼ÅŸÃ¼k bir eÄŸitim hatasÄ± ve yÃ¼ksek bir doÄŸrulama hatasÄ± ile tespit edilebilir. Normalde eÄŸitim sÄ±rasÄ±nda hem eÄŸitim hem de doÄŸrulama hatalarÄ±nÄ±n azalmaya baÅŸladÄ±ÄŸÄ±nÄ± gÃ¶rÃ¼rÃ¼z ve ardÄ±ndan bir noktada doÄŸrulama hatasÄ± azalmayÄ± durdurup artmaya baÅŸlayabilir. Bu, aÅŸÄ±rÄ± Ã¶ÄŸrenmenin bir iÅŸareti olacak ve eÄŸitimi muhtemelen bu noktada durdurmamÄ±z gerektiÄŸini (veya en azÄ±ndan modelin bir anlÄ±k gÃ¶rÃ¼ntÃ¼sÃ¼nÃ¼ almamÄ±z gerektiÄŸini) gÃ¶sterecektir.

![overfitting](../../../../../translated_images/Overfitting.408ad91cd90b4371d0a81f4287e1409c359751adeb1ae450332af50e84f08c3e.tr.png)

## AÅŸÄ±rÄ± Ã–ÄŸrenme NasÄ±l Ã–nlenir?

AÅŸÄ±rÄ± Ã¶ÄŸrenmenin meydana geldiÄŸini gÃ¶rÃ¼yorsanÄ±z, aÅŸaÄŸÄ±dakilerden birini yapabilirsiniz:

 * EÄŸitim verilerinin miktarÄ±nÄ± artÄ±rÄ±n
 * Modelin karmaÅŸÄ±klÄ±ÄŸÄ±nÄ± azaltÄ±n
 * [Dropout](../../4-ComputerVision/08-TransferLearning/TrainingTricks.md#Dropout) gibi bazÄ± [dÃ¼zenleme tekniklerini](../../4-ComputerVision/08-TransferLearning/TrainingTricks.md) kullanÄ±n. BunlarÄ± daha sonra ele alacaÄŸÄ±z.

## AÅŸÄ±rÄ± Ã–ÄŸrenme ve YanlÄ±lÄ±k-Varyans Dengesi

AÅŸÄ±rÄ± Ã¶ÄŸrenme, aslÄ±nda istatistikte [YanlÄ±lÄ±k-Varyans Dengesi](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff) adÄ± verilen daha genel bir problemin bir Ã¶rneÄŸidir. Modelimizdeki hata kaynaklarÄ±nÄ± dÃ¼ÅŸÃ¼ndÃ¼ÄŸÃ¼mÃ¼zde, iki tÃ¼r hata gÃ¶rebiliriz:

* **YanlÄ±lÄ±k hatalarÄ±**, algoritmamÄ±zÄ±n eÄŸitim verileri arasÄ±ndaki iliÅŸkiyi doÄŸru bir ÅŸekilde yakalayamamasÄ±ndan kaynaklanÄ±r. Bu, modelimizin yeterince gÃ¼Ã§lÃ¼ olmamasÄ±ndan (**eksik Ã¶ÄŸrenme**) kaynaklanabilir.
* **Varyans hatalarÄ±**, modelin giriÅŸ verilerindeki gÃ¼rÃ¼ltÃ¼yÃ¼ anlamlÄ± bir iliÅŸki yerine yaklaÅŸÄ±k olarak tahmin etmesinden kaynaklanÄ±r (**aÅŸÄ±rÄ± Ã¶ÄŸrenme**).

EÄŸitim sÄ±rasÄ±nda, yanlÄ±lÄ±k hatasÄ± azalÄ±r (modelimiz verileri yaklaÅŸÄ±k olarak Ã¶ÄŸrenir) ve varyans hatasÄ± artar. AÅŸÄ±rÄ± Ã¶ÄŸrenmeyi Ã¶nlemek iÃ§in eÄŸitimi - ya manuel olarak (aÅŸÄ±rÄ± Ã¶ÄŸrenmeyi tespit ettiÄŸimizde) ya da otomatik olarak (dÃ¼zenleme teknikleri kullanarak) - durdurmak Ã¶nemlidir.

## SonuÃ§

Bu derste, iki en popÃ¼ler yapay zeka Ã§erÃ§evesi olan TensorFlow ve PyTorch'un Ã§eÅŸitli API'leri arasÄ±ndaki farklarÄ± Ã¶ÄŸrendiniz. AyrÄ±ca, Ã§ok Ã¶nemli bir konu olan aÅŸÄ±rÄ± Ã¶ÄŸrenme hakkÄ±nda bilgi edindiniz.

## ğŸš€ Zorluk

EÅŸlik eden not defterlerinde, 'gÃ¶revler' bÃ¶lÃ¼mÃ¼nÃ¼ bulacaksÄ±nÄ±z; not defterlerini inceleyin ve gÃ¶revleri tamamlayÄ±n.

## [Ders SonrasÄ± Test](https://ff-quizzes.netlify.app/en/ai/quiz/10)

## GÃ¶zden GeÃ§irme ve Kendi Kendine Ã‡alÄ±ÅŸma

AÅŸaÄŸÄ±daki konular hakkÄ±nda biraz araÅŸtÄ±rma yapÄ±n:

- TensorFlow
- PyTorch
- AÅŸÄ±rÄ± Ã¶ÄŸrenme

Kendinize ÅŸu sorularÄ± sorun:

- TensorFlow ve PyTorch arasÄ±ndaki fark nedir?
- AÅŸÄ±rÄ± Ã¶ÄŸrenme ve eksik Ã¶ÄŸrenme arasÄ±ndaki fark nedir?

## [Ã–dev](lab/README.md)

Bu laboratuvarda, PyTorch veya TensorFlow kullanarak tek katmanlÄ± ve Ã§ok katmanlÄ± tam baÄŸlantÄ±lÄ± aÄŸlarla iki sÄ±nÄ±flandÄ±rma problemini Ã§Ã¶zmeniz isteniyor.

* [Talimatlar](lab/README.md)
* [Not Defteri](../../../../../lessons/3-NeuralNetworks/05-Frameworks/lab/LabFrameworks.ipynb)

**Feragatname**:  
Bu belge, AI Ã§eviri hizmeti [Co-op Translator](https://github.com/Azure/co-op-translator) kullanÄ±larak Ã§evrilmiÅŸtir. DoÄŸruluk iÃ§in Ã§aba gÃ¶stersek de, otomatik Ã§evirilerin hata veya yanlÄ±ÅŸlÄ±klar iÃ§erebileceÄŸini lÃ¼tfen unutmayÄ±n. Belgenin orijinal dili, yetkili kaynak olarak kabul edilmelidir. Kritik bilgiler iÃ§in profesyonel insan Ã§evirisi Ã¶nerilir. Bu Ã§evirinin kullanÄ±mÄ±ndan kaynaklanan yanlÄ±ÅŸ anlamalar veya yanlÄ±ÅŸ yorumlamalar iÃ§in sorumluluk kabul etmiyoruz.