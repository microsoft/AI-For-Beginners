# Sinir AÄŸÄ± Ã‡erÃ§eveleri

Daha Ã¶nce Ã¶ÄŸrendiÄŸimiz gibi, sinir aÄŸlarÄ±nÄ± verimli bir ÅŸekilde eÄŸitebilmek iÃ§in iki ÅŸeyi yapmamÄ±z gerekiyor:

* TensÃ¶rler Ã¼zerinde iÅŸlem yapmak, Ã¶rneÄŸin Ã§arpma, toplama ve sigmoid veya softmax gibi bazÄ± fonksiyonlarÄ± hesaplamak
* TÃ¼m ifadelerin gradyanlarÄ±nÄ± hesaplamak, bÃ¶ylece gradyan iniÅŸi optimizasyonu gerÃ§ekleÅŸtirebilmek

## [Ders Ã–ncesi Test](https://ff-quizzes.netlify.app/en/ai/quiz/9)

`numpy` kÃ¼tÃ¼phanesi ilk kÄ±smÄ± yapabilse de, gradyanlarÄ± hesaplamak iÃ§in bir mekanizmaya ihtiyacÄ±mÄ±z var. Ã–nceki bÃ¶lÃ¼mde geliÅŸtirdiÄŸimiz [kendi Ã§erÃ§evemizde](../04-OwnFramework/OwnFramework.ipynb), `backward` metodunun iÃ§inde tÃ¼m tÃ¼rev fonksiyonlarÄ±nÄ± manuel olarak programlamamÄ±z gerekiyordu. Ä°deal olarak, bir Ã§erÃ§eve bize tanÄ±mlayabileceÄŸimiz *herhangi bir ifadenin* gradyanlarÄ±nÄ± hesaplama fÄ±rsatÄ± sunmalÄ±dÄ±r.

Bir diÄŸer Ã¶nemli ÅŸey ise GPU veya [TPU](https://en.wikipedia.org/wiki/Tensor_Processing_Unit) gibi diÄŸer Ã¶zel iÅŸlem birimlerinde hesaplama yapabilmektir. Derin sinir aÄŸÄ± eÄŸitimi *Ã§ok fazla* hesaplama gerektirir ve bu hesaplamalarÄ± GPU'larda paralelleÅŸtirebilmek Ã§ok Ã¶nemlidir.

> âœ… 'ParalelleÅŸtirme' terimi, hesaplamalarÄ± birden fazla cihaz arasÄ±nda daÄŸÄ±tmak anlamÄ±na gelir.

Åu anda en popÃ¼ler iki sinir aÄŸÄ± Ã§erÃ§evesi: [TensorFlow](http://TensorFlow.org) ve [PyTorch](https://pytorch.org/). Her ikisi de hem CPU hem de GPU Ã¼zerinde tensÃ¶rlerle Ã§alÄ±ÅŸmak iÃ§in dÃ¼ÅŸÃ¼k seviyeli bir API saÄŸlar. DÃ¼ÅŸÃ¼k seviyeli API'nin yanÄ± sÄ±ra, sÄ±rasÄ±yla [Keras](https://keras.io/) ve [PyTorch Lightning](https://pytorchlightning.ai/) olarak adlandÄ±rÄ±lan yÃ¼ksek seviyeli bir API de bulunmaktadÄ±r.

DÃ¼ÅŸÃ¼k Seviyeli API | [TensorFlow](http://TensorFlow.org) | [PyTorch](https://pytorch.org/)
--------------------|-------------------------------------|--------------------------------
YÃ¼ksek Seviyeli API | [Keras](https://keras.io/) | [PyTorch Lightning](https://pytorchlightning.ai/)

**DÃ¼ÅŸÃ¼k seviyeli API'ler**, her iki Ã§erÃ§evede de **hesaplama grafikleri** oluÅŸturmanÄ±za olanak tanÄ±r. Bu grafik, verilen giriÅŸ parametreleriyle Ã§Ä±ktÄ±nÄ±n (genellikle kayÄ±p fonksiyonu) nasÄ±l hesaplanacaÄŸÄ±nÄ± tanÄ±mlar ve GPU'da hesaplama iÃ§in gÃ¶nderilebilir. Bu hesaplama grafiÄŸini tÃ¼retmek ve gradyanlarÄ± hesaplamak iÃ§in fonksiyonlar vardÄ±r, bu gradyanlar daha sonra model parametrelerini optimize etmek iÃ§in kullanÄ±labilir.

**YÃ¼ksek seviyeli API'ler**, sinir aÄŸlarÄ±nÄ± genellikle bir **katmanlar dizisi** olarak ele alÄ±r ve Ã§oÄŸu sinir aÄŸÄ±nÄ± oluÅŸturmayÄ± Ã§ok daha kolay hale getirir. Modeli eÄŸitmek genellikle veriyi hazÄ±rlamayÄ± ve ardÄ±ndan iÅŸi yapmak iÃ§in bir `fit` fonksiyonunu Ã§aÄŸÄ±rmayÄ± gerektirir.

YÃ¼ksek seviyeli API, tipik sinir aÄŸlarÄ±nÄ± Ã§ok hÄ±zlÄ± bir ÅŸekilde oluÅŸturmanÄ±za olanak tanÄ±r ve birÃ§ok detayÄ± dÃ¼ÅŸÃ¼nmenize gerek kalmaz. AynÄ± zamanda, dÃ¼ÅŸÃ¼k seviyeli API, eÄŸitim sÃ¼reci Ã¼zerinde Ã§ok daha fazla kontrol saÄŸlar ve bu nedenle yeni sinir aÄŸÄ± mimarileriyle Ã§alÄ±ÅŸÄ±rken araÅŸtÄ±rmalarda Ã§ok kullanÄ±lÄ±r.

Her iki API'yi birlikte kullanabileceÄŸinizi anlamak da Ã¶nemlidir. Ã–rneÄŸin, dÃ¼ÅŸÃ¼k seviyeli API kullanarak kendi aÄŸ katmanÄ± mimarinizi geliÅŸtirebilir ve ardÄ±ndan bunu daha bÃ¼yÃ¼k bir aÄŸ iÃ§inde kullanabilir ve yÃ¼ksek seviyeli API ile eÄŸitebilirsiniz. Ya da katmanlar dizisi olarak yÃ¼ksek seviyeli API kullanarak bir aÄŸ tanÄ±mlayabilir ve ardÄ±ndan kendi dÃ¼ÅŸÃ¼k seviyeli eÄŸitim dÃ¶ngÃ¼nÃ¼zÃ¼ kullanarak optimizasyon yapabilirsiniz. Her iki API de aynÄ± temel kavramlarÄ± kullanÄ±r ve birlikte iyi Ã§alÄ±ÅŸacak ÅŸekilde tasarlanmÄ±ÅŸtÄ±r.

## Ã–ÄŸrenme

Bu derste, iÃ§eriÄŸin Ã§oÄŸunu hem PyTorch hem de TensorFlow iÃ§in sunuyoruz. Tercih ettiÄŸiniz Ã§erÃ§eveyi seÃ§ebilir ve yalnÄ±zca ilgili not defterlerini inceleyebilirsiniz. Hangi Ã§erÃ§eveyi seÃ§eceÄŸinizden emin deÄŸilseniz, **PyTorch vs. TensorFlow** hakkÄ±nda internetteki bazÄ± tartÄ±ÅŸmalarÄ± okuyabilirsiniz. AyrÄ±ca her iki Ã§erÃ§eveye de gÃ¶z atarak daha iyi bir anlayÄ±ÅŸ elde edebilirsiniz.

MÃ¼mkÃ¼n olduÄŸunda, basitlik iÃ§in YÃ¼ksek Seviyeli API'leri kullanacaÄŸÄ±z. Ancak, sinir aÄŸlarÄ±nÄ±n temelden nasÄ±l Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ± anlamanÄ±n Ã¶nemli olduÄŸuna inanÄ±yoruz, bu nedenle baÅŸlangÄ±Ã§ta dÃ¼ÅŸÃ¼k seviyeli API ve tensÃ¶rlerle Ã§alÄ±ÅŸmaya baÅŸlÄ±yoruz. Ancak, hÄ±zlÄ± bir ÅŸekilde baÅŸlamak ve bu detaylarÄ± Ã¶ÄŸrenmek iÃ§in Ã§ok fazla zaman harcamak istemiyorsanÄ±z, bunlarÄ± atlayabilir ve doÄŸrudan yÃ¼ksek seviyeli API not defterlerine geÃ§ebilirsiniz.

## âœï¸ Egzersizler: Ã‡erÃ§eveler

AÅŸaÄŸÄ±daki not defterlerinde Ã¶ÄŸrenmeye devam edin:

DÃ¼ÅŸÃ¼k Seviyeli API | [TensorFlow+Keras Not Defteri](IntroKerasTF.ipynb) | [PyTorch](IntroPyTorch.ipynb)
--------------------|-------------------------------------|--------------------------------
YÃ¼ksek Seviyeli API | [Keras](IntroKeras.ipynb) | *PyTorch Lightning*

Ã‡erÃ§eveleri Ã¶ÄŸrendikten sonra, aÅŸÄ±rÄ± Ã¶ÄŸrenme (overfitting) kavramÄ±nÄ± tekrar gÃ¶zden geÃ§irelim.

# AÅŸÄ±rÄ± Ã–ÄŸrenme (Overfitting)

AÅŸÄ±rÄ± Ã¶ÄŸrenme, makine Ã¶ÄŸreniminde son derece Ã¶nemli bir kavramdÄ±r ve doÄŸru anlamak Ã§ok Ã¶nemlidir!

AÅŸaÄŸÄ±daki 5 noktayÄ± (grafiklerde `x` ile gÃ¶sterilen) yaklaÅŸÄ±k olarak tahmin etme problemini dÃ¼ÅŸÃ¼nÃ¼n:

![linear](../../../../../translated_images/tr/overfit1.f24b71c6f652e59e.webp) | ![overfit](../../../../../translated_images/tr/overfit2.131f5800ae10ca5e.webp)
-------------------------|--------------------------
**DoÄŸrusal model, 2 parametre** | **DoÄŸrusal olmayan model, 7 parametre**
EÄŸitim hatasÄ± = 5.3 | EÄŸitim hatasÄ± = 0
DoÄŸrulama hatasÄ± = 5.1 | DoÄŸrulama hatasÄ± = 20

* Solda, iyi bir doÄŸru Ã§izgisi yaklaÅŸÄ±mÄ± gÃ¶rÃ¼yoruz. Parametre sayÄ±sÄ± uygun olduÄŸu iÃ§in model, nokta daÄŸÄ±lÄ±mÄ±nÄ±n arkasÄ±ndaki fikri doÄŸru bir ÅŸekilde kavrÄ±yor.
* SaÄŸda, model Ã§ok gÃ¼Ã§lÃ¼. Sadece 5 noktamÄ±z olduÄŸu ve modelin 7 parametresi olduÄŸu iÃ§in, tÃ¼m noktalardan geÃ§ecek ÅŸekilde ayarlanabiliyor, bu da eÄŸitim hatasÄ±nÄ± 0 yapÄ±yor. Ancak bu, modelin verinin arkasÄ±ndaki doÄŸru deseni anlamasÄ±nÄ± engelliyor, dolayÄ±sÄ±yla doÄŸrulama hatasÄ± Ã§ok yÃ¼ksek.

Modelin zenginliÄŸi (parametre sayÄ±sÄ±) ile eÄŸitim Ã¶rneklerinin sayÄ±sÄ± arasÄ±nda doÄŸru bir denge kurmak Ã§ok Ã¶nemlidir.

## AÅŸÄ±rÄ± Ã–ÄŸrenme Neden OluÅŸur?

  * Yeterli eÄŸitim verisinin olmamasÄ±
  * Ã‡ok gÃ¼Ã§lÃ¼ bir model
  * GiriÅŸ verilerinde Ã§ok fazla gÃ¼rÃ¼ltÃ¼

## AÅŸÄ±rÄ± Ã–ÄŸrenme NasÄ±l Tespit Edilir?

YukarÄ±daki grafikten gÃ¶rebileceÄŸiniz gibi, aÅŸÄ±rÄ± Ã¶ÄŸrenme Ã§ok dÃ¼ÅŸÃ¼k bir eÄŸitim hatasÄ± ve yÃ¼ksek bir doÄŸrulama hatasÄ± ile tespit edilebilir. Normalde eÄŸitim sÄ±rasÄ±nda hem eÄŸitim hem de doÄŸrulama hatalarÄ±nÄ±n azalmaya baÅŸladÄ±ÄŸÄ±nÄ± gÃ¶rÃ¼rÃ¼z, ancak bir noktada doÄŸrulama hatasÄ± azalmayÄ± durdurabilir ve artmaya baÅŸlayabilir. Bu, aÅŸÄ±rÄ± Ã¶ÄŸrenmenin bir iÅŸareti ve eÄŸitimi muhtemelen bu noktada durdurmamÄ±z gerektiÄŸinin (veya en azÄ±ndan modelin bir anlÄ±k gÃ¶rÃ¼ntÃ¼sÃ¼nÃ¼ almamÄ±z gerektiÄŸinin) gÃ¶stergesidir.

![overfitting](../../../../../translated_images/tr/Overfitting.408ad91cd90b4371.webp)

## AÅŸÄ±rÄ± Ã–ÄŸrenme NasÄ±l Ã–nlenir?

AÅŸÄ±rÄ± Ã¶ÄŸrenmenin meydana geldiÄŸini gÃ¶rÃ¼yorsanÄ±z, aÅŸaÄŸÄ±dakilerden birini yapabilirsiniz:

 * EÄŸitim verilerinin miktarÄ±nÄ± artÄ±rÄ±n
 * Modelin karmaÅŸÄ±klÄ±ÄŸÄ±nÄ± azaltÄ±n
 * [Dropout](../../4-ComputerVision/08-TransferLearning/TrainingTricks.md#Dropout) gibi bazÄ± [dÃ¼zenleme tekniklerini](../../4-ComputerVision/08-TransferLearning/TrainingTricks.md) kullanÄ±n, bunlarÄ± daha sonra ele alacaÄŸÄ±z.

## AÅŸÄ±rÄ± Ã–ÄŸrenme ve Hata-Varyans Dengesi

AÅŸÄ±rÄ± Ã¶ÄŸrenme aslÄ±nda istatistikte [Hata-Varyans Dengesi](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff) olarak adlandÄ±rÄ±lan daha genel bir problemin bir Ã¶rneÄŸidir. Modelimizdeki hata kaynaklarÄ±nÄ± dÃ¼ÅŸÃ¼nÃ¼rsek, iki tÃ¼r hata gÃ¶rebiliriz:

* **Hata hatalarÄ±**, algoritmamÄ±zÄ±n eÄŸitim verileri arasÄ±ndaki iliÅŸkiyi doÄŸru bir ÅŸekilde yakalayamamasÄ±ndan kaynaklanÄ±r. Bu, modelimizin yeterince gÃ¼Ã§lÃ¼ olmamasÄ±ndan kaynaklanabilir (**eksik Ã¶ÄŸrenme**).
* **Varyans hatalarÄ±**, modelin giriÅŸ verilerindeki gÃ¼rÃ¼ltÃ¼yÃ¼ anlamlÄ± bir iliÅŸki yerine tahmin etmesinden kaynaklanÄ±r (**aÅŸÄ±rÄ± Ã¶ÄŸrenme**).

EÄŸitim sÄ±rasÄ±nda, hata hatasÄ± azalÄ±r (modelimiz veriyi tahmin etmeyi Ã¶ÄŸrenir) ve varyans hatasÄ± artar. AÅŸÄ±rÄ± Ã¶ÄŸrenmeyi Ã¶nlemek iÃ§in eÄŸitimi durdurmak - ya manuel olarak (aÅŸÄ±rÄ± Ã¶ÄŸrenmeyi tespit ettiÄŸimizde) ya da otomatik olarak (dÃ¼zenleme ekleyerek) - Ã¶nemlidir.

## SonuÃ§

Bu derste, iki en popÃ¼ler AI Ã§erÃ§evesi olan TensorFlow ve PyTorch iÃ§in Ã§eÅŸitli API'ler arasÄ±ndaki farklarÄ± Ã¶ÄŸrendiniz. AyrÄ±ca, Ã§ok Ã¶nemli bir konu olan aÅŸÄ±rÄ± Ã¶ÄŸrenmeyi Ã¶ÄŸrendiniz.

## ğŸš€ Meydan Okuma

EÅŸlik eden not defterlerinde, 'gÃ¶revler' bÃ¶lÃ¼mÃ¼nÃ¼ bulacaksÄ±nÄ±z; not defterlerini inceleyin ve gÃ¶revleri tamamlayÄ±n.

## [Ders SonrasÄ± Test](https://ff-quizzes.netlify.app/en/ai/quiz/10)

## GÃ¶zden GeÃ§irme ve Kendi Kendine Ã‡alÄ±ÅŸma

AÅŸaÄŸÄ±daki konular hakkÄ±nda biraz araÅŸtÄ±rma yapÄ±n:

- TensorFlow
- PyTorch
- AÅŸÄ±rÄ± Ã¶ÄŸrenme

Kendinize ÅŸu sorularÄ± sorun:

- TensorFlow ve PyTorch arasÄ±ndaki fark nedir?
- AÅŸÄ±rÄ± Ã¶ÄŸrenme ile eksik Ã¶ÄŸrenme arasÄ±ndaki fark nedir?

## [Ã–dev](lab/README.md)

Bu laboratuvarda, PyTorch veya TensorFlow kullanarak tek katmanlÄ± ve Ã§ok katmanlÄ± tam baÄŸlantÄ±lÄ± aÄŸlarla iki sÄ±nÄ±flandÄ±rma problemini Ã§Ã¶zmeniz isteniyor.

* [Talimatlar](lab/README.md)
* [Not Defteri](lab/LabFrameworks.ipynb)

---

