<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "f07c85bbf05a1f67505da98f4ecc124c",
  "translation_date": "2025-08-26T07:26:24+00:00",
  "source_file": "lessons/4-ComputerVision/10-GANs/README.md",
  "language_code": "tr"
}
-->
# Ãœretici Ã‡ekiÅŸmeli AÄŸlar (Generative Adversarial Networks)

Ã–nceki bÃ¶lÃ¼mde, **Ã¼retici modeller** hakkÄ±nda bilgi edindik: eÄŸitim veri kÃ¼mesindeki gÃ¶rÃ¼ntÃ¼lere benzer yeni gÃ¶rÃ¼ntÃ¼ler Ã¼retebilen modeller. VAE, Ã¼retici bir modele iyi bir Ã¶rnekti.

## [Ders Ã–ncesi Test](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/110)

Ancak, VAE ile makul bir Ã§Ã¶zÃ¼nÃ¼rlÃ¼kte anlamlÄ± bir ÅŸey, Ã¶rneÄŸin bir tablo, Ã¼retmeye Ã§alÄ±ÅŸÄ±rsak, eÄŸitimin iyi bir ÅŸekilde yakÄ±nsama saÄŸlamadÄ±ÄŸÄ±nÄ± gÃ¶receÄŸiz. Bu kullanÄ±m durumu iÃ§in, Ã¶zellikle Ã¼retici modellere yÃ¶nelik baÅŸka bir mimariyi Ã¶ÄŸrenmeliyiz - **Ãœretici Ã‡ekiÅŸmeli AÄŸlar** veya GAN'ler.

GAN'in temel fikri, birbirine karÅŸÄ± eÄŸitilecek iki sinir aÄŸÄ±na sahip olmaktÄ±r:

<img src="images/gan_architecture.png" width="70%"/>

> GÃ¶rsel: [Dmitry Soshnikov](http://soshnikov.com)

> âœ… KÃ¼Ã§Ã¼k bir kelime bilgisi:
> * **Ãœretici (Generator)**, rastgele bir vektÃ¶r alÄ±p sonuÃ§ olarak bir gÃ¶rÃ¼ntÃ¼ Ã¼reten bir aÄŸdÄ±r.
> * **AyrÄ±mcÄ± (Discriminator)**, bir gÃ¶rÃ¼ntÃ¼ alÄ±r ve bunun gerÃ§ek bir gÃ¶rÃ¼ntÃ¼ mÃ¼ (eÄŸitim veri kÃ¼mesinden) yoksa bir Ã¼retici tarafÄ±ndan mÄ± Ã¼retildiÄŸini belirlemelidir. Esasen bir gÃ¶rÃ¼ntÃ¼ sÄ±nÄ±flandÄ±rÄ±cÄ±sÄ±dÄ±r.

### AyrÄ±mcÄ± (Discriminator)

AyrÄ±mcÄ±nÄ±n mimarisi, sÄ±radan bir gÃ¶rÃ¼ntÃ¼ sÄ±nÄ±flandÄ±rma aÄŸÄ±ndan farklÄ± deÄŸildir. En basit durumda, tamamen baÄŸlÄ± bir sÄ±nÄ±flandÄ±rÄ±cÄ± olabilir, ancak bÃ¼yÃ¼k olasÄ±lÄ±kla bir [konvolÃ¼syonel aÄŸ](../07-ConvNets/README.md) olacaktÄ±r.

> âœ… KonvolÃ¼syonel aÄŸlara dayalÄ± bir GAN, [DCGAN](https://arxiv.org/pdf/1511.06434.pdf) olarak adlandÄ±rÄ±lÄ±r.

Bir CNN ayrÄ±mcÄ±sÄ± ÅŸu katmanlardan oluÅŸur: birkaÃ§ konvolÃ¼syon + havuzlama (azalan uzamsal boyutlarla) ve bir veya daha fazla tamamen baÄŸlÄ± katman, "Ã¶zellik vektÃ¶rÃ¼" elde etmek iÃ§in, ardÄ±ndan ikili bir sÄ±nÄ±flandÄ±rÄ±cÄ±.

> âœ… 'Havuzlama' (pooling) bu baÄŸlamda, gÃ¶rÃ¼ntÃ¼nÃ¼n boyutunu kÃ¼Ã§Ã¼ltme tekniÄŸidir. "Havuzlama katmanlarÄ±, bir katmandaki nÃ¶ron kÃ¼melerinin Ã§Ä±ktÄ±sÄ±nÄ± bir sonraki katmandaki tek bir nÃ¶ronda birleÅŸtirerek verilerin boyutlarÄ±nÄ± azaltÄ±r." - [kaynak](https://wikipedia.org/wiki/Convolutional_neural_network#Pooling_layers)

### Ãœretici (Generator)

Ãœretici biraz daha karmaÅŸÄ±ktÄ±r. Bunu tersine Ã§evrilmiÅŸ bir ayrÄ±mcÄ± olarak dÃ¼ÅŸÃ¼nebilirsiniz. Bir Ã¶zellik vektÃ¶rÃ¼ yerine bir gizli vektÃ¶rden baÅŸlayarak, gerekli boyut/ÅŸekle dÃ¶nÃ¼ÅŸtÃ¼rmek iÃ§in tamamen baÄŸlÄ± bir katmana sahiptir, ardÄ±ndan dekonvolÃ¼syonlar + Ã¶lÃ§ek bÃ¼yÃ¼tme gelir. Bu, [otoenkoderin](../09-Autoencoders/README.md) *kod Ã§Ã¶zÃ¼cÃ¼* kÄ±smÄ±na benzer.

> âœ… KonvolÃ¼syon katmanÄ±, gÃ¶rÃ¼ntÃ¼ Ã¼zerinde bir doÄŸrusal filtre olarak uygulandÄ±ÄŸÄ±ndan, dekonvolÃ¼syon esasen konvolÃ¼syona benzerdir ve aynÄ± katman mantÄ±ÄŸÄ± kullanÄ±larak uygulanabilir.

<img src="images/gan_arch_detail.png" width="70%"/>

> GÃ¶rsel: [Dmitry Soshnikov](http://soshnikov.com)

### GAN'in EÄŸitimi

GAN'ler **Ã§ekiÅŸmeli** olarak adlandÄ±rÄ±lÄ±r Ã§Ã¼nkÃ¼ Ã¼retici ve ayrÄ±mcÄ± arasÄ±nda sÃ¼rekli bir rekabet vardÄ±r. Bu rekabet sÄ±rasÄ±nda hem Ã¼retici hem de ayrÄ±mcÄ± geliÅŸir, bÃ¶ylece aÄŸ daha iyi ve daha iyi gÃ¶rÃ¼ntÃ¼ler Ã¼retmeyi Ã¶ÄŸrenir.

EÄŸitim iki aÅŸamada gerÃ§ekleÅŸir:

* **AyrÄ±mcÄ±nÄ±n eÄŸitimi**. Bu gÃ¶rev oldukÃ§a basittir: Ã¼retici tarafÄ±ndan bir gÃ¶rÃ¼ntÃ¼ grubu oluÅŸtururuz, bunlarÄ± sahte gÃ¶rÃ¼ntÃ¼ anlamÄ±na gelen 0 ile etiketleriz ve giriÅŸ veri kÃ¼mesinden bir grup gÃ¶rÃ¼ntÃ¼ alÄ±rÄ±z (1 etiketiyle, gerÃ§ek gÃ¶rÃ¼ntÃ¼). Bir *ayrÄ±mcÄ± kaybÄ±* elde ederiz ve geri yayÄ±lÄ±m yaparÄ±z.
* **Ãœreticinin eÄŸitimi**. Bu biraz daha karmaÅŸÄ±ktÄ±r Ã§Ã¼nkÃ¼ Ã¼retici iÃ§in beklenen Ã§Ä±ktÄ±yÄ± doÄŸrudan bilmiyoruz. Ãœretici ve ayrÄ±mcÄ±dan oluÅŸan tÃ¼m GAN aÄŸÄ±na bazÄ± rastgele vektÃ¶rler besleriz ve sonucun 1 (gerÃ§ek gÃ¶rÃ¼ntÃ¼lere karÅŸÄ±lÄ±k gelen) olmasÄ±nÄ± bekleriz. ArdÄ±ndan ayrÄ±mcÄ±nÄ±n parametrelerini dondururuz (bu adÄ±mda eÄŸitilmesini istemeyiz) ve geri yayÄ±lÄ±m yaparÄ±z.

Bu sÃ¼reÃ§ sÄ±rasÄ±nda, hem Ã¼retici hem de ayrÄ±mcÄ± kayÄ±plarÄ± Ã¶nemli Ã¶lÃ§Ã¼de azalmaz. Ä°deal durumda, her iki aÄŸÄ±n performansÄ±nÄ± geliÅŸtirdiÄŸini gÃ¶steren bir salÄ±nÄ±m yapmalÄ±dÄ±rlar.

## âœï¸ AlÄ±ÅŸtÄ±rmalar: GAN'ler

* [TensorFlow/Keras ile GAN Defteri](../../../../../lessons/4-ComputerVision/10-GANs/GANTF.ipynb)
* [PyTorch ile GAN Defteri](../../../../../lessons/4-ComputerVision/10-GANs/GANPyTorch.ipynb)

### GAN EÄŸitimiyle Ä°lgili Sorunlar

GAN'lerin eÄŸitimi Ã¶zellikle zordur. Ä°ÅŸte birkaÃ§ sorun:

* **Mod Ã‡Ã¶kmesi**. Bu terimle, Ã¼reticinin ayrÄ±mcÄ±yÄ± kandÄ±ran tek bir baÅŸarÄ±lÄ± gÃ¶rÃ¼ntÃ¼ Ã¼retmeyi Ã¶ÄŸrenmesi ve farklÄ± gÃ¶rÃ¼ntÃ¼ler Ã¼retmemesi kastedilir.
* **Hiperparametrelere duyarlÄ±lÄ±k**. Ã‡oÄŸu zaman, bir GAN'in hiÃ§ yakÄ±nsamadÄ±ÄŸÄ±nÄ± ve ardÄ±ndan Ã¶ÄŸrenme oranÄ±ndaki ani bir dÃ¼ÅŸÃ¼ÅŸle yakÄ±nsamaya baÅŸladÄ±ÄŸÄ±nÄ± gÃ¶rebilirsiniz.
* Ãœretici ve ayrÄ±mcÄ± arasÄ±nda bir **denge** saÄŸlamak. Ã‡oÄŸu durumda, ayrÄ±mcÄ± kaybÄ± nispeten hÄ±zlÄ± bir ÅŸekilde sÄ±fÄ±ra dÃ¼ÅŸebilir, bu da Ã¼reticinin daha fazla eÄŸitim yapamamasÄ±na neden olur. Bunu aÅŸmak iÃ§in, Ã¼retici ve ayrÄ±mcÄ± iÃ§in farklÄ± Ã¶ÄŸrenme oranlarÄ± ayarlamayÄ± deneyebilir veya kayÄ±p zaten Ã§ok dÃ¼ÅŸÃ¼kse ayrÄ±mcÄ± eÄŸitimini atlayabiliriz.
* **YÃ¼ksek Ã§Ã¶zÃ¼nÃ¼rlÃ¼k** iÃ§in eÄŸitim. Otoenkoderlerde olduÄŸu gibi, bu sorun, Ã§ok fazla konvolÃ¼syonel aÄŸ katmanÄ±nÄ± yeniden yapÄ±landÄ±rmanÄ±n artefaktlara yol aÃ§masÄ± nedeniyle ortaya Ã§Ä±kar. Bu sorun genellikle, Ã¶nce birkaÃ§ katmanÄ±n dÃ¼ÅŸÃ¼k Ã§Ã¶zÃ¼nÃ¼rlÃ¼klÃ¼ gÃ¶rÃ¼ntÃ¼ler Ã¼zerinde eÄŸitildiÄŸi ve ardÄ±ndan katmanlarÄ±n "aÃ§Ä±ldÄ±ÄŸÄ±" veya eklendiÄŸi **aÅŸamalÄ± bÃ¼yÃ¼me** ile Ã§Ã¶zÃ¼lÃ¼r. Bir diÄŸer Ã§Ã¶zÃ¼m, katmanlar arasÄ±nda ek baÄŸlantÄ±lar eklemek ve birden fazla Ã§Ã¶zÃ¼nÃ¼rlÃ¼ÄŸÃ¼ aynÄ± anda eÄŸitmek olabilir - ayrÄ±ntÄ±lar iÃ§in bu [Ã‡ok Ã–lÃ§ekli Gradient GAN'ler makalesine](https://arxiv.org/abs/1903.06048) bakabilirsiniz.

## Stil Transferi

GAN'ler sanatsal gÃ¶rÃ¼ntÃ¼ler Ã¼retmek iÃ§in harika bir yÃ¶ntemdir. Bir diÄŸer ilginÃ§ teknik ise **stil transferi** olarak adlandÄ±rÄ±lÄ±r. Bu teknik, bir **iÃ§erik gÃ¶rÃ¼ntÃ¼sÃ¼** alÄ±r ve onu farklÄ± bir tarzda yeniden Ã§izer, **stil gÃ¶rÃ¼ntÃ¼sÃ¼nden** filtreler uygular.

Bu yÃ¶ntem ÅŸu ÅŸekilde Ã§alÄ±ÅŸÄ±r:
* Rastgele bir gÃ¼rÃ¼ltÃ¼ gÃ¶rÃ¼ntÃ¼sÃ¼yle baÅŸlarÄ±z (veya bir iÃ§erik gÃ¶rÃ¼ntÃ¼sÃ¼yle, ancak anlamak iÃ§in rastgele gÃ¼rÃ¼ltÃ¼yle baÅŸlamak daha kolaydÄ±r)
* AmacÄ±mÄ±z, hem iÃ§erik gÃ¶rÃ¼ntÃ¼sÃ¼ne hem de stil gÃ¶rÃ¼ntÃ¼sÃ¼ne yakÄ±n olacak bir gÃ¶rÃ¼ntÃ¼ oluÅŸturmaktÄ±r. Bu, iki kayÄ±p fonksiyonu ile belirlenir:
   - **Ä°Ã§erik kaybÄ±**, mevcut gÃ¶rÃ¼ntÃ¼ ve iÃ§erik gÃ¶rÃ¼ntÃ¼sÃ¼nden CNN tarafÄ±ndan bazÄ± katmanlarda Ã§Ä±karÄ±lan Ã¶zelliklere dayanarak hesaplanÄ±r.
   - **Stil kaybÄ±**, mevcut gÃ¶rÃ¼ntÃ¼ ve stil gÃ¶rÃ¼ntÃ¼sÃ¼ arasÄ±nda, Gram matrisleri kullanÄ±larak (daha fazla ayrÄ±ntÄ± iÃ§in [Ã¶rnek deftere](../../../../../lessons/4-ComputerVision/10-GANs/StyleTransfer.ipynb) bakabilirsiniz) hesaplanÄ±r.
* GÃ¶rÃ¼ntÃ¼yÃ¼ daha pÃ¼rÃ¼zsÃ¼z hale getirmek ve gÃ¼rÃ¼ltÃ¼yÃ¼ kaldÄ±rmak iÃ§in, **Varyasyon kaybÄ±** da ekleriz, bu kayÄ±p komÅŸu pikseller arasÄ±ndaki ortalama mesafeyi hesaplar.
* Ana optimizasyon dÃ¶ngÃ¼sÃ¼, toplam kaybÄ± (Ã¼Ã§ kaybÄ±n aÄŸÄ±rlÄ±klÄ± toplamÄ±) minimize etmek iÃ§in mevcut gÃ¶rÃ¼ntÃ¼yÃ¼ gradyan iniÅŸi (veya baÅŸka bir optimizasyon algoritmasÄ±) kullanarak ayarlar.

## âœï¸ Ã–rnek: [Stil Transferi](../../../../../lessons/4-ComputerVision/10-GANs/StyleTransfer.ipynb)

## [Ders SonrasÄ± Test](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/210)

## SonuÃ§

Bu derste, GAN'ler ve bunlarÄ± nasÄ±l eÄŸiteceÄŸinizi Ã¶ÄŸrendiniz. AyrÄ±ca, bu tÃ¼r Sinir AÄŸlarÄ±nÄ±n karÅŸÄ±laÅŸabileceÄŸi Ã¶zel zorluklarÄ± ve bunlarÄ±n Ã¼stesinden nasÄ±l gelinebileceÄŸine dair bazÄ± stratejileri Ã¶ÄŸrendiniz.

## ğŸš€ Meydan Okuma

[Stil Transferi defterini](../../../../../lessons/4-ComputerVision/10-GANs/StyleTransfer.ipynb) kendi gÃ¶rÃ¼ntÃ¼lerinizle Ã§alÄ±ÅŸtÄ±rÄ±n.

## Ä°nceleme ve Kendi Kendine Ã‡alÄ±ÅŸma

Referans iÃ§in, GAN'ler hakkÄ±nda daha fazla bilgi edinmek iÃ§in ÅŸu kaynaklarÄ± okuyun:

* Marco Pasini, [GAN'leri Bir YÄ±l Boyunca EÄŸitmekten Ã–ÄŸrendiÄŸim 10 Ders](https://towardsdatascience.com/10-lessons-i-learned-training-generative-adversarial-networks-gans-for-a-year-c9071159628)
* [StyleGAN](https://en.wikipedia.org/wiki/StyleGAN), dikkate alÄ±nmasÄ± gereken bir *de facto* GAN mimarisi
* [Azure ML'de GAN'ler Kullanarak Ãœretici Sanat OluÅŸturma](https://soshnikov.com/scienceart/creating-generative-art-using-gan-on-azureml/)

## Ã–dev

Bu derse ait iki defterden birini tekrar gÃ¶zden geÃ§irin ve GAN'i kendi gÃ¶rÃ¼ntÃ¼lerinizle yeniden eÄŸitin. Neler oluÅŸturabilirsiniz?

**Feragatname**:  
Bu belge, AI Ã§eviri hizmeti [Co-op Translator](https://github.com/Azure/co-op-translator) kullanÄ±larak Ã§evrilmiÅŸtir. DoÄŸruluk iÃ§in Ã§aba gÃ¶stersek de, otomatik Ã§evirilerin hata veya yanlÄ±ÅŸlÄ±k iÃ§erebileceÄŸini lÃ¼tfen unutmayÄ±n. Belgenin orijinal dili, yetkili kaynak olarak kabul edilmelidir. Kritik bilgiler iÃ§in profesyonel insan Ã§evirisi Ã¶nerilir. Bu Ã§evirinin kullanÄ±mÄ±ndan kaynaklanan yanlÄ±ÅŸ anlamalar veya yanlÄ±ÅŸ yorumlamalar iÃ§in sorumluluk kabul etmiyoruz.