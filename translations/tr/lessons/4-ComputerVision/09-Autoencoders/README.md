<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "0b306c04f5337b6e7430e5c0b16bb5c0",
  "translation_date": "2025-08-26T07:25:14+00:00",
  "source_file": "lessons/4-ComputerVision/09-Autoencoders/README.md",
  "language_code": "tr"
}
-->
# Otomatik KodlayÄ±cÄ±lar (Autoencoders)

CNN'leri (Convolutional Neural Networks) eÄŸitirken karÅŸÄ±laÅŸÄ±lan sorunlardan biri, Ã§ok fazla etiketlenmiÅŸ veriye ihtiyaÃ§ duymamÄ±zdÄ±r. GÃ¶rÃ¼ntÃ¼ sÄ±nÄ±flandÄ±rma durumunda, gÃ¶rÃ¼ntÃ¼leri farklÄ± sÄ±nÄ±flara ayÄ±rmamÄ±z gerekir ve bu manuel bir Ã§abadÄ±r.

## [Ders Ã–ncesi Testi](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/109)

Ancak, CNN Ã¶zellik Ã§Ä±karÄ±cÄ±larÄ±nÄ± eÄŸitmek iÃ§in ham (etiketlenmemiÅŸ) verileri kullanmak isteyebiliriz; bu, **kendinden denetimli Ã¶ÄŸrenme** (self-supervised learning) olarak adlandÄ±rÄ±lÄ±r. Etiketler yerine, eÄŸitim gÃ¶rÃ¼ntÃ¼lerini hem aÄŸ giriÅŸi hem de Ã§Ä±kÄ±ÅŸÄ± olarak kullanacaÄŸÄ±z. **Otomatik kodlayÄ±cÄ±** (autoencoder) fikrinin temelinde, bir **kodlayÄ±cÄ± aÄŸ** (encoder network) ile giriÅŸ gÃ¶rÃ¼ntÃ¼sÃ¼nÃ¼ bir **gizli uzaya** (latent space) dÃ¶nÃ¼ÅŸtÃ¼rmek (genellikle daha kÃ¼Ã§Ã¼k boyutlu bir vektÃ¶rdÃ¼r) ve ardÄ±ndan **kod Ã§Ã¶zÃ¼cÃ¼ aÄŸ** (decoder network) ile orijinal gÃ¶rÃ¼ntÃ¼yÃ¼ yeniden oluÅŸturmak yer alÄ±r.

> âœ… Bir [otomatik kodlayÄ±cÄ±](https://wikipedia.org/wiki/Autoencoder), "etiketlenmemiÅŸ verilerin verimli kodlamalarÄ±nÄ± Ã¶ÄŸrenmek iÃ§in kullanÄ±lan bir yapay sinir aÄŸÄ± tÃ¼rÃ¼dÃ¼r."

Otomatik kodlayÄ±cÄ±yÄ±, orijinal gÃ¶rÃ¼ntÃ¼den mÃ¼mkÃ¼n olduÄŸunca fazla bilgi yakalamak ve doÄŸru bir ÅŸekilde yeniden oluÅŸturmak iÃ§in eÄŸittiÄŸimizden, aÄŸ, giriÅŸ gÃ¶rÃ¼ntÃ¼lerinin anlamÄ±nÄ± yakalamak iÃ§in en iyi **gÃ¶mÃ¼lÃ¼ temsili** (embedding) bulmaya Ã§alÄ±ÅŸÄ±r.

![AutoEncoder ÅemasÄ±](../../../../../translated_images/autoencoder_schema.5e6fc9ad98a5eb6197f3513cf3baf4dfbe1389a6ae74daebda64de9f1c99f142.tr.jpg)

> GÃ¶rsel: [Keras blog](https://blog.keras.io/building-autoencoders-in-keras.html)

## Otomatik KodlayÄ±cÄ±larÄ±n KullanÄ±m SenaryolarÄ±

Orijinal gÃ¶rÃ¼ntÃ¼leri yeniden oluÅŸturmak kendi baÅŸÄ±na Ã§ok faydalÄ± gÃ¶rÃ¼nmese de, otomatik kodlayÄ±cÄ±larÄ±n Ã¶zellikle faydalÄ± olduÄŸu birkaÃ§ senaryo vardÄ±r:

* **GÃ¶rÃ¼ntÃ¼lerin boyutunu dÃ¼ÅŸÃ¼rmek** veya **gÃ¶rÃ¼ntÃ¼ gÃ¶mÃ¼lÃ¼ temsilleri eÄŸitmek**. Otomatik kodlayÄ±cÄ±lar genellikle PCA'dan daha iyi sonuÃ§lar verir, Ã§Ã¼nkÃ¼ gÃ¶rÃ¼ntÃ¼lerin mekansal doÄŸasÄ±nÄ± ve hiyerarÅŸik Ã¶zelliklerini dikkate alÄ±r.
* **GÃ¼rÃ¼ltÃ¼ giderme**, yani gÃ¶rÃ¼ntÃ¼den gÃ¼rÃ¼ltÃ¼yÃ¼ kaldÄ±rma. GÃ¼rÃ¼ltÃ¼ genellikle Ã§ok fazla gereksiz bilgi taÅŸÄ±r, bu nedenle otomatik kodlayÄ±cÄ±, nispeten kÃ¼Ã§Ã¼k bir gizli uzaya tÃ¼m bu bilgiyi sÄ±ÄŸdÄ±ramaz ve yalnÄ±zca gÃ¶rÃ¼ntÃ¼nÃ¼n Ã¶nemli kÄ±smÄ±nÄ± yakalar. GÃ¼rÃ¼ltÃ¼ gidericileri eÄŸitirken, orijinal gÃ¶rÃ¼ntÃ¼lerle baÅŸlarÄ±z ve otomatik kodlayÄ±cÄ±ya giriÅŸ olarak yapay olarak eklenmiÅŸ gÃ¼rÃ¼ltÃ¼ iÃ§eren gÃ¶rÃ¼ntÃ¼leri kullanÄ±rÄ±z.
* **SÃ¼per Ã§Ã¶zÃ¼nÃ¼rlÃ¼k**, gÃ¶rÃ¼ntÃ¼ Ã§Ã¶zÃ¼nÃ¼rlÃ¼ÄŸÃ¼nÃ¼ artÄ±rma. YÃ¼ksek Ã§Ã¶zÃ¼nÃ¼rlÃ¼klÃ¼ gÃ¶rÃ¼ntÃ¼lerle baÅŸlarÄ±z ve otomatik kodlayÄ±cÄ±ya giriÅŸ olarak daha dÃ¼ÅŸÃ¼k Ã§Ã¶zÃ¼nÃ¼rlÃ¼klÃ¼ gÃ¶rÃ¼ntÃ¼yÃ¼ kullanÄ±rÄ±z.
* **Ãœretici modeller**. Otomatik kodlayÄ±cÄ±yÄ± eÄŸittikten sonra, kod Ã§Ã¶zÃ¼cÃ¼ kÄ±smÄ± rastgele gizli vektÃ¶rlerden baÅŸlayarak yeni nesneler oluÅŸturmak iÃ§in kullanÄ±labilir.

## Varyasyonel Otomatik KodlayÄ±cÄ±lar (VAE)

Geleneksel otomatik kodlayÄ±cÄ±lar, giriÅŸ verilerinin boyutunu bir ÅŸekilde azaltÄ±r ve giriÅŸ gÃ¶rÃ¼ntÃ¼lerinin Ã¶nemli Ã¶zelliklerini belirler. Ancak, gizli vektÃ¶rler genellikle Ã§ok anlamlÄ± deÄŸildir. Ã–rneÄŸin, MNIST veri setini ele alÄ±rsak, farklÄ± gizli vektÃ¶rlerin hangi rakamlara karÅŸÄ±lÄ±k geldiÄŸini anlamak kolay deÄŸildir, Ã§Ã¼nkÃ¼ yakÄ±n gizli vektÃ¶rler mutlaka aynÄ± rakamlara karÅŸÄ±lÄ±k gelmez.

Ã–te yandan, *Ã¼retici* modelleri eÄŸitmek iÃ§in gizli uzayÄ± anlamak daha iyidir. Bu fikir bizi **varyasyonel otomatik kodlayÄ±cÄ±**ya (VAE) gÃ¶tÃ¼rÃ¼r.

VAE, gizli parametrelerin *istatistiksel daÄŸÄ±lÄ±mÄ±nÄ±* (latent distribution) tahmin etmeyi Ã¶ÄŸrenen bir otomatik kodlayÄ±cÄ±dÄ±r. Ã–rneÄŸin, gizli vektÃ¶rlerin belirli bir ortalama z<sub>mean</sub> ve standart sapma z<sub>sigma</sub> (her ikisi de belirli bir boyut d'ye sahip vektÃ¶rlerdir) ile normal olarak daÄŸÄ±tÄ±lmasÄ±nÄ± isteyebiliriz. VAE'deki kodlayÄ±cÄ± bu parametreleri tahmin etmeyi Ã¶ÄŸrenir ve ardÄ±ndan kod Ã§Ã¶zÃ¼cÃ¼, bu daÄŸÄ±lÄ±mdan rastgele bir vektÃ¶r alarak nesneyi yeniden oluÅŸturur.

Ã–zetle:

* GiriÅŸ vektÃ¶rÃ¼nden, `z_mean` ve `z_log_sigma` tahmin edilir (standart sapmanÄ±n kendisini tahmin etmek yerine, logaritmasÄ± tahmin edilir).
* `sample` adlÄ± bir vektÃ¶r, N(z<sub>mean</sub>,exp(z<sub>log_sigma</sub>)) daÄŸÄ±lÄ±mÄ±ndan Ã¶rneklenir.
* Kod Ã§Ã¶zÃ¼cÃ¼, `sample` vektÃ¶rÃ¼nÃ¼ giriÅŸ olarak kullanarak orijinal gÃ¶rÃ¼ntÃ¼yÃ¼ yeniden oluÅŸturmaya Ã§alÄ±ÅŸÄ±r.

<img src="images/vae.png" width="50%">

> GÃ¶rsel: [Bu blog yazÄ±sÄ±](https://ijdykeman.github.io/ml/2016/12/21/cvae.html) - Isaak Dykeman

Varyasyonel otomatik kodlayÄ±cÄ±lar, iki bÃ¶lÃ¼mden oluÅŸan karmaÅŸÄ±k bir kayÄ±p fonksiyonu kullanÄ±r:

* **Yeniden yapÄ±landÄ±rma kaybÄ±** (Reconstruction loss), yeniden oluÅŸturulan gÃ¶rÃ¼ntÃ¼nÃ¼n hedefe ne kadar yakÄ±n olduÄŸunu gÃ¶steren kayÄ±p fonksiyonudur (Ã¶rneÄŸin, Ortalama Kare HatasÄ± - MSE). Bu, normal otomatik kodlayÄ±cÄ±lardaki kayÄ±p fonksiyonuyla aynÄ±dÄ±r.
* **KL kaybÄ±**, gizli deÄŸiÅŸken daÄŸÄ±lÄ±mlarÄ±nÄ±n normal daÄŸÄ±lÄ±ma yakÄ±n kalmasÄ±nÄ± saÄŸlar. Bu, iki istatistiksel daÄŸÄ±lÄ±mÄ±n ne kadar benzer olduÄŸunu tahmin etmek iÃ§in kullanÄ±lan [Kullback-Leibler sapmasÄ±](https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained) kavramÄ±na dayanÄ±r.

VAEs'in Ã¶nemli bir avantajÄ±, yeni gÃ¶rÃ¼ntÃ¼leri nispeten kolay bir ÅŸekilde oluÅŸturabilmemize olanak tanÄ±masÄ±dÄ±r, Ã§Ã¼nkÃ¼ hangi daÄŸÄ±lÄ±mdan gizli vektÃ¶rlerin Ã¶rneklenmesi gerektiÄŸini biliriz. Ã–rneÄŸin, MNIST Ã¼zerinde 2D gizli vektÃ¶rle bir VAE eÄŸitirsek, gizli vektÃ¶rÃ¼n bileÅŸenlerini deÄŸiÅŸtirerek farklÄ± rakamlar elde edebiliriz:

<img alt="vaemnist" src="images/vaemnist.png" width="50%"/>

> GÃ¶rsel: [Dmitry Soshnikov](http://soshnikov.com)

Gizli parametre uzayÄ±nÄ±n farklÄ± bÃ¶lÃ¼mlerinden gizli vektÃ¶rler almaya baÅŸladÄ±kÃ§a, gÃ¶rÃ¼ntÃ¼lerin birbirine nasÄ±l karÄ±ÅŸtÄ±ÄŸÄ±nÄ± gÃ¶zlemleyin. Bu uzayÄ± ayrÄ±ca 2D olarak gÃ¶rselleÅŸtirebiliriz:

<img alt="vaemnist cluster" src="images/vaemnist-diag.png" width="50%"/> 

> GÃ¶rsel: [Dmitry Soshnikov](http://soshnikov.com)

## âœï¸ Egzersizler: Otomatik KodlayÄ±cÄ±lar

Otomatik kodlayÄ±cÄ±lar hakkÄ±nda daha fazla bilgi edinmek iÃ§in ÅŸu not defterlerini inceleyin:

* [TensorFlow'da Otomatik KodlayÄ±cÄ±lar](../../../../../lessons/4-ComputerVision/09-Autoencoders/AutoencodersTF.ipynb)
* [PyTorch'ta Otomatik KodlayÄ±cÄ±lar](../../../../../lessons/4-ComputerVision/09-Autoencoders/AutoEncodersPyTorch.ipynb)

## Otomatik KodlayÄ±cÄ±larÄ±n Ã–zellikleri

* **Veriye Ã–zgÃ¼** - yalnÄ±zca eÄŸitildikleri gÃ¶rÃ¼ntÃ¼ tÃ¼rleriyle iyi Ã§alÄ±ÅŸÄ±rlar. Ã–rneÄŸin, bir sÃ¼per Ã§Ã¶zÃ¼nÃ¼rlÃ¼k aÄŸÄ± Ã§iÃ§ekler Ã¼zerinde eÄŸitilirse, portrelerde iyi Ã§alÄ±ÅŸmaz. Bunun nedeni, aÄŸÄ±n daha yÃ¼ksek Ã§Ã¶zÃ¼nÃ¼rlÃ¼klÃ¼ bir gÃ¶rÃ¼ntÃ¼ Ã¼retebilmek iÃ§in eÄŸitim veri setinden Ã¶ÄŸrendiÄŸi ince detaylarÄ± kullanmasÄ±dÄ±r.
* **KayÄ±plÄ±** - yeniden oluÅŸturulan gÃ¶rÃ¼ntÃ¼, orijinal gÃ¶rÃ¼ntÃ¼yle aynÄ± deÄŸildir. KayÄ±p tÃ¼rÃ¼, eÄŸitim sÄ±rasÄ±nda kullanÄ±lan *kayÄ±p fonksiyonu* ile tanÄ±mlanÄ±r.
* **EtiketlenmemiÅŸ veri** Ã¼zerinde Ã§alÄ±ÅŸÄ±r.

## [Ders SonrasÄ± Testi](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/209)

## SonuÃ§

Bu derste, bir AI bilim insanÄ±nÄ±n kullanabileceÄŸi Ã§eÅŸitli otomatik kodlayÄ±cÄ± tÃ¼rlerini Ã¶ÄŸrendiniz. BunlarÄ± nasÄ±l inÅŸa edeceÄŸinizi ve gÃ¶rÃ¼ntÃ¼leri yeniden oluÅŸturmak iÃ§in nasÄ±l kullanacaÄŸÄ±nÄ±zÄ± Ã¶ÄŸrendiniz. AyrÄ±ca VAE'yi ve yeni gÃ¶rÃ¼ntÃ¼ler oluÅŸturmak iÃ§in nasÄ±l kullanÄ±lacaÄŸÄ±nÄ± Ã¶ÄŸrendiniz.

## ğŸš€ Meydan Okuma

Bu derste, gÃ¶rÃ¼ntÃ¼ler iÃ§in otomatik kodlayÄ±cÄ±larÄ±n kullanÄ±mÄ±nÄ± Ã¶ÄŸrendiniz. Ancak, bunlar mÃ¼zik iÃ§in de kullanÄ±labilir! Magenta projesinin [MusicVAE](https://magenta.tensorflow.org/music-vae) projesine gÃ¶z atÄ±n; bu proje, mÃ¼ziÄŸi yeniden oluÅŸturmayÄ± Ã¶ÄŸrenmek iÃ§in otomatik kodlayÄ±cÄ±larÄ± kullanÄ±r. Bu kÃ¼tÃ¼phane ile bazÄ± [deneyler](https://colab.research.google.com/github/magenta/magenta-demos/blob/master/colab-notebooks/Multitrack_MusicVAE.ipynb) yaparak neler yaratabileceÄŸinizi gÃ¶rÃ¼n.

## [Ders SonrasÄ± Testi](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/208)

## Ä°nceleme ve Kendi Kendine Ã‡alÄ±ÅŸma

Referans iÃ§in, otomatik kodlayÄ±cÄ±lar hakkÄ±nda daha fazla bilgi edinmek iÃ§in ÅŸu kaynaklarÄ± okuyun:

* [Keras'ta Otomatik KodlayÄ±cÄ±lar Ä°nÅŸa Etmek](https://blog.keras.io/building-autoencoders-in-keras.html)
* [NeuroHive'daki Blog YazÄ±sÄ±](https://neurohive.io/ru/osnovy-data-science/variacionnyj-avtojenkoder-vae/)
* [Varyasyonel Otomatik KodlayÄ±cÄ±lar AÃ§Ä±klamasÄ±](https://kvfrans.com/variational-autoencoders-explained/)
* [KoÅŸullu Varyasyonel Otomatik KodlayÄ±cÄ±lar](https://ijdykeman.github.io/ml/2016/12/21/cvae.html)

## Ã–dev

[TensorFlow kullanarak bu not defterinin](../../../../../lessons/4-ComputerVision/09-Autoencoders/AutoencodersTF.ipynb) sonunda bir 'gÃ¶rev' bulacaksÄ±nÄ±z - bunu Ã¶deviniz olarak kullanÄ±n.

**Feragatname**:  
Bu belge, [Co-op Translator](https://github.com/Azure/co-op-translator) adlÄ± yapay zeka Ã§eviri hizmeti kullanÄ±larak Ã§evrilmiÅŸtir. DoÄŸruluk iÃ§in Ã§aba gÃ¶stersek de, otomatik Ã§evirilerin hata veya yanlÄ±ÅŸlÄ±klar iÃ§erebileceÄŸini lÃ¼tfen unutmayÄ±n. Belgenin orijinal dili, yetkili kaynak olarak kabul edilmelidir. Kritik bilgiler iÃ§in profesyonel insan Ã§evirisi Ã¶nerilir. Bu Ã§evirinin kullanÄ±mÄ±ndan kaynaklanan herhangi bir yanlÄ±ÅŸ anlama veya yanlÄ±ÅŸ yorumlama durumunda sorumluluk kabul edilmez.