# Otomatik KodlayÄ±cÄ±lar

CNN'leri eÄŸitirken, karÅŸÄ±laÅŸtÄ±ÄŸÄ±mÄ±z sorunlardan biri Ã§ok fazla etiketli veriye ihtiyaÃ§ duymamÄ±zdÄ±r. GÃ¶rÃ¼ntÃ¼ sÄ±nÄ±flandÄ±rmasÄ± durumunda, gÃ¶rÃ¼ntÃ¼leri farklÄ± sÄ±nÄ±flara ayÄ±rmamÄ±z gerekir ki bu da manuel bir Ã§abadÄ±r.

## [Ã–n ders sÄ±navÄ±](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/109)

Ancak, CNN Ã¶zellik Ã§Ä±karÄ±cÄ±larÄ±nÄ± eÄŸitmek iÃ§in ham (etiketsiz) verileri kullanmak isteyebiliriz ki buna **kendinden denetimli Ã¶ÄŸrenme** denir. Etiketler yerine, eÄŸitim gÃ¶rÃ¼ntÃ¼lerini hem aÄŸ giriÅŸi hem de Ã§Ä±kÄ±ÅŸÄ± olarak kullanacaÄŸÄ±z. **Otomatik kodlayÄ±cÄ±**nÄ±n ana fikri, girdi gÃ¶rÃ¼ntÃ¼sÃ¼nÃ¼ bazÄ± **gizli alanlara** (genellikle daha kÃ¼Ã§Ã¼k boyutlu bir vektÃ¶r) dÃ¶nÃ¼ÅŸtÃ¼ren bir **kodlayÄ±cÄ± aÄŸÄ±**na sahip olmamÄ±zdÄ±r; ardÄ±ndan orijinal gÃ¶rÃ¼ntÃ¼yÃ¼ yeniden oluÅŸturmayÄ± amaÃ§layan **Ã§Ã¶zÃ¼cÃ¼ aÄŸÄ±** gelir.

> âœ… Bir [otomatik kodlayÄ±cÄ±](https://wikipedia.org/wiki/Autoencoder), "etiketsiz verilerin verimli kodlamalarÄ±nÄ± Ã¶ÄŸrenmek iÃ§in kullanÄ±lan bir tÃ¼r yapay sinir aÄŸÄ±dÄ±r."

Otomatik kodlayÄ±cÄ±yÄ±, orijinal gÃ¶rÃ¼ntÃ¼den mÃ¼mkÃ¼n olduÄŸunca fazla bilgiyi yakalamak iÃ§in eÄŸittiÄŸimiz iÃ§in, aÄŸ girdi gÃ¶rÃ¼ntÃ¼lerinin anlamÄ±nÄ± yakalamak iÃ§in en iyi **gÃ¶mÃ¼lÃ¼ alanÄ±** bulmaya Ã§alÄ±ÅŸÄ±r.

![Otomatik KodlayÄ±cÄ± DiyagramÄ±](../../../../../translated_images/autoencoder_schema.5e6fc9ad98a5eb6197f3513cf3baf4dfbe1389a6ae74daebda64de9f1c99f142.tr.jpg)

> GÃ¶rsel [Keras blogu](https://blog.keras.io/building-autoencoders-in-keras.html) kaynaÄŸÄ±ndandÄ±r.

## Otomatik KodlayÄ±cÄ±larÄ±n KullanÄ±m SenaryolarÄ±

Orijinal gÃ¶rÃ¼ntÃ¼leri yeniden oluÅŸturmanÄ±n kendi baÅŸÄ±na faydalÄ± gÃ¶rÃ¼nmemesi bir yana, otomatik kodlayÄ±cÄ±larÄ±n Ã¶zellikle yararlÄ± olduÄŸu birkaÃ§ senaryo vardÄ±r:

* **GÃ¶rÃ¼ntÃ¼lerin boyutunu dÃ¼ÅŸÃ¼rmek iÃ§in gÃ¶rselleÅŸtirme** veya **gÃ¶rÃ¼ntÃ¼ gÃ¶mÃ¼lÃ¼ alanlarÄ± eÄŸitimi**. Genellikle otomatik kodlayÄ±cÄ±lar, PCA'dan daha iyi sonuÃ§lar verir Ã§Ã¼nkÃ¼ gÃ¶rÃ¼ntÃ¼lerin mekansal doÄŸasÄ±nÄ± ve hiyerarÅŸik Ã¶zelliklerini dikkate alÄ±r.
* **GÃ¼rÃ¼ltÃ¼ azaltma**, yani gÃ¶rÃ¼ntÃ¼den gÃ¼rÃ¼ltÃ¼yÃ¼ kaldÄ±rma. Ã‡Ã¼nkÃ¼ gÃ¼rÃ¼ltÃ¼ birÃ§ok gereksiz bilgi taÅŸÄ±r, otomatik kodlayÄ±cÄ± bunu nispeten kÃ¼Ã§Ã¼k gizli alana sÄ±ÄŸdÄ±ramaz ve dolayÄ±sÄ±yla yalnÄ±zca gÃ¶rÃ¼ntÃ¼nÃ¼n Ã¶nemli kÄ±smÄ±nÄ± yakalar. GÃ¼rÃ¼ltÃ¼ gidericileri eÄŸitirken, orijinal gÃ¶rÃ¼ntÃ¼lerle baÅŸlarÄ±z ve otomatik kodlayÄ±cÄ± iÃ§in girdi olarak yapay olarak eklenmiÅŸ gÃ¼rÃ¼ltÃ¼ iÃ§eren gÃ¶rÃ¼ntÃ¼leri kullanÄ±rÄ±z.
* **SÃ¼per Ã§Ã¶zÃ¼nÃ¼rlÃ¼k**, gÃ¶rÃ¼ntÃ¼ Ã§Ã¶zÃ¼nÃ¼rlÃ¼ÄŸÃ¼nÃ¼ artÄ±rma. YÃ¼ksek Ã§Ã¶zÃ¼nÃ¼rlÃ¼klÃ¼ gÃ¶rÃ¼ntÃ¼lerle baÅŸlarÄ±z ve daha dÃ¼ÅŸÃ¼k Ã§Ã¶zÃ¼nÃ¼rlÃ¼klÃ¼ gÃ¶rÃ¼ntÃ¼yÃ¼ otomatik kodlayÄ±cÄ± giriÅŸi olarak kullanÄ±rÄ±z.
* **Ãœretken modeller**. Otomatik kodlayÄ±cÄ±yÄ± eÄŸittikten sonra, Ã§Ã¶zÃ¼cÃ¼ kÄ±smÄ± rastgele gizli vektÃ¶rlerden yeni nesneler oluÅŸturmak iÃ§in kullanÄ±labilir.

## Varyasyonel Otomatik KodlayÄ±cÄ±lar (VAE)

Geleneksel otomatik kodlayÄ±cÄ±lar, girdi verilerinin boyutunu bir ÅŸekilde azaltarak, girdi gÃ¶rÃ¼ntÃ¼lerinin Ã¶nemli Ã¶zelliklerini belirler. Ancak, gizli vektÃ¶rler genellikle pek anlam ifade etmez. BaÅŸka bir deyiÅŸle, MNIST veri setini Ã¶rnek alÄ±rsak, farklÄ± gizli vektÃ¶rlerin hangi rakamlara karÅŸÄ±lÄ±k geldiÄŸini bulmak kolay bir iÅŸ deÄŸildir, Ã§Ã¼nkÃ¼ yakÄ±n gizli vektÃ¶rler aynÄ± rakamlara karÅŸÄ±lÄ±k gelmeyebilir.

DiÄŸer yandan, *Ã¼retken* modelleri eÄŸitmek iÃ§in gizli alanÄ± anlamak daha iyidir. Bu fikir bizi **varyasyonel otomatik kodlayÄ±cÄ±** (VAE) kavramÄ±na gÃ¶tÃ¼rÃ¼r.

VAE, gizli parametrelerin *istatistiksel daÄŸÄ±lÄ±mÄ±nÄ±* tahmin etmeyi Ã¶ÄŸrenen bir otomatik kodlayÄ±cÄ±dÄ±r, buna **gizli daÄŸÄ±lÄ±m** denir. Ã–rneÄŸin, gizli vektÃ¶rlerin belirli bir ortalama z<sub>mean</sub> ve standart sapma z<sub>sigma</sub> ile normal daÄŸÄ±lÄ±ma sahip olmasÄ±nÄ± isteyebiliriz (hem ortalama hem de standart sapma belirli bir boyut d'ye sahip vektÃ¶rlerdir). VAE'deki kodlayÄ±cÄ± bu parametreleri tahmin etmeyi Ã¶ÄŸrenir ve ardÄ±ndan Ã§Ã¶zÃ¼cÃ¼, nesneyi yeniden oluÅŸturmak iÃ§in bu daÄŸÄ±lÄ±mdan rastgele bir vektÃ¶r alÄ±r.

Ã–zetlemek gerekirse:

* Girdi vektÃ¶rÃ¼nden `z_mean` ve `z_log_sigma` tahmin ediyoruz (standart sapmayÄ± tahmin etmek yerine, onun logaritmasÄ±nÄ± tahmin ediyoruz)
* DaÄŸÄ±lÄ±mdan N(z<sub>mean</sub>,exp(z<sub>log_sigma</sub>)) daÄŸÄ±lÄ±mÄ±ndan bir vektÃ¶r `sample` Ã¶rnekliyoruz
* Ã‡Ã¶zÃ¼cÃ¼, `sample`'yi girdi vektÃ¶rÃ¼ olarak kullanarak orijinal gÃ¶rÃ¼ntÃ¼yÃ¼ Ã§Ã¶zmeye Ã§alÄ±ÅŸÄ±r

 <img src="images/vae.png" width="50%">

> GÃ¶rsel [bu blog yazÄ±sÄ±ndan](https://ijdykeman.github.io/ml/2016/12/21/cvae.html) Isaak Dykeman'a aittir.

Varyasyonel otomatik kodlayÄ±cÄ±lar, iki bÃ¶lÃ¼mden oluÅŸan karmaÅŸÄ±k bir kayÄ±p fonksiyonu kullanÄ±r:

* **Yeniden yapÄ±landÄ±rma kaybÄ±**, yeniden oluÅŸturulan bir gÃ¶rÃ¼ntÃ¼nÃ¼n hedefe ne kadar yakÄ±n olduÄŸunu gÃ¶steren kayÄ±p fonksiyonudur (Bu, Ortalama Kare HatasÄ± veya MSE olabilir). Bu, normal otomatik kodlayÄ±cÄ±lardaki kayÄ±p fonksiyonu ile aynÄ±dÄ±r.
* **KL kaybÄ±**, gizli deÄŸiÅŸken daÄŸÄ±lÄ±mlarÄ±nÄ±n normal daÄŸÄ±lÄ±ma yakÄ±n kalmasÄ±nÄ± saÄŸlar. Bu, iki istatistiksel daÄŸÄ±lÄ±mÄ±n ne kadar benzer olduÄŸunu tahmin etmek iÃ§in kullanÄ±lan [Kullback-Leibler sapmasÄ±](https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained) kavramÄ±na dayanÄ±r.

VAE'lerin Ã¶nemli bir avantajÄ±, yeni gÃ¶rÃ¼ntÃ¼ler oluÅŸturmayÄ± nispeten kolaylaÅŸtÄ±rmalarÄ±dÄ±r Ã§Ã¼nkÃ¼ gizli vektÃ¶rleri Ã¶rneklemek iÃ§in hangi daÄŸÄ±lÄ±mdan yararlanacaÄŸÄ±mÄ±zÄ± biliyoruz. Ã–rneÄŸin, MNIST Ã¼zerinde 2D gizli vektÃ¶rle VAE eÄŸitimi yaptÄ±ÄŸÄ±mÄ±zda, farklÄ± rakamlar elde etmek iÃ§in gizli vektÃ¶rÃ¼n bileÅŸenlerini deÄŸiÅŸtirebiliriz:

<img alt="vaemnist" src="images/vaemnist.png" width="50%"/>

> GÃ¶rsel [Dmitry Soshnikov](http://soshnikov.com) tarafÄ±ndan saÄŸlanmÄ±ÅŸtÄ±r.

Gizli parametre alanÄ±nÄ±n farklÄ± kÄ±sÄ±mlarÄ±ndan gizli vektÃ¶rler almaya baÅŸladÄ±ÄŸÄ±mÄ±zda, gÃ¶rÃ¼ntÃ¼lerin birbirine nasÄ±l karÄ±ÅŸtÄ±ÄŸÄ±nÄ± gÃ¶zlemleyin. Bu alanÄ± 2D olarak da gÃ¶rselleÅŸtirebiliriz:

<img alt="vaemnist cluster" src="images/vaemnist-diag.png" width="50%"/> 

> GÃ¶rsel [Dmitry Soshnikov](http://soshnikov.com) tarafÄ±ndan saÄŸlanmÄ±ÅŸtÄ±r.

## âœï¸ AlÄ±ÅŸtÄ±rmalar: Otomatik KodlayÄ±cÄ±lar

Otomatik kodlayÄ±cÄ±lar hakkÄ±nda daha fazla bilgi edinmek iÃ§in ilgili not defterlerini inceleyin:

* [TensorFlow'da Otomatik KodlayÄ±cÄ±lar](../../../../../lessons/4-ComputerVision/09-Autoencoders/AutoencodersTF.ipynb)
* [PyTorch'ta Otomatik KodlayÄ±cÄ±lar](../../../../../lessons/4-ComputerVision/09-Autoencoders/AutoEncodersPyTorch.ipynb)

## Otomatik KodlayÄ±cÄ±larÄ±n Ã–zellikleri

* **Veri Spesifik** - yalnÄ±zca eÄŸitildikleri gÃ¶rÃ¼ntÃ¼ tÃ¼rleriyle iyi Ã§alÄ±ÅŸÄ±rlar. Ã–rneÄŸin, Ã§iÃ§ekler Ã¼zerinde sÃ¼per Ã§Ã¶zÃ¼nÃ¼rlÃ¼k aÄŸÄ± eÄŸitirsek, portreler Ã¼zerinde iyi Ã§alÄ±ÅŸmaz. Bunun nedeni, aÄŸÄ±n eÄŸitim veri setinden Ã¶ÄŸrenilen Ã¶zelliklerden ince detaylar alarak daha yÃ¼ksek Ã§Ã¶zÃ¼nÃ¼rlÃ¼klÃ¼ gÃ¶rÃ¼ntÃ¼ Ã¼retebilmesidir.
* **KaybÄ±** - yeniden oluÅŸturulan gÃ¶rÃ¼ntÃ¼, orijinal gÃ¶rÃ¼ntÃ¼yle aynÄ± deÄŸildir. KayÄ±p doÄŸasÄ±, eÄŸitim sÄ±rasÄ±nda kullanÄ±lan *kayÄ±p fonksiyonu* tarafÄ±ndan tanÄ±mlanÄ±r.
* **Etiketsiz veriler** Ã¼zerinde Ã§alÄ±ÅŸÄ±r.

## [Ders sonrasÄ± sÄ±nav](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/209)

## SonuÃ§

Bu derste, AI bilimcisine sunulan Ã§eÅŸitli otomatik kodlayÄ±cÄ± tÃ¼rlerini Ã¶ÄŸrendiniz. OnlarÄ± nasÄ±l inÅŸa edeceÄŸinizi ve gÃ¶rÃ¼ntÃ¼leri nasÄ±l yeniden oluÅŸturmak iÃ§in kullanacaÄŸÄ±nÄ±zÄ± Ã¶ÄŸrendiniz. AyrÄ±ca VAE'yi ve yeni gÃ¶rÃ¼ntÃ¼ler oluÅŸturmak iÃ§in nasÄ±l kullanacaÄŸÄ±nÄ±zÄ± Ã¶ÄŸrendiniz.

## ğŸš€ Meydan Okuma

Bu derste, otomatik kodlayÄ±cÄ±larÄ±n gÃ¶rÃ¼ntÃ¼ler iÃ§in nasÄ±l kullanÄ±ldÄ±ÄŸÄ±nÄ± Ã¶ÄŸrendiniz. Ancak mÃ¼zik iÃ§in de kullanÄ±labilirler! Otomatik kodlayÄ±cÄ±larÄ± mÃ¼ziÄŸi yeniden yapÄ±landÄ±rmayÄ± Ã¶ÄŸrenmek iÃ§in kullanan Magenta projesinin [MusicVAE](https://magenta.tensorflow.org/music-vae) projesine gÃ¶z atÄ±n. Ne yaratabileceÄŸinizi gÃ¶rmek iÃ§in bu kÃ¼tÃ¼phane ile bazÄ± [deneyler](https://colab.research.google.com/github/magenta/magenta-demos/blob/master/colab-notebooks/Multitrack_MusicVAE.ipynb) yapÄ±n.

## [Ders sonrasÄ± sÄ±nav](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/208)

## GÃ¶zden GeÃ§irme & Kendi Kendine Ã‡alÄ±ÅŸma

Otomatik kodlayÄ±cÄ±lar hakkÄ±nda daha fazla bilgi iÃ§in bu kaynaklarÄ± okuyun:

* [Keras'da Otomatik KodlayÄ±cÄ±lar Ä°nÅŸasÄ±](https://blog.keras.io/building-autoencoders-in-keras.html)
* [NeuroHive'daki Blog YazÄ±sÄ±](https://neurohive.io/ru/osnovy-data-science/variacionnyj-avtojenkoder-vae/)
* [Varyasyonel Otomatik KodlayÄ±cÄ±lar AÃ§Ä±klamasÄ±](https://kvfrans.com/variational-autoencoders-explained/)
* [KoÅŸullu Varyasyonel Otomatik KodlayÄ±cÄ±lar](https://ijdykeman.github.io/ml/2016/12/21/cvae.html)

## GÃ¶rev

[TensorFlow kullanan bu not defterinin](../../../../../lessons/4-ComputerVision/09-Autoencoders/AutoEncodersTF.ipynb) sonunda bir 'gÃ¶rev' bulacaksÄ±nÄ±z - bunu Ã¶dev olarak kullanÄ±n.

**AÃ§Ä±klama**:  
Bu belge, makine tabanlÄ± AI Ã§eviri hizmetleri kullanÄ±larak Ã§evrilmiÅŸtir. DoÄŸruluk iÃ§in Ã§aba gÃ¶stersek de, otomatik Ã§evirilerin hatalar veya yanlÄ±ÅŸ anlamalar iÃ§erebileceÄŸini lÃ¼tfen dikkate alÄ±n. Orijinal belge, kendi dilinde otorite kaynaÄŸÄ± olarak kabul edilmelidir. Kritik bilgiler iÃ§in profesyonel insan Ã§evirisi Ã¶nerilmektedir. Bu Ã§evirinin kullanÄ±mÄ± sonucu ortaya Ã§Ä±kan yanlÄ±ÅŸ anlamalardan veya yanlÄ±ÅŸ yorumlamalardan sorumlu deÄŸiliz.