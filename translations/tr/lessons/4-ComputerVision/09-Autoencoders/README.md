# Otomatik KodlayÄ±cÄ±lar

CNN'leri eÄŸitirken karÅŸÄ±laÅŸÄ±lan sorunlardan biri, Ã§ok fazla etiketlenmiÅŸ veriye ihtiyaÃ§ duymamÄ±zdÄ±r. GÃ¶rÃ¼ntÃ¼ sÄ±nÄ±flandÄ±rma durumunda, gÃ¶rÃ¼ntÃ¼leri farklÄ± sÄ±nÄ±flara ayÄ±rmamÄ±z gerekir ve bu manuel bir Ã§abadÄ±r.

## [Ders Ã–ncesi Test](https://ff-quizzes.netlify.app/en/ai/quiz/17)

Ancak, CNN Ã¶zellik Ã§Ä±karÄ±cÄ±larÄ±nÄ± eÄŸitmek iÃ§in ham (etiketlenmemiÅŸ) veriyi kullanmak isteyebiliriz, bu yÃ¶nteme **kendinden denetimli Ã¶ÄŸrenme** denir. Etiketler yerine, eÄŸitim gÃ¶rÃ¼ntÃ¼lerini hem aÄŸ giriÅŸi hem de Ã§Ä±kÄ±ÅŸÄ± olarak kullanacaÄŸÄ±z. **Otomatik kodlayÄ±cÄ±** fikrinin temelinde, bir **kodlayÄ±cÄ± aÄŸ** ile giriÅŸ gÃ¶rÃ¼ntÃ¼sÃ¼nÃ¼ bir **gizli uzaya** (genellikle daha kÃ¼Ã§Ã¼k boyutlu bir vektÃ¶r) dÃ¶nÃ¼ÅŸtÃ¼rmek ve ardÄ±ndan **kod Ã§Ã¶zÃ¼cÃ¼ aÄŸ** ile orijinal gÃ¶rÃ¼ntÃ¼yÃ¼ yeniden oluÅŸturmak yer alÄ±r.

> âœ… Bir [otomatik kodlayÄ±cÄ±](https://wikipedia.org/wiki/Autoencoder), "etiketlenmemiÅŸ verilerin verimli kodlamalarÄ±nÄ± Ã¶ÄŸrenmek iÃ§in kullanÄ±lan bir tÃ¼r yapay sinir aÄŸÄ±dÄ±r."

Otomatik kodlayÄ±cÄ±yÄ±, orijinal gÃ¶rÃ¼ntÃ¼den mÃ¼mkÃ¼n olduÄŸunca fazla bilgi yakalamak ve doÄŸru bir ÅŸekilde yeniden oluÅŸturmak iÃ§in eÄŸittiÄŸimizden, aÄŸ en iyi **gÃ¶mÃ¼lÃ¼ temsili** bulmaya Ã§alÄ±ÅŸÄ±r.

![Otomatik KodlayÄ±cÄ± ÅemasÄ±](../../../../../translated_images/tr/autoencoder_schema.5e6fc9ad98a5eb61.webp)

> GÃ¶rsel [Keras blogundan](https://blog.keras.io/building-autoencoders-in-keras.html)

## Otomatik KodlayÄ±cÄ±larÄ±n KullanÄ±m SenaryolarÄ±

Orijinal gÃ¶rÃ¼ntÃ¼leri yeniden oluÅŸturmak kendi baÅŸÄ±na Ã§ok faydalÄ± gÃ¶rÃ¼nmese de, otomatik kodlayÄ±cÄ±larÄ±n Ã¶zellikle faydalÄ± olduÄŸu birkaÃ§ senaryo vardÄ±r:

* **GÃ¶rÃ¼ntÃ¼lerin boyutunu dÃ¼ÅŸÃ¼rmek iÃ§in gÃ¶rselleÅŸtirme** veya **gÃ¶rÃ¼ntÃ¼ gÃ¶mÃ¼lÃ¼ temsilleri eÄŸitmek**. Genellikle otomatik kodlayÄ±cÄ±lar PCA'dan daha iyi sonuÃ§lar verir, Ã§Ã¼nkÃ¼ gÃ¶rÃ¼ntÃ¼lerin mekansal doÄŸasÄ±nÄ± ve hiyerarÅŸik Ã¶zelliklerini dikkate alÄ±r.
* **GÃ¼rÃ¼ltÃ¼ giderme**, yani gÃ¶rÃ¼ntÃ¼den gÃ¼rÃ¼ltÃ¼yÃ¼ kaldÄ±rma. GÃ¼rÃ¼ltÃ¼ Ã§ok fazla gereksiz bilgi taÅŸÄ±dÄ±ÄŸÄ± iÃ§in, otomatik kodlayÄ±cÄ± bunu nispeten kÃ¼Ã§Ã¼k gizli uzaya sÄ±ÄŸdÄ±ramaz ve bu nedenle yalnÄ±zca gÃ¶rÃ¼ntÃ¼nÃ¼n Ã¶nemli kÄ±smÄ±nÄ± yakalar. GÃ¼rÃ¼ltÃ¼ gidericileri eÄŸitirken, orijinal gÃ¶rÃ¼ntÃ¼lerle baÅŸlarÄ±z ve otomatik kodlayÄ±cÄ±ya giriÅŸ olarak yapay olarak eklenmiÅŸ gÃ¼rÃ¼ltÃ¼ iÃ§eren gÃ¶rÃ¼ntÃ¼leri kullanÄ±rÄ±z.
* **SÃ¼per Ã§Ã¶zÃ¼nÃ¼rlÃ¼k**, gÃ¶rÃ¼ntÃ¼ Ã§Ã¶zÃ¼nÃ¼rlÃ¼ÄŸÃ¼nÃ¼ artÄ±rma. YÃ¼ksek Ã§Ã¶zÃ¼nÃ¼rlÃ¼klÃ¼ gÃ¶rÃ¼ntÃ¼lerle baÅŸlarÄ±z ve dÃ¼ÅŸÃ¼k Ã§Ã¶zÃ¼nÃ¼rlÃ¼klÃ¼ gÃ¶rÃ¼ntÃ¼yÃ¼ otomatik kodlayÄ±cÄ±ya giriÅŸ olarak kullanÄ±rÄ±z.
* **Ãœretici modeller**. Otomatik kodlayÄ±cÄ±yÄ± eÄŸittikten sonra, kod Ã§Ã¶zÃ¼cÃ¼ kÄ±smÄ± rastgele gizli vektÃ¶rlerden baÅŸlayarak yeni nesneler oluÅŸturmak iÃ§in kullanÄ±labilir.

## Varyasyonel Otomatik KodlayÄ±cÄ±lar (VAE)

Geleneksel otomatik kodlayÄ±cÄ±lar, giriÅŸ verisinin boyutunu bir ÅŸekilde azaltÄ±r ve giriÅŸ gÃ¶rÃ¼ntÃ¼lerinin Ã¶nemli Ã¶zelliklerini belirler. Ancak, gizli vektÃ¶rler genellikle Ã§ok anlamlÄ± deÄŸildir. Ã–rneÄŸin, MNIST veri setini ele alÄ±rsak, farklÄ± gizli vektÃ¶rlerin hangi rakamlara karÅŸÄ±lÄ±k geldiÄŸini anlamak kolay deÄŸildir, Ã§Ã¼nkÃ¼ yakÄ±n gizli vektÃ¶rler mutlaka aynÄ± rakamlara karÅŸÄ±lÄ±k gelmez.

Ã–te yandan, *Ã¼retici* modelleri eÄŸitmek iÃ§in gizli uzay hakkÄ±nda bir anlayÄ±ÅŸa sahip olmak daha iyidir. Bu fikir bizi **varyasyonel otomatik kodlayÄ±cÄ±ya** (VAE) gÃ¶tÃ¼rÃ¼r.

VAE, gizli parametrelerin *istatistiksel daÄŸÄ±lÄ±mÄ±nÄ±* tahmin etmeyi Ã¶ÄŸrenen bir otomatik kodlayÄ±cÄ±dÄ±r, buna **gizli daÄŸÄ±lÄ±m** denir. Ã–rneÄŸin, gizli vektÃ¶rlerin z<sub>mean</sub> ve z<sub>sigma</sub> (her ikisi de belirli bir boyut d'ye sahip vektÃ¶rlerdir) ile normal olarak daÄŸÄ±tÄ±lmasÄ±nÄ± isteyebiliriz. VAE'deki kodlayÄ±cÄ± bu parametreleri tahmin etmeyi Ã¶ÄŸrenir ve ardÄ±ndan kod Ã§Ã¶zÃ¼cÃ¼, bu daÄŸÄ±lÄ±mdan rastgele bir vektÃ¶r alarak nesneyi yeniden oluÅŸturur.

Ã–zetlemek gerekirse:

 * GiriÅŸ vektÃ¶rÃ¼nden `z_mean` ve `z_log_sigma` tahmin edilir (standart sapmanÄ±n kendisini tahmin etmek yerine, logaritmasÄ± tahmin edilir)
 * N(z<sub>mean</sub>,exp(z<sub>log\_sigma</sub>)) daÄŸÄ±lÄ±mÄ±ndan `sample` adlÄ± bir vektÃ¶r Ã¶rneklenir
 * Kod Ã§Ã¶zÃ¼cÃ¼, `sample` vektÃ¶rÃ¼nÃ¼ giriÅŸ olarak kullanarak orijinal gÃ¶rÃ¼ntÃ¼yÃ¼ Ã§Ã¶zmeye Ã§alÄ±ÅŸÄ±r

 <img src="../../../../../translated_images/tr/vae.464c465a5b6a9e25.webp" width="50%">

> GÃ¶rsel [bu blog yazÄ±sÄ±ndan](https://ijdykeman.github.io/ml/2016/12/21/cvae.html) Isaak Dykeman tarafÄ±ndan

Varyasyonel otomatik kodlayÄ±cÄ±lar, iki bÃ¶lÃ¼mden oluÅŸan karmaÅŸÄ±k bir kayÄ±p fonksiyonu kullanÄ±r:

* **Yeniden yapÄ±landÄ±rma kaybÄ±**, yeniden yapÄ±landÄ±rÄ±lmÄ±ÅŸ bir gÃ¶rÃ¼ntÃ¼nÃ¼n hedefe ne kadar yakÄ±n olduÄŸunu gÃ¶steren kayÄ±p fonksiyonudur (Ã¶rneÄŸin Ortalama Kare Hata veya MSE olabilir). Bu, normal otomatik kodlayÄ±cÄ±lardaki kayÄ±p fonksiyonuyla aynÄ±dÄ±r.
* **KL kaybÄ±**, gizli deÄŸiÅŸken daÄŸÄ±lÄ±mlarÄ±nÄ±n normal daÄŸÄ±lÄ±ma yakÄ±n kalmasÄ±nÄ± saÄŸlar. Bu, iki istatistiksel daÄŸÄ±lÄ±mÄ±n ne kadar benzer olduÄŸunu tahmin etmek iÃ§in kullanÄ±lan bir metrik olan [Kullback-Leibler sapmasÄ±](https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained) temel alÄ±narak hesaplanÄ±r.

VAE'lerin Ã¶nemli bir avantajÄ±, yeni gÃ¶rÃ¼ntÃ¼leri nispeten kolay bir ÅŸekilde oluÅŸturabilmemize olanak tanÄ±masÄ±dÄ±r, Ã§Ã¼nkÃ¼ gizli vektÃ¶rlerin Ã¶rnekleneceÄŸi daÄŸÄ±lÄ±mÄ± biliriz. Ã–rneÄŸin, MNIST Ã¼zerinde 2D gizli vektÃ¶rle VAE eÄŸitirsek, gizli vektÃ¶rÃ¼n bileÅŸenlerini deÄŸiÅŸtirerek farklÄ± rakamlar elde edebiliriz:

<img alt="vaemnist" src="../../../../../translated_images/tr/vaemnist.cab9e602dc08dc50.webp" width="50%"/>

> GÃ¶rsel [Dmitry Soshnikov](http://soshnikov.com) tarafÄ±ndan

Gizli parametre uzayÄ±nÄ±n farklÄ± bÃ¶lÃ¼mlerinden gizli vektÃ¶rler almaya baÅŸladÄ±kÃ§a, gÃ¶rÃ¼ntÃ¼lerin birbirine nasÄ±l karÄ±ÅŸtÄ±ÄŸÄ±nÄ± gÃ¶zlemleyin. Bu uzayÄ± ayrÄ±ca 2D olarak gÃ¶rselleÅŸtirebiliriz:

<img alt="vaemnist cluster" src="../../../../../translated_images/tr/vaemnist-diag.694315f775d5d666.webp" width="50%"/> 

> GÃ¶rsel [Dmitry Soshnikov](http://soshnikov.com) tarafÄ±ndan

## âœï¸ Egzersizler: Otomatik KodlayÄ±cÄ±lar

Otomatik kodlayÄ±cÄ±lar hakkÄ±nda daha fazla bilgi edinmek iÃ§in ÅŸu ilgili not defterlerini inceleyin:

* [TensorFlow'da Otomatik KodlayÄ±cÄ±lar](AutoencodersTF.ipynb)
* [PyTorch'ta Otomatik KodlayÄ±cÄ±lar](AutoEncodersPyTorch.ipynb)

## Otomatik KodlayÄ±cÄ±larÄ±n Ã–zellikleri

* **Veriye Ã–zgÃ¼** - yalnÄ±zca eÄŸitildikleri gÃ¶rÃ¼ntÃ¼ tÃ¼rleriyle iyi Ã§alÄ±ÅŸÄ±rlar. Ã–rneÄŸin, bir sÃ¼per Ã§Ã¶zÃ¼nÃ¼rlÃ¼k aÄŸÄ± Ã§iÃ§ekler Ã¼zerinde eÄŸitilirse, portrelerde iyi Ã§alÄ±ÅŸmaz. Bunun nedeni, aÄŸÄ±n daha yÃ¼ksek Ã§Ã¶zÃ¼nÃ¼rlÃ¼klÃ¼ gÃ¶rÃ¼ntÃ¼ Ã¼retebilmesi iÃ§in eÄŸitim veri setinden Ã¶ÄŸrenilen Ã¶zelliklerden ince detaylar almasÄ±dÄ±r.
* **KayÄ±plÄ±** - yeniden yapÄ±landÄ±rÄ±lmÄ±ÅŸ gÃ¶rÃ¼ntÃ¼, orijinal gÃ¶rÃ¼ntÃ¼yle aynÄ± deÄŸildir. KaybÄ±n doÄŸasÄ±, eÄŸitim sÄ±rasÄ±nda kullanÄ±lan *kayÄ±p fonksiyonu* ile tanÄ±mlanÄ±r.
* **EtiketlenmemiÅŸ veri** Ã¼zerinde Ã§alÄ±ÅŸÄ±r.

## [Ders SonrasÄ± Test](https://ff-quizzes.netlify.app/en/ai/quiz/18)

## SonuÃ§

Bu derste, bir AI bilim insanÄ±nÄ±n kullanabileceÄŸi Ã§eÅŸitli otomatik kodlayÄ±cÄ± tÃ¼rlerini Ã¶ÄŸrendiniz. BunlarÄ± nasÄ±l oluÅŸturacaÄŸÄ±nÄ±zÄ± ve gÃ¶rÃ¼ntÃ¼leri yeniden yapÄ±landÄ±rmak iÃ§in nasÄ±l kullanacaÄŸÄ±nÄ±zÄ± Ã¶ÄŸrendiniz. AyrÄ±ca VAE'yi ve yeni gÃ¶rÃ¼ntÃ¼ler oluÅŸturmak iÃ§in nasÄ±l kullanÄ±lacaÄŸÄ±nÄ± Ã¶ÄŸrendiniz.

## ğŸš€ Meydan Okuma

Bu derste, otomatik kodlayÄ±cÄ±larÄ± gÃ¶rÃ¼ntÃ¼ler iÃ§in kullanmayÄ± Ã¶ÄŸrendiniz. Ancak, mÃ¼zik iÃ§in de kullanÄ±labilirler! Magenta projesinin [MusicVAE](https://magenta.tensorflow.org/music-vae) projesine gÃ¶z atÄ±n; bu proje, mÃ¼ziÄŸi yeniden yapÄ±landÄ±rmayÄ± Ã¶ÄŸrenmek iÃ§in otomatik kodlayÄ±cÄ±larÄ± kullanÄ±r. Bu kÃ¼tÃ¼phane ile bazÄ± [deneyler](https://colab.research.google.com/github/magenta/magenta-demos/blob/master/colab-notebooks/Multitrack_MusicVAE.ipynb) yaparak neler yaratabileceÄŸinizi gÃ¶rÃ¼n.

## [Ders SonrasÄ± Test](https://ff-quizzes.netlify.app/en/ai/quiz/16)

## Ä°nceleme ve Kendi Kendine Ã‡alÄ±ÅŸma

Referans iÃ§in, otomatik kodlayÄ±cÄ±lar hakkÄ±nda daha fazla bilgi edinmek iÃ§in ÅŸu kaynaklarÄ± okuyun:

* [Keras'ta Otomatik KodlayÄ±cÄ±lar OluÅŸturma](https://blog.keras.io/building-autoencoders-in-keras.html)
* [NeuroHive'daki Blog YazÄ±sÄ±](https://neurohive.io/ru/osnovy-data-science/variacionnyj-avtojenkoder-vae/)
* [Varyasyonel Otomatik KodlayÄ±cÄ±lar AÃ§Ä±klamasÄ±](https://kvfrans.com/variational-autoencoders-explained/)
* [KoÅŸullu Varyasyonel Otomatik KodlayÄ±cÄ±lar](https://ijdykeman.github.io/ml/2016/12/21/cvae.html)

## Ã–dev

[TensorFlow kullanÄ±larak hazÄ±rlanan bu not defterinin](AutoencodersTF.ipynb) sonunda bir 'gÃ¶rev' bulacaksÄ±nÄ±z - bunu Ã¶deviniz olarak kullanÄ±n.

---

