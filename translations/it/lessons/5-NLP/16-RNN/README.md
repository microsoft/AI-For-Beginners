# Redes Neurais Recorrentes

## [Quiz pr√©-aula](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/116)

Nas se√ß√µes anteriores, temos utilizado representa√ß√µes sem√¢nticas ricas de texto e um classificador linear simples sobre as incorpora√ß√µes. O que essa arquitetura faz √© capturar o significado agregado das palavras em uma frase, mas n√£o leva em considera√ß√£o a **ordem** das palavras, pois a opera√ß√£o de agrega√ß√£o sobre as incorpora√ß√µes removeu essa informa√ß√£o do texto original. Como esses modelos n√£o conseguem modelar a ordem das palavras, eles n√£o conseguem resolver tarefas mais complexas ou amb√≠guas, como gera√ß√£o de texto ou resposta a perguntas.

Para capturar o significado de uma sequ√™ncia de texto, precisamos usar outra arquitetura de rede neural, chamada de **rede neural recorrente**, ou RNN. Na RNN, passamos nossa frase pela rede um s√≠mbolo de cada vez, e a rede produz um **estado**, que ent√£o passamos para a rede novamente com o pr√≥ximo s√≠mbolo.

![RNN](../../../../../translated_images/rnn.27f5c29c53d727b546ad3961637a267f0fe9ec5ab01f2a26a853c92fcefbb574.it.png)

> Imagem do autor

Dada a sequ√™ncia de entrada de tokens X<sub>0</sub>,...,X<sub>n</sub>, a RNN cria uma sequ√™ncia de blocos de rede neural e treina essa sequ√™ncia de ponta a ponta usando retropropaga√ß√£o. Cada bloco da rede recebe um par (X<sub>i</sub>,S<sub>i</sub>) como entrada e produz S<sub>i+1</sub> como resultado. O estado final S<sub>n</sub> ou (sa√≠da Y<sub>n</sub>) vai para um classificador linear para produzir o resultado. Todos os blocos da rede compartilham os mesmos pesos e s√£o treinados de ponta a ponta usando uma √∫nica passagem de retropropaga√ß√£o.

Como os vetores de estado S<sub>0</sub>,...,S<sub>n</sub> s√£o passados pela rede, ela consegue aprender as depend√™ncias sequenciais entre as palavras. Por exemplo, quando a palavra *n√£o* aparece em algum lugar na sequ√™ncia, ela pode aprender a negar certos elementos dentro do vetor de estado, resultando em nega√ß√£o.

> ‚úÖ Como os pesos de todos os blocos RNN na imagem acima s√£o compartilhados, a mesma imagem pode ser representada como um √∫nico bloco (√† direita) com um loop de feedback recorrente, que passa o estado de sa√≠da da rede de volta para a entrada.

## Anatomia de uma C√©lula RNN

Vamos ver como uma c√©lula RNN simples √© organizada. Ela aceita o estado anterior S<sub>i-1</sub> e o s√≠mbolo atual X<sub>i</sub> como entradas e precisa produzir o estado de sa√≠da S<sub>i</sub> (e, √†s vezes, tamb√©m estamos interessados em alguma outra sa√≠da Y<sub>i</sub>, como no caso de redes generativas).

Uma c√©lula RNN simples possui duas matrizes de pesos internas: uma transforma um s√≠mbolo de entrada (vamos cham√°-la de W) e a outra transforma um estado de entrada (H). Nesse caso, a sa√≠da da rede √© calculada como œÉ(W√óX<sub>i</sub>+H√óS<sub>i-1</sub>+b), onde œÉ √© a fun√ß√£o de ativa√ß√£o e b √© um vi√©s adicional.

<img alt="Anatomia da C√©lula RNN" src="images/rnn-anatomy.png" width="50%"/>

> Imagem do autor

Em muitos casos, os tokens de entrada s√£o passados pela camada de incorpora√ß√£o antes de entrar na RNN para reduzir a dimensionalidade. Nesse caso, se a dimens√£o dos vetores de entrada √© *emb_size*, e o vetor de estado √© *hid_size* - o tamanho de W √© *emb_size*√ó*hid_size*, e o tamanho de H √© *hid_size*√ó*hid_size*.

## Mem√≥ria de Longo e Curto Prazo (LSTM)

Um dos principais problemas das RNNs cl√°ssicas √© o chamado problema dos **gradientes que desaparecem**. Como as RNNs s√£o treinadas de ponta a ponta em uma √∫nica passagem de retropropaga√ß√£o, elas t√™m dificuldade em propagar o erro para as primeiras camadas da rede, e assim a rede n√£o consegue aprender relacionamentos entre tokens distantes. Uma das maneiras de evitar esse problema √© introduzir **gerenciamento de estado expl√≠cito** usando os chamados **port√µes**. Existem duas arquiteturas bem conhecidas desse tipo: **Mem√≥ria de Longo e Curto Prazo** (LSTM) e **Unidade de Rel√© Gated** (GRU).

![Imagem mostrando um exemplo de c√©lula de mem√≥ria de longo e curto prazo](../../../../../lessons/5-NLP/16-RNN/images/long-short-term-memory-cell.svg)

> Fonte da imagem TBD

A Rede LSTM √© organizada de maneira semelhante √† RNN, mas h√° dois estados que est√£o sendo passados de camada para camada: o estado real C e o vetor oculto H. Em cada unidade, o vetor oculto H<sub>i</sub> √© concatenado com a entrada X<sub>i</sub>, e eles controlam o que acontece com o estado C atrav√©s dos **port√µes**. Cada port√£o √© uma rede neural com ativa√ß√£o sigmoide (sa√≠da na faixa [0,1]), que pode ser pensada como uma m√°scara bit a bit quando multiplicada pelo vetor de estado. Existem os seguintes port√µes (da esquerda para a direita na imagem acima):

* O **port√£o de esquecimento** recebe um vetor oculto e determina quais componentes do vetor C devemos esquecer e quais devemos passar adiante.
* O **port√£o de entrada** pega algumas informa√ß√µes dos vetores de entrada e ocultos e as insere no estado.
* O **port√£o de sa√≠da** transforma o estado atrav√©s de uma camada linear com ativa√ß√£o *tanh*, e ent√£o seleciona alguns de seus componentes usando um vetor oculto H<sub>i</sub> para produzir um novo estado C<sub>i+1</sub>.

Os componentes do estado C podem ser pensados como algumas bandeiras que podem ser ativadas ou desativadas. Por exemplo, quando encontramos um nome *Alice* na sequ√™ncia, podemos querer assumir que se refere a um personagem feminino e ativar a bandeira no estado que temos um substantivo feminino na frase. Quando encontramos mais adiante as frases *e Tom*, elevamos a bandeira de que temos um substantivo plural. Assim, manipulando o estado, podemos supostamente acompanhar as propriedades gramaticais das partes da frase.

> ‚úÖ Um excelente recurso para entender os detalhes do LSTM √© este √≥timo artigo [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) de Christopher Olah.

## RNNs Bidirecionais e Multicamadas

Discutimos redes recorrentes que operam em uma dire√ß√£o, do in√≠cio de uma sequ√™ncia at√© o final. Isso parece natural, pois se assemelha √† maneira como lemos e ouvimos a fala. No entanto, como em muitos casos pr√°ticos temos acesso aleat√≥rio √† sequ√™ncia de entrada, pode fazer sentido executar o c√°lculo recorrente em ambas as dire√ß√µes. Essas redes s√£o chamadas de RNNs **bidirecionais**. Ao lidar com uma rede bidirecional, precisar√≠amos de dois vetores de estado oculto, um para cada dire√ß√£o.

Uma rede recorrente, seja unidirecional ou bidirecional, captura certos padr√µes dentro de uma sequ√™ncia e pode armazen√°-los em um vetor de estado ou pass√°-los para a sa√≠da. Assim como nas redes convolucionais, podemos construir outra camada recorrente sobre a primeira para capturar padr√µes de n√≠vel superior e construir a partir de padr√µes de baixo n√≠vel extra√≠dos pela primeira camada. Isso nos leva √† no√ß√£o de uma **RNN de m√∫ltiplas camadas**, que consiste em duas ou mais redes recorrentes, onde a sa√≠da da camada anterior √© passada para a pr√≥xima camada como entrada.

![Imagem mostrando uma RNN de mem√≥ria de longo e curto prazo multicamada](../../../../../translated_images/multi-layer-lstm.dd975e29bb2a59fe58b429db833932d734c81f211cad2783797a9608984acb8c.it.jpg)

*Imagem de [este maravilhoso post](https://towardsdatascience.com/from-a-lstm-cell-to-a-multilayer-lstm-network-with-pytorch-2899eb5696f3) de Fernando L√≥pez*

## ‚úçÔ∏è Exerc√≠cios: Incorpora√ß√µes

Continue seu aprendizado nos seguintes notebooks:

* [RNNs com PyTorch](../../../../../lessons/5-NLP/16-RNN/RNNPyTorch.ipynb)
* [RNNs com TensorFlow](../../../../../lessons/5-NLP/16-RNN/RNNTF.ipynb)

## Conclus√£o

Nesta unidade, vimos que as RNNs podem ser usadas para classifica√ß√£o de sequ√™ncias, mas na verdade, elas podem lidar com muitas outras tarefas, como gera√ß√£o de texto, tradu√ß√£o autom√°tica e mais. Consideraremos essas tarefas na pr√≥xima unidade.

## üöÄ Desafio

Leia um pouco sobre LSTMs e considere suas aplica√ß√µes:

- [Grid Long Short-Term Memory](https://arxiv.org/pdf/1507.01526v1.pdf)
- [Show, Attend and Tell: Gera√ß√£o de Legendas de Imagens Neurais com Aten√ß√£o Visual](https://arxiv.org/pdf/1502.03044v2.pdf)

## [Quiz p√≥s-aula](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/216)

## Revis√£o e Autoestudo

- [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) de Christopher Olah.

## [Tarefa: Notebooks](assignment.md)

**Disclaimer**: 
Este documento ha sido traducido utilizando servicios de traducci√≥n autom√°tica basados en IA. Aunque nos esforzamos por lograr precisi√≥n, tenga en cuenta que las traducciones automatizadas pueden contener errores o inexactitudes. El documento original en su idioma nativo debe considerarse la fuente autorizada. Para informaci√≥n cr√≠tica, se recomienda la traducci√≥n profesional por parte de un humano. No somos responsables de ning√∫n malentendido o mala interpretaci√≥n que surja del uso de esta traducci√≥n.