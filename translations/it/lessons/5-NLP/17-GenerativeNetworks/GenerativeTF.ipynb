{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reti generative\n",
    "\n",
    "Le Reti Neurali Ricorrenti (RNN) e le loro varianti con celle gated, come le celle Long Short Term Memory (LSTM) e le Gated Recurrent Units (GRU), hanno fornito un meccanismo per il modellamento del linguaggio, ovvero possono apprendere l'ordine delle parole e fornire previsioni per la parola successiva in una sequenza. Questo ci permette di utilizzare le RNN per **compiti generativi**, come la generazione di testo ordinario, la traduzione automatica e persino la descrizione di immagini.\n",
    "\n",
    "Nell'architettura RNN che abbiamo discusso nell'unità precedente, ogni unità RNN produceva il prossimo stato nascosto come output. Tuttavia, possiamo anche aggiungere un altro output a ciascuna unità ricorrente, che ci consentirebbe di generare una **sequenza** (di lunghezza uguale alla sequenza originale). Inoltre, possiamo utilizzare unità RNN che non accettano un input a ogni passo, ma prendono solo un vettore di stato iniziale e poi producono una sequenza di output.\n",
    "\n",
    "In questo notebook, ci concentreremo su modelli generativi semplici che ci aiutano a generare testo. Per semplicità, costruiamo una **rete a livello di carattere**, che genera testo lettera per lettera. Durante l'addestramento, dobbiamo prendere un corpus di testo e suddividerlo in sequenze di lettere.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "\n",
    "ds_train, ds_test = tfds.load('ag_news_subset').values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Costruire un vocabolario di caratteri\n",
    "\n",
    "Per costruire una rete generativa a livello di caratteri, dobbiamo suddividere il testo in singoli caratteri invece che in parole. Il livello `TextVectorization` che abbiamo utilizzato in precedenza non può farlo, quindi abbiamo due opzioni:\n",
    "\n",
    "* Caricare manualmente il testo e fare la tokenizzazione \"a mano\", come mostrato in [questo esempio ufficiale di Keras](https://keras.io/examples/generative/lstm_character_level_text_generation/)\n",
    "* Utilizzare la classe `Tokenizer` per la tokenizzazione a livello di caratteri.\n",
    "\n",
    "Opteremo per la seconda opzione. `Tokenizer` può essere utilizzato anche per la tokenizzazione in parole, quindi dovrebbe essere possibile passare facilmente dalla tokenizzazione a livello di caratteri a quella a livello di parole.\n",
    "\n",
    "Per effettuare la tokenizzazione a livello di caratteri, dobbiamo passare il parametro `char_level=True`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(x):\n",
    "    return x['title']+' '+x['description']\n",
    "\n",
    "def tupelize(x):\n",
    "    return (extract_text(x),x['label'])\n",
    "\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(char_level=True,lower=False)\n",
    "tokenizer.fit_on_texts([x['title'].numpy().decode('utf-8') for x in ds_train])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vogliamo anche utilizzare un token speciale per indicare **fine della sequenza**, che chiameremo `<eos>`. Aggiungiamolo manualmente al vocabolario:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "eos_token = len(tokenizer.word_index)+1\n",
    "tokenizer.word_index['<eos>'] = eos_token\n",
    "\n",
    "vocab_size = eos_token + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[48, 2, 10, 10, 5, 44, 1, 25, 5, 8, 10, 13, 78]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences(['Hello, world!'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Addestrare un RNN generativo per generare titoli\n",
    "\n",
    "Il modo in cui addestreremo l'RNN per generare titoli di notizie è il seguente. A ogni passo, prenderemo un titolo, che verrà fornito a un RNN, e per ogni carattere di input chiederemo alla rete di generare il carattere di output successivo:\n",
    "\n",
    "![Immagine che mostra un esempio di generazione RNN della parola 'HELLO'.](../../../../../translated_images/it/rnn-generate.56c54afb52f9781d.webp)\n",
    "\n",
    "Per l'ultimo carattere della nostra sequenza, chiederemo alla rete di generare il token `<eos>`.\n",
    "\n",
    "La principale differenza con l'RNN generativo che stiamo utilizzando qui è che prenderemo un output da ogni passo dell'RNN, e non solo dalla cella finale. Questo può essere ottenuto specificando il parametro `return_sequences` alla cella RNN.\n",
    "\n",
    "Quindi, durante l'addestramento, un input per la rete sarà una sequenza di caratteri codificati di una certa lunghezza, e un output sarà una sequenza della stessa lunghezza, ma spostata di un elemento e terminata con `<eos>`. Il minibatch sarà composto da diverse di queste sequenze, e sarà necessario utilizzare **padding** per allineare tutte le sequenze.\n",
    "\n",
    "Creiamo funzioni che trasformeranno il dataset per noi. Poiché vogliamo aggiungere padding alle sequenze a livello di minibatch, prima raggrupperemo il dataset chiamando `.batch()`, e poi lo `map` per effettuare la trasformazione. Quindi, la funzione di trasformazione prenderà un intero minibatch come parametro:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def title_batch(x):\n",
    "    x = [t.numpy().decode('utf-8') for t in x]\n",
    "    z = tokenizer.texts_to_sequences(x)\n",
    "    z = tf.keras.preprocessing.sequence.pad_sequences(z)\n",
    "    return tf.one_hot(z,vocab_size), tf.one_hot(tf.concat([z[:,1:],tf.constant(eos_token,shape=(len(z),1))],axis=1),vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alcune cose importanti che facciamo qui:\n",
    "* Per prima cosa estraiamo il testo effettivo dal tensore di stringhe\n",
    "* `text_to_sequences` converte l'elenco di stringhe in un elenco di tensori interi\n",
    "* `pad_sequences` riempie quei tensori fino alla loro lunghezza massima\n",
    "* Infine, codifichiamo tutti i caratteri in one-hot, e facciamo anche lo spostamento e l'aggiunta di `<eos>`. Presto vedremo perché abbiamo bisogno di caratteri codificati in one-hot.\n",
    "\n",
    "Tuttavia, questa funzione è **Pythonic**, ovvero non può essere tradotta automaticamente nel grafo computazionale di Tensorflow. Otterremo errori se proviamo a utilizzare questa funzione direttamente nella funzione `Dataset.map`. Dobbiamo racchiudere questa chiamata Pythonic utilizzando il wrapper `py_function`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def title_batch_fn(x):\n",
    "    x = x['title']\n",
    "    a,b = tf.py_function(title_batch,inp=[x],Tout=(tf.float32,tf.float32))\n",
    "    return a,b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Nota**: Differenziare tra funzioni di trasformazione Pythonic e Tensorflow potrebbe sembrare un po' troppo complesso, e potresti chiederti perché non trasformiamo il dataset utilizzando funzioni Python standard prima di passarlo a `fit`. Sebbene ciò sia sicuramente possibile, utilizzare `Dataset.map` ha un enorme vantaggio, poiché la pipeline di trasformazione dei dati viene eseguita utilizzando il grafo computazionale di Tensorflow, che sfrutta i calcoli della GPU e riduce al minimo la necessità di trasferire dati tra CPU e GPU.\n",
    "\n",
    "Ora possiamo costruire la nostra rete generativa e iniziare l'addestramento. Può essere basata su qualsiasi cella ricorrente di cui abbiamo discusso nell'unità precedente (semplice, LSTM o GRU). Nel nostro esempio utilizzeremo LSTM.\n",
    "\n",
    "Poiché la rete prende caratteri come input e la dimensione del vocabolario è piuttosto piccola, non abbiamo bisogno di uno strato di embedding: l'input codificato in one-hot può essere passato direttamente alla cella LSTM. Lo strato di output sarà un classificatore `Dense` che convertirà l'output dell'LSTM in numeri di token codificati in one-hot.\n",
    "\n",
    "Inoltre, poiché stiamo lavorando con sequenze di lunghezza variabile, possiamo utilizzare uno strato `Masking` per creare una maschera che ignorerà la parte riempita della stringa. Questo non è strettamente necessario, poiché non siamo particolarmente interessati a tutto ciò che va oltre il token `<eos>`, ma lo utilizzeremo per acquisire un po' di esperienza con questo tipo di strato. L'`input_shape` sarà `(None, vocab_size)`, dove `None` indica la sequenza di lunghezza variabile, e la forma dell'output sarà anch'essa `(None, vocab_size)`, come puoi vedere dal `summary`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "masking (Masking)            (None, None, 84)          0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, None, 128)         109056    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, None, 84)          10836     \n",
      "=================================================================\n",
      "Total params: 119,892\n",
      "Trainable params: 119,892\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "15000/15000 [==============================] - 229s 15ms/step - loss: 1.5385\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fa40c1245e0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Masking(input_shape=(None,vocab_size)),\n",
    "    keras.layers.LSTM(128,return_sequences=True),\n",
    "    keras.layers.Dense(vocab_size,activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy')\n",
    "\n",
    "model.fit(ds_train.batch(8).map(title_batch_fn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generazione dell'output\n",
    "\n",
    "Ora che abbiamo addestrato il modello, vogliamo usarlo per generare un output. Prima di tutto, abbiamo bisogno di un modo per decodificare il testo rappresentato da una sequenza di numeri di token. Per fare questo, potremmo utilizzare la funzione `tokenizer.sequences_to_texts`; tuttavia, non funziona bene con la tokenizzazione a livello di carattere. Pertanto, prenderemo un dizionario di token dal tokenizer (chiamato `word_index`), costruiremo una mappa inversa e scriveremo la nostra funzione di decodifica:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_map = {val:key for key, val in tokenizer.word_index.items()}\n",
    "\n",
    "def decode(x):\n",
    "    return ''.join([reverse_map[t] for t in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ora iniziamo la generazione. Partiremo da una stringa `start`, la codificheremo in una sequenza `inp`, e ad ogni passo chiameremo la nostra rete per inferire il carattere successivo.\n",
    "\n",
    "L'output della rete `out` è un vettore di `vocab_size` elementi che rappresentano le probabilità di ciascun token, e possiamo trovare il numero del token più probabile utilizzando `argmax`. Successivamente, aggiungiamo questo carattere alla lista generata di token e procediamo con la generazione. Questo processo di generazione di un carattere viene ripetuto `size` volte per generare il numero richiesto di caratteri, e terminiamo anticipatamente quando viene incontrato `eos_token`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Today #39;s lead to strike for the strike for the strike for the strike (AFP)'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate(model,size=100,start='Today '):\n",
    "        inp = tokenizer.texts_to_sequences([start])[0]\n",
    "        chars = inp\n",
    "        for i in range(size):\n",
    "            out = model(tf.expand_dims(tf.one_hot(inp,vocab_size),0))[0][-1]\n",
    "            nc = tf.argmax(out)\n",
    "            if nc==eos_token:\n",
    "                break\n",
    "            chars.append(nc.numpy())\n",
    "            inp = inp+[nc]\n",
    "        return decode(chars)\n",
    "    \n",
    "generate(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Campionamento dell'output durante l'addestramento\n",
    "\n",
    "Poiché non abbiamo metriche utili come *accuratezza*, l'unico modo per verificare che il nostro modello stia migliorando è **campionare** le stringhe generate durante l'addestramento. Per farlo, utilizzeremo **callback**, ovvero funzioni che possiamo passare alla funzione `fit` e che verranno chiamate periodicamente durante l'addestramento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "15000/15000 [==============================] - 226s 15ms/step - loss: 1.2703\n",
      "Today #39;s a lead in the company for the strike\n",
      "Epoch 2/3\n",
      "15000/15000 [==============================] - 227s 15ms/step - loss: 1.2057\n",
      "Today #39;s the Market Service on Security Start (AP)\n",
      "Epoch 3/3\n",
      "15000/15000 [==============================] - 226s 15ms/step - loss: 1.1752\n",
      "Today #39;s a line on the strike to start for the start\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fa40c74e3d0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampling_callback = keras.callbacks.LambdaCallback(\n",
    "  on_epoch_end = lambda batch, logs: print(generate(model))\n",
    ")\n",
    "\n",
    "model.fit(ds_train.batch(8).map(title_batch_fn),callbacks=[sampling_callback],epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questo esempio genera già un testo piuttosto buono, ma può essere ulteriormente migliorato in diversi modi:\n",
    "\n",
    "* **Più testo**. Abbiamo utilizzato solo titoli per il nostro compito, ma potresti voler sperimentare con il testo completo. Ricorda che gli RNN non sono molto efficaci nel gestire sequenze lunghe, quindi ha senso suddividerle in frasi più brevi o allenarsi sempre su una lunghezza di sequenza fissa di un valore predefinito `num_chars` (ad esempio, 256). Potresti provare a modificare l'esempio sopra in un'architettura simile, utilizzando il [tutorial ufficiale di Keras](https://keras.io/examples/generative/lstm_character_level_text_generation/) come ispirazione.\n",
    "\n",
    "* **LSTM multilivello**. Ha senso provare 2 o 3 livelli di celle LSTM. Come abbiamo menzionato nell'unità precedente, ogni livello di LSTM estrae determinati schemi dal testo e, nel caso di un generatore a livello di carattere, possiamo aspettarci che il livello inferiore di LSTM sia responsabile dell'estrazione delle sillabe, mentre i livelli superiori si occupano di parole e combinazioni di parole. Questo può essere implementato semplicemente passando un parametro relativo al numero di livelli al costruttore di LSTM.\n",
    "\n",
    "* Potresti anche voler sperimentare con **unità GRU** per vedere quali offrono prestazioni migliori e con **dimensioni diverse dei livelli nascosti**. Un livello nascosto troppo grande potrebbe portare a overfitting (ad esempio, la rete apprenderà il testo esatto), mentre una dimensione più piccola potrebbe non produrre risultati soddisfacenti.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generazione di testo morbido e temperatura\n",
    "\n",
    "Nella precedente definizione di `generate`, sceglievamo sempre il carattere con la probabilità più alta come prossimo carattere nel testo generato. Questo portava spesso al fatto che il testo \"ciclasse\" tra le stesse sequenze di caratteri più e più volte, come in questo esempio:\n",
    "```\n",
    "today of the second the company and a second the company ...\n",
    "```\n",
    "\n",
    "Tuttavia, se osserviamo la distribuzione di probabilità per il prossimo carattere, potrebbe accadere che la differenza tra alcune delle probabilità più alte non sia così grande, ad esempio un carattere potrebbe avere una probabilità di 0,2, un altro di 0,19, ecc. Per esempio, quando cerchiamo il prossimo carattere nella sequenza '*play*', il carattere successivo potrebbe essere ugualmente uno spazio o **e** (come nella parola *player*).\n",
    "\n",
    "Questo ci porta alla conclusione che non è sempre \"giusto\" selezionare il carattere con la probabilità più alta, perché scegliere il secondo più probabile potrebbe comunque portarci a un testo significativo. È più saggio **campionare** i caratteri dalla distribuzione di probabilità fornita dall'output della rete.\n",
    "\n",
    "Questo campionamento può essere effettuato utilizzando la funzione `np.multinomial`, che implementa la cosiddetta **distribuzione multinomiale**. Una funzione che implementa questa generazione di testo **morbida** è definita di seguito:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Temperature = 0.3\n",
      "Today #39;s strike #39; to start at the store return\n",
      "On Sunday PO to Be Data Profit Up (Reuters)\n",
      "Moscow, SP wins straight to the Microsoft #39;s control of the space start\n",
      "President olding of the blast start for the strike to pay &lt;b&gt;...&lt;/b&gt;\n",
      "Little red riding hood ficed to the spam countered in European &lt;b&gt;...&lt;/b&gt;\n",
      "\n",
      "--- Temperature = 0.8\n",
      "Today countie strikes ryder missile faces food market blut\n",
      "On Sunday collores lose-toppy of sale of Bullment in &lt;b&gt;...&lt;/b&gt;\n",
      "Moscow, IBM Diffeiting in Afghan Software Hotels (Reuters)\n",
      "President Ol Luster for Profit Peaced Raised (AP)\n",
      "Little red riding hood dace on depart talks #39; bank up\n",
      "\n",
      "--- Temperature = 1.0\n",
      "Today wits House buiting debate fixes #39; supervice stake again\n",
      "On Sunday arling digital poaching In for level\n",
      "Moscow, DS Up 7, Top Proble Protest Caprey Mamarian Strike\n",
      "President teps help of roubler stepted lessabul-Dhalitics (AFP)\n",
      "Little red riding hood signs on cash in Carter-youb\n",
      "\n",
      "--- Temperature = 1.3\n",
      "Today wits flawer ro, pSIA figat's co DroftwavesIs Talo up\n",
      "On Sunday hround elitwing wint EU Powerburlinetien\n",
      "Moscow, Bazz #39;s sentries olymen winnelds' next for Olympite Huc?\n",
      "President lost securitys from power Elections in Smiltrials\n",
      "Little red riding hood vides profit, exponituity, profitmainalist-at said listers\n",
      "\n",
      "--- Temperature = 1.8\n",
      "Today #39;It: He deat: N.KA Asside\n",
      "On Sunday i arry Par aldeup patient Wo stele1\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-db32367a0feb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\n--- Temperature = {i}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerate_soft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-33-db32367a0feb>\u001b[0m in \u001b[0;36mgenerate_soft\u001b[0;34m(model, size, start, temperature)\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mchars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'Today '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'On Sunday '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Moscow, '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'President '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Little red riding hood '\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-3f5fa6130b1d>\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mreverse_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-3f5fa6130b1d>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mreverse_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "def generate_soft(model,size=100,start='Today ',temperature=1.0):\n",
    "        inp = tokenizer.texts_to_sequences([start])[0]\n",
    "        chars = inp\n",
    "        for i in range(size):\n",
    "            out = model(tf.expand_dims(tf.one_hot(inp,vocab_size),0))[0][-1]\n",
    "            probs = tf.exp(tf.math.log(out)/temperature).numpy().astype(np.float64)\n",
    "            probs = probs/np.sum(probs)\n",
    "            nc = np.argmax(np.random.multinomial(1,probs,1))\n",
    "            if nc==eos_token:\n",
    "                break\n",
    "            chars.append(nc)\n",
    "            inp = inp+[nc]\n",
    "        return decode(chars)\n",
    "\n",
    "words = ['Today ','On Sunday ','Moscow, ','President ','Little red riding hood ']\n",
    "    \n",
    "for i in [0.3,0.8,1.0,1.3,1.8]:\n",
    "    print(f\"\\n--- Temperature = {i}\")\n",
    "    for j in range(5):\n",
    "        print(generate_soft(model,size=300,start=words[j],temperature=i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abbiamo introdotto un altro parametro chiamato **temperatura**, che viene utilizzato per indicare quanto strettamente dobbiamo aderire alla probabilità più alta. Se la temperatura è 1.0, facciamo un campionamento multinomiale equo, e quando la temperatura va all'infinito - tutte le probabilità diventano uguali, e selezioniamo casualmente il prossimo carattere. Nell'esempio qui sotto possiamo osservare che il testo diventa privo di senso quando aumentiamo troppo la temperatura, e somiglia a un testo \"ciclato\" generato rigidamente quando si avvicina a 0.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Disclaimer**:  \nQuesto documento è stato tradotto utilizzando il servizio di traduzione automatica [Co-op Translator](https://github.com/Azure/co-op-translator). Sebbene ci impegniamo per garantire l'accuratezza, si prega di notare che le traduzioni automatiche possono contenere errori o imprecisioni. Il documento originale nella sua lingua nativa dovrebbe essere considerato la fonte autorevole. Per informazioni critiche, si raccomanda una traduzione professionale effettuata da un traduttore umano. Non siamo responsabili per eventuali fraintendimenti o interpretazioni errate derivanti dall'uso di questa traduzione.\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "16af2a8bbb083ea23e5e41c7f5787656b2ce26968575d8763f2c4b17f9cd711f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('py38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "coopTranslator": {
   "original_hash": "9fbb7d5fda708537649f71f5f646fcde",
   "translation_date": "2025-08-28T14:04:11+00:00",
   "source_file": "lessons/5-NLP/17-GenerativeNetworks/GenerativeTF.ipynb",
   "language_code": "it"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}