# Representaci칩n de Texto como Tensores

## [Cuestionario previo a la clase](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/113)

## Clasificaci칩n de Texto

A lo largo de la primera parte de esta secci칩n, nos enfocaremos en la tarea de **clasificaci칩n de texto**. Utilizaremos el conjunto de datos [AG News](https://www.kaggle.com/amananandrai/ag-news-classification-dataset), que contiene art칤culos de noticias como los siguientes:

* Categor칤a: Sci/Tech
* T칤tulo: La empresa de Ky. gana una subvenci칩n para estudiar p칠ptidos (AP)
* Cuerpo: AP - Una empresa fundada por un investigador en qu칤mica de la Universidad de Louisville gan칩 una subvenci칩n para desarrollar...

Nuestro objetivo ser치 clasificar el art칤culo de noticias en una de las categor칤as bas치ndonos en el texto.

## Representaci칩n de texto

Si queremos resolver tareas de Procesamiento de Lenguaje Natural (NLP) con redes neuronales, necesitamos alguna forma de representar el texto como tensores. Las computadoras ya representan los caracteres textuales como n칰meros que se mapean a fuentes en tu pantalla utilizando codificaciones como ASCII o UTF-8.

<img alt="Imagen que muestra un diagrama que mapea un car치cter a una representaci칩n ASCII y binaria" src="images/ascii-character-map.png" width="50%"/>

> [Fuente de la imagen](https://www.seobility.net/en/wiki/ASCII)

Como humanos, entendemos lo que cada letra **representa** y c칩mo todos los caracteres se combinan para formar las palabras de una oraci칩n. Sin embargo, las computadoras por s칤 solas no tienen tal comprensi칩n, y la red neuronal tiene que aprender el significado durante el entrenamiento.

Por lo tanto, podemos usar diferentes enfoques al representar texto:

* **Representaci칩n a nivel de car치cter**, cuando representamos el texto tratando cada car치cter como un n칰mero. Dado que tenemos *C* caracteres diferentes en nuestro corpus de texto, la palabra *Hola* se representar칤a mediante un tensor de 5x*C*. Cada letra corresponder칤a a una columna de tensor en codificaci칩n one-hot.
* **Representaci칩n a nivel de palabra**, en la que creamos un **vocabulario** de todas las palabras en nuestro texto, y luego representamos las palabras utilizando codificaci칩n one-hot. Este enfoque es algo mejor, porque cada letra por s칤 sola no tiene mucho significado, y as칤 al usar conceptos sem치nticos de mayor nivel - palabras - simplificamos la tarea para la red neuronal. Sin embargo, dado el gran tama침o del diccionario, necesitamos lidiar con tensores dispersos de alta dimensi칩n.

Independientemente de la representaci칩n, primero necesitamos convertir el texto en una secuencia de **tokens**, siendo un token ya sea un car치cter, una palabra, o a veces incluso parte de una palabra. Luego, convertimos el token en un n칰mero, t칤picamente usando un **vocabulario**, y este n칰mero puede ser alimentado a una red neuronal usando codificaci칩n one-hot.

## N-Grams

En el lenguaje natural, el significado preciso de las palabras solo puede determinarse en contexto. Por ejemplo, los significados de *red neuronal* y *red de pesca* son completamente diferentes. Una de las formas de tener esto en cuenta es construir nuestro modelo en pares de palabras, considerando los pares de palabras como tokens de vocabulario separados. De esta manera, la oraci칩n *Me gusta ir a pescar* se representar치 mediante la siguiente secuencia de tokens: *Me gusta*, *gusta ir*, *ir a*, *a pescar*. El problema con este enfoque es que el tama침o del diccionario crece significativamente, y combinaciones como *ir a pescar* y *ir de compras* se presentan con diferentes tokens, que no comparten ninguna similitud sem치ntica a pesar de tener el mismo verbo.

En algunos casos, tambi칠n podemos considerar usar tri-gramas -- combinaciones de tres palabras --. Por lo tanto, este enfoque se conoce a menudo como **n-grams**. Tambi칠n tiene sentido usar n-grams con representaci칩n a nivel de car치cter, en cuyo caso los n-grams corresponder치n aproximadamente a diferentes s칤labas.

## Bolsa de Palabras y TF/IDF

Al resolver tareas como la clasificaci칩n de texto, necesitamos ser capaces de representar el texto mediante un vector de tama침o fijo, que utilizaremos como entrada para el clasificador denso final. Una de las formas m치s simples de hacerlo es combinar todas las representaciones individuales de palabras, por ejemplo, sum치ndolas. Si sumamos las codificaciones one-hot de cada palabra, terminaremos con un vector de frecuencias, que muestra cu치ntas veces aparece cada palabra dentro del texto. Tal representaci칩n de texto se llama **bolsa de palabras** (BoW).

<img src="images/bow.png" width="90%"/>

> Imagen del autor

Un BoW representa esencialmente qu칠 palabras aparecen en el texto y en qu칠 cantidades, lo que puede ser una buena indicaci칩n de sobre qu칠 trata el texto. Por ejemplo, un art칤culo de noticias sobre pol칤tica probablemente contendr치 palabras como *presidente* y *pa칤s*, mientras que una publicaci칩n cient칤fica tendr칤a algo como *colisionador*, *descubierto*, etc. Por lo tanto, las frecuencias de palabras pueden ser en muchos casos un buen indicador del contenido del texto.

El problema con BoW es que ciertas palabras comunes, como *y*, *es*, etc., aparecen en la mayor칤a de los textos, y tienen las frecuencias m치s altas, ocultando las palabras que realmente son importantes. Podemos disminuir la importancia de esas palabras al tener en cuenta la frecuencia con la que ocurren las palabras en toda la colecci칩n de documentos. Esta es la idea principal detr치s del enfoque TF/IDF, que se cubre con m치s detalle en los cuadernos adjuntos a esta lecci칩n.

Sin embargo, ninguno de estos enfoques puede tener en cuenta completamente la **sem치ntica** del texto. Necesitamos modelos de redes neuronales m치s poderosos para hacer esto, que discutiremos m치s adelante en esta secci칩n.

## 九꽲잺 Ejercicios: Representaci칩n de Texto

Contin칰a tu aprendizaje en los siguientes cuadernos:

* [Representaci칩n de Texto con PyTorch](../../../../../lessons/5-NLP/13-TextRep/TextRepresentationPyTorch.ipynb)
* [Representaci칩n de Texto con TensorFlow](../../../../../lessons/5-NLP/13-TextRep/TextRepresentationTF.ipynb)

## Conclusi칩n

Hasta ahora, hemos estudiado t칠cnicas que pueden agregar peso de frecuencia a diferentes palabras. Sin embargo, no son capaces de representar el significado o el orden. Como dijo el famoso ling칲ista J. R. Firth en 1935, "El significado completo de una palabra siempre es contextual, y ning칰n estudio del significado separado del contexto puede tomarse en serio." Aprenderemos m치s adelante en el curso c칩mo capturar informaci칩n contextual del texto utilizando modelado de lenguaje.

## 游 Desaf칤o

Intenta algunos otros ejercicios utilizando bolsa de palabras y diferentes modelos de datos. Podr칤as inspirarte en esta [competencia en Kaggle](https://www.kaggle.com/competitions/word2vec-nlp-tutorial/overview/part-1-for-beginners-bag-of-words)

## [Cuestionario posterior a la clase](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/213)

## Revisi칩n y Autoestudio

Practica tus habilidades con t칠cnicas de embeddings de texto y bolsa de palabras en [Microsoft Learn](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-pytorch/?WT.mc_id=academic-77998-cacaste)

## [Asignaci칩n: Cuadernos](assignment.md)

**Disclaimer**:  
This document has been translated using machine-based AI translation services. While we strive for accuracy, please be aware that automated translations may contain errors or inaccuracies. The original document in its native language should be considered the authoritative source. For critical information, professional human translation is recommended. We are not liable for any misunderstandings or misinterpretations arising from the use of this translation.