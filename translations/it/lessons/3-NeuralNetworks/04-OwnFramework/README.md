<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "186bf7eeab776b36f557357ea56d4751",
  "translation_date": "2025-08-26T07:08:38+00:00",
  "source_file": "lessons/3-NeuralNetworks/04-OwnFramework/README.md",
  "language_code": "it"
}
-->
# Introduzione alle Reti Neurali. Perceptron Multistrato

Nella sezione precedente, hai imparato il modello di rete neurale piÃ¹ semplice: il perceptron a un livello, un modello lineare per la classificazione a due classi.

In questa sezione estenderemo questo modello in un framework piÃ¹ flessibile, che ci permetterÃ  di:

* eseguire la **classificazione multi-classe** oltre alla classificazione a due classi
* risolvere problemi di **regressione** oltre alla classificazione
* separare classi che non sono linearmente separabili

Svilupperemo anche un framework modulare in Python che ci consentirÃ  di costruire diverse architetture di reti neurali.

## [Quiz pre-lezione](https://ff-quizzes.netlify.app/en/ai/quiz/7)

## Formalizzazione del Machine Learning

Iniziamo formalizzando il problema del Machine Learning. Supponiamo di avere un dataset di addestramento **X** con etichette **Y**, e dobbiamo costruire un modello *f* che produca le previsioni piÃ¹ accurate. La qualitÃ  delle previsioni viene misurata dalla **funzione di perdita** â„’. Le seguenti funzioni di perdita sono spesso utilizzate:

* Per problemi di regressione, quando dobbiamo prevedere un numero, possiamo usare **errore assoluto** âˆ‘<sub>i</sub>|f(x<sup>(i)</sup>)-y<sup>(i)</sup>|, o **errore quadratico** âˆ‘<sub>i</sub>(f(x<sup>(i)</sup>)-y<sup>(i)</sup>)<sup>2</sup>
* Per la classificazione, utilizziamo **0-1 loss** (che Ã¨ essenzialmente lo stesso dell'**accuratezza** del modello), o **logistic loss**.

Per il perceptron a un livello, la funzione *f* era definita come una funzione lineare *f(x)=wx+b* (qui *w* Ã¨ la matrice dei pesi, *x* Ã¨ il vettore delle caratteristiche di input, e *b* Ã¨ il vettore di bias). Per diverse architetture di reti neurali, questa funzione puÃ² assumere una forma piÃ¹ complessa.

> Nel caso della classificazione, Ã¨ spesso desiderabile ottenere le probabilitÃ  delle classi corrispondenti come output della rete. Per convertire numeri arbitrari in probabilitÃ  (ad esempio per normalizzare l'output), utilizziamo spesso la funzione **softmax** Ïƒ, e la funzione *f* diventa *f(x)=Ïƒ(wx+b)*

Nella definizione di *f* sopra, *w* e *b* sono chiamati **parametri** Î¸=âŸ¨*w,b*âŸ©. Dato il dataset âŸ¨**X**,**Y**âŸ©, possiamo calcolare un errore complessivo sull'intero dataset come funzione dei parametri Î¸.

> âœ… **L'obiettivo dell'addestramento della rete neurale Ã¨ minimizzare l'errore variando i parametri Î¸**

## Ottimizzazione con Discesa del Gradiente

Esiste un metodo ben noto per l'ottimizzazione delle funzioni chiamato **discesa del gradiente**. L'idea Ã¨ che possiamo calcolare una derivata (nel caso multidimensionale chiamata **gradiente**) della funzione di perdita rispetto ai parametri, e variare i parametri in modo tale che l'errore diminuisca. Questo puÃ² essere formalizzato come segue:

* Inizializzare i parametri con alcuni valori casuali w<sup>(0)</sup>, b<sup>(0)</sup>
* Ripetere il seguente passo molte volte:
    - w<sup>(i+1)</sup> = w<sup>(i)</sup>-Î·âˆ‚â„’/âˆ‚w
    - b<sup>(i+1)</sup> = b<sup>(i)</sup>-Î·âˆ‚â„’/âˆ‚b

Durante l'addestramento, i passi di ottimizzazione dovrebbero essere calcolati considerando l'intero dataset (ricorda che la perdita Ã¨ calcolata come somma su tutti i campioni di addestramento). Tuttavia, nella pratica prendiamo piccole porzioni del dataset chiamate **minibatch**, e calcoliamo i gradienti basandoci su un sottoinsieme di dati. PoichÃ© il sottoinsieme viene preso casualmente ogni volta, tale metodo Ã¨ chiamato **discesa del gradiente stocastica** (SGD).

## Perceptron Multistrato e Backpropagation

La rete a un livello, come abbiamo visto sopra, Ã¨ in grado di classificare classi linearmente separabili. Per costruire un modello piÃ¹ ricco, possiamo combinare diversi livelli della rete. Matematicamente ciÃ² significherebbe che la funzione *f* avrebbe una forma piÃ¹ complessa e verrebbe calcolata in diversi passaggi:
* z<sub>1</sub>=w<sub>1</sub>x+b<sub>1</sub>
* z<sub>2</sub>=w<sub>2</sub>Î±(z<sub>1</sub>)+b<sub>2</sub>
* f = Ïƒ(z<sub>2</sub>)

Qui, Î± Ã¨ una **funzione di attivazione non lineare**, Ïƒ Ã¨ una funzione softmax, e i parametri Î¸=<*w<sub>1</sub>,b<sub>1</sub>,w<sub>2</sub>,b<sub>2</sub>*>.

L'algoritmo di discesa del gradiente rimarrebbe lo stesso, ma sarebbe piÃ¹ difficile calcolare i gradienti. Dato il principio della derivazione a catena, possiamo calcolare le derivate come:

* âˆ‚â„’/âˆ‚w<sub>2</sub> = (âˆ‚â„’/âˆ‚Ïƒ)(âˆ‚Ïƒ/âˆ‚z<sub>2</sub>)(âˆ‚z<sub>2</sub>/âˆ‚w<sub>2</sub>)
* âˆ‚â„’/âˆ‚w<sub>1</sub> = (âˆ‚â„’/âˆ‚Ïƒ)(âˆ‚Ïƒ/âˆ‚z<sub>2</sub>)(âˆ‚z<sub>2</sub>/âˆ‚Î±)(âˆ‚Î±/âˆ‚z<sub>1</sub>)(âˆ‚z<sub>1</sub>/âˆ‚w<sub>1</sub>)

> âœ… La regola della derivazione a catena viene utilizzata per calcolare le derivate della funzione di perdita rispetto ai parametri.

Nota che la parte piÃ¹ a sinistra di tutte queste espressioni Ã¨ la stessa, e quindi possiamo calcolare efficacemente le derivate partendo dalla funzione di perdita e andando "indietro" attraverso il grafo computazionale. Pertanto, il metodo di addestramento di un perceptron multistrato Ã¨ chiamato **backpropagation**, o 'backprop'.

<img alt="compute graph" src="images/ComputeGraphGrad.png"/>

> TODO: citazione immagine

> âœ… Approfondiremo il backpropagation in modo molto piÃ¹ dettagliato nel nostro esempio nel notebook.  

## Conclusione

In questa lezione, abbiamo costruito la nostra libreria di reti neurali e l'abbiamo utilizzata per un semplice compito di classificazione bidimensionale.

## ðŸš€ Sfida

Nel notebook allegato, implementerai il tuo framework per costruire e addestrare perceptron multistrato. Potrai vedere in dettaglio come operano le reti neurali moderne.

Procedi al notebook [OwnFramework](../../../../../lessons/3-NeuralNetworks/04-OwnFramework/OwnFramework.ipynb) e segui le istruzioni.

## [Quiz post-lezione](https://ff-quizzes.netlify.app/en/ai/quiz/8)

## Revisione e Studio Autonomo

Il backpropagation Ã¨ un algoritmo comune utilizzato in AI e ML, vale la pena studiarlo [in modo piÃ¹ dettagliato](https://wikipedia.org/wiki/Backpropagation)

## [Compito](lab/README.md)

In questo laboratorio, ti viene chiesto di utilizzare il framework che hai costruito in questa lezione per risolvere la classificazione delle cifre scritte a mano MNIST.

* [Istruzioni](lab/README.md)
* [Notebook](../../../../../lessons/3-NeuralNetworks/04-OwnFramework/lab/MyFW_MNIST.ipynb)

**Disclaimer**:  
Questo documento Ã¨ stato tradotto utilizzando il servizio di traduzione automatica [Co-op Translator](https://github.com/Azure/co-op-translator). Sebbene ci impegniamo per garantire l'accuratezza, si prega di notare che le traduzioni automatiche potrebbero contenere errori o imprecisioni. Il documento originale nella sua lingua nativa dovrebbe essere considerato la fonte autorevole. Per informazioni critiche, si raccomanda una traduzione professionale effettuata da un traduttore umano. Non siamo responsabili per eventuali fraintendimenti o interpretazioni errate derivanti dall'uso di questa traduzione.