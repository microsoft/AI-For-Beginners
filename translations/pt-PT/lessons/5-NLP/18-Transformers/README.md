# Mecanismos de Aten√ß√£o e Transformers

## [Question√°rio pr√©-aula](https://ff-quizzes.netlify.app/en/ai/quiz/35)

Um dos problemas mais importantes no dom√≠nio de NLP √© a **tradu√ß√£o autom√°tica**, uma tarefa essencial que sustenta ferramentas como o Google Translate. Nesta se√ß√£o, vamos focar na tradu√ß√£o autom√°tica ou, de forma mais geral, em qualquer tarefa de *sequ√™ncia para sequ√™ncia* (tamb√©m chamada de **transdu√ß√£o de frases**).

Com RNNs, a tarefa de sequ√™ncia para sequ√™ncia √© implementada por duas redes recorrentes, onde uma rede, o **codificador**, condensa uma sequ√™ncia de entrada num estado oculto, enquanto outra rede, o **descodificador**, expande este estado oculto num resultado traduzido. Existem alguns problemas com esta abordagem:

* O estado final da rede codificadora tem dificuldade em lembrar-se do in√≠cio de uma frase, causando uma qualidade inferior do modelo para frases longas.
* Todas as palavras numa sequ√™ncia t√™m o mesmo impacto no resultado. Na realidade, no entanto, palavras espec√≠ficas na sequ√™ncia de entrada frequentemente t√™m mais impacto nos resultados sequenciais do que outras.

Os **Mecanismos de Aten√ß√£o** fornecem um meio de ponderar o impacto contextual de cada vetor de entrada em cada previs√£o de sa√≠da da RNN. Isto √© implementado criando atalhos entre estados intermedi√°rios da RNN de entrada e a RNN de sa√≠da. Desta forma, ao gerar o s√≠mbolo de sa√≠da y<sub>t</sub>, consideramos todos os estados ocultos de entrada h<sub>i</sub>, com diferentes coeficientes de peso &alpha;<sub>t,i</sub>.

![Imagem mostrando um modelo codificador/descodificador com uma camada de aten√ß√£o aditiva](../../../../../translated_images/pt-PT/encoder-decoder-attention.7a726296894fb567.webp)

> O modelo codificador-descodificador com mecanismo de aten√ß√£o aditiva em [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf), citado deste [post de blog](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)

A matriz de aten√ß√£o {&alpha;<sub>i,j</sub>} representaria o grau em que certas palavras de entrada influenciam a gera√ß√£o de uma determinada palavra na sequ√™ncia de sa√≠da. Abaixo est√° um exemplo de tal matriz:

![Imagem mostrando um alinhamento de exemplo encontrado por RNNsearch-50, retirada de Bahdanau - arviz.org](../../../../../translated_images/pt-PT/bahdanau-fig3.09ba2d37f202a6af.webp)

> Figura de [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) (Fig.3)

Os mecanismos de aten√ß√£o s√£o respons√°veis por grande parte do estado da arte atual ou pr√≥ximo ao estado da arte em NLP. No entanto, adicionar aten√ß√£o aumenta significativamente o n√∫mero de par√¢metros do modelo, o que levou a problemas de escalabilidade com RNNs. Uma restri√ß√£o chave na escalabilidade das RNNs √© que a natureza recorrente dos modelos torna desafiador agrupar e paralelizar o treino. Numa RNN, cada elemento de uma sequ√™ncia precisa ser processado em ordem sequencial, o que significa que n√£o pode ser facilmente paralelizado.

![Codificador Descodificador com Aten√ß√£o](../../../../../lessons/5-NLP/18-Transformers/images/EncDecAttention.gif)

> Figura do [Blog do Google](https://research.googleblog.com/2016/09/a-neural-network-for-machine.html)

A ado√ß√£o de mecanismos de aten√ß√£o combinada com esta restri√ß√£o levou √† cria√ß√£o dos modelos Transformer, agora estado da arte, que conhecemos e usamos hoje, como BERT e Open-GPT3.

## Modelos Transformer

Uma das principais ideias por tr√°s dos transformers √© evitar a natureza sequencial das RNNs e criar um modelo que seja paraleliz√°vel durante o treino. Isto √© alcan√ßado implementando duas ideias:

* codifica√ß√£o posicional
* uso de mecanismo de auto-aten√ß√£o para capturar padr√µes em vez de RNNs (ou CNNs) (√© por isso que o artigo que introduz os transformers se chama *[Attention is all you need](https://arxiv.org/abs/1706.03762)*)

### Codifica√ß√£o/Embeddings Posicionais

A ideia da codifica√ß√£o posicional √© a seguinte. 
1. Ao usar RNNs, a posi√ß√£o relativa dos tokens √© representada pelo n√∫mero de passos e, portanto, n√£o precisa ser explicitamente representada. 
2. No entanto, ao mudar para aten√ß√£o, precisamos saber as posi√ß√µes relativas dos tokens dentro de uma sequ√™ncia. 
3. Para obter codifica√ß√£o posicional, aumentamos a nossa sequ√™ncia de tokens com uma sequ√™ncia de posi√ß√µes dos tokens na sequ√™ncia (ou seja, uma sequ√™ncia de n√∫meros 0,1, ...).
4. Depois misturamos a posi√ß√£o do token com um vetor de embedding do token. Para transformar a posi√ß√£o (inteiro) num vetor, podemos usar diferentes abordagens:

* Embedding trein√°vel, semelhante ao embedding de tokens. Esta √© a abordagem que consideramos aqui. Aplicamos camadas de embedding tanto nos tokens quanto nas suas posi√ß√µes, resultando em vetores de embedding com as mesmas dimens√µes, que depois somamos.
* Fun√ß√£o de codifica√ß√£o posicional fixa, como proposto no artigo original.

<img src="../../../../../translated_images/pt-PT/pos-embedding.e41ce9b6cf6078af.webp" width="50%"/>

> Imagem do autor

O resultado que obtemos com o embedding posicional incorpora tanto o token original quanto a sua posi√ß√£o dentro de uma sequ√™ncia.

### Auto-Aten√ß√£o Multi-Head

A seguir, precisamos capturar alguns padr√µes dentro da nossa sequ√™ncia. Para isso, os transformers usam um mecanismo de **auto-aten√ß√£o**, que √© essencialmente aten√ß√£o aplicada √† mesma sequ√™ncia como entrada e sa√≠da. Aplicar auto-aten√ß√£o permite-nos levar em conta o **contexto** dentro da frase e ver quais palavras est√£o inter-relacionadas. Por exemplo, permite-nos ver quais palavras s√£o referidas por correfer√™ncias, como *it*, e tamb√©m considerar o contexto:

![](../../../../../translated_images/pt-PT/CoreferenceResolution.861924d6d384a7d6.webp)

> Imagem do [Blog do Google](https://research.googleblog.com/2017/08/transformer-novel-neural-network.html)

Nos transformers, usamos **Aten√ß√£o Multi-Head** para dar √† rede o poder de capturar v√°rios tipos diferentes de depend√™ncias, como rela√ß√µes de palavras de longo prazo vs. curto prazo, correfer√™ncia vs. algo diferente, etc.

[Notebook TensorFlow](TransformersTF.ipynb) cont√©m mais detalhes sobre a implementa√ß√£o de camadas de transformer.

### Aten√ß√£o Codificador-Descodificador

Nos transformers, a aten√ß√£o √© usada em dois lugares:

* Para capturar padr√µes dentro do texto de entrada usando auto-aten√ß√£o
* Para realizar tradu√ß√£o de sequ√™ncia - √© a camada de aten√ß√£o entre codificador e descodificador.

A aten√ß√£o codificador-descodificador √© muito semelhante ao mecanismo de aten√ß√£o usado em RNNs, conforme descrito no in√≠cio desta se√ß√£o. Este diagrama animado explica o papel da aten√ß√£o codificador-descodificador.

![GIF animado mostrando como as avalia√ß√µes s√£o realizadas em modelos transformer.](../../../../../lessons/5-NLP/18-Transformers/images/transformer-animated-explanation.gif)

Como cada posi√ß√£o de entrada √© mapeada independentemente para cada posi√ß√£o de sa√≠da, os transformers podem paralelizar melhor do que as RNNs, o que permite modelos de linguagem muito maiores e mais expressivos. Cada cabe√ßa de aten√ß√£o pode ser usada para aprender diferentes rela√ß√µes entre palavras, melhorando tarefas de Processamento de Linguagem Natural.

## BERT

**BERT** (Bidirectional Encoder Representations from Transformers) √© uma rede transformer muito grande com v√°rias camadas: 12 camadas para o *BERT-base* e 24 para o *BERT-large*. O modelo √© primeiro pr√©-treinado num grande corpus de dados de texto (WikiPedia + livros) usando treino n√£o supervisionado (prevendo palavras mascaradas numa frase). Durante o pr√©-treino, o modelo absorve n√≠veis significativos de compreens√£o da linguagem, que podem ser aproveitados com outros conjuntos de dados usando ajuste fino. Este processo √© chamado de **aprendizagem por transfer√™ncia**.

![imagem de http://jalammar.github.io/illustrated-bert/](../../../../../translated_images/pt-PT/jalammarBERT-language-modeling-masked-lm.34f113ea5fec4362.webp)

> Imagem [fonte](http://jalammar.github.io/illustrated-bert/)

## ‚úçÔ∏è Exerc√≠cios: Transformers

Continue a sua aprendizagem nos seguintes notebooks:

* [Transformers em PyTorch](TransformersPyTorch.ipynb)
* [Transformers em TensorFlow](TransformersTF.ipynb)

## Conclus√£o

Nesta li√ß√£o, aprendeu sobre Transformers e Mecanismos de Aten√ß√£o, ferramentas essenciais na caixa de ferramentas de NLP. Existem muitas varia√ß√µes de arquiteturas Transformer, incluindo BERT, DistilBERT, BigBird, OpenGPT3 e mais, que podem ser ajustadas. O pacote [HuggingFace](https://github.com/huggingface/) fornece um reposit√≥rio para treinar muitas destas arquiteturas com PyTorch e TensorFlow.

## üöÄ Desafio

## [Question√°rio p√≥s-aula](https://ff-quizzes.netlify.app/en/ai/quiz/36)

## Revis√£o e Autoestudo

* [Post de blog](https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/), explicando o cl√°ssico artigo [Attention is all you need](https://arxiv.org/abs/1706.03762) sobre transformers.
* [Uma s√©rie de posts de blog](https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452) sobre transformers, explicando a arquitetura em detalhe.

## [Tarefa](assignment.md)

---

