# Introdu√ß√£o √†s Redes Neuronais. Perceptr√£o Multicamadas

Na sec√ß√£o anterior, aprendeste sobre o modelo mais simples de rede neuronal - o perceptr√£o de uma camada, um modelo linear de classifica√ß√£o de duas classes.

Nesta sec√ß√£o, vamos expandir este modelo para um framework mais flex√≠vel, permitindo-nos:

* realizar **classifica√ß√£o de m√∫ltiplas classes** al√©m de duas classes
* resolver **problemas de regress√£o** al√©m de classifica√ß√£o
* separar classes que n√£o s√£o linearmente separ√°veis

Tamb√©m iremos desenvolver o nosso pr√≥prio framework modular em Python, que nos permitir√° construir diferentes arquiteturas de redes neuronais.

## [Question√°rio pr√©-aula](https://ff-quizzes.netlify.app/en/ai/quiz/7)

## Formaliza√ß√£o de Aprendizagem Autom√°tica

Vamos come√ßar por formalizar o problema de Aprendizagem Autom√°tica. Suponhamos que temos um conjunto de dados de treino **X** com etiquetas **Y**, e precisamos construir um modelo *f* que fa√ßa previs√µes o mais precisas poss√≠vel. A qualidade das previs√µes √© medida pela **fun√ß√£o de perda** &lagran;. As seguintes fun√ß√µes de perda s√£o frequentemente utilizadas:

* Para problemas de regress√£o, quando precisamos prever um n√∫mero, podemos usar **erro absoluto** &sum;<sub>i</sub>|f(x<sup>(i)</sup>)-y<sup>(i)</sup>|, ou **erro quadr√°tico** &sum;<sub>i</sub>(f(x<sup>(i)</sup>)-y<sup>(i)</sup>)<sup>2</sup>
* Para classifica√ß√£o, usamos **perda 0-1** (que √© essencialmente o mesmo que **precis√£o** do modelo), ou **perda log√≠stica**.

Para o perceptr√£o de uma camada, a fun√ß√£o *f* foi definida como uma fun√ß√£o linear *f(x)=wx+b* (aqui *w* √© a matriz de pesos, *x* √© o vetor de caracter√≠sticas de entrada, e *b* √© o vetor de bias). Para diferentes arquiteturas de redes neuronais, esta fun√ß√£o pode assumir uma forma mais complexa.

> No caso de classifica√ß√£o, √© frequentemente desej√°vel obter probabilidades das classes correspondentes como sa√≠da da rede. Para converter n√∫meros arbitr√°rios em probabilidades (por exemplo, para normalizar a sa√≠da), usamos frequentemente a fun√ß√£o **softmax** &sigma;, e a fun√ß√£o *f* torna-se *f(x)=&sigma;(wx+b)*

Na defini√ß√£o de *f* acima, *w* e *b* s√£o chamados **par√¢metros** &theta;=‚ü®*w,b*‚ü©. Dado o conjunto de dados ‚ü®**X**,**Y**‚ü©, podemos calcular um erro geral em todo o conjunto de dados como uma fun√ß√£o dos par√¢metros &theta;.

> ‚úÖ **O objetivo do treino de redes neuronais √© minimizar o erro variando os par√¢metros &theta;**

## Otimiza√ß√£o por Gradiente Descendente

Existe um m√©todo bem conhecido de otimiza√ß√£o de fun√ß√µes chamado **gradiente descendente**. A ideia √© que podemos calcular uma derivada (no caso multidimensional chamada **gradiente**) da fun√ß√£o de perda em rela√ß√£o aos par√¢metros, e variar os par√¢metros de forma que o erro diminua. Isto pode ser formalizado da seguinte forma:

* Inicializar os par√¢metros com alguns valores aleat√≥rios w<sup>(0)</sup>, b<sup>(0)</sup>
* Repetir o seguinte passo v√°rias vezes:
    - w<sup>(i+1)</sup> = w<sup>(i)</sup>-&eta;&part;&lagran;/&part;w
    - b<sup>(i+1)</sup> = b<sup>(i)</sup>-&eta;&part;&lagran;/&part;b

Durante o treino, os passos de otimiza√ß√£o devem ser calculados considerando todo o conjunto de dados (lembra-te que a perda √© calculada como uma soma atrav√©s de todas as amostras de treino). No entanto, na pr√°tica, usamos pequenas por√ß√µes do conjunto de dados chamadas **minibatches**, e calculamos os gradientes com base num subconjunto de dados. Como o subconjunto √© escolhido aleatoriamente cada vez, tal m√©todo √© chamado **gradiente descendente estoc√°stico** (SGD).

## Perceptr√µes Multicamadas e Retropropaga√ß√£o

A rede de uma camada, como vimos acima, √© capaz de classificar classes linearmente separ√°veis. Para construir um modelo mais rico, podemos combinar v√°rias camadas da rede. Matematicamente, isso significaria que a fun√ß√£o *f* teria uma forma mais complexa e seria calculada em v√°rios passos:
* z<sub>1</sub>=w<sub>1</sub>x+b<sub>1</sub>
* z<sub>2</sub>=w<sub>2</sub>&alpha;(z<sub>1</sub>)+b<sub>2</sub>
* f = &sigma;(z<sub>2</sub>)

Aqui, &alpha; √© uma **fun√ß√£o de ativa√ß√£o n√£o linear**, &sigma; √© uma fun√ß√£o softmax, e os par√¢metros &theta;=<*w<sub>1</sub>,b<sub>1</sub>,w<sub>2</sub>,b<sub>2</sub>*>.

O algoritmo de gradiente descendente permaneceria o mesmo, mas seria mais dif√≠cil calcular os gradientes. Dado o princ√≠pio da regra da cadeia de diferencia√ß√£o, podemos calcular as derivadas como:

* &part;&lagran;/&part;w<sub>2</sub> = (&part;&lagran;/&part;&sigma;)(&part;&sigma;/&part;z<sub>2</sub>)(&part;z<sub>2</sub>/&part;w<sub>2</sub>)
* &part;&lagran;/&part;w<sub>1</sub> = (&part;&lagran;/&part;&sigma;)(&part;&sigma;/&part;z<sub>2</sub>)(&part;z<sub>2</sub>/&part;&alpha;)(&part;&alpha;/&part;z<sub>1</sub>)(&part;z<sub>1</sub>/&part;w<sub>1</sub>)

> ‚úÖ A regra da cadeia de diferencia√ß√£o √© usada para calcular as derivadas da fun√ß√£o de perda em rela√ß√£o aos par√¢metros.

Nota que a parte mais √† esquerda de todas essas express√µes √© a mesma, e assim podemos calcular eficazmente as derivadas come√ßando pela fun√ß√£o de perda e indo "para tr√°s" atrav√©s do gr√°fico computacional. Assim, o m√©todo de treino de um perceptr√£o multicamadas √© chamado **retropropaga√ß√£o**, ou 'backprop'.

<img alt="compute graph" src="../../../../../translated_images/pt-PT/ComputeGraphGrad.4626252c0de03507.webp"/>

> TODO: cita√ß√£o da imagem

> ‚úÖ Vamos abordar a retropropaga√ß√£o com muito mais detalhe no nosso exemplo em notebook.  

## Conclus√£o

Nesta aula, constru√≠mos a nossa pr√≥pria biblioteca de redes neuronais e utiliz√°mo-la para uma tarefa simples de classifica√ß√£o bidimensional.

## üöÄ Desafio

No notebook que acompanha, vais implementar o teu pr√≥prio framework para construir e treinar perceptr√µes multicamadas. Vais poder ver em detalhe como funcionam as redes neuronais modernas.

Segue para o notebook [OwnFramework](OwnFramework.ipynb) e trabalha nele.

## [Question√°rio p√≥s-aula](https://ff-quizzes.netlify.app/en/ai/quiz/8)

## Revis√£o e Estudo Aut√≥nomo

A retropropaga√ß√£o √© um algoritmo comum usado em IA e ML, vale a pena estudar [em mais detalhe](https://wikipedia.org/wiki/Backpropagation)

## [Tarefa](lab/README.md)

Neste laborat√≥rio, √©s convidado a usar o framework que constru√≠ste nesta aula para resolver a classifica√ß√£o de d√≠gitos manuscritos MNIST.

* [Instru√ß√µes](lab/README.md)
* [Notebook](lab/MyFW_MNIST.ipynb)

---

