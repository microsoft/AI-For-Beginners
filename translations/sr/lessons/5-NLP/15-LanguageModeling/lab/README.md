# Тренирање Skip-Gram модела

Задатак из [AI for Beginners Curriculum](https://github.com/microsoft/ai-for-beginners).

## Задатак

У овом задатку, изазивамо вас да обучите Word2Vec модел користећи Skip-Gram технику. Обучите мрежу са уграђивањем (embedding) да предвиђа суседне речи у Skip-Gram прозору ширине $N$ токена. Можете користити [код из овог часа](../../../../../../lessons/5-NLP/15-LanguageModeling/CBoW-TF.ipynb) и мало га модификовати.

## Скуп података

Добродошли сте да користите било коју књигу. Можете пронаћи много бесплатних текстова на [Project Gutenberg](https://www.gutenberg.org/), на пример, овде је директан линк ка [Алиса у земљи чуда](https://www.gutenberg.org/files/11/11-0.txt)) од Луиса Керола. Или, можете користити Шекспирове драме, које можете добити користећи следећи код:

```python
path_to_file = tf.keras.utils.get_file(
   'shakespeare.txt', 
   'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')
text = open(path_to_file, 'rb').read().decode(encoding='utf-8')
```

## Истражите!

Ако имате времена и желите да дубље уђете у тему, пробајте да истражите неколико ствари:

* Како величина уграђивања (embedding size) утиче на резултате?
* Како различити стилови текста утичу на резултат?
* Узмите неколико веома различитих типова речи и њихових синонима, добијте њихове векторске репрезентације, примените PCA да смањите димензије на 2, и прикажите их у 2D простору. Да ли видите неке обрасце?

**Одрицање од одговорности**:  
Овај документ је преведен коришћењем услуге за превођење помоћу вештачке интелигенције [Co-op Translator](https://github.com/Azure/co-op-translator). Иако се трудимо да обезбедимо тачност, молимо вас да имате у виду да аутоматски преводи могу садржати грешке или нетачности. Оригинални документ на његовом изворном језику треба сматрати меродавним извором. За критичне информације препоручује се професионални превод од стране људи. Не преузимамо одговорност за било каква погрешна тумачења или неспоразуме који могу настати услед коришћења овог превода.