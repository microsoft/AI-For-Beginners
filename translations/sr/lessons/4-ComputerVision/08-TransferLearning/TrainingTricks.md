# Трикови за тренинг дубоког учења

Како неуронске мреже постају дубље, процес њиховог тренинга постаје све изазовнији. Један од главних проблема су такозвани [проблем нестајућих градијената](https://en.wikipedia.org/wiki/Vanishing_gradient_problem) или [експлодирајући градијенти](https://deepai.org/machine-learning-glossary-and-terms/exploding-gradient-problem#:~:text=Exploding%20gradients%20are%20a%20problem,updates%20are%20small%20and%20controlled.). [Овај текст](https://towardsdatascience.com/the-vanishing-exploding-gradient-problem-in-deep-neural-networks-191358470c11) пружа добар увод у те проблеме.

Да би тренинг дубоких мрежа био ефикаснији, постоји неколико техника које се могу применити.

## Одржавање вредности у разумном интервалу

Да би нумеричке рачунице биле стабилније, важно је осигурати да све вредности унутар наше неуронске мреже буду у разумном опсегу, обично [-1..1] или [0..1]. Ово није веома строг захтев, али природа рачунања са покретним зарезом је таква да вредности различитих величина не могу бити прецизно обрађене заједно. На пример, ако саберемо 10<sup>-10</sup> и 10<sup>10</sup>, вероватно ћемо добити 10<sup>10</sup>, јер ће мања вредност бити "конвертована" на исти ред величине као већа, и тако ће мантиса бити изгубљена.

Већина функција активације има нелинеарности око [-1..1], па је логично скалирати све улазне податке на интервал [-1..1] или [0..1].

## Иницијализација тежина

Идеално, желимо да вредности остану у истом опсегу након проласка кроз слојеве мреже. Због тога је важно иницијализовати тежине на начин који чува дистрибуцију вредности.

Нормална дистрибуција **N(0,1)** није добра идеја, јер ако имамо *n* улаза, стандардна девијација излаза биће *n*, и вредности ће вероватно изаћи из интервала [0..1].

Следеће иницијализације се често користе:

 * Униформна дистрибуција -- `uniform`
 * **N(0,1/n)** -- `gaussian`
 * **N(0,1/√n_in)** гарантује да за улазе са средњом вредношћу нула и стандардном девијацијом 1, исте средње вредности/стандардне девијације остају
 * **N(0,√2/(n_in+n_out))** -- такозвана **Xavier иницијализација** (`glorot`), помаже да сигнали остану у опсегу током напредне и повратне пропагације

## Нормализација партије (Batch Normalization)

Чак и уз правилну иницијализацију тежина, тежине могу постати произвољно велике или мале током тренинга, што ће избацити сигнале из одговарајућег опсега. Сигнале можемо вратити у опсег коришћењем једне од техника **нормализације**. Иако их има неколико (нормализација тежина, нормализација слоја), најчешће коришћена је нормализација партије.

Идеја **нормализације партије** је да узмемо у обзир све вредности унутар мини-партије и извршимо нормализацију (тј. одузмемо средњу вредност и поделимо са стандардном девијацијом) на основу тих вредности. Она се имплементира као слој мреже који врши ову нормализацију након примене тежина, али пре функције активације. Као резултат, вероватно ћемо видети већу коначну тачност и бржи тренинг.

Ево [оригиналног рада](https://arxiv.org/pdf/1502.03167.pdf) о нормализацији партије, [објашњења на Википедији](https://en.wikipedia.org/wiki/Batch_normalization), и [добар уводни блог пост](https://towardsdatascience.com/batch-normalization-in-3-levels-of-understanding-14c2da90a338) (и један [на руском](https://habrahabr.ru/post/309302/)).

## Dropout

**Dropout** је занимљива техника која уклања одређени проценат случајних неурона током тренинга. Такође се имплементира као слој са једним параметром (проценат неурона који се уклања, обично 10%-50%), и током тренинга он нулира случајне елементе улазног вектора пре него што их проследи следећем слоју.

Иако ово може звучати као чудна идеја, можете видети ефекат dropout-а на тренинг класификатора цифара MNIST у [`Dropout.ipynb`](../../../../../lessons/4-ComputerVision/08-TransferLearning/Dropout.ipynb) нотебуку. Убрзава тренинг и омогућава постизање веће тачности у мање епоха тренинга.

Овај ефекат се може објаснити на неколико начина:

 * Може се сматрати као случајни шок фактор за модел, који га изводи из локалног минимума
 * Може се сматрати као *имплицитно просечно моделирање*, јер можемо рећи да током dropout-а тренирамо мало другачији модел

> *Неки људи кажу да када пијана особа покушава да научи нешто, она ће то боље запамтити следећег јутра у поређењу са трезном особом, јер мозак са неким нефункционалним неуронима покушава боље да се прилагоди да схвати значење. Никада нисмо тестирали да ли је ово тачно или не.*

## Спречавање пренапрегнутости (overfitting)

Један од веома важних аспеката дубоког учења је способност да се спречи [пренапрегнутост](../../3-NeuralNetworks/05-Frameworks/Overfitting.md). Иако може бити примамљиво користити веома моћан модел неуронске мреже, увек треба балансирати број параметара модела са бројем узорака за тренинг.

> Уверите се да разумете концепт [пренапрегнутости](../../3-NeuralNetworks/05-Frameworks/Overfitting.md) који смо раније објаснили!

Постоји неколико начина да се спречи пренапрегнутост:

 * Рано заустављање -- континуирано праћење грешке на валидационом сету и заустављање тренинга када грешка на валидационом сету почне да расте.
 * Експлицитно распадање тежина / регуларизација -- додавање додатне казне функцији губитка за високе апсолутне вредности тежина, што спречава модел да добије веома нестабилне резултате
 * Просечно моделирање -- тренинг неколико модела и затим просечно израчунавање резултата. Ово помаже у минимизирању варијансе.
 * Dropout (имплицитно просечно моделирање)

## Оптимизатори / алгоритми за тренинг

Још један важан аспект тренинга је избор доброг алгоритма за тренинг. Иако је класични **градијентни спуст** разумна опција, понекад може бити превише спор или довести до других проблема.

У дубоком учењу користимо **стохастички градијентни спуст** (SGD), који је градијентни спуст примењен на мини-партије, насумично изабране из скупа за тренинг. Тежине се прилагођавају помоћу ове формуле:

w<sup>t+1</sup> = w<sup>t</sup> - η∇ℒ

### Моментум

У **моментум SGD**, задржавамо део градијента из претходних корака. То је слично као када се крећемо негде са инерцијом, и добијемо ударац у другом правцу, наша трајекторија се не мења одмах, већ задржава део оригиналног кретања. Овде уводимо још један вектор v који представља *брзину*:

* v<sup>t+1</sup> = γ v<sup>t</sup> - η∇ℒ
* w<sup>t+1</sup> = w<sup>t</sup>+v<sup>t+1</sup>

Овде параметар γ указује на степен до којег узимамо инерцију у обзир: γ=0 одговара класичном SGD; γ=1 је чиста једначина кретања.

### Adam, Adagrad, итд.

Пошто у сваком слоју множимо сигнале са неком матрицом W<sub>i</sub>, у зависности од ||W<sub>i</sub>||, градијент може или да се смањи и буде близу 0, или да расте бесконачно. То је суштина проблема експлодирајућих/нестајућих градијената.

Једно од решења овог проблема је да се у једначини користи само правац градијента, а да се апсолутна вредност игнорише, тј.

w<sup>t+1</sup> = w<sup>t</sup> - η(∇ℒ/||∇ℒ||), где ||∇ℒ|| = √∑(∇ℒ)<sup>2</sup>

Овај алгоритам се зове **Adagrad**. Други алгоритми који користе исту идеју: **RMSProp**, **Adam**

> **Adam** се сматра веома ефикасним алгоритмом за многе примене, па ако нисте сигурни који да користите - користите Adam.

### Сечење градијента (Gradient clipping)

Сечење градијента је проширење горе наведене идеје. Када је ||∇ℒ|| ≤ θ, узимамо оригинални градијент у оптимизацији тежина, а када је ||∇ℒ|| > θ - делимо градијент са његовом нормом. Овде је θ параметар, у већини случајева можемо узети θ=1 или θ=10.

### Смањење стопе учења

Успех тренинга често зависи од параметра стопе учења η. Логично је претпоставити да веће вредности η доводе до бржег тренинга, што је нешто што обично желимо на почетку тренинга, а затим мање вредности η омогућавају фино подешавање мреже. Због тога у већини случајева желимо да смањимо η током процеса тренинга.

Ово се може урадити множењем η са неким бројем (нпр. 0.98) након сваке епохе тренинга, или коришћењем сложенијег **распореда стопе учења**.

## Различите архитектуре мрежа

Избор праве архитектуре мреже за ваш проблем може бити тежак. Обично бисмо узели архитектуру која се показала успешном за наш специфичан задатак (или сличан). Ево [добар преглед](https://www.topbots.com/a-brief-history-of-neural-network-architectures/) архитектура неуронских мрежа за рачунарски вид.

> Важно је изабрати архитектуру која ће бити довољно моћна за број узорака за тренинг које имамо. Избор превише моћног модела може довести до [пренапрегнутости](../../3-NeuralNetworks/05-Frameworks/Overfitting.md).

Још један добар начин би био коришћење архитектуре која ће се аутоматски прилагодити потребној сложености. До одређене мере, **ResNet** архитектура и **Inception** су само-прилагодљиве. [Више о архитектурама за рачунарски вид](../07-ConvNets/CNN_Architectures.md)

**Одрицање од одговорности**:  
Овај документ је преведен коришћењем услуге за превођење помоћу вештачке интелигенције [Co-op Translator](https://github.com/Azure/co-op-translator). Иако се трудимо да обезбедимо тачност, молимо вас да имате у виду да аутоматски преводи могу садржати грешке или нетачности. Оригинални документ на његовом изворном језику треба сматрати меродавним извором. За критичне информације препоручује се професионални превод од стране људи. Не преузимамо одговорност за било каква погрешна тумачења или неспоразуме који могу настати услед коришћења овог превода.