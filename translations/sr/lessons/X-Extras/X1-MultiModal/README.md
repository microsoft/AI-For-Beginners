# Мултимодалне мреже

Након успеха трансформер модела у решавању задатака обраде природног језика (NLP), исте или сличне архитектуре примењене су и на задатке рачунарског вида. Расте интересовање за изградњу модела који би *комбиновали* могућности визије и природног језика. Један од таквих покушаја је направио OpenAI, а назива се CLIP и DALL.E.

## Контрастивно претходно обучавање слика (CLIP)

Главна идеја CLIP-а је могућност поређења текстуалних упита са сликом и одређивање колико добро слика одговара упиту.

![CLIP Архитектура](../../../../../translated_images/sr/clip-arch.b3dbf20b4e8ed8be.webp)

> *Слика из [овог блога](https://openai.com/blog/clip/)*

Модел је обучен на сликама преузетим са интернета и њиховим описима. За сваки пакет, узимамо N парова (слика, текст) и претварамо их у неке векторске репрезентације I, ..., T. Те репрезентације се затим упарују. Функција губитка је дефинисана тако да максимизује косинусну сличност између вектора који одговарају једном пару (нпр. I и T), а минимизује косинусну сличност између свих осталих парова. Због тога се овај приступ назива **контрастивним**.

CLIP модел/библиотека доступан је на [OpenAI GitHub](https://github.com/openai/CLIP). Приступ је описан у [овом блогу](https://openai.com/blog/clip/), а детаљније у [овом раду](https://arxiv.org/pdf/2103.00020.pdf).

Када је овај модел претходно обучен, можемо му дати пакет слика и пакет текстуалних упита, а он ће вратити тензор са вероватноћама. CLIP се може користити за неколико задатака:

**Класификација слика**

Претпоставимо да треба да класификујемо слике, рецимо, између мачака, паса и људи. У том случају, можемо моделу дати слику и низ текстуалних упита: "*слика мачке*", "*слика пса*", "*слика човека*". У резултујућем вектору од 3 вероватноће само треба изабрати индекс са највишом вредношћу.

![CLIP за класификацију слика](../../../../../translated_images/sr/clip-class.3af42ef0b2b19369.webp)

> *Слика из [овог блога](https://openai.com/blog/clip/)*

**Претрага слика на основу текста**

Можемо урадити и супротно. Ако имамо колекцију слика, можемо ту колекцију проследити моделу, заједно са текстуалним упитом - ово ће нам дати слику која је најсличнија датом упиту.

## ✍️ Пример: [Коришћење CLIP-а за класификацију слика и претрагу слика](../../../../../lessons/X-Extras/X1-MultiModal/Clip.ipynb)

Отворите [Clip.ipynb](../../../../../lessons/X-Extras/X1-MultiModal/Clip.ipynb) бележницу да видите CLIP у акцији.

## Генерисање слика са VQGAN+CLIP

CLIP се такође може користити за **генерисање слика** на основу текстуалног упита. Да бисмо то урадили, потребан нам је **генеративни модел** који ће моћи да генерише слике на основу неког векторског уноса. Један од таквих модела назива се [VQGAN](https://compvis.github.io/taming-transformers/) (Vector-Quantized GAN).

Главне идеје VQGAN-а које га разликују од обичних [GAN](../../4-ComputerVision/10-GANs/README.md) су следеће:
* Коришћење ауторегресивне трансформер архитектуре за генерисање секвенце визуелних делова богатих контекстом који чине слику. Ти визуелни делови се уче помоћу [CNN](../../4-ComputerVision/07-ConvNets/README.md).
* Коришћење дискриминатора за подслике који открива да ли су делови слике "стварни" или "лажни" (за разлику од "све-или-ништа" приступа у традиционалним GAN-овима).

Сазнајте више о VQGAN-у на веб сајту [Taming Transformers](https://compvis.github.io/taming-transformers/).

Једна од важних разлика између VQGAN-а и традиционалног GAN-а је та што други може произвести пристојну слику из било ког улазног вектора, док VQGAN вероватно производи слику која није кохерентна. Због тога је потребно додатно усмеравати процес креирања слике, а то се може урадити помоћу CLIP-а.

![VQGAN+CLIP Архитектура](../../../../../translated_images/sr/vqgan.5027fe05051dfa31.webp)

Да бисмо генерисали слику која одговара текстуалном упиту, почињемо са неким насумичним вектором кодирања који се прослеђује кроз VQGAN да би се произвела слика. Затим се CLIP користи за креирање функције губитка која показује колико добро слика одговара текстуалном упиту. Циљ је затим минимизовати овај губитак, користећи уназадно ширење да би се прилагодили параметри улазног вектора.

Одлична библиотека која имплементира VQGAN+CLIP је [Pixray](http://github.com/pixray/pixray).

![Слика коју је произвео Pixray](../../../../../translated_images/sr/a_closeup_watercolor_portrait_of_young_male_teacher_of_literature_with_a_book.2384968e9db8a0d0.webp) |  ![Слика коју је произвео Pixray](../../../../../translated_images/sr/a_closeup_oil_portrait_of_young_female_teacher_of_computer_science_with_a_computer.e0b6495f210a4390.webp) | ![Слика коју је произвео Pixray](../../../../../translated_images/sr/a_closeup_oil_portrait_of_old_male_teacher_of_math.5362e67aa7fc2683.webp)
----|----|----
Слика генерисана на основу упита *акварелски портрет младог мушког професора књижевности са књигом* | Слика генерисана на основу упита *уљани портрет младе женске професорке рачунарских наука са рачунаром* | Слика генерисана на основу упита *уљани портрет старог мушког професора математике испред табле*

> Слике из колекције **Вештачки професори** аутора [Дмитрија Сошњикова](http://soshnikov.com)

## DALL-E
### [DALL-E 1](https://openai.com/research/dall-e)
DALL-E је верзија GPT-3 обучена за генерисање слика на основу упита. Обучен је са 12 милијарди параметара.

За разлику од CLIP-а, DALL-E прима и текст и слику као јединствен ток токена за обе врсте података. Стога, из више упита, можете генерисати слике на основу текста.

### [DALL-E 2](https://openai.com/dall-e-2)
Главна разлика између DALL-E 1 и 2 је у томе што други генерише реалистичније слике и уметничка дела.

Примери генерисања слика са DALL-E:
![Слика коју је произвео Pixray](../../../../../translated_images/sr/DALL·E%202023-06-20%2015.56.56%20-%20a%20closeup%20watercolor%20portrait%20of%20young%20male%20teacher%20of%20literature%20with%20a%20book.6c235e8271d9ed10ce985d86aeb241a58518958647973af136912116b9518fce.png) |  ![Слика коју је произвео Pixray](../../../../../translated_images/sr/DALL·E%202023-06-20%2015.57.43%20-%20a%20closeup%20oil%20portrait%20of%20young%20female%20teacher%20of%20computer%20science%20with%20a%20computer.f21dc4166340b6c8b4d1cb57efd1e22127407f9b28c9ac7afe11344065369e64.png) | ![Слика коју је произвео Pixray](../../../../../translated_images/sr/DALL·E%202023-06-20%2015.58.42%20-%20%20a%20closeup%20oil%20portrait%20of%20old%20male%20teacher%20of%20mathematics%20in%20front%20of%20blackboard.d331c2dfbdc3f7c46aa65c0809066f5e7ed4b49609cd259852e760df21051e4a.png)
----|----|----
Слика генерисана на основу упита *акварелски портрет младог мушког професора књижевности са књигом* | Слика генерисана на основу упита *уљани портрет младе женске професорке рачунарских наука са рачунаром* | Слика генерисана на основу упита *уљани портрет старог мушког професора математике испред табле*

## Референце

* Рад о VQGAN-у: [Taming Transformers for High-Resolution Image Synthesis](https://compvis.github.io/taming-transformers/paper/paper.pdf)
* Рад о CLIP-у: [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/pdf/2103.00020.pdf)

**Одрицање од одговорности**:  
Овај документ је преведен коришћењем услуге за превођење помоћу вештачке интелигенције [Co-op Translator](https://github.com/Azure/co-op-translator). Иако се трудимо да обезбедимо тачност, молимо вас да имате у виду да аутоматски преводи могу садржати грешке или нетачности. Оригинални документ на његовом изворном језику треба сматрати меродавним извором. За критичне информације препоручује се професионални превод од стране људи. Не преузимамо одговорност за било каква погрешна тумачења или неспоразуме који могу настати услед коришћења овог превода.