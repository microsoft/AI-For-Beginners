# Neur√°lis h√°l√≥zati keretrendszerek

Ahogy m√°r megtanultuk, a neur√°lis h√°l√≥zatok hat√©kony tan√≠t√°s√°hoz k√©t dolgot kell megtenn√ºnk:

* Tensorokon kell m≈±veleteket v√©gezni, p√©ld√°ul szorz√°s, √∂sszead√°s, valamint bizonyos f√ºggv√©nyek, mint a sigmoid vagy softmax kisz√°m√≠t√°sa.
* Az √∂sszes kifejez√©s gradiens√©t ki kell sz√°m√≠tani, hogy gradient descent optimaliz√°ci√≥t v√©gezhess√ºnk.

## [El≈ëad√°s el≈ëtti kv√≠z](https://ff-quizzes.netlify.app/en/ai/quiz/9)

M√≠g a `numpy` k√∂nyvt√°r k√©pes az els≈ë feladatot elv√©gezni, sz√ºks√©g√ºnk van egy mechanizmusra a gradiens sz√°m√≠t√°s√°hoz. Az [√°ltalunk fejlesztett keretrendszerben](../04-OwnFramework/OwnFramework.ipynb), amelyet az el≈ëz≈ë szakaszban k√©sz√≠tett√ºnk, manu√°lisan kellett programoznunk az √∂sszes deriv√°lt f√ºggv√©nyt a `backward` met√≥dusban, amely a visszaterjeszt√©st v√©gzi. Ide√°lis esetben egy keretrendszer lehet≈ës√©get kell adjon arra, hogy *b√°rmilyen kifejez√©s* gradiens√©t kisz√°m√≠tsuk, amelyet defini√°lni tudunk.

Egy m√°sik fontos dolog, hogy k√©pesek legy√ºnk sz√°m√≠t√°sokat v√©gezni GPU-n vagy m√°s speci√°lis sz√°m√≠t√°si egys√©geken, mint p√©ld√°ul [TPU](https://en.wikipedia.org/wiki/Tensor_Processing_Unit). A m√©ly neur√°lis h√°l√≥zatok tan√≠t√°sa *rengeteg* sz√°m√≠t√°st ig√©nyel, √©s nagyon fontos, hogy ezeket a sz√°m√≠t√°sokat p√°rhuzamos√≠tsuk GPU-kon.

> ‚úÖ A 'p√°rhuzamos√≠t√°s' kifejez√©s azt jelenti, hogy a sz√°m√≠t√°sokat t√∂bb eszk√∂z k√∂z√∂tt osztjuk el.

Jelenleg a k√©t legn√©pszer≈±bb neur√°lis keretrendszer: [TensorFlow](http://TensorFlow.org) √©s [PyTorch](https://pytorch.org/). Mindkett≈ë alacsony szint≈± API-t biztos√≠t a tensorokkal val√≥ m≈±veletekhez, mind CPU-n, mind GPU-n. Az alacsony szint≈± API mellett van egy magasabb szint≈± API is, amelyet [Keras](https://keras.io/) √©s [PyTorch Lightning](https://pytorchlightning.ai/) n√©ven ismer√ºnk.

Alacsony szint≈± API | [TensorFlow](http://TensorFlow.org) | [PyTorch](https://pytorch.org/)
--------------------|-------------------------------------|--------------------------------
Magas szint≈± API   | [Keras](https://keras.io/)         | [PyTorch Lightning](https://pytorchlightning.ai/)

**Az alacsony szint≈± API-k** mindk√©t keretrendszerben lehet≈ëv√© teszik √∫gynevezett **sz√°m√≠t√°si gr√°fok** l√©trehoz√°s√°t. Ez a gr√°f meghat√°rozza, hogyan kell kisz√°m√≠tani a kimenetet (√°ltal√°ban a vesztes√©gf√ºggv√©nyt) adott bemeneti param√©terekkel, √©s GPU-ra is k√ºldhet≈ë sz√°m√≠t√°sra, ha el√©rhet≈ë. Vannak funkci√≥k, amelyekkel differenci√°lhatjuk ezt a sz√°m√≠t√°si gr√°fot, √©s kisz√°m√≠thatjuk a gradiens √©rt√©keket, amelyeket azt√°n a modell param√©tereinek optimaliz√°l√°s√°ra haszn√°lhatunk.

**A magas szint≈± API-k** a neur√°lis h√°l√≥zatokat gyakorlatilag **r√©tegek sorozatak√©nt** kezelik, √©s a legt√∂bb neur√°lis h√°l√≥zat fel√©p√≠t√©s√©t jelent≈ësen leegyszer≈±s√≠tik. A modell tan√≠t√°sa √°ltal√°ban az adatok el≈ëk√©sz√≠t√©s√©t √©s egy `fit` f√ºggv√©ny megh√≠v√°s√°t ig√©nyli.

A magas szint≈± API lehet≈ëv√© teszi, hogy tipikus neur√°lis h√°l√≥zatokat nagyon gyorsan fel√©p√≠ts√ºnk an√©lk√ºl, hogy sok r√©szlettel kellene foglalkoznunk. Ugyanakkor az alacsony szint≈± API sokkal nagyobb kontrollt biztos√≠t a tan√≠t√°si folyamat felett, ez√©rt gyakran haszn√°lj√°k kutat√°sok sor√°n, amikor √∫j neur√°lis h√°l√≥zati architekt√∫r√°kkal dolgozunk.

Fontos meg√©rteni, hogy mindk√©t API egy√ºtt is haszn√°lhat√≥, p√©ld√°ul saj√°t h√°l√≥zati r√©tegarchitekt√∫r√°t fejleszthet√ºnk alacsony szint≈± API-val, majd haszn√°lhatjuk azt egy nagyobb h√°l√≥zatban, amelyet magas szint≈± API-val √©p√≠tett√ºnk √©s tan√≠tottunk. Vagy defini√°lhatunk egy h√°l√≥zatot magas szint≈± API-val r√©tegek sorozatak√©nt, majd saj√°t alacsony szint≈± tan√≠t√°si ciklust haszn√°lhatunk az optimaliz√°ci√≥hoz. Mindk√©t API ugyanazokat az alapvet≈ë koncepci√≥kat haszn√°lja, √©s √∫gy tervezt√©k ≈ëket, hogy j√≥l m≈±k√∂djenek egy√ºtt.

## Tanul√°s

Ebben a kurzusban a legt√∂bb tartalmat mind PyTorch, mind TensorFlow keretrendszerhez k√≠n√°ljuk. V√°laszthatod a prefer√°lt keretrendszert, √©s csak a megfelel≈ë jegyzetf√ºzeteket tanulm√°nyozhatod. Ha nem vagy biztos benne, melyik keretrendszert v√°laszd, olvass el n√©h√°ny vit√°t az interneten a **PyTorch vs. TensorFlow** t√©m√°ban. Megn√©zheted mindk√©t keretrendszert is, hogy jobban meg√©rtsd ≈ëket.

Ahol lehets√©ges, magas szint≈± API-kat fogunk haszn√°lni az egyszer≈±s√©g kedv√©√©rt. Ugyanakkor fontosnak tartjuk, hogy meg√©rts√ºk, hogyan m≈±k√∂dnek a neur√°lis h√°l√≥zatok az alapokt√≥l kezdve, ez√©rt az elej√©n alacsony szint≈± API-val √©s tensorokkal kezd√ºnk dolgozni. Ha azonban gyorsan szeretn√©l haladni, √©s nem akarsz sok id≈ët t√∂lteni ezeknek a r√©szleteknek a tanul√°s√°val, kihagyhatod ezeket, √©s k√∂zvetlen√ºl a magas szint≈± API jegyzetf√ºzetekbe l√©phetsz.

## ‚úçÔ∏è Gyakorlatok: Keretrendszerek

Folytasd a tanul√°st az al√°bbi jegyzetf√ºzetekben:

Alacsony szint≈± API | [TensorFlow+Keras Jegyzetf√ºzet](IntroKerasTF.ipynb) | [PyTorch](IntroPyTorch.ipynb)
--------------------|-------------------------------------|--------------------------------
Magas szint≈± API   | [Keras](IntroKeras.ipynb)         | *PyTorch Lightning*

Miut√°n elsaj√°t√≠tottad a keretrendszereket, tekints√ºk √°t az overfitting fogalm√°t.

# Overfitting

Az overfitting rendk√≠v√ºl fontos fogalom a g√©pi tanul√°sban, √©s nagyon fontos, hogy helyesen √©rts√ºk meg!

Vegy√ºk p√©ld√°ul az al√°bbi probl√©m√°t, amelyben 5 pontot pr√≥b√°lunk k√∂zel√≠teni (a grafikonokon `x` jel√∂li a pontokat):

![line√°ris](../../../../../translated_images/hu/overfit1.f24b71c6f652e59e.webp) | ![overfitting](../../../../../translated_images/hu/overfit2.131f5800ae10ca5e.webp)
-------------------------|--------------------------
**Line√°ris modell, 2 param√©ter** | **Nemline√°ris modell, 7 param√©ter**
Tan√≠t√°si hiba = 5.3 | Tan√≠t√°si hiba = 0
Valid√°ci√≥s hiba = 5.1 | Valid√°ci√≥s hiba = 20

* A bal oldalon egy j√≥ egyenes vonal k√∂zel√≠t√©st l√°tunk. Mivel a param√©terek sz√°ma megfelel≈ë, a modell helyesen √©rti a pontok eloszl√°s√°nak l√©nyeg√©t.
* A jobb oldalon a modell t√∫l er≈ës. Mivel csak 5 pontunk van, √©s a modellnek 7 param√©tere van, √∫gy tudja be√°ll√≠tani mag√°t, hogy minden ponton √°thaladjon, √≠gy a tan√≠t√°si hiba 0 lesz. Ez azonban megakad√°lyozza, hogy a modell meg√©rtse az adatok m√∂g√∂tti helyes mint√°zatot, √≠gy a valid√°ci√≥s hiba nagyon magas.

Nagyon fontos megtal√°lni a megfelel≈ë egyens√∫lyt a modell gazdags√°ga (param√©terek sz√°ma) √©s a tan√≠t√°si mint√°k sz√°ma k√∂z√∂tt.

## Mi√©rt fordul el≈ë overfitting?

  * Nem el√©g tan√≠t√°si adat
  * T√∫l er≈ës modell
  * T√∫l sok zaj a bemeneti adatokban

## Hogyan √©szlelhet≈ë az overfitting?

Ahogy a fenti grafikonon l√°that√≥, az overfittinget nagyon alacsony tan√≠t√°si hiba √©s magas valid√°ci√≥s hiba jelezheti. √Åltal√°ban a tan√≠t√°s sor√°n mind a tan√≠t√°si, mind a valid√°ci√≥s hib√°k cs√∂kkenni kezdenek, majd egy ponton a valid√°ci√≥s hiba meg√°llhat a cs√∂kken√©sben, √©s n√∂vekedni kezdhet. Ez az overfitting jele, √©s annak indik√°tora, hogy val√≥sz√≠n≈±leg abba kell hagynunk a tan√≠t√°st (vagy legal√°bbis k√©sz√≠ten√ºnk kell egy pillanatk√©pet a modellr≈ël).

![overfitting](../../../../../translated_images/hu/Overfitting.408ad91cd90b4371.webp)

## Hogyan el≈ëzhet≈ë meg az overfitting?

Ha l√°tod, hogy overfitting t√∂rt√©nik, az al√°bbiakat teheted:

 * N√∂veld a tan√≠t√°si adatok mennyis√©g√©t
 * Cs√∂kkentsd a modell komplexit√°s√°t
 * Haszn√°lj valamilyen [regulariz√°ci√≥s technik√°t](../../4-ComputerVision/08-TransferLearning/TrainingTricks.md), p√©ld√°ul [Dropout](../../4-ComputerVision/08-TransferLearning/TrainingTricks.md#Dropout), amelyet k√©s≈ëbb megvizsg√°lunk.

## Overfitting √©s Bias-Variance Tradeoff

Az overfitting val√≥j√°ban egy √°ltal√°nosabb statisztikai probl√©ma, amelyet [Bias-Variance Tradeoff](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff) n√©ven ismer√ºnk. Ha megvizsg√°ljuk a modell√ºnk hib√°inak lehets√©ges forr√°sait, k√©tf√©le hib√°t l√°thatunk:

* **Bias hib√°k**, amelyeket az algoritmusunk okoz, mert nem k√©pes helyesen megragadni a tan√≠t√°si adatok k√∂z√∂tti kapcsolatot. Ez abb√≥l ad√≥dhat, hogy a modell√ºnk nem el√©g er≈ës (**underfitting**).
* **Variance hib√°k**, amelyeket az okoz, hogy a modell a bemeneti adatok zaj√°t k√∂zel√≠ti, ahelyett, hogy a jelent≈ës kapcsolatot ragadn√° meg (**overfitting**).

A tan√≠t√°s sor√°n a bias hiba cs√∂kken (ahogy a modell megtanulja k√∂zel√≠teni az adatokat), m√≠g a variance hiba n√∂vekszik. Fontos, hogy meg√°ll√≠tsuk a tan√≠t√°st - ak√°r manu√°lisan (amikor √©szlelj√ºk az overfittinget), ak√°r automatikusan (regulariz√°ci√≥ bevezet√©s√©vel) -, hogy megel≈ëzz√ºk az overfittinget.

## √ñsszegz√©s

Ebben a leck√©ben megtanultad a k√ºl√∂nbs√©geket a k√©t legn√©pszer≈±bb AI keretrendszer, a TensorFlow √©s a PyTorch k√ºl√∂nb√∂z≈ë API-i k√∂z√∂tt. Emellett megismerkedt√©l egy nagyon fontos t√©m√°val, az overfittinggel.

## üöÄ Kih√≠v√°s

Az ehhez kapcsol√≥d√≥ jegyzetf√ºzetek alj√°n 'feladatokat' tal√°lsz; dolgozd √°t a jegyzetf√ºzeteket, √©s v√©gezd el a feladatokat.

## [El≈ëad√°s ut√°ni kv√≠z](https://ff-quizzes.netlify.app/en/ai/quiz/10)

## √Åttekint√©s √©s √∂n√°ll√≥ tanul√°s

V√©gezz kutat√°st az al√°bbi t√©m√°kban:

- TensorFlow
- PyTorch
- Overfitting

Tedd fel magadnak az al√°bbi k√©rd√©seket:

- Mi a k√ºl√∂nbs√©g a TensorFlow √©s a PyTorch k√∂z√∂tt?
- Mi a k√ºl√∂nbs√©g az overfitting √©s az underfitting k√∂z√∂tt?

## [Feladat](lab/README.md)

Ebben a laborban k√©t oszt√°lyoz√°si probl√©m√°t kell megoldanod egy- √©s t√∂bbr√©teg≈± teljesen √∂sszekapcsolt h√°l√≥zatokkal, PyTorch vagy TensorFlow haszn√°lat√°val.

* [√ötmutat√≥](lab/README.md)
* [Jegyzetf√ºzet](lab/LabFrameworks.ipynb)

---

