# Bevezet√©s a Neur√°lis H√°l√≥zatokba. T√∂bbr√©teg≈± Perceptron

Az el≈ëz≈ë r√©szben megismerkedt√©l a legegyszer≈±bb neur√°lis h√°l√≥zati modellel - az egyr√©teg≈± perceptronnal, amely egy line√°ris k√©toszt√°lyos oszt√°lyoz√°si modell.

Ebben a r√©szben kiterjesztj√ºk ezt a modellt egy rugalmasabb keretrendszerre, amely lehet≈ëv√© teszi sz√°munkra:

* **t√∂bboszt√°lyos oszt√°lyoz√°s** v√©grehajt√°s√°t a k√©toszt√°lyos oszt√°lyoz√°s mellett
* **regresszi√≥s probl√©m√°k** megold√°s√°t az oszt√°lyoz√°s mellett
* nem line√°risan szepar√°lhat√≥ oszt√°lyok elk√ºl√∂n√≠t√©s√©t

Ezenk√≠v√ºl saj√°t modul√°ris keretrendszert fejleszt√ºnk Pythonban, amely lehet≈ëv√© teszi k√ºl√∂nb√∂z≈ë neur√°lis h√°l√≥zati architekt√∫r√°k l√©trehoz√°s√°t.

## [El≈ëad√°s el≈ëtti kv√≠z](https://ff-quizzes.netlify.app/en/ai/quiz/7)

## G√©pi Tanul√°s Formaliz√°l√°sa

Kezdj√ºk a g√©pi tanul√°si probl√©ma formaliz√°l√°s√°val. Tegy√ºk fel, hogy van egy **X** tan√≠t√≥ adathalmazunk c√≠mk√©kkel **Y**, √©s egy olyan modellt kell √©p√≠ten√ºnk (*f*), amely a lehet≈ë legpontosabb el≈ërejelz√©seket adja. Az el≈ërejelz√©sek min≈ës√©g√©t a **vesztes√©gf√ºggv√©ny** (&lagran;) m√©ri. Gyakran haszn√°lt vesztes√©gf√ºggv√©nyek:

* Regresszi√≥s probl√©m√°k eset√©n, amikor egy sz√°mot kell el≈ëre jelezn√ºnk, haszn√°lhatjuk az **abszol√∫t hib√°t** &sum;<sub>i</sub>|f(x<sup>(i)</sup>)-y<sup>(i)</sup>|, vagy a **n√©gyzetes hib√°t** &sum;<sub>i</sub>(f(x<sup>(i)</sup>)-y<sup>(i)</sup>)<sup>2</sup>.
* Oszt√°lyoz√°s eset√©n haszn√°ljuk a **0-1 vesztes√©get** (ami l√©nyeg√©ben a modell **pontoss√°ga**), vagy a **logisztikus vesztes√©get**.

Az egyszint≈± perceptron eset√©ben az *f* f√ºggv√©nyt line√°ris f√ºggv√©nyk√©nt defini√°ltuk: *f(x)=wx+b* (ahol *w* a s√∫lym√°trix, *x* a bemeneti jellemz≈ëk vektora, √©s *b* az eltol√°si vektor). K√ºl√∂nb√∂z≈ë neur√°lis h√°l√≥zati architekt√∫r√°k eset√©n ez a f√ºggv√©ny bonyolultabb form√°t √∂lthet.

> Oszt√°lyoz√°s eset√©n gyakran k√≠v√°natos, hogy a h√°l√≥zat kimenete az adott oszt√°lyok val√≥sz√≠n≈±s√©geit adja meg. Az √©rt√©kek val√≥sz√≠n≈±s√©gekk√© t√∂rt√©n≈ë √°talak√≠t√°s√°hoz (pl. a kimenet normaliz√°l√°s√°hoz) gyakran haszn√°ljuk a **softmax** f√ºggv√©nyt (&sigma;), √≠gy az *f* f√ºggv√©ny *f(x)=&sigma;(wx+b)* form√°t √∂lt.

Az *f* defin√≠ci√≥j√°ban *w* √©s *b* az √∫gynevezett **param√©terek** (&theta;=‚ü®*w,b*‚ü©). Az ‚ü®**X**,**Y**‚ü© adathalmaz alapj√°n kisz√°m√≠thatjuk az eg√©sz adathalmazra vonatkoz√≥ √∂sszes√≠tett hib√°t a param√©terek (&theta;) f√ºggv√©ny√©ben.

> ‚úÖ **A neur√°lis h√°l√≥zat tan√≠t√°s√°nak c√©lja a hiba minimaliz√°l√°sa a param√©terek (&theta;) v√°ltoztat√°s√°val.**

## Gradiens Descent Optimaliz√°l√°s

Van egy j√≥l ismert f√ºggv√©nyoptimaliz√°l√°si m√≥dszer, az √∫gynevezett **gradiens descent**. Az √∂tlet az, hogy kisz√°m√≠thatjuk a vesztes√©gf√ºggv√©ny deriv√°ltj√°t (t√∂bbdimenzi√≥s esetben **gradiensnek** nevezz√ºk) a param√©terekre vonatkoz√≥an, √©s √∫gy v√°ltoztathatjuk a param√©tereket, hogy a hiba cs√∂kkenjen. Ez a k√∂vetkez≈ëk√©ppen formaliz√°lhat√≥:

* Inicializ√°ljuk a param√©tereket v√©letlenszer≈± √©rt√©kekkel: w<sup>(0)</sup>, b<sup>(0)</sup>.
* Ism√©telj√ºk meg az al√°bbi l√©p√©st sokszor:
    - w<sup>(i+1)</sup> = w<sup>(i)</sup>-&eta;&part;&lagran;/&part;w
    - b<sup>(i+1)</sup> = b<sup>(i)</sup>-&eta;&part;&lagran;/&part;b

A tan√≠t√°s sor√°n az optimaliz√°l√°si l√©p√©seket az eg√©sz adathalmaz figyelembev√©tel√©vel kellene kisz√°m√≠tani (eml√©kezz√ºnk, hogy a vesztes√©get az √∂sszes tan√≠t√≥ minta √∂sszegz√©s√©vel sz√°m√≠tjuk). Azonban a val√≥s√°gban az adathalmaz kis r√©szeit, √∫gynevezett **minibatch-eket** haszn√°lunk, √©s a gradienseket az adatok egy r√©szhalmaz√°n alapulva sz√°m√≠tjuk ki. Mivel a r√©szhalmazt minden alkalommal v√©letlenszer≈±en v√°lasztjuk ki, ezt a m√≥dszert **stochasztikus gradiens descentnek** (SGD) nevezz√ºk.

## T√∂bbr√©teg≈± Perceptronok √©s Backpropagation

Az egyr√©teg≈± h√°l√≥zat, ahogy fentebb l√°ttuk, k√©pes line√°risan szepar√°lhat√≥ oszt√°lyok oszt√°lyoz√°s√°ra. Gazdagabb modellt √©p√≠thet√ºnk, ha t√∂bb r√©teget kombin√°lunk a h√°l√≥zatban. Matematikailag ez azt jelenti, hogy az *f* f√ºggv√©ny bonyolultabb form√°t √∂lt, √©s t√∂bb l√©p√©sben sz√°m√≠tjuk ki:
* z<sub>1</sub>=w<sub>1</sub>x+b<sub>1</sub>
* z<sub>2</sub>=w<sub>2</sub>&alpha;(z<sub>1</sub>)+b<sub>2</sub>
* f = &sigma;(z<sub>2</sub>)

Itt &alpha; egy **nemline√°ris aktiv√°ci√≥s f√ºggv√©ny**, &sigma; egy softmax f√ºggv√©ny, √©s a param√©terek &theta;=<*w<sub>1</sub>,b<sub>1</sub>,w<sub>2</sub>,b<sub>2</sub>*>.

A gradiens descent algoritmus ugyanaz marad, de a gradiensek kisz√°m√≠t√°sa bonyolultabb√° v√°lik. A l√°ncszab√°ly alapj√°n a deriv√°ltakat a k√∂vetkez≈ëk√©ppen sz√°m√≠thatjuk ki:

* &part;&lagran;/&part;w<sub>2</sub> = (&part;&lagran;/&part;&sigma;)(&part;&sigma;/&part;z<sub>2</sub>)(&part;z<sub>2</sub>/&part;w<sub>2</sub>)
* &part;&lagran;/&part;w<sub>1</sub> = (&part;&lagran;/&part;&sigma;)(&part;&sigma;/&part;z<sub>2</sub>)(&part;z<sub>2</sub>/&part;&alpha;)(&part;&alpha;/&part;z<sub>1</sub>)(&part;z<sub>1</sub>/&part;w<sub>1</sub>)

> ‚úÖ A l√°ncszab√°lyt haszn√°ljuk a vesztes√©gf√ºggv√©ny param√©terekre vonatkoz√≥ deriv√°ltjainak kisz√°m√≠t√°s√°hoz.

Figyelj√ºk meg, hogy ezeknek a kifejez√©seknek a bal sz√©ls≈ë r√©sze ugyanaz, √≠gy hat√©konyan kisz√°m√≠thatjuk a deriv√°ltakat a vesztes√©gf√ºggv√©nyt≈ël kiindulva, "visszafel√©" haladva a sz√°m√≠t√°si gr√°fon. Ez√©rt a t√∂bbr√©teg≈± perceptron tan√≠t√°s√°nak m√≥dszer√©t **backpropagationnek**, vagy r√∂viden 'backprop'-nak nevezz√ºk.

<img alt="sz√°m√≠t√°si gr√°f" src="../../../../../translated_images/hu/ComputeGraphGrad.4626252c0de03507.webp"/>

> TODO: k√©p forr√°smegjel√∂l√©s

> ‚úÖ A backpropagation-t sokkal r√©szletesebben fogjuk t√°rgyalni a notebook p√©ld√°nkban.  

## √ñsszegz√©s

Ebben a leck√©ben saj√°t neur√°lis h√°l√≥zati k√∂nyvt√°rat √©p√≠tett√ºnk, √©s egy egyszer≈± k√©tdimenzi√≥s oszt√°lyoz√°si feladatra haszn√°ltuk.

## üöÄ Kih√≠v√°s

A mell√©kelt notebookban saj√°t keretrendszert fogsz megval√≥s√≠tani t√∂bbr√©teg≈± perceptronok √©p√≠t√©s√©re √©s tan√≠t√°s√°ra. R√©szletesen megismerheted, hogyan m≈±k√∂dnek a modern neur√°lis h√°l√≥zatok.

L√©pj tov√°bb az [OwnFramework](OwnFramework.ipynb) notebookra, √©s dolgozd ki.

## [El≈ëad√°s ut√°ni kv√≠z](https://ff-quizzes.netlify.app/en/ai/quiz/8)

## √Åttekint√©s √©s √ñn√°ll√≥ Tanul√°s

A backpropagation egy gyakori algoritmus az AI √©s ML ter√ºlet√©n, √©rdemes [r√©szletesebben tanulm√°nyozni](https://wikipedia.org/wiki/Backpropagation).

## [Feladat](lab/README.md)

Ebben a laborban arra k√©r√ºnk, hogy haszn√°ld az ebben a leck√©ben meg√©p√≠tett keretrendszert az MNIST k√©zzel √≠rott sz√°mjegyek oszt√°lyoz√°s√°nak megold√°s√°ra.

* [√ötmutat√≥](lab/README.md)
* [Notebook](lab/MyFW_MNIST.ipynb)

---

