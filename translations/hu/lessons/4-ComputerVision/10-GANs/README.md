# Generat√≠v Adverz√°lis H√°l√≥zatok

Az el≈ëz≈ë r√©szben megismerkedt√ºnk a **generat√≠v modellekkel**: olyan modellekkel, amelyek k√©pesek √∫j k√©peket gener√°lni, hasonl√≥an a tan√≠t√≥ adathalmazban l√©v≈ëkh√∂z. A VAE j√≥ p√©ld√°ja volt egy generat√≠v modellnek.

## [El≈ëad√°s el≈ëtti kv√≠z](https://ff-quizzes.netlify.app/en/ai/quiz/19)

Azonban, ha valami igaz√°n jelent≈ës dolgot szeretn√©nk gener√°lni, p√©ld√°ul egy festm√©nyt megfelel≈ë felbont√°sban, a VAE-vel azt tapasztaljuk, hogy a tan√≠t√°s nem konverg√°l j√≥l. Ehhez az esethez egy m√°sik architekt√∫r√°t kell megismern√ºnk, amely kifejezetten generat√≠v modellekre ir√°nyul - **Generat√≠v Adverz√°lis H√°l√≥zatok**, vagy GAN-ek.

A GAN f≈ë √∂tlete, hogy k√©t neur√°lis h√°l√≥zatot tan√≠tunk egym√°s ellen:

<img src="../../../../../translated_images/hu/gan_architecture.8f3a5ab62b8d5d69.webp" width="70%"/>

> K√©p: [Dmitry Soshnikov](http://soshnikov.com)

> ‚úÖ Egy kis sz√≥szedet:
> * **Gener√°tor**: egy h√°l√≥zat, amely egy v√©letlenszer≈± vektort vesz, √©s eredm√©nyk√©nt k√©pet √°ll√≠t el≈ë.
> * **Diszkrimin√°tor**: egy h√°l√≥zat, amely egy k√©pet vesz, √©s meg kell mondania, hogy az val√≥di k√©p-e (a tan√≠t√≥ adathalmazb√≥l), vagy a gener√°tor √°ltal l√©trehozott. L√©nyeg√©ben egy k√©poszt√°lyoz√≥.

### Diszkrimin√°tor

A diszkrimin√°tor architekt√∫r√°ja nem k√ºl√∂nb√∂zik egy szok√°sos k√©poszt√°lyoz√≥ h√°l√≥zatt√≥l. A legegyszer≈±bb esetben lehet egy teljesen √∂sszekapcsolt oszt√°lyoz√≥, de val√≥sz√≠n≈±leg egy [konvol√∫ci√≥s h√°l√≥zat](../07-ConvNets/README.md) lesz.

> ‚úÖ A konvol√∫ci√≥s h√°l√≥zatokon alapul√≥ GAN-t [DCGAN](https://arxiv.org/pdf/1511.06434.pdf)-nek h√≠vj√°k.

Egy CNN diszkrimin√°tor a k√∂vetkez≈ë r√©tegekb≈ël √°ll: t√∂bb konvol√∫ci√≥+pooling (cs√∂kken≈ë t√©rbeli m√©rettel), √©s egy vagy t√∂bb teljesen √∂sszekapcsolt r√©teg, hogy "jellemz≈ë vektort" kapjunk, v√©g√ºl egy bin√°ris oszt√°lyoz√≥.

> ‚úÖ A 'pooling' ebben az √∂sszef√ºgg√©sben egy olyan technika, amely cs√∂kkenti a k√©p m√©ret√©t. "A pooling r√©tegek cs√∂kkentik az adatok dimenzi√≥it az√°ltal, hogy az egyik r√©teg neuronklasztereinek kimeneteit egyetlen neuronba kombin√°lj√°k a k√∂vetkez≈ë r√©tegben." - [forr√°s](https://wikipedia.org/wiki/Convolutional_neural_network#Pooling_layers)

### Gener√°tor

A gener√°tor valamivel bonyolultabb. √ögy tekinthetj√ºk, mint egy ford√≠tott diszkrimin√°tort. Egy l√°tens vektorb√≥l kiindulva (a jellemz≈ë vektor helyett) van egy teljesen √∂sszekapcsolt r√©tege, amely √°talak√≠tja a sz√ºks√©ges m√©retre/alakra, majd dekonvol√∫ci√≥+felbont√°s n√∂vel√©s k√∂vetkezik. Ez hasonl√≥ az [autoencoder](../09-Autoencoders/README.md) *dek√≥der* r√©sz√©hez.

> ‚úÖ Mivel a konvol√∫ci√≥s r√©teg line√°ris sz≈±r≈ëk√©nt m≈±k√∂dik, amely v√©gighalad a k√©pen, a dekonvol√∫ci√≥ l√©nyeg√©ben hasonl√≥ a konvol√∫ci√≥hoz, √©s ugyanazzal a r√©teglogik√°val megval√≥s√≠that√≥.

<img src="../../../../../translated_images/hu/gan_arch_detail.46b95fd366f8e543.webp" width="70%"/>

> K√©p: [Dmitry Soshnikov](http://soshnikov.com)

### A GAN tan√≠t√°sa

A GAN-eket **adverz√°lisnak** nevezik, mert folyamatos verseny zajlik a gener√°tor √©s a diszkrimin√°tor k√∂z√∂tt. E verseny sor√°n mind a gener√°tor, mind a diszkrimin√°tor javul, √≠gy a h√°l√≥zat egyre jobb k√©pek el≈ë√°ll√≠t√°s√°t tanulja meg.

A tan√≠t√°s k√©t szakaszban t√∂rt√©nik:

* **A diszkrimin√°tor tan√≠t√°sa**. Ez a feladat viszonylag egyszer≈±: gener√°lunk egy k√©pcsomagot a gener√°torral, 0-val c√≠mk√©zve ≈ëket, ami hamis k√©pet jelent, √©s vesz√ºnk egy k√©pcsomagot a bemeneti adathalmazb√≥l (1-es c√≠mk√©vel, val√≥di k√©p). Kapunk egy *diszkrimin√°tor vesztes√©get*, √©s v√©grehajtjuk a backpropot.
* **A gener√°tor tan√≠t√°sa**. Ez valamivel bonyolultabb, mert nem ismerj√ºk k√∂zvetlen√ºl a gener√°tor v√°rt kimenet√©t. Az eg√©sz GAN h√°l√≥zatot, amely egy gener√°torb√≥l √©s egy diszkrimin√°torb√≥l √°ll, v√©letlenszer≈± vektorokkal t√°pl√°ljuk, √©s azt v√°rjuk, hogy az eredm√©ny 1 legyen (ami a val√≥di k√©peknek felel meg). Ezut√°n befagyasztjuk a diszkrimin√°tor param√©tereit (nem akarjuk, hogy ebben a l√©p√©sben tanuljon), √©s v√©grehajtjuk a backpropot.

E folyamat sor√°n a gener√°tor √©s a diszkrimin√°tor vesztes√©gei nem cs√∂kkennek jelent≈ësen. Ide√°lis esetben oszcill√°lniuk kellene, ami azt jelzi, hogy mindk√©t h√°l√≥zat jav√≠tja teljes√≠tm√©ny√©t.

## ‚úçÔ∏è Feladatok: GAN-ek

* [GAN Notebook TensorFlow/Keras-ban](GANTF.ipynb)
* [GAN Notebook PyTorch-ban](GANPyTorch.ipynb)

### Probl√©m√°k a GAN-ek tan√≠t√°s√°val

A GAN-ekr≈ël ismert, hogy k√ºl√∂n√∂sen neh√©z ≈ëket tan√≠tani. √çme n√©h√°ny probl√©ma:

* **M√≥dus √∂sszeoml√°s**. Ez azt jelenti, hogy a gener√°tor megtanul egy sikeres k√©pet el≈ë√°ll√≠tani, amely becsapja a diszkrimin√°tort, de nem k√©pes k√ºl√∂nb√∂z≈ë k√©pek sokas√°g√°t l√©trehozni.
* **√ârz√©kenys√©g a hiperparam√©terekre**. Gyakran el≈ëfordul, hogy egy GAN egy√°ltal√°n nem konverg√°l, majd hirtelen a tanul√°si r√°ta cs√∂kkent√©se konvergenci√°t eredm√©nyez.
* **Egyens√∫ly fenntart√°sa** a gener√°tor √©s a diszkrimin√°tor k√∂z√∂tt. Sok esetben a diszkrimin√°tor vesztes√©ge viszonylag gyorsan null√°ra cs√∂kkenhet, ami azt eredm√©nyezi, hogy a gener√°tor nem k√©pes tov√°bb tanulni. Ennek lek√ºzd√©s√©re megpr√≥b√°lhatunk k√ºl√∂nb√∂z≈ë tanul√°si r√°t√°kat be√°ll√≠tani a gener√°tor √©s a diszkrimin√°tor sz√°m√°ra, vagy kihagyhatjuk a diszkrimin√°tor tan√≠t√°s√°t, ha a vesztes√©g m√°r t√∫l alacsony.
* **Magas felbont√°s√∫ tan√≠t√°s**. Ugyanaz a probl√©ma, mint az autoencoder-ekn√©l, ez a probl√©ma akkor jelentkezik, amikor t√∫l sok konvol√∫ci√≥s h√°l√≥zati r√©teget kell rekonstru√°lni, ami artefaktumokat eredm√©nyez. Ezt a probl√©m√°t √°ltal√°ban √∫gynevezett **progressz√≠v n√∂veked√©ssel** oldj√°k meg, amikor el≈ësz√∂r n√©h√°ny r√©teget alacsony felbont√°s√∫ k√©peken tan√≠tanak, majd a r√©tegeket "feloldj√°k" vagy hozz√°adj√°k. Egy m√°sik megold√°s az lenne, ha extra kapcsolatokat adn√°nk a r√©tegek k√∂z√∂tt, √©s egyszerre t√∂bb felbont√°st tan√≠tan√°nk - r√©szletek√©rt l√°sd ezt a [Multi-Scale Gradient GANs tanulm√°nyt](https://arxiv.org/abs/1903.06048).

## St√≠lus√°tvitel

A GAN-ek nagyszer≈± m√≥dja m≈±v√©szi k√©pek gener√°l√°s√°nak. Egy m√°sik √©rdekes technika az √∫gynevezett **st√≠lus√°tvitel**, amely egy **tartalomk√©pet** vesz, √©s egy m√°sik st√≠lusban √∫jrarajzolja, alkalmazva a **st√≠lusk√©p** sz≈±r≈ëit.

A m≈±k√∂d√©se a k√∂vetkez≈ë:
* V√©letlenszer≈± zajk√©ppel kezd√ºnk (vagy egy tartalomk√©ppel, de a meg√©rt√©s kedv√©√©rt egyszer≈±bb zajk√©ppel kezdeni).
* C√©lunk egy olyan k√©p l√©trehoz√°sa, amely k√∂zel √°ll mind a tartalomk√©phez, mind a st√≠lusk√©phez. Ezt k√©t vesztes√©gf√ºggv√©ny hat√°rozza meg:
   - **Tartalomvesztes√©g**, amelyet a CNN √°ltal bizonyos r√©tegeken kinyert jellemz≈ëk alapj√°n sz√°m√≠tunk ki az aktu√°lis k√©p √©s a tartalomk√©p k√∂z√∂tt.
   - **St√≠lusvesztes√©g**, amelyet az aktu√°lis k√©p √©s a st√≠lusk√©p k√∂z√∂tt sz√°m√≠tunk ki egy okos m√≥dszerrel Gram-m√°trixok seg√≠ts√©g√©vel (tov√°bbi r√©szletek az [p√©lda notebookban](StyleTransfer.ipynb)).
* A k√©p sim√°bb√° t√©tele √©s a zaj elt√°vol√≠t√°sa √©rdek√©ben bevezetj√ºk a **Vari√°ci√≥s vesztes√©get**, amely a szomsz√©dos pixelek k√∂z√∂tti √°tlagos t√°vols√°got sz√°m√≠tja ki.
* A f≈ë optimaliz√°ci√≥s ciklus az aktu√°lis k√©pet gradient descent (vagy m√°s optimaliz√°ci√≥s algoritmus) seg√≠ts√©g√©vel m√≥dos√≠tja, hogy minimaliz√°lja a teljes vesztes√©get, amely az √∂sszes vesztes√©g s√∫lyozott √∂sszege.

## ‚úçÔ∏è P√©lda: [St√≠lus√°tvitel](StyleTransfer.ipynb)

## [El≈ëad√°s ut√°ni kv√≠z](https://ff-quizzes.netlify.app/en/ai/quiz/20)

## √ñsszegz√©s

Ebben a leck√©ben megismerkedt√©l a GAN-ekkel √©s azok tan√≠t√°s√°val. Megtanultad azokat a k√ºl√∂nleges kih√≠v√°sokat is, amelyekkel ez a neur√°lis h√°l√≥zat t√≠pus szembes√ºlhet, valamint n√©h√°ny strat√©gi√°t ezek lek√ºzd√©s√©re.

## üöÄ Kih√≠v√°s

Futtasd v√©gig a [St√≠lus√°tvitel notebookot](StyleTransfer.ipynb) saj√°t k√©peiddel.

## √Åttekint√©s √©s √∂n√°ll√≥ tanul√°s

Tov√°bbi inform√°ci√≥√©rt olvass a GAN-ekr≈ël az al√°bbi forr√°sokban:

* Marco Pasini, [10 tanuls√°g, amit egy √©v alatt GAN-ek tan√≠t√°s√°b√≥l tanultam](https://towardsdatascience.com/10-lessons-i-learned-training-generative-adversarial-networks-gans-for-a-year-c9071159628)
* [StyleGAN](https://en.wikipedia.org/wiki/StyleGAN), egy *de facto* GAN architekt√∫ra, amit √©rdemes megfontolni
* [Generat√≠v m≈±v√©szet l√©trehoz√°sa GAN-ekkel az Azure ML-en](https://soshnikov.com/scienceart/creating-generative-art-using-gan-on-azureml/)

## Feladat

N√©zd √°t a leck√©hez kapcsol√≥d√≥ k√©t notebook egyik√©t, √©s tan√≠tsd √∫jra a GAN-t saj√°t k√©peiden. Mit tudsz l√©trehozni?

---

