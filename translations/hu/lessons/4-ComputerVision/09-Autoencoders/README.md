# Autoenk√≥derek

Amikor CNN-eket (konvol√∫ci√≥s neur√°lis h√°l√≥zatokat) tan√≠tunk, az egyik probl√©ma az, hogy sok c√≠mk√©zett adatra van sz√ºks√©g√ºnk. K√©pklasszifik√°ci√≥ eset√©n p√©ld√°ul manu√°lisan kell azonos√≠tani √©s k√ºl√∂nb√∂z≈ë oszt√°lyokba sorolni a k√©peket.

## [El≈ëad√°s el≈ëtti kv√≠z](https://ff-quizzes.netlify.app/en/ai/quiz/17)

Azonban el≈ëfordulhat, hogy nyers (c√≠mk√©zetlen) adatokat szeretn√©nk haszn√°lni a CNN-ek jellemz≈ëkivon√≥inak tan√≠t√°s√°hoz, amit **√∂nfel√ºgyelt tanul√°snak** nevez√ºnk. C√≠mk√©k helyett a tan√≠t√≥k√©peket haszn√°ljuk a h√°l√≥zat bemenetek√©nt √©s kimenetek√©nt. Az **autoenk√≥der** f≈ë √∂tlete az, hogy lesz egy **enk√≥der h√°l√≥zat**, amely a bemeneti k√©pet valamilyen **latens t√©rbe** (√°ltal√°ban egy kisebb m√©ret≈± vektorba) alak√≠tja, majd egy **dek√≥der h√°l√≥zat**, amelynek c√©lja az eredeti k√©p rekonstru√°l√°sa.

> ‚úÖ Az [autoenk√≥der](https://wikipedia.org/wiki/Autoencoder) "egy mesters√©ges neur√°lis h√°l√≥zat t√≠pus, amelyet c√≠mk√©zetlen adatok hat√©kony k√≥dol√°s√°nak megtanul√°s√°ra haszn√°lnak."

Mivel az autoenk√≥dert arra tan√≠tjuk, hogy min√©l t√∂bb inform√°ci√≥t meg≈ërizzen az eredeti k√©pb≈ël a pontos rekonstrukci√≥ √©rdek√©ben, a h√°l√≥zat megpr√≥b√°lja megtal√°lni a legjobb **be√°gyaz√°st** a bemeneti k√©pek jelent√©s√©nek megragad√°s√°hoz.

![Autoenk√≥der diagram](../../../../../translated_images/hu/autoencoder_schema.5e6fc9ad98a5eb61.webp)

> K√©p a [Keras blogb√≥l](https://blog.keras.io/building-autoencoders-in-keras.html)

## Autoenk√≥derek haszn√°lati forgat√≥k√∂nyvei

B√°r az eredeti k√©pek rekonstru√°l√°sa √∂nmag√°ban nem t≈±nik hasznosnak, vannak olyan forgat√≥k√∂nyvek, ahol az autoenk√≥derek k√ºl√∂n√∂sen hasznosak:

* **Dimenzi√≥cs√∂kkent√©s vizualiz√°ci√≥hoz** vagy **k√©pbe√°gyaz√°sok tan√≠t√°sa**. Az autoenk√≥derek √°ltal√°ban jobb eredm√©nyeket adnak, mint a PCA, mivel figyelembe veszik a k√©pek t√©rbeli jelleg√©t √©s hierarchikus jellemz≈ëit.
* **Zajcs√∂kkent√©s**, azaz a zaj elt√°vol√≠t√°sa a k√©pr≈ël. Mivel a zaj sok haszontalan inform√°ci√≥t hordoz, az autoenk√≥der nem tudja mindezt beilleszteni a viszonylag kis latens t√©rbe, √≠gy csak a k√©p fontos r√©sz√©t ragadja meg. Zajcs√∂kkent≈ë tan√≠t√°skor az eredeti k√©pekkel kezd√ºnk, √©s mesters√©gesen zajt adunk hozz√°juk, hogy bemenetk√©nt szolg√°ljanak az autoenk√≥der sz√°m√°ra.
* **Szuperfelbont√°s**, azaz a k√©p felbont√°s√°nak n√∂vel√©se. Magas felbont√°s√∫ k√©pekkel kezd√ºnk, √©s az alacsonyabb felbont√°s√∫ k√©pet haszn√°ljuk az autoenk√≥der bemenetek√©nt.
* **Generat√≠v modellek**. Miut√°n az autoenk√≥dert betan√≠tottuk, a dek√≥der r√©szt √∫j objektumok l√©trehoz√°s√°ra haszn√°lhatjuk v√©letlenszer≈± latens vektorokb√≥l kiindulva.

## Vari√°ci√≥s Autoenk√≥derek (VAE)

A hagyom√°nyos autoenk√≥derek valamilyen m√≥don cs√∂kkentik a bemeneti adatok dimenzi√≥j√°t, √©s megpr√≥b√°lj√°k azonos√≠tani a bemeneti k√©pek fontos jellemz≈ëit. Azonban a latens vektorok gyakran nem √©rtelmezhet≈ëk. M√°s sz√≥val, ha p√©ld√°ul az MNIST adathalmazt vessz√ºk, nem k√∂nny≈± meghat√°rozni, hogy mely sz√°mjegyek felelnek meg k√ºl√∂nb√∂z≈ë latens vektoroknak, mivel a k√∂zeli latens vektorok nem felt√©tlen√ºl ugyanazokat a sz√°mjegyeket jelentik.

Ezzel szemben, ha *generat√≠v* modelleket szeretn√©nk tan√≠tani, jobb, ha van n√©mi meg√©rt√©s√ºnk a latens t√©rr≈ël. Ez az √∂tlet vezet el minket a **vari√°ci√≥s autoenk√≥derhez** (VAE).

A VAE egy olyan autoenk√≥der, amely megtanulja a latens param√©terek **statisztikai eloszl√°s√°t** el≈ëre jelezni, az √∫gynevezett **latens eloszl√°st**. P√©ld√°ul azt szeretn√©nk, hogy a latens vektorok norm√°lisan legyenek elosztva egy z<sub>mean</sub> √°tlaggal √©s z<sub>sigma</sub> sz√≥r√°ssal (mind az √°tlag, mind a sz√≥r√°s egy d dimenzi√≥s vektor). A VAE enk√≥der megtanulja ezeket a param√©tereket el≈ëre jelezni, majd a dek√≥der egy v√©letlenszer≈± vektort vesz ebb≈ël az eloszl√°sb√≥l, hogy rekonstru√°lja az objektumot.

√ñsszefoglalva:

 * A bemeneti vektorb√≥l el≈ëre jelezz√ºk `z_mean`-t √©s `z_log_sigma`-t (a sz√≥r√°s helyett annak logaritmus√°t jelezz√ºk el≈ëre)
 * Egy `sample` vektort mint√°zunk az N(z<sub>mean</sub>,exp(z<sub>log\_sigma</sub>)) eloszl√°sb√≥l
 * A dek√≥der megpr√≥b√°lja dek√≥dolni az eredeti k√©pet a `sample` vektort bemenetk√©nt haszn√°lva

 <img src="../../../../../translated_images/hu/vae.464c465a5b6a9e25.webp" width="50%">

> K√©p [Isaak Dykeman blogbejegyz√©s√©b≈ël](https://ijdykeman.github.io/ml/2016/12/21/cvae.html)

A vari√°ci√≥s autoenk√≥derek egy √∂sszetett vesztes√©gf√ºggv√©nyt haszn√°lnak, amely k√©t r√©szb≈ël √°ll:

* **Rekonstrukci√≥s vesztes√©g**, amely azt mutatja, hogy a rekonstru√°lt k√©p mennyire k√∂zel √°ll a c√©lhoz (ez lehet p√©ld√°ul a Mean Squared Error, vagy MSE). Ez ugyanaz a vesztes√©gf√ºggv√©ny, mint a norm√°l autoenk√≥derekn√©l.
* **KL vesztes√©g**, amely biztos√≠tja, hogy a latens v√°ltoz√≥ eloszl√°sa k√∂zel maradjon a norm√°l eloszl√°shoz. Ez a [Kullback-Leibler divergencia](https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained) fogalm√°n alapul - egy metrika, amely k√©t statisztikai eloszl√°s hasonl√≥s√°g√°t becs√ºli.

A VAE-k egyik fontos el≈ënye, hogy viszonylag k√∂nnyen lehet √∫j k√©peket gener√°lni, mivel tudjuk, mely eloszl√°sb√≥l kell mint√°zni a latens vektorokat. P√©ld√°ul, ha egy VAE-t tan√≠tunk 2D latens vektorral az MNIST adathalmazon, akkor a latens vektor komponenseit v√°ltoztatva k√ºl√∂nb√∂z≈ë sz√°mjegyeket kapunk:

<img alt="vaemnist" src="../../../../../translated_images/hu/vaemnist.cab9e602dc08dc50.webp" width="50%"/>

> K√©p [Dmitry Soshnikovt√≥l](http://soshnikov.com)

Figyelj√ºk meg, hogyan olvadnak √∂ssze a k√©pek, ahogy a latens param√©tert√©r k√ºl√∂nb√∂z≈ë r√©szeib≈ël kezd√ºnk latens vektorokat venni. Ezt a teret 2D-ben is vizualiz√°lhatjuk:

<img alt="vaemnist cluster" src="../../../../../translated_images/hu/vaemnist-diag.694315f775d5d666.webp" width="50%"/> 

> K√©p [Dmitry Soshnikovt√≥l](http://soshnikov.com)

## ‚úçÔ∏è Gyakorlatok: Autoenk√≥derek

Tov√°bbi inform√°ci√≥kat az autoenk√≥derekr≈ël az al√°bbi notebookokban tal√°lhatsz:

* [Autoenk√≥derek TensorFlow-ban](AutoencodersTF.ipynb)
* [Autoenk√≥derek PyTorch-ban](AutoEncodersPyTorch.ipynb)

## Az autoenk√≥derek tulajdons√°gai

* **Adatspecifikusak** - csak azon k√©pt√≠pusokkal m≈±k√∂dnek j√≥l, amelyeken tan√≠tott√°k ≈ëket. P√©ld√°ul, ha egy szuperfelbont√°s√∫ h√°l√≥zatot vir√°gokon tan√≠tunk, nem fog j√≥l m≈±k√∂dni portr√©kon. Ennek oka, hogy a h√°l√≥zat a magasabb felbont√°s√∫ k√©pet √∫gy √°ll√≠tja el≈ë, hogy finom r√©szleteket vesz a tan√≠t√≥ adathalmazb√≥l tanult jellemz≈ëkb≈ël.
* **Vesztes√©gesek** - a rekonstru√°lt k√©p nem ugyanaz, mint az eredeti k√©p. A vesztes√©g jellege a tan√≠t√°s sor√°n haszn√°lt *vesztes√©gf√ºggv√©nyt≈ël* f√ºgg.
* **C√≠mk√©zetlen adatokkal m≈±k√∂dik**

## [El≈ëad√°s ut√°ni kv√≠z](https://ff-quizzes.netlify.app/en/ai/quiz/18)

## K√∂vetkeztet√©s

Ebben a leck√©ben megismerkedt√©l az autoenk√≥derek k√ºl√∂nb√∂z≈ë t√≠pusaival, amelyek az AI kutat√≥k rendelkez√©s√©re √°llnak. Megtanultad, hogyan √©p√≠tsd fel ≈ëket, √©s hogyan haszn√°ld ≈ëket k√©pek rekonstru√°l√°s√°ra. Emellett megismerkedt√©l a VAE-vel, √©s megtanultad, hogyan haszn√°lhatod √∫j k√©pek gener√°l√°s√°ra.

## üöÄ Kih√≠v√°s

Ebben a leck√©ben megtanultad, hogyan haszn√°lhat√≥k az autoenk√≥derek k√©pekhez. De zen√©hez is haszn√°lhat√≥k! N√©zd meg a Magenta projekt [MusicVAE](https://magenta.tensorflow.org/music-vae) projektj√©t, amely autoenk√≥dereket haszn√°l a zene rekonstru√°l√°s√°nak megtanul√°s√°ra. V√©gezzen n√©h√°ny [k√≠s√©rletet](https://colab.research.google.com/github/magenta/magenta-demos/blob/master/colab-notebooks/Multitrack_MusicVAE.ipynb) ezzel a k√∂nyvt√°rral, hogy l√°ssa, mit tud l√©trehozni.

## [El≈ëad√°s ut√°ni kv√≠z](https://ff-quizzes.netlify.app/en/ai/quiz/16)

## √Åttekint√©s √©s √∂n√°ll√≥ tanul√°s

Tov√°bbi inform√°ci√≥k√©rt olvass az autoenk√≥derekr≈ël az al√°bbi forr√°sokban:

* [Autoenk√≥derek √©p√≠t√©se Kerasban](https://blog.keras.io/building-autoencoders-in-keras.html)
* [Blogbejegyz√©s a NeuroHive-on](https://neurohive.io/ru/osnovy-data-science/variacionnyj-avtojenkoder-vae/)
* [Vari√°ci√≥s autoenk√≥derek magyar√°zata](https://kvfrans.com/variational-autoencoders-explained/)
* [Felt√©teles vari√°ci√≥s autoenk√≥derek](https://ijdykeman.github.io/ml/2016/12/21/cvae.html)

## Feladat

A [TensorFlow-t haszn√°l√≥ notebook](AutoencodersTF.ipynb) v√©g√©n tal√°lhat√≥ egy "feladat" - haszn√°ld ezt a h√°zi feladatk√©nt.

---

