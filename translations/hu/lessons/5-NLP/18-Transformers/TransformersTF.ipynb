{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figyelemmechanizmusok és transzformerek\n",
    "\n",
    "A rekurens hálózatok egyik fő hátránya, hogy egy szekvencia minden szava azonos mértékben hat a végeredményre. Ez az alapértelmezett LSTM kódoló-dekódoló modellek esetében nem optimális teljesítményt eredményez szekvencia-szekvencia feladatoknál, például a néventitás-felismerésnél vagy a gépi fordításnál. Valójában az input szekvencia bizonyos szavai gyakran nagyobb hatással vannak a kimeneti szekvenciára, mint mások.\n",
    "\n",
    "Vegyünk például egy szekvencia-szekvencia modellt, mint a gépi fordítás. Ez két rekurens hálózattal valósul meg, ahol az egyik hálózat (**kódoló**) az input szekvenciát egy rejtett állapotba sűríti, míg a másik (**dekódoló**) ezt a rejtett állapotot bontja ki a fordított eredményre. Ennek a megközelítésnek az a problémája, hogy a hálózat végső állapota nehezen tudja megjegyezni a mondat elejét, ami gyenge modellminőséget eredményez hosszú mondatok esetén.\n",
    "\n",
    "A **figyelemmechanizmusok** lehetőséget adnak arra, hogy súlyozzuk az egyes input vektorok kontextuális hatását az RNN kimeneti előrejelzéseire. Ez úgy valósul meg, hogy rövidítéseket hozunk létre az input RNN köztes állapotai és a kimeneti RNN között. Ily módon, amikor a $y_t$ kimeneti szimbólumot generáljuk, figyelembe vesszük az összes input rejtett állapotot $h_i$, különböző súlykoefficiensekkel $\\alpha_{t,i}$. \n",
    "\n",
    "![Kép egy kódoló/dekódoló modellről additív figyelemréteggel](../../../../../translated_images/hu/encoder-decoder-attention.7a726296894fb567.webp)\n",
    "*A kódoló-dekódoló modell additív figyelemmechanizmussal [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) alapján, idézve [ebből a blogbejegyzésből](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)*\n",
    "\n",
    "A figyelem mátrix $\\{\\alpha_{i,j}\\}$ azt mutatja, hogy az egyes input szavak milyen mértékben járulnak hozzá egy adott szó generálásához a kimeneti szekvenciában. Az alábbiakban egy ilyen mátrix példáját láthatjuk:\n",
    "\n",
    "![Kép egy mintaillesztésről, amelyet az RNNsearch-50 talált, Bahdanau - arviz.org](../../../../../translated_images/hu/bahdanau-fig3.09ba2d37f202a6af.webp)\n",
    "\n",
    "*Ábra [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) (3. ábra) alapján*\n",
    "\n",
    "A figyelemmechanizmusok felelősek a természetes nyelvfeldolgozás jelenlegi vagy közel jelenlegi csúcsteljesítményéért. A figyelem hozzáadása azonban jelentősen növeli a modell paramétereinek számát, ami méretezési problémákat okozott az RNN-eknél. Az RNN-ek méretezésének egyik kulcsfontosságú korlátja, hogy a modellek rekurzív jellege megnehezíti az edzés batch-elését és párhuzamosítását. Egy RNN-ben a szekvencia minden elemét sorrendben kell feldolgozni, ami azt jelenti, hogy nem lehet könnyen párhuzamosítani.\n",
    "\n",
    "A figyelemmechanizmusok alkalmazása és ez a korlát vezettek a ma ismert és használt csúcsteljesítményű transzformer modellek létrejöttéhez, mint például a BERT vagy az OpenGPT3.\n",
    "\n",
    "## Transzformer modellek\n",
    "\n",
    "Ahelyett, hogy az előző előrejelzések kontextusát továbbítanák a következő értékelési lépésbe, a **transzformer modellek** **pozíciós kódolásokat** és **figyelmet** használnak, hogy megragadják az adott input kontextusát egy megadott szövegablakon belül. Az alábbi kép bemutatja, hogyan képesek a pozíciós kódolások és a figyelem megragadni a kontextust egy adott ablakon belül.\n",
    "\n",
    "![Animált GIF, amely bemutatja, hogyan történik az értékelés a transzformer modellekben.](../../../../../lessons/5-NLP/18-Transformers/images/transformer-animated-explanation.gif) \n",
    "\n",
    "Mivel minden input pozíciót függetlenül térképeznek a kimeneti pozíciókhoz, a transzformerek jobban párhuzamosíthatók, mint az RNN-ek, ami lehetővé teszi sokkal nagyobb és kifejezőbb nyelvi modellek létrehozását. Minden figyelemfej különböző kapcsolatok megtanulására használható a szavak között, ami javítja a természetes nyelvfeldolgozási feladatok eredményét.\n",
    "\n",
    "## Egyszerű transzformer modell építése\n",
    "\n",
    "A Keras nem tartalmaz beépített transzformer réteget, de saját magunk is felépíthetjük. Mint korábban, most is az AG News adathalmaz szövegklasszifikációjára fogunk összpontosítani, de érdemes megjegyezni, hogy a transzformer modellek a legjobb eredményeket a nehezebb NLP feladatoknál mutatják.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "\n",
    "ds_train, ds_test = tfds.load('ag_news_subset').values()\n",
    "\n",
    "def extract_text(x):\n",
    "    return x['title']+' '+x['description']\n",
    "\n",
    "def tupelize(x):\n",
    "    return (extract_text(x),x['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Az új rétegeknek a Kerasban alosztályozniuk kell a `Layer` osztályt, és implementálniuk kell a `call` metódust. Kezdjük a **Positional Embedding** réteggel. [Az alábbi kódot az hivatalos Keras dokumentációból](https://keras.io/examples/nlp/text_classification_with_transformer/) fogjuk használni. Feltételezzük, hogy az összes bemeneti szekvenciát `maxlen` hosszúságúra töltjük fel.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(keras.layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = keras.layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "        self.maxlen = maxlen\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = self.maxlen\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x+positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ez a réteg két `Embedding` rétegből áll: az egyik a tokenek beágyazására szolgál (ahogy azt korábban megbeszéltük), a másik pedig a tokenek pozícióinak beágyazására. A tokenek pozícióit a 0-tól `maxlen`-ig terjedő természetes számok sorozataként hozzuk létre a `tf.range` segítségével, majd ezt átadjuk a beágyazási rétegnek. A két eredményül kapott beágyazási vektort összeadjuk, így létrejön a bemenet pozícióval beágyazott reprezentációja, amelynek alakja `maxlen`$\\times$`embed_dim`.\n",
    "\n",
    "Most implementáljuk a transformer blokkot. Ez a korábban definiált beágyazási réteg kimenetét fogja használni:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim, name='attn')\n",
    "        self.ffn = keras.Sequential(\n",
    "            [keras.layers.Dense(ff_dim, activation=\"relu\"), keras.layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = keras.layers.Dropout(rate)\n",
    "        self.dropout2 = keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most már készen állunk arra, hogy meghatározzuk a teljes transformer modellt:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "text_vectorization (TextVect (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "token_and_position_embedding (None, 256, 32)           648192    \n",
      "_________________________________________________________________\n",
      "transformer_block (Transform (None, 256, 32)           10656     \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 20)                660       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 84        \n",
      "=================================================================\n",
      "Total params: 659,592\n",
      "Trainable params: 659,592\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 32  # Embedding size for each token\n",
    "num_heads = 2  # Number of attention heads\n",
    "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
    "maxlen = 256\n",
    "vocab_size = 20000\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.experimental.preprocessing.TextVectorization(max_tokens=vocab_size,output_sequence_length=maxlen, input_shape=(1,)),\n",
    "    TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim),\n",
    "    TransformerBlock(embed_dim, num_heads, ff_dim),\n",
    "    keras.layers.GlobalAveragePooling1D(),\n",
    "    keras.layers.Dropout(0.1),\n",
    "    keras.layers.Dense(20, activation=\"relu\"),\n",
    "    keras.layers.Dropout(0.1),\n",
    "    keras.layers.Dense(4, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokenizer\n",
      "938/938 [==============================] - 45s 39ms/step - loss: 0.4978 - acc: 0.8068 - val_loss: 0.2808 - val_acc: 0.9124\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9c2427a0d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Training tokenizer')\n",
    "model.layers[0].adapt(ds_train.map(extract_text))\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'], optimizer='adam')\n",
    "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT Transformer Modellek\n",
    "\n",
    "A **BERT** (Bidirectional Encoder Representations from Transformers) egy nagyon nagy, többrétegű transformer hálózat, amely *BERT-base* esetén 12 rétegből, míg *BERT-large* esetén 24 rétegből áll. A modellt először egy nagy szövegkorpuszra (WikiPedia + könyvek) tanítják be felügyelet nélküli tanulással (maszkolt szavak előrejelzése egy mondatban). Az előképzés során a modell jelentős nyelvi megértést sajátít el, amelyet később más adathalmazokkal finomhangolással lehet hasznosítani. Ezt a folyamatot **transzfer tanulásnak** nevezzük.\n",
    "\n",
    "![kép forrása: http://jalammar.github.io/illustrated-bert/](../../../../../translated_images/hu/jalammarBERT-language-modeling-masked-lm.34f113ea5fec4362.webp)\n",
    "\n",
    "Számos Transformer architektúra létezik, például BERT, DistilBERT, BigBird, OpenGPT3 és még sok más, amelyek finomhangolhatók.\n",
    "\n",
    "Nézzük meg, hogyan használhatunk egy előre betanított BERT modellt hagyományos szekvenciaosztályozási problémák megoldására. Az ötletet és némi kódot az [hivatalos dokumentációból](https://www.tensorflow.org/text/tutorials/classify_text_with_bert) vesszük kölcsön.\n",
    "\n",
    "Az előre betanított modellek betöltéséhez a **Tensorflow hubot** fogjuk használni. Először töltsük be a BERT-specifikus vektorizálót:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_41180/4216669875.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow_text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow_hub\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mhub\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhub\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mKerasLayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_text'"
     ]
    }
   ],
   "source": [
    "import tensorflow_text \n",
    "import tensorflow_hub as hub\n",
    "vectorizer = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_type_ids': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       " array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "       dtype=int32)>,\n",
       " 'input_word_ids': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       " array([[  101,  1045,  2293, 19081,   102,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]], dtype=int32)>,\n",
       " 'input_mask': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       " array([[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "       dtype=int32)>}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer(['I love transformers'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fontos, hogy ugyanazt a vektorizálót használd, amelyet az eredeti hálózat betanításához használtak. A BERT vektorizáló három komponenst ad vissza:\n",
    "* `input_word_ids`, amely az input mondat token számainak sorozata\n",
    "* `input_mask`, amely megmutatja, hogy a sorozat mely része tartalmaz tényleges bemenetet, és melyik a kitöltés. Ez hasonló a `Masking` réteg által előállított maszkhoz\n",
    "* `input_type_ids`, amelyet nyelvi modellezési feladatokhoz használnak, és lehetővé teszi két bemeneti mondat megadását egy sorozatban.\n",
    "\n",
    "Ezután példányosíthatjuk a BERT jellemzők kinyerőjét:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = hub.KerasLayer('https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pooled_output -> (1, 128)\n",
      "encoder_outputs -> 4\n",
      "sequence_output -> (1, 128, 128)\n",
      "default -> (1, 128)\n"
     ]
    }
   ],
   "source": [
    "z = bert(vectorizer(['I love transformers']))\n",
    "for i,x in z.items():\n",
    "    print(f\"{i} -> { len(x) if isinstance(x, list) else x.shape }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tehát a BERT réteg számos hasznos eredményt ad vissza:\n",
    "* `pooled_output` az összes token átlagolásának eredménye a szekvenciában. Ezt tekinthetjük az egész hálózat intelligens szemantikai beágyazásának. Ez egyenértékű a korábbi modellünkben használt `GlobalAveragePooling1D` réteg kimenetével.\n",
    "* `sequence_output` az utolsó transformer réteg kimenete (megfelel a fenti modellünkben található `TransformerBlock` kimenetének).\n",
    "* `encoder_outputs` az összes transformer réteg kimenete. Mivel egy 4-rétegű BERT modellt töltöttünk be (ahogy valószínűleg a névből is sejthető, amely tartalmazza a `4_H` jelölést), ez 4 tenzort tartalmaz. Az utolsó ugyanaz, mint a `sequence_output`.\n",
    "\n",
    "Most definiáljuk az elejétől a végéig tartó osztályozó modellt. *Funkcionális modelldefiníciót* fogunk használni, ahol meghatározzuk a modell bemenetét, majd egy sor kifejezést adunk meg a kimenet kiszámításához. A BERT modell súlyait nem tesszük taníthatóvá, és csak a végső osztályozót fogjuk tanítani:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer (KerasLayer)        {'input_type_ids': ( 0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer_1 (KerasLayer)      {'pooled_output': (N 4782465     keras_layer[0][0]                \n",
      "                                                                 keras_layer[0][1]                \n",
      "                                                                 keras_layer[0][2]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 128)          0           keras_layer_1[0][5]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 4)            516         dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 4,782,981\n",
      "Trainable params: 516\n",
      "Non-trainable params: 4,782,465\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inp = keras.Input(shape=(),dtype=tf.string)\n",
    "x = vectorizer(inp)\n",
    "x = bert(x)\n",
    "x = keras.layers.Dropout(0.1)(x['pooled_output'])\n",
    "out = keras.layers.Dense(4,activation='softmax')(x)\n",
    "model = keras.models.Model(inp,out)\n",
    "bert.trainable = False\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 528s 559ms/step - loss: 0.8056 - acc: 0.6983 - val_loss: 0.5953 - val_acc: 0.7888\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9bb1e36d00>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'], optimizer='adam')\n",
    "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Annak ellenére, hogy kevés a tanítható paraméter, a folyamat meglehetősen lassú, mivel a BERT jellemzők kinyerése számításigényes. Úgy tűnik, nem sikerült elfogadható pontosságot elérni, akár a nem megfelelő tanítás, akár a modellparaméterek hiánya miatt.\n",
    "\n",
    "Próbáljuk meg feloldani a BERT súlyok zárolását, és azt is betanítani. Ehhez nagyon kicsi tanulási rátára van szükség, valamint egy körültekintőbb tanítási stratégiára **warmup**-pal, az **AdamW** optimalizáló használatával. Az optimalizáló létrehozásához a `tf-models-official` csomagot fogjuk használni:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer (KerasLayer)        {'input_type_ids': ( 0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer_1 (KerasLayer)      {'pooled_output': (N 4782465     keras_layer[0][0]                \n",
      "                                                                 keras_layer[0][1]                \n",
      "                                                                 keras_layer[0][2]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 128)          0           keras_layer_1[0][5]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 4)            516         dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 4,782,981\n",
      "Trainable params: 4,782,980\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n",
      "938/938 [==============================] - 629s 664ms/step - loss: 0.6344 - acc: 0.7658 - val_loss: 0.4876 - val_acc: 0.8247\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9bb0bd0070>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from official.nlp import optimization \n",
    "bert.trainable=True\n",
    "model.summary()\n",
    "epochs = 3\n",
    "opt = optimization.create_optimizer(\n",
    "    init_lr=3e-5,\n",
    "    num_train_steps=epochs*len(ds_train),\n",
    "    num_warmup_steps=0.1*epochs*len(ds_train),\n",
    "    optimizer_type='adamw')\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'], optimizer=opt)\n",
    "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amint láthatod, a tanítás meglehetősen lassan halad – de érdemes lehet kísérletezni, és néhány epochon (5-10) keresztül tanítani a modellt, hogy összehasonlítsd az eredményeket az általunk korábban alkalmazott megközelítésekkel.\n",
    "\n",
    "## Huggingface Transformers könyvtár\n",
    "\n",
    "Egy másik nagyon elterjedt (és valamivel egyszerűbb) módja a Transformer modellek használatának a [HuggingFace csomag](https://github.com/huggingface/), amely egyszerű építőelemeket biztosít különböző NLP feladatokhoz. Ez elérhető mind Tensorflow, mind PyTorch számára, amelyek szintén nagyon népszerű neurális hálózati keretrendszerek. \n",
    "\n",
    "> **Note**: Ha nem érdekel, hogyan működik a Transformers könyvtár, akkor átugorhatod a notebook végét, mert nem fogsz lényegesen eltérő dolgokat látni ahhoz képest, amit korábban csináltunk. Ugyanazokat a lépéseket fogjuk ismételni a BERT modell tanításához, csak egy másik könyvtárat és lényegesen nagyobb modellt használva. Ezért a folyamat meglehetősen hosszú tanítást igényel, így lehet, hogy csak a kódot szeretnéd átnézni.\n",
    "\n",
    "Nézzük meg, hogyan oldható meg a problémánk a [Huggingface Transformers](http://huggingface.co) segítségével.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Először ki kell választanunk a modellt, amelyet használni fogunk. A beépített modellek mellett a Huggingface rendelkezik egy [online modell-adattárral](https://huggingface.co/models), ahol a közösség által előre betanított modellek széles választékát találhatod. Ezeket a modelleket mindössze a modell nevének megadásával lehet betölteni és használni. Az összes szükséges bináris fájl automatikusan letöltésre kerül.\n",
    "\n",
    "Bizonyos esetekben szükséged lehet arra, hogy saját modelleket tölts be. Ilyenkor megadhatod azt a könyvtárat, amely tartalmazza az összes releváns fájlt, beleértve a tokenizáló paramétereit, a `config.json` fájlt a modell paramétereivel, bináris súlyokat stb.\n",
    "\n",
    "A modell nevéből létrehozhatjuk mind a modellt, mind a tokenizálót. Kezdjük a tokenizálóval:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "# To load the model from Internet repository using model name. \n",
    "# Use this if you are running from your own copy of the notebooks\n",
    "bert_model = 'bert-base-uncased' \n",
    "\n",
    "# To load the model from the directory on disk. Use this for Microsoft Learn module, because we have\n",
    "# prepared all required files for you.\n",
    "#bert_model = './bert'\n",
    "\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(bert_model)\n",
    "\n",
    "MAX_SEQ_LEN = 128\n",
    "PAD_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "UNK_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `tokenizer` objektum tartalmazza az `encode` függvényt, amely közvetlenül használható szöveg kódolására:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 23435, 12314, 2003, 1037, 2307, 7705, 2005, 17953, 2361, 102]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('Tensorflow is a great framework for NLP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Használhatjuk a tokenizálót arra, hogy egy sorozatot olyan módon kódoljunk, amely alkalmas a modellnek való átadásra, azaz tartalmazza a `token_ids`, `input_mask` mezőket stb. Megadhatjuk azt is, hogy Tensorflow tenzorokat szeretnénk, ha megadjuk a `return_tensors='tf'` argumentumot:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[ 101, 7592, 1010, 2045,  102]], dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[0, 0, 0, 0, 0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[1, 1, 1, 1, 1]], dtype=int32)>}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(['Hello, there'],return_tensors='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ebben az esetben egy előre betanított BERT modellt fogunk használni, amelynek neve `bert-base-uncased`. Az *uncased* azt jelzi, hogy a modell nem érzékeny a kis- és nagybetűkre.\n",
    "\n",
    "A modell betanításakor tokenizált szekvenciát kell bemenetként megadnunk, ezért egy adatfeldolgozási folyamatot fogunk kialakítani. Mivel a `tokenizer.encode` egy Python függvény, ugyanazt a megközelítést fogjuk alkalmazni, mint az előző egységben, azaz a `py_function` hívásával:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(x):\n",
    "    return tokenizer.encode(x.numpy().decode('utf-8'),return_tensors='tf',padding='max_length',max_length=MAX_SEQ_LEN,truncation=True)[0]\n",
    "\n",
    "def process_fn(x):\n",
    "    s = x['title']+' '+x['description']\n",
    "    e = tf.py_function(process,inp=[s],Tout=(tf.int32))\n",
    "    e.set_shape(MAX_SEQ_LEN)\n",
    "    return e,x['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most már betölthetjük a tényleges modellt a `BertForSequenceClassification` csomag használatával. Ez biztosítja, hogy a modellünk már rendelkezik a szükséges architektúrával az osztályozáshoz, beleértve a végső osztályozót is. Figyelmeztető üzenetet fogsz látni, amely azt jelzi, hogy a végső osztályozó súlyai nincsenek inicializálva, és a modell előzetes tanítást igényel - ez teljesen rendben van, mivel pontosan ezt fogjuk tenni!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = transformers.TFBertForSequenceClassification.from_pretrained(bert_model,num_labels=4,output_attentions=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  109482240 \n",
      "_________________________________________________________________\n",
      "dropout_75 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  3076      \n",
      "=================================================================\n",
      "Total params: 109,485,316\n",
      "Trainable params: 109,485,316\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amint látható a `summary()` alapján, a modell közel 110 millió paramétert tartalmaz! Feltételezhetően, ha egyszerű osztályozási feladatot szeretnénk egy viszonylag kis adathalmazon, nem akarjuk betanítani a BERT alapréteget:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  109482240 \n",
      "_________________________________________________________________\n",
      "dropout_75 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  3076      \n",
      "=================================================================\n",
      "Total params: 109,485,316\n",
      "Trainable params: 3,076\n",
      "Non-trainable params: 109,482,240\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.layers[0].trainable = False\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most már készen állunk az edzés megkezdésére!\n",
    "\n",
    "> **Megjegyzés**: A teljes méretű BERT modell betanítása rendkívül időigényes lehet! Ezért csak az első 32 batch-re fogjuk betanítani. Ez csupán arra szolgál, hogy bemutassuk, hogyan van beállítva a modell betanítása. Ha szeretnéd kipróbálni a teljes méretű betanítást, egyszerűen távolítsd el a `steps_per_epoch` és `validation_steps` paramétereket, és készülj fel a várakozásra!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 142s 4s/step - loss: 1.3896 - acc: 0.2500 - val_loss: 1.3863 - val_acc: 0.2480\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f1d40a4b6a0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile('adam','sparse_categorical_crossentropy',['acc'])\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "model.fit(ds_train.map(process_fn).batch(32),validation_data=ds_test.map(process_fn).batch(32),steps_per_epoch=32,validation_steps=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ha növeled az iterációk számát, elég sokáig vársz, és több epochon keresztül tanítasz, akkor várhatóan a BERT osztályozás adja a legjobb pontosságot! Ez azért van, mert a BERT már eleve elég jól érti a nyelv szerkezetét, és nekünk csak a végső osztályozót kell finomhangolnunk. Azonban, mivel a BERT egy nagy modell, az egész tanítási folyamat sok időt vesz igénybe, és komoly számítási kapacitást igényel! (GPU, és lehetőleg több is).\n",
    "\n",
    "> **Megjegyzés:** Példánkban az egyik legkisebb előre betanított BERT modellt használtuk. Vannak nagyobb modellek is, amelyek valószínűleg jobb eredményeket hoznak.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Legfontosabb tanulság\n",
    "\n",
    "Ebben az egységben megismerkedtünk a legújabb **transformer** alapú modellarchitektúrákkal. Alkalmaztuk őket szövegklasszifikációs feladatunkra, de hasonlóan a BERT modellek használhatók entitáskinyerésre, kérdés-válaszolásra és más NLP feladatokra is.\n",
    "\n",
    "A transformer modellek jelenleg az NLP csúcstechnológiáját képviselik, és a legtöbb esetben ezekkel érdemes először kísérletezni, amikor egyedi NLP megoldásokat valósítasz meg. Ugyanakkor rendkívül fontos megérteni az ebben a modulban tárgyalt visszatérő neurális hálózatok alapelveit, ha fejlettebb neurális modelleket szeretnél építeni.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Felelősség kizárása**:  \nEz a dokumentum az AI fordítási szolgáltatás [Co-op Translator](https://github.com/Azure/co-op-translator) segítségével lett lefordítva. Bár törekszünk a pontosságra, kérjük, vegye figyelembe, hogy az automatikus fordítások hibákat vagy pontatlanságokat tartalmazhatnak. Az eredeti dokumentum az eredeti nyelvén tekintendő hiteles forrásnak. Fontos információk esetén javasolt professzionális emberi fordítást igénybe venni. Nem vállalunk felelősséget semmilyen félreértésért vagy téves értelmezésért, amely a fordítás használatából eredhet.\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0cb620c6d4b9f7a635928804c26cf22403d89d98d79684e4529119355ee6d5a5"
  },
  "kernelspec": {
   "display_name": "py38_tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "coopTranslator": {
   "original_hash": "ab59c532409774988ab875f2260e8e53",
   "translation_date": "2025-08-29T16:04:54+00:00",
   "source_file": "lessons/5-NLP/18-Transformers/TransformersTF.ipynb",
   "language_code": "hu"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}