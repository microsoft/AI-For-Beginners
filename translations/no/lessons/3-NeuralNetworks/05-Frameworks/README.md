# Rammeverk for nevrale nettverk

Som vi allerede har l√¶rt, for √• kunne trene nevrale nettverk effektivt m√• vi gj√∏re to ting:

* Operere p√• tensorer, f.eks. multiplisere, legge til og beregne funksjoner som sigmoid eller softmax
* Beregne gradienter av alle uttrykk for √• utf√∏re gradientnedstigningsoptimalisering

## [Quiz f√∏r forelesning](https://ff-quizzes.netlify.app/en/ai/quiz/9)

Mens `numpy`-biblioteket kan utf√∏re den f√∏rste delen, trenger vi en mekanisme for √• beregne gradienter. I [v√•rt rammeverk](../04-OwnFramework/OwnFramework.ipynb) som vi utviklet i forrige seksjon, m√•tte vi manuelt programmere alle deriverte funksjoner i `backward`-metoden, som utf√∏rer tilbakepropagering. Ideelt sett b√∏r et rammeverk gi oss muligheten til √• beregne gradienter av *ethvert uttrykk* vi kan definere.

En annen viktig ting er √• kunne utf√∏re beregninger p√• GPU, eller andre spesialiserte beregningsenheter, som [TPU](https://en.wikipedia.org/wiki/Tensor_Processing_Unit). Trening av dype nevrale nettverk krever *mange* beregninger, og det er sv√¶rt viktig √• kunne parallellisere disse beregningene p√• GPU-er.

> ‚úÖ Begrepet 'parallellisere' betyr √• fordele beregningene over flere enheter.

For √∏yeblikket er de to mest popul√¶re rammeverkene for nevrale nettverk: [TensorFlow](http://TensorFlow.org) og [PyTorch](https://pytorch.org/). Begge tilbyr et lavniv√•-API for √• operere med tensorer p√• b√•de CPU og GPU. I tillegg til lavniv√•-API finnes det ogs√• h√∏yniv√•-API, kalt [Keras](https://keras.io/) og [PyTorch Lightning](https://pytorchlightning.ai/) henholdsvis.

Low-Level API | [TensorFlow](http://TensorFlow.org) | [PyTorch](https://pytorch.org/)
--------------|-------------------------------------|--------------------------------
High-level API| [Keras](https://keras.io/) | [PyTorch Lightning](https://pytorchlightning.ai/)

**Lavniv√•-API** i begge rammeverk lar deg bygge s√•kalte **beregningsgrafer**. Denne grafen definerer hvordan man beregner utdata (vanligvis tapsfunksjonen) med gitte inngangsparametere, og kan sendes til beregning p√• GPU hvis tilgjengelig. Det finnes funksjoner for √• differensiere denne beregningsgrafen og beregne gradienter, som deretter kan brukes til √• optimalisere modellparametere.

**H√∏yniv√•-API** betrakter nevrale nettverk som en **sekvens av lag**, og gj√∏r det mye enklere √• konstruere de fleste nevrale nettverk. √Ö trene modellen krever vanligvis at man forbereder dataene og deretter kaller en `fit`-funksjon for √• utf√∏re jobben.

H√∏yniv√•-API lar deg konstruere typiske nevrale nettverk veldig raskt uten √• bekymre deg for mange detaljer. Samtidig gir lavniv√•-API mye mer kontroll over treningsprosessen, og brukes derfor ofte i forskning n√•r man arbeider med nye arkitekturer for nevrale nettverk.

Det er ogs√• viktig √• forst√• at du kan bruke begge API-ene sammen, f.eks. kan du utvikle din egen arkitektur for nettverkslag ved hjelp av lavniv√•-API og deretter bruke den i et st√∏rre nettverk konstruert og trent med h√∏yniv√•-API. Eller du kan definere et nettverk ved hjelp av h√∏yniv√•-API som en sekvens av lag, og deretter bruke din egen lavniv√• treningssl√∏yfe for √• utf√∏re optimalisering. Begge API-ene bruker de samme grunnleggende konseptene og er designet for √• fungere godt sammen.

## L√¶ring

I dette kurset tilbyr vi det meste av innholdet b√•de for PyTorch og TensorFlow. Du kan velge ditt foretrukne rammeverk og kun g√• gjennom de tilsvarende notatb√∏kene. Hvis du ikke er sikker p√• hvilket rammeverk du skal velge, les noen diskusjoner p√• nettet om **PyTorch vs. TensorFlow**. Du kan ogs√• ta en titt p√• begge rammeverkene for √• f√• bedre forst√•else.

Der det er mulig, vil vi bruke h√∏yniv√•-API for enkelhetens skyld. Imidlertid mener vi det er viktig √• forst√• hvordan nevrale nettverk fungerer fra grunnen av, derfor starter vi med √• arbeide med lavniv√•-API og tensorer. Men hvis du √∏nsker √• komme i gang raskt og ikke vil bruke mye tid p√• √• l√¶re disse detaljene, kan du hoppe over dem og g√• direkte til notatb√∏kene for h√∏yniv√•-API.

## ‚úçÔ∏è √òvelser: Rammeverk

Fortsett l√¶ringen i f√∏lgende notatb√∏ker:

Low-Level API | [TensorFlow+Keras Notebook](IntroKerasTF.ipynb) | [PyTorch](IntroPyTorch.ipynb)
--------------|-------------------------------------|--------------------------------
High-level API| [Keras](IntroKeras.ipynb) | *PyTorch Lightning*

Etter √• ha mestret rammeverkene, la oss oppsummere begrepet overtilpasning.

# Overtilpasning

Overtilpasning er et ekstremt viktig konsept innen maskinl√¶ring, og det er veldig viktig √• forst√• det riktig!

Tenk p√• f√∏lgende problem med √• tiln√¶rme 5 punkter (representert med `x` p√• grafene nedenfor):

![linear](../../../../../translated_images/no/overfit1.f24b71c6f652e59e.webp) | ![overfit](../../../../../translated_images/no/overfit2.131f5800ae10ca5e.webp)
-------------------------|--------------------------
**Line√¶r modell, 2 parametere** | **Ikke-line√¶r modell, 7 parametere**
Treningsfeil = 5.3 | Treningsfeil = 0
Valideringsfeil = 5.1 | Valideringsfeil = 20

* Til venstre ser vi en god rett linje-tiln√¶rming. Fordi antall parametere er passende, f√•r modellen riktig forst√•else av punktfordelingen.
* Til h√∏yre er modellen for kraftig. Fordi vi bare har 5 punkter og modellen har 7 parametere, kan den justere seg slik at den passer gjennom alle punktene, noe som gj√∏r treningsfeilen til 0. Dette hindrer imidlertid modellen i √• forst√• det korrekte m√∏nsteret bak dataene, og dermed er valideringsfeilen veldig h√∏y.

Det er veldig viktig √• finne en riktig balanse mellom modellens kompleksitet (antall parametere) og antall treningspr√∏ver.

## Hvorfor oppst√•r overtilpasning

  * Ikke nok treningsdata
  * For kraftig modell
  * For mye st√∏y i inngangsdataene

## Hvordan oppdage overtilpasning

Som du kan se fra grafen ovenfor, kan overtilpasning oppdages ved en veldig lav treningsfeil og en h√∏y valideringsfeil. Normalt under trening vil vi se b√•de trenings- og valideringsfeil begynne √• avta, og deretter p√• et tidspunkt kan valideringsfeilen slutte √• avta og begynne √• stige. Dette vil v√¶re et tegn p√• overtilpasning og en indikator p√• at vi sannsynligvis b√∏r stoppe treningen p√• dette punktet (eller i det minste ta et √∏yeblikksbilde av modellen).

![overfitting](../../../../../translated_images/no/Overfitting.408ad91cd90b4371.webp)

## Hvordan forhindre overtilpasning

Hvis du ser at overtilpasning oppst√•r, kan du gj√∏re f√∏lgende:

 * √òke mengden treningsdata
 * Redusere modellens kompleksitet
 * Bruke noen [regulariseringsteknikker](../../4-ComputerVision/08-TransferLearning/TrainingTricks.md), som [Dropout](../../4-ComputerVision/08-TransferLearning/TrainingTricks.md#Dropout), som vi vil se n√¶rmere p√• senere.

## Overtilpasning og Bias-Variance Tradeoff

Overtilpasning er faktisk et tilfelle av et mer generelt problem innen statistikk kalt [Bias-Variance Tradeoff](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff). Hvis vi vurderer de mulige kildene til feil i modellen v√•r, kan vi se to typer feil:

* **Bias-feil** skyldes at algoritmen v√•r ikke klarer √• fange forholdet mellom treningsdataene korrekt. Dette kan skyldes at modellen v√•r ikke er kraftig nok (**undertilpasning**).
* **Variansfeil**, som skyldes at modellen tiln√¶rmer seg st√∏y i inngangsdataene i stedet for meningsfulle relasjoner (**overtilpasning**).

Under trening avtar bias-feilen (ettersom modellen v√•r l√¶rer √• tiln√¶rme dataene), og variansfeilen √∏ker. Det er viktig √• stoppe treningen - enten manuelt (n√•r vi oppdager overtilpasning) eller automatisk (ved √• introdusere regularisering) - for √• forhindre overtilpasning.

## Konklusjon

I denne leksjonen l√¶rte du om forskjellene mellom de ulike API-ene for de to mest popul√¶re AI-rammeverkene, TensorFlow og PyTorch. I tillegg l√¶rte du om et veldig viktig tema, overtilpasning.

## üöÄ Utfordring

I de medf√∏lgende notatb√∏kene finner du 'oppgaver' nederst; arbeid gjennom notatb√∏kene og fullf√∏r oppgavene.

## [Quiz etter forelesning](https://ff-quizzes.netlify.app/en/ai/quiz/10)

## Gjennomgang og selvstudium

Gj√∏r litt research p√• f√∏lgende temaer:

- TensorFlow
- PyTorch
- Overtilpasning

Still deg selv f√∏lgende sp√∏rsm√•l:

- Hva er forskjellen mellom TensorFlow og PyTorch?
- Hva er forskjellen mellom overtilpasning og undertilpasning?

## [Oppgave](lab/README.md)

I denne labben blir du bedt om √• l√∏se to klassifiseringsproblemer ved hjelp av enkelt- og flerlags fullt tilkoblede nettverk ved bruk av PyTorch eller TensorFlow.

* [Instruksjoner](lab/README.md)
* [Notatbok](lab/LabFrameworks.ipynb)

---

