# Autoencodere

N√•r vi trener CNN-er, er en av utfordringene at vi trenger mye merket data. N√•r det gjelder bildeklassifisering, m√• vi dele bilder inn i ulike klasser, noe som krever manuell innsats.

## [Quiz f√∏r forelesning](https://ff-quizzes.netlify.app/en/ai/quiz/17)

Vi kan imidlertid √∏nske √• bruke r√• (umerkede) data for √• trene CNN-funksjonsekstraktorer, noe som kalles **selv-supervisert l√¶ring**. I stedet for etiketter bruker vi treningsbilder b√•de som nettverksinput og output. Hovedideen med **autoencoder** er at vi har et **encoder-nettverk** som konverterer input-bildet til et **latent rom** (vanligvis bare en vektor av mindre st√∏rrelse), og deretter et **decoder-nettverk**, som har som m√•l √• rekonstruere det originale bildet.

> ‚úÖ En [autoencoder](https://wikipedia.org/wiki/Autoencoder) er "en type kunstig nevralt nettverk som brukes til √• l√¶re effektive kodinger av umarkert data."

Siden vi trener en autoencoder til √• fange s√• mye informasjon som mulig fra det originale bildet for n√∏yaktig rekonstruksjon, pr√∏ver nettverket √• finne den beste **embedding** av input-bilder for √• fange meningen.

![AutoEncoder Diagram](../../../../../translated_images/no/autoencoder_schema.5e6fc9ad98a5eb61.webp)

> Bilde fra [Keras-blogg](https://blog.keras.io/building-autoencoders-in-keras.html)

## Scenarier for bruk av autoencodere

Selv om det √• rekonstruere originale bilder ikke virker nyttig i seg selv, finnes det noen scenarier der autoencodere er spesielt nyttige:

* **Redusere dimensjonen p√• bilder for visualisering** eller **trene bildeembeddings**. Vanligvis gir autoencodere bedre resultater enn PCA, fordi de tar hensyn til bildenes romlige natur og hierarkiske funksjoner.
* **Fjerning av st√∏y**, dvs. fjerne st√∏y fra bildet. Fordi st√∏y inneholder mye unyttig informasjon, kan autoencoder ikke passe alt inn i det relativt lille latente rommet, og dermed fanger den bare den viktige delen av bildet. N√•r vi trener st√∏yfjernere, starter vi med originale bilder og bruker bilder med kunstig lagt til st√∏y som input for autoencoder.
* **Superoppl√∏sning**, √∏ke bildekvaliteten. Vi starter med bilder med h√∏y oppl√∏sning og bruker bildet med lavere oppl√∏sning som input for autoencoder.
* **Generative modeller**. N√•r vi har trent autoencoder, kan decoder-delen brukes til √• lage nye objekter basert p√• tilfeldige latente vektorer.

## Variasjonelle autoencodere (VAE)

Tradisjonelle autoencodere reduserer dimensjonen p√• input-data p√• en eller annen m√•te, og finner de viktige funksjonene i input-bildene. Imidlertid gir latente vektorer ofte ikke mye mening. Med andre ord, hvis vi tar MNIST-datasettet som et eksempel, er det ikke lett √• finne ut hvilke sifre som tilsvarer ulike latente vektorer, fordi n√¶rliggende latente vektorer ikke n√∏dvendigvis tilsvarer de samme sifrene.

P√• den annen side, for √• trene *generative* modeller er det bedre √• ha en viss forst√•else av det latente rommet. Denne ideen leder oss til **variational autoencoder** (VAE).

VAE er en autoencoder som l√¶rer √• forutsi *statistisk distribusjon* av de latente parameterne, det s√•kalte **latente distribusjonen**. For eksempel kan vi √∏nske at latente vektorer skal v√¶re normalt fordelt med en viss gjennomsnitt z<sub>mean</sub> og standardavvik z<sub>sigma</sub> (b√•de gjennomsnitt og standardavvik er vektorer med en viss dimensjonalitet d). Encoder i VAE l√¶rer √• forutsi disse parameterne, og deretter tar decoder en tilfeldig vektor fra denne distribusjonen for √• rekonstruere objektet.

Oppsummert:

 * Fra input-vektoren forutsier vi `z_mean` og `z_log_sigma` (i stedet for √• forutsi standardavviket direkte, forutsier vi logaritmen av det)
 * Vi tar en pr√∏vevektor `sample` fra distribusjonen N(z<sub>mean</sub>,exp(z<sub>log\_sigma</sub>))
 * Decoder pr√∏ver √• dekode det originale bildet ved √• bruke `sample` som input-vektor

 <img src="../../../../../translated_images/no/vae.464c465a5b6a9e25.webp" width="50%">

> Bilde fra [denne bloggposten](https://ijdykeman.github.io/ml/2016/12/21/cvae.html) av Isaak Dykeman

Variasjonelle autoencodere bruker en kompleks tapsfunksjon som best√•r av to deler:

* **Rekonstruksjonstap** er tapsfunksjonen som viser hvor n√¶rt et rekonstruert bilde er til m√•let (det kan v√¶re Mean Squared Error, eller MSE). Det er den samme tapsfunksjonen som i vanlige autoencodere.
* **KL-tap**, som sikrer at latente variabeldistribusjoner holder seg n√¶r normalfordelingen. Det er basert p√• begrepet [Kullback-Leibler-divergens](https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained) - en metrikk for √• estimere hvor like to statistiske distribusjoner er.

En viktig fordel med VAE-er er at de lar oss generere nye bilder relativt enkelt, fordi vi vet hvilken distribusjon vi skal ta latente vektorer fra. For eksempel, hvis vi trener VAE med 2D latente vektorer p√• MNIST, kan vi deretter variere komponentene i den latente vektoren for √• f√• ulike sifre:

<img alt="vaemnist" src="../../../../../translated_images/no/vaemnist.cab9e602dc08dc50.webp" width="50%"/>

> Bilde av [Dmitry Soshnikov](http://soshnikov.com)

Legg merke til hvordan bildene glir over i hverandre, ettersom vi begynner √• ta latente vektorer fra ulike deler av det latente parameterrommet. Vi kan ogs√• visualisere dette rommet i 2D:

<img alt="vaemnist cluster" src="../../../../../translated_images/no/vaemnist-diag.694315f775d5d666.webp" width="50%"/> 

> Bilde av [Dmitry Soshnikov](http://soshnikov.com)

## ‚úçÔ∏è √òvelser: Autoencodere

L√¶r mer om autoencodere i disse tilh√∏rende notatb√∏kene:

* [Autoencodere i TensorFlow](AutoencodersTF.ipynb)
* [Autoencodere i PyTorch](AutoEncodersPyTorch.ipynb)

## Egenskaper ved autoencodere

* **Dataspesifikke** - de fungerer bare godt med den typen bilder de er trent p√•. For eksempel, hvis vi trener et superoppl√∏sningsnettverk p√• blomster, vil det ikke fungere godt p√• portretter. Dette er fordi nettverket kan produsere bilder med h√∏yere oppl√∏sning ved √• ta fine detaljer fra funksjoner l√¶rt fra treningsdatasettet.
* **Tapsbaserte** - det rekonstruerte bildet er ikke det samme som det originale bildet. Naturen til tapet er definert av *tapsfunksjonen* som brukes under trening.
* Fungerer p√• **umarkert data**

## [Quiz etter forelesning](https://ff-quizzes.netlify.app/en/ai/quiz/18)

## Konklusjon

I denne leksjonen l√¶rte du om de ulike typene autoencodere som er tilgjengelige for AI-forskeren. Du l√¶rte hvordan du bygger dem, og hvordan du bruker dem til √• rekonstruere bilder. Du l√¶rte ogs√• om VAE og hvordan du bruker det til √• generere nye bilder.

## üöÄ Utfordring

I denne leksjonen l√¶rte du om bruk av autoencodere for bilder. Men de kan ogs√• brukes for musikk! Sjekk ut Magenta-prosjektets [MusicVAE](https://magenta.tensorflow.org/music-vae)-prosjekt, som bruker autoencodere til √• l√¶re √• rekonstruere musikk. Gj√∏r noen [eksperimenter](https://colab.research.google.com/github/magenta/magenta-demos/blob/master/colab-notebooks/Multitrack_MusicVAE.ipynb) med dette biblioteket for √• se hva du kan skape.

## [Quiz etter forelesning](https://ff-quizzes.netlify.app/en/ai/quiz/16)

## Gjennomgang & Selvstudium

For referanse, les mer om autoencodere i disse ressursene:

* [Bygge autoencodere i Keras](https://blog.keras.io/building-autoencoders-in-keras.html)
* [Bloggpost p√• NeuroHive](https://neurohive.io/ru/osnovy-data-science/variacionnyj-avtojenkoder-vae/)
* [Variasjonelle autoencodere forklart](https://kvfrans.com/variational-autoencoders-explained/)
* [Betingede variasjonelle autoencodere](https://ijdykeman.github.io/ml/2016/12/21/cvae.html)

## Oppgave

P√• slutten av [denne notatboken med TensorFlow](AutoencodersTF.ipynb), finner du en 'oppgave' - bruk denne som din oppgave.

---

