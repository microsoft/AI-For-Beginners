# Spr친kmodellering

Semantiske embeddinger, som Word2Vec og GloVe, er faktisk et f칮rste steg mot **spr친kmodellering** - 친 lage modeller som p친 en eller annen m친te *forst친r* (eller *representerer*) spr친kets natur.

## [Quiz f칮r forelesning](https://ff-quizzes.netlify.app/en/ai/quiz/29)

Hovedideen bak spr친kmodellering er 친 trene dem p친 umerkede datasett p친 en usupervisert m친te. Dette er viktig fordi vi har enorme mengder umerket tekst tilgjengelig, mens mengden merket tekst alltid vil v칝re begrenset av innsatsen vi kan legge ned i merking. Oftest kan vi bygge spr친kmodeller som kan **forutsi manglende ord** i teksten, fordi det er enkelt 친 maskere et tilfeldig ord i teksten og bruke det som en treningspr칮ve.

## Trening av embeddinger

I v친re tidligere eksempler brukte vi forh친ndstrente semantiske embeddinger, men det er interessant 친 se hvordan disse embeddingene kan trenes. Det finnes flere mulige ideer som kan brukes:

* **N-Gram** spr친kmodellering, der vi forutsier et token ved 친 se p친 N tidligere token (N-gram).
* **Continuous Bag-of-Words** (CBoW), der vi forutsier det midterste tokenet $W_0$ i en sekvens av token $W_{-N}$, ..., $W_N$.
* **Skip-gram**, der vi forutsier et sett av nabotoken {$W_{-N},\dots, W_{-1}, W_1,\dots, W_N$} fra det midterste tokenet $W_0$.

![bilde fra artikkel om konvertering av ord til vektorer](../../../../../translated_images/no/example-algorithms-for-converting-words-to-vectors.fbe9207a726922f6.webp)

> Bilde fra [denne artikkelen](https://arxiv.org/pdf/1301.3781.pdf)

## 九꽲잺 Eksempelnotatb칮ker: Trening av CBoW-modell

Fortsett l칝ringen i f칮lgende notatb칮ker:

* [Trening av CBoW Word2Vec med TensorFlow](CBoW-TF.ipynb)
* [Trening av CBoW Word2Vec med PyTorch](CBoW-PyTorch.ipynb)

## Konklusjon

I forrige leksjon s친 vi at ordembeddinger fungerer som magi! N친 vet vi at trening av ordembeddinger ikke er en veldig kompleks oppgave, og vi b칮r kunne trene v친re egne ordembeddinger for domenespesifikk tekst hvis n칮dvendig.

## [Quiz etter forelesning](https://ff-quizzes.netlify.app/en/ai/quiz/30)

## Gjennomgang og selvstudium

* [Offisiell PyTorch-veiledning om spr친kmodellering](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html).
* [Offisiell TensorFlow-veiledning om trening av Word2Vec-modell](https://www.TensorFlow.org/tutorials/text/word2vec).
* Bruk av **gensim**-rammeverket for 친 trene de mest brukte embeddingene med noen f친 linjer kode er beskrevet [i denne dokumentasjonen](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html).

## 游 [Oppgave: Tren Skip-Gram-modell](lab/README.md)

I laboratoriet utfordrer vi deg til 친 endre koden fra denne leksjonen for 친 trene en skip-gram-modell i stedet for CBoW. [Les detaljene](lab/README.md)

---

