{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oppmerksomhetsmekanismer og transformatorer\n",
    "\n",
    "En stor ulempe med rekurrente nettverk er at alle ord i en sekvens har samme innvirkning på resultatet. Dette fører til suboptimal ytelse med standard LSTM-encoder-decoder-modeller for sekvens-til-sekvens-oppgaver, som for eksempel navngitt enhetsgjenkjenning og maskinoversettelse. I virkeligheten har spesifikke ord i inngangssekvensen ofte større innvirkning på sekvensielle utganger enn andre.\n",
    "\n",
    "Tenk på en sekvens-til-sekvens-modell, som maskinoversettelse. Den implementeres ved hjelp av to rekurrente nettverk, der ett nettverk (**encoder**) komprimerer inngangssekvensen til en skjult tilstand, og et annet nettverk, **decoder**, ruller ut denne skjulte tilstanden til et oversatt resultat. Problemet med denne tilnærmingen er at den endelige tilstanden til nettverket vil ha vanskeligheter med å huske begynnelsen av en setning, noe som fører til dårlig modellkvalitet for lange setninger.\n",
    "\n",
    "**Oppmerksomhetsmekanismer** gir en måte å vekte den kontekstuelle innvirkningen av hver inngangsvektor på hver utgangsprediksjon av RNN. Dette implementeres ved å lage snarveier mellom mellomliggende tilstander i inngangs-RNN og utgangs-RNN. På denne måten, når vi genererer utgangssymbolet $y_t$, tar vi hensyn til alle skjulte inngangstilstander $h_i$, med forskjellige vektkoeffisienter $\\alpha_{t,i}$.\n",
    "\n",
    "![Bilde som viser en encoder/decoder-modell med et additivt oppmerksomhetslag](../../../../../translated_images/no/encoder-decoder-attention.7a726296894fb567.webp)\n",
    "*Encoder-decoder-modellen med additiv oppmerksomhetsmekanisme i [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf), sitert fra [denne bloggposten](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)*\n",
    "\n",
    "Oppmerksomhetsmatrisen $\\{\\alpha_{i,j}\\}$ representerer graden til hvilken visse inngangsord spiller en rolle i genereringen av et gitt ord i utgangssekvensen. Nedenfor er et eksempel på en slik matrise:\n",
    "\n",
    "![Bilde som viser en eksempeljustering funnet av RNNsearch-50, hentet fra Bahdanau - arviz.org](../../../../../translated_images/no/bahdanau-fig3.09ba2d37f202a6af.webp)\n",
    "\n",
    "*Figur hentet fra [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) (Fig.3)*\n",
    "\n",
    "Oppmerksomhetsmekanismer er ansvarlige for mye av dagens eller nesten dagens toppytelse innen naturlig språkbehandling. Å legge til oppmerksomhet øker imidlertid antall modellparametere betydelig, noe som førte til skaleringsproblemer med RNN-er. En viktig begrensning ved å skalere RNN-er er at modellens rekurrente natur gjør det utfordrende å batch-prosessere og parallellisere trening. I en RNN må hvert element i en sekvens behandles i sekvensiell rekkefølge, noe som betyr at det ikke enkelt kan parallelliseres.\n",
    "\n",
    "Adopsjonen av oppmerksomhetsmekanismer kombinert med denne begrensningen førte til opprettelsen av de nåværende toppmoderne transformator-modellene som vi kjenner og bruker i dag, fra BERT til OpenGPT3.\n",
    "\n",
    "## Transformator-modeller\n",
    "\n",
    "I stedet for å føre konteksten fra hver tidligere prediksjon inn i neste evalueringssteg, bruker **transformator-modeller** **posisjonelle kodinger** og oppmerksomhet for å fange konteksten til en gitt inngang innenfor et gitt tekstvindu. Bildet nedenfor viser hvordan posisjonelle kodinger med oppmerksomhet kan fange kontekst innenfor et gitt vindu.\n",
    "\n",
    "![Animasjon som viser hvordan evalueringene utføres i transformator-modeller.](../../../../../lessons/5-NLP/18-Transformers/images/transformer-animated-explanation.gif)\n",
    "\n",
    "Siden hver inngangsposisjon kartlegges uavhengig til hver utgangsposisjon, kan transformatorer parallellisere bedre enn RNN-er, noe som muliggjør mye større og mer uttrykksfulle språkmodeller. Hver oppmerksomhetshode kan brukes til å lære forskjellige relasjoner mellom ord som forbedrer oppgaver innen naturlig språkbehandling.\n",
    "\n",
    "**BERT** (Bidirectional Encoder Representations from Transformers) er et svært stort flerlags transformatornettverk med 12 lag for *BERT-base* og 24 for *BERT-large*. Modellen blir først forhåndstrent på en stor tekstkorpus (Wikipedia + bøker) ved hjelp av usupervisert trening (predikere maskerte ord i en setning). Under forhåndstreningen absorberer modellen et betydelig nivå av språkforståelse som deretter kan utnyttes med andre datasett ved hjelp av finjustering. Denne prosessen kalles **overføringslæring**.\n",
    "\n",
    "![Bilde fra http://jalammar.github.io/illustrated-bert/](../../../../../translated_images/no/jalammarBERT-language-modeling-masked-lm.34f113ea5fec4362.webp)\n",
    "\n",
    "Det finnes mange varianter av transformator-arkitekturer, inkludert BERT, DistilBERT, BigBird, OpenGPT3 og flere, som kan finjusteres. [HuggingFace-pakken](https://github.com/huggingface/) gir et bibliotek for å trene mange av disse arkitekturene med PyTorch.\n",
    "\n",
    "## Bruke BERT til tekstklassifisering\n",
    "\n",
    "La oss se hvordan vi kan bruke en forhåndstrent BERT-modell for å løse vår tradisjonelle oppgave: sekvensklassifisering. Vi skal klassifisere vårt originale AG News-datasett.\n",
    "\n",
    "Først laster vi inn HuggingFace-biblioteket og datasettet vårt:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Building vocab...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "from torchnlp import *\n",
    "import transformers\n",
    "train_dataset, test_dataset, classes, vocab = load_dataset()\n",
    "vocab_len = len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fordi vi skal bruke en forhåndstrent BERT-modell, må vi bruke en spesifikk tokenizer. Først laster vi inn en tokenizer som er knyttet til den forhåndstrente BERT-modellen.\n",
    "\n",
    "HuggingFace-biblioteket inneholder et arkiv med forhåndstrente modeller, som du kan bruke ved å spesifisere navnene deres som argumenter til `from_pretrained`-funksjoner. Alle nødvendige binærfiler for modellen vil automatisk bli lastet ned.\n",
    "\n",
    "Imidlertid vil du noen ganger trenge å laste inn dine egne modeller. I slike tilfeller kan du spesifisere katalogen som inneholder alle relevante filer, inkludert parametere for tokenizer, `config.json`-filen med modellparametere, binære vekter, osv.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load the model from Internet repository using model name. \n",
    "# Use this if you are running from your own copy of the notebooks\n",
    "bert_model = 'bert-base-uncased' \n",
    "\n",
    "# To load the model from the directory on disk. Use this for Microsoft Learn module, because we have\n",
    "# prepared all required files for you.\n",
    "bert_model = './bert'\n",
    "\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(bert_model)\n",
    "\n",
    "MAX_SEQ_LEN = 128\n",
    "PAD_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "UNK_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tokenizer`-objektet inneholder `encode`-funksjonen som kan brukes direkte til å kode tekst:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 1052, 22123, 2953, 2818, 2003, 1037, 2307, 7705, 2005, 17953, 2361, 102]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('PyTorch is a great framework for NLP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da la oss lage iteratorer som vi vil bruke under trening for å få tilgang til dataene. Fordi BERT bruker sin egen kodingsfunksjon, må vi definere en utfyllingsfunksjon som ligner på `padify` vi har definert tidligere:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_bert(b):\n",
    "    # b is the list of tuples of length batch_size\n",
    "    #   - first element of a tuple = label, \n",
    "    #   - second = feature (text sequence)\n",
    "    # build vectorized sequence\n",
    "    v = [tokenizer.encode(x[1]) for x in b]\n",
    "    # compute max length of a sequence in this minibatch\n",
    "    l = max(map(len,v))\n",
    "    return ( # tuple of two tensors - labels and features\n",
    "        torch.LongTensor([t[0] for t in b]),\n",
    "        torch.stack([torch.nn.functional.pad(torch.tensor(t),(0,l-len(t)),mode='constant',value=0) for t in v])\n",
    "    )\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=8, collate_fn=pad_bert, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=8, collate_fn=pad_bert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I vårt tilfelle vil vi bruke en forhåndstrent BERT-modell kalt `bert-base-uncased`. La oss laste inn modellen ved hjelp av pakken `BertForSequenceClassification`. Dette sikrer at modellen vår allerede har en nødvendig arkitektur for klassifisering, inkludert den endelige klassifikatoren. Du vil se en advarsel som sier at vektene til den endelige klassifikatoren ikke er initialisert, og at modellen vil kreve forhåndstrening - det er helt greit, fordi det er akkurat det vi skal gjøre!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./bert were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./bert and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = transformers.BertForSequenceClassification.from_pretrained(bert_model,num_labels=4).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nå er vi klare til å begynne treningen! Siden BERT allerede er forhåndstrent, ønsker vi å starte med en ganske lav læringsrate for å unngå å ødelegge de opprinnelige vektene.\n",
    "\n",
    "Alt det harde arbeidet utføres av `BertForSequenceClassification`-modellen. Når vi kjører modellen på treningsdataene, returnerer den både tap og nettverksutgang for minibatch-inndata. Vi bruker tapet for parameteroptimalisering (`loss.backward()` utfører bakoverpasset), og `out` for å beregne treningsnøyaktighet ved å sammenligne de oppnådde etikettene `labs` (beregnet ved hjelp av `argmax`) med forventede `labels`.\n",
    "\n",
    "For å kontrollere prosessen akkumulerer vi tap og nøyaktighet over flere iterasjoner, og skriver dem ut hver `report_freq` treningssyklus.\n",
    "\n",
    "Denne treningen vil sannsynligvis ta ganske lang tid, så vi begrenser antall iterasjoner.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 1.1254194641113282, Accuracy = 0.585\n",
      "Loss = 0.6194715118408203, Accuracy = 0.83\n",
      "Loss = 0.46665248870849607, Accuracy = 0.8475\n",
      "Loss = 0.4309701919555664, Accuracy = 0.8575\n",
      "Loss = 0.35427074432373046, Accuracy = 0.8825\n",
      "Loss = 0.3306886291503906, Accuracy = 0.8975\n",
      "Loss = 0.30340143203735354, Accuracy = 0.8975\n",
      "Loss = 0.26139299392700194, Accuracy = 0.915\n",
      "Loss = 0.26708646774291994, Accuracy = 0.9225\n",
      "Loss = 0.3667240524291992, Accuracy = 0.8675\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
    "\n",
    "report_freq = 50\n",
    "iterations = 500 # make this larger to train for longer time!\n",
    "\n",
    "model.train()\n",
    "\n",
    "i,c = 0,0\n",
    "acc_loss = 0\n",
    "acc_acc = 0\n",
    "\n",
    "for labels,texts in train_loader:\n",
    "    labels = labels.to(device)-1 # get labels in the range 0-3         \n",
    "    texts = texts.to(device)\n",
    "    loss, out = model(texts, labels=labels)[:2]\n",
    "    labs = out.argmax(dim=1)\n",
    "    acc = torch.mean((labs==labels).type(torch.float32))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    acc_loss += loss\n",
    "    acc_acc += acc\n",
    "    i+=1\n",
    "    c+=1\n",
    "    if i%report_freq==0:\n",
    "        print(f\"Loss = {acc_loss.item()/c}, Accuracy = {acc_acc.item()/c}\")\n",
    "        c = 0\n",
    "        acc_loss = 0\n",
    "        acc_acc = 0\n",
    "    iterations-=1\n",
    "    if not iterations:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Du kan se (spesielt hvis du øker antall iterasjoner og venter lenge nok) at BERT-klassifisering gir oss ganske god nøyaktighet! Dette er fordi BERT allerede forstår språkstrukturen ganske godt, og vi trenger bare å finjustere den endelige klassifikatoren. Men siden BERT er en stor modell, tar hele treningsprosessen lang tid og krever betydelig datakraft! (GPU, og helst mer enn én).\n",
    "\n",
    "> **Merk:** I vårt eksempel har vi brukt en av de minste forhåndstrente BERT-modellene. Det finnes større modeller som sannsynligvis vil gi bedre resultater.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluere modellens ytelse\n",
    "\n",
    "Nå kan vi evaluere ytelsen til modellen vår på testdatasettet. Evalueringsløkken ligner ganske mye på treningsløkken, men vi må ikke glemme å sette modellen i evalueringsmodus ved å kalle `model.eval()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy: 0.9047029702970297\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "iterations = 100\n",
    "acc = 0\n",
    "i = 0\n",
    "for labels,texts in test_loader:\n",
    "    labels = labels.to(device)-1      \n",
    "    texts = texts.to(device)\n",
    "    _, out = model(texts, labels=labels)[:2]\n",
    "    labs = out.argmax(dim=1)\n",
    "    acc += torch.mean((labs==labels).type(torch.float32))\n",
    "    i+=1\n",
    "    if i>iterations: break\n",
    "        \n",
    "print(f\"Final accuracy: {acc.item()/i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viktige punkter\n",
    "\n",
    "I denne enheten har vi sett hvor enkelt det er å ta en forhåndstrent språkmodell fra **transformers**-biblioteket og tilpasse den til vår tekstklassifiseringsoppgave. På samme måte kan BERT-modeller brukes til enhetsuttrekking, spørsmål-svar og andre NLP-oppgaver.\n",
    "\n",
    "Transformer-modeller representerer dagens toppmoderne teknologi innen NLP, og i de fleste tilfeller bør dette være den første løsningen du eksperimenterer med når du implementerer skreddersydde NLP-løsninger. Likevel er det svært viktig å forstå de grunnleggende prinsippene for rekurrente nevrale nettverk som ble diskutert i denne modulen, dersom du ønsker å bygge avanserte nevrale modeller.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Ansvarsfraskrivelse**:  \nDette dokumentet er oversatt ved hjelp av AI-oversettelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selv om vi streber etter nøyaktighet, vær oppmerksom på at automatiserte oversettelser kan inneholde feil eller unøyaktigheter. Det originale dokumentet på sitt opprinnelige språk bør anses som den autoritative kilden. For kritisk informasjon anbefales profesjonell menneskelig oversettelse. Vi er ikke ansvarlige for misforståelser eller feiltolkninger som oppstår ved bruk av denne oversettelsen.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37_pytorch",
   "language": "python",
   "name": "conda-env-py37_pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "coopTranslator": {
   "original_hash": "753865967678a92dbce7d7efbd36d980",
   "translation_date": "2025-08-28T17:33:53+00:00",
   "source_file": "lessons/5-NLP/18-Transformers/TransformersPyTorch.ipynb",
   "language_code": "no"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}