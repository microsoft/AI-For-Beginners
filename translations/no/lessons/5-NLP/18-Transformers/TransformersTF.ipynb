{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oppmerksomhetsmekanismer og transformere\n",
    "\n",
    "En stor ulempe med rekurrente nettverk er at alle ord i en sekvens har samme påvirkning på resultatet. Dette fører til suboptimal ytelse med standard LSTM-encoder-decoder-modeller for sekvens-til-sekvens-oppgaver, som for eksempel navngjenkjenning og maskinoversettelse. I virkeligheten har spesifikke ord i inngangssekvensen ofte større påvirkning på sekvensielle utganger enn andre.\n",
    "\n",
    "Tenk på en sekvens-til-sekvens-modell, som maskinoversettelse. Den implementeres ved hjelp av to rekurrente nettverk, der ett nettverk (**encoder**) komprimerer inngangssekvensen til en skjult tilstand, og et annet nettverk, **decoder**, ruller ut denne skjulte tilstanden til et oversatt resultat. Problemet med denne tilnærmingen er at nettverkets endelige tilstand har vanskeligheter med å huske begynnelsen av en setning, noe som fører til dårlig modellkvalitet for lange setninger.\n",
    "\n",
    "**Oppmerksomhetsmekanismer** gir en måte å vekte den kontekstuelle påvirkningen av hver inngangsvektor på hver utgangsprediksjon i RNN. Dette implementeres ved å lage snarveier mellom mellomliggende tilstander i inngangs-RNN og utgangs-RNN. På denne måten, når vi genererer utgangssymbolet $y_t$, tar vi hensyn til alle skjulte inngangstilstander $h_i$, med forskjellige vektkoeffisienter $\\alpha_{t,i}$. \n",
    "\n",
    "![Bilde som viser en encoder/decoder-modell med et additivt oppmerksomhetslag](../../../../../translated_images/no/encoder-decoder-attention.7a726296894fb567.webp)\n",
    "*Encoder-decoder-modellen med additiv oppmerksomhetsmekanisme i [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf), sitert fra [denne bloggposten](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)*\n",
    "\n",
    "Oppmerksomhetsmatrisen $\\{\\alpha_{i,j}\\}$ representerer graden til hvilken visse inngangsord spiller en rolle i genereringen av et gitt ord i utgangssekvensen. Nedenfor er et eksempel på en slik matrise:\n",
    "\n",
    "![Bilde som viser et eksempel på justering funnet av RNNsearch-50, hentet fra Bahdanau - arviz.org](../../../../../translated_images/no/bahdanau-fig3.09ba2d37f202a6af.webp)\n",
    "\n",
    "*Figur hentet fra [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) (Fig.3)*\n",
    "\n",
    "Oppmerksomhetsmekanismer er ansvarlige for mye av dagens eller nesten dagens toppytelse innen naturlig språkbehandling. Å legge til oppmerksomhet øker imidlertid antallet modellparametere betydelig, noe som førte til skaleringsproblemer med RNN-er. En viktig begrensning ved skalering av RNN-er er at modellens rekurrente natur gjør det utfordrende å batch-prosessere og parallellisere treningen. I en RNN må hvert element i en sekvens behandles i sekvensiell rekkefølge, noe som betyr at det ikke enkelt kan parallelliseres.\n",
    "\n",
    "Bruken av oppmerksomhetsmekanismer kombinert med denne begrensningen førte til opprettelsen av de nåværende toppmoderne transformermodellene vi kjenner og bruker i dag, fra BERT til OpenGPT3.\n",
    "\n",
    "## Transformermodeller\n",
    "\n",
    "I stedet for å sende konteksten fra hver tidligere prediksjon videre til neste evalueringssteg, bruker **transformermodeller** **posisjonelle kodinger** og **oppmerksomhet** for å fange konteksten til en gitt inngang innenfor et gitt tekstvindu. Bildet nedenfor viser hvordan posisjonelle kodinger med oppmerksomhet kan fange kontekst innenfor et gitt vindu.\n",
    "\n",
    "![Animasjon som viser hvordan evalueringene utføres i transformermodeller.](../../../../../lessons/5-NLP/18-Transformers/images/transformer-animated-explanation.gif) \n",
    "\n",
    "Siden hver inngangsposisjon kartlegges uavhengig til hver utgangsposisjon, kan transformere parallellisere bedre enn RNN-er, noe som muliggjør mye større og mer uttrykksfulle språkmodeller. Hver oppmerksomhetshode kan brukes til å lære forskjellige relasjoner mellom ord som forbedrer oppgaver innen naturlig språkbehandling.\n",
    "\n",
    "## Bygge en enkel transformermodell\n",
    "\n",
    "Keras inneholder ikke et innebygd Transformer-lag, men vi kan bygge vårt eget. Som før vil vi fokusere på tekstklassifisering av AG News-datasettet, men det er verdt å nevne at transformermodeller gir best resultater på mer krevende NLP-oppgaver.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "\n",
    "ds_train, ds_test = tfds.load('ag_news_subset').values()\n",
    "\n",
    "def extract_text(x):\n",
    "    return x['title']+' '+x['description']\n",
    "\n",
    "def tupelize(x):\n",
    "    return (extract_text(x),x['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nye lag i Keras bør arve `Layer`-klassen og implementere `call`-metoden. La oss starte med **Positional Embedding**-laget. Vi vil bruke [noe kode fra den offisielle Keras-dokumentasjonen](https://keras.io/examples/nlp/text_classification_with_transformer/). Vi vil anta at vi fyller alle inngangssekvenser til lengde `maxlen`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(keras.layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = keras.layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "        self.maxlen = maxlen\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = self.maxlen\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x+positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dette laget består av to `Embedding`-lag: ett for å embedde tokens (på en måte vi har diskutert tidligere) og ett for tokenposisjoner. Tokenposisjoner opprettes som en sekvens av naturlige tall fra 0 til `maxlen` ved hjelp av `tf.range`, og sendes deretter gjennom embed-laget. De to resulterende embedding-vektorene legges sammen, og gir en posisjonsbasert representasjon av input med formen `maxlen`$\\times$`embed_dim`.\n",
    "\n",
    "Nå skal vi implementere transformer-blokken. Den vil ta utdataene fra det tidligere definerte embed-laget:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim, name='attn')\n",
    "        self.ffn = keras.Sequential(\n",
    "            [keras.layers.Dense(ff_dim, activation=\"relu\"), keras.layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = keras.layers.Dropout(rate)\n",
    "        self.dropout2 = keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nå er vi klare til å definere den komplette transformer-modellen:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "text_vectorization (TextVect (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "token_and_position_embedding (None, 256, 32)           648192    \n",
      "_________________________________________________________________\n",
      "transformer_block (Transform (None, 256, 32)           10656     \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 20)                660       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 84        \n",
      "=================================================================\n",
      "Total params: 659,592\n",
      "Trainable params: 659,592\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 32  # Embedding size for each token\n",
    "num_heads = 2  # Number of attention heads\n",
    "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
    "maxlen = 256\n",
    "vocab_size = 20000\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.experimental.preprocessing.TextVectorization(max_tokens=vocab_size,output_sequence_length=maxlen, input_shape=(1,)),\n",
    "    TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim),\n",
    "    TransformerBlock(embed_dim, num_heads, ff_dim),\n",
    "    keras.layers.GlobalAveragePooling1D(),\n",
    "    keras.layers.Dropout(0.1),\n",
    "    keras.layers.Dense(20, activation=\"relu\"),\n",
    "    keras.layers.Dropout(0.1),\n",
    "    keras.layers.Dense(4, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokenizer\n",
      "938/938 [==============================] - 45s 39ms/step - loss: 0.4978 - acc: 0.8068 - val_loss: 0.2808 - val_acc: 0.9124\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9c2427a0d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Training tokenizer')\n",
    "model.layers[0].adapt(ds_train.map(extract_text))\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'], optimizer='adam')\n",
    "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT Transformer-modeller\n",
    "\n",
    "**BERT** (Bidirectional Encoder Representations from Transformers) er et svært stort flerlags transformatornettverk med 12 lag for *BERT-base* og 24 for *BERT-large*. Modellen blir først forhåndstrent på en stor mengde tekstdata (Wikipedia + bøker) ved hjelp av usupervisert trening (forutsi maskerte ord i en setning). Under forhåndstreningen absorberer modellen et betydelig nivå av språkforståelse som deretter kan utnyttes med andre datasett ved hjelp av finjustering. Denne prosessen kalles **transfer learning**.\n",
    "\n",
    "![bilde fra http://jalammar.github.io/illustrated-bert/](../../../../../translated_images/no/jalammarBERT-language-modeling-masked-lm.34f113ea5fec4362.webp)\n",
    "\n",
    "Det finnes mange varianter av transformatorarkitekturer, inkludert BERT, DistilBERT, BigBird, OpenGPT3 og flere, som kan finjusteres.\n",
    "\n",
    "La oss se hvordan vi kan bruke en forhåndstrent BERT-modell for å løse vårt tradisjonelle sekvensklassifiseringsproblem. Vi vil låne ideen og noe kode fra [offisiell dokumentasjon](https://www.tensorflow.org/text/tutorials/classify_text_with_bert).\n",
    "\n",
    "For å laste inn forhåndstrente modeller, vil vi bruke **Tensorflow hub**. Først, la oss laste inn den BERT-spesifikke vektorisatoren:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_41180/4216669875.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow_text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow_hub\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mhub\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhub\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mKerasLayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_text'"
     ]
    }
   ],
   "source": [
    "import tensorflow_text \n",
    "import tensorflow_hub as hub\n",
    "vectorizer = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_type_ids': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       " array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "       dtype=int32)>,\n",
       " 'input_word_ids': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       " array([[  101,  1045,  2293, 19081,   102,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]], dtype=int32)>,\n",
       " 'input_mask': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       " array([[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "       dtype=int32)>}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer(['I love transformers'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Det er viktig at du bruker den samme vektoriseringen som det opprinnelige nettverket ble trent med. I tillegg returnerer BERT-vektoriseringen tre komponenter:\n",
    "* `input_word_ids`, som er en sekvens av token-numre for inndata-setningen\n",
    "* `input_mask`, som viser hvilken del av sekvensen som inneholder faktisk inndata, og hvilken del som er utfylling. Dette ligner på masken som produseres av `Masking`-laget\n",
    "* `input_type_ids` brukes for oppgaver innen språklig modellering, og gjør det mulig å spesifisere to inndata-setninger i én sekvens.\n",
    "\n",
    "Deretter kan vi opprette en BERT-funksjonsekstraktor:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = hub.KerasLayer('https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pooled_output -> (1, 128)\n",
      "encoder_outputs -> 4\n",
      "sequence_output -> (1, 128, 128)\n",
      "default -> (1, 128)\n"
     ]
    }
   ],
   "source": [
    "z = bert(vectorizer(['I love transformers']))\n",
    "for i,x in z.items():\n",
    "    print(f\"{i} -> { len(x) if isinstance(x, list) else x.shape }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Så BERT-laget returnerer en rekke nyttige resultater:\n",
    "* `pooled_output` er et resultat av å gjennomsnittliggjøre alle tokens i sekvensen. Du kan se på det som en intelligent semantisk representasjon av hele nettverket. Det tilsvarer utdataene fra `GlobalAveragePooling1D`-laget i vår forrige modell.\n",
    "* `sequence_output` er utdataene fra det siste transformer-laget (tilsvarer utdataene fra `TransformerBlock` i modellen vår ovenfor).\n",
    "* `encoder_outputs` er utdataene fra alle transformer-lagene. Siden vi har lastet inn en 4-lags BERT-modell (som du sikkert kan gjette fra navnet, som inneholder `4_H`), har den 4 tensorer. Den siste er den samme som `sequence_output`.\n",
    "\n",
    "Nå skal vi definere den ende-til-ende klassifiseringsmodellen. Vi vil bruke *funksjonell modelldefinisjon*, der vi definerer modellens input og deretter gir en serie uttrykk for å beregne dens output. Vi vil også gjøre vektene til BERT-modellen ikke-trainable og kun trene den siste klassifikatoren:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer (KerasLayer)        {'input_type_ids': ( 0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer_1 (KerasLayer)      {'pooled_output': (N 4782465     keras_layer[0][0]                \n",
      "                                                                 keras_layer[0][1]                \n",
      "                                                                 keras_layer[0][2]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 128)          0           keras_layer_1[0][5]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 4)            516         dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 4,782,981\n",
      "Trainable params: 516\n",
      "Non-trainable params: 4,782,465\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inp = keras.Input(shape=(),dtype=tf.string)\n",
    "x = vectorizer(inp)\n",
    "x = bert(x)\n",
    "x = keras.layers.Dropout(0.1)(x['pooled_output'])\n",
    "out = keras.layers.Dense(4,activation='softmax')(x)\n",
    "model = keras.models.Model(inp,out)\n",
    "bert.trainable = False\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 528s 559ms/step - loss: 0.8056 - acc: 0.6983 - val_loss: 0.5953 - val_acc: 0.7888\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9bb1e36d00>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'], optimizer='adam')\n",
    "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selv om det er få trenbare parametere, er prosessen ganske treg, fordi BERT-funksjonsekstraktoren er beregningsmessig tung. Det ser ut til at vi ikke klarte å oppnå rimelig nøyaktighet, enten på grunn av mangel på trening eller mangel på modellparametere.\n",
    "\n",
    "La oss prøve å låse opp BERT-vektene og trene den også. Dette krever en veldig liten læringsrate, og en mer forsiktig treningsstrategi med **warmup**, ved bruk av **AdamW**-optimalisatoren. Vi vil bruke `tf-models-official`-pakken for å opprette optimalisatoren:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer (KerasLayer)        {'input_type_ids': ( 0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer_1 (KerasLayer)      {'pooled_output': (N 4782465     keras_layer[0][0]                \n",
      "                                                                 keras_layer[0][1]                \n",
      "                                                                 keras_layer[0][2]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 128)          0           keras_layer_1[0][5]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 4)            516         dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 4,782,981\n",
      "Trainable params: 4,782,980\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n",
      "938/938 [==============================] - 629s 664ms/step - loss: 0.6344 - acc: 0.7658 - val_loss: 0.4876 - val_acc: 0.8247\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9bb0bd0070>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from official.nlp import optimization \n",
    "bert.trainable=True\n",
    "model.summary()\n",
    "epochs = 3\n",
    "opt = optimization.create_optimizer(\n",
    "    init_lr=3e-5,\n",
    "    num_train_steps=epochs*len(ds_train),\n",
    "    num_warmup_steps=0.1*epochs*len(ds_train),\n",
    "    optimizer_type='adamw')\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'], optimizer=opt)\n",
    "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Som du kan se, går treningen ganske sakte – men du kan eksperimentere og trene modellen i noen få epoker (5-10) for å se om du kan oppnå et bedre resultat sammenlignet med tilnærmingene vi har brukt tidligere.\n",
    "\n",
    "## Huggingface Transformers-bibliotek\n",
    "\n",
    "En annen veldig vanlig (og litt enklere) måte å bruke Transformer-modeller på er [HuggingFace-pakken](https://github.com/huggingface/), som tilbyr enkle byggeklosser for ulike NLP-oppgaver. Den er tilgjengelig både for Tensorflow og PyTorch, et annet veldig populært rammeverk for nevrale nettverk.\n",
    "\n",
    "> **Merk**: Hvis du ikke er interessert i å se hvordan Transformers-biblioteket fungerer, kan du hoppe til slutten av denne notatboken, fordi du ikke vil se noe vesentlig annerledes enn det vi har gjort ovenfor. Vi vil gjenta de samme trinnene for å trene BERT-modellen ved å bruke et annet bibliotek og en vesentlig større modell. Dermed innebærer prosessen en del langvarig trening, så du kan bare velge å se gjennom koden.\n",
    "\n",
    "La oss se hvordan problemet vårt kan løses ved hjelp av [Huggingface Transformers](http://huggingface.co).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Det første vi må gjøre er å velge modellen vi skal bruke. I tillegg til noen innebygde modeller, har Huggingface et [nettbasert modellbibliotek](https://huggingface.co/models), hvor du kan finne mange flere forhåndstrente modeller fra fellesskapet. Alle disse modellene kan lastes inn og brukes bare ved å oppgi et modellnavn. Alle nødvendige binærfiler for modellen vil automatisk bli lastet ned.\n",
    "\n",
    "Noen ganger vil du kanskje måtte laste inn dine egne modeller. I så fall kan du spesifisere katalogen som inneholder alle relevante filer, inkludert parametere for tokenizer, `config.json`-filen med modellparametere, binære vekter osv.\n",
    "\n",
    "Fra modellnavnet kan vi opprette både modellen og tokenizer. La oss starte med en tokenizer:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "# To load the model from Internet repository using model name. \n",
    "# Use this if you are running from your own copy of the notebooks\n",
    "bert_model = 'bert-base-uncased' \n",
    "\n",
    "# To load the model from the directory on disk. Use this for Microsoft Learn module, because we have\n",
    "# prepared all required files for you.\n",
    "#bert_model = './bert'\n",
    "\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(bert_model)\n",
    "\n",
    "MAX_SEQ_LEN = 128\n",
    "PAD_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "UNK_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tokenizer`-objektet inneholder `encode`-funksjonen som kan brukes direkte til å kode tekst:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 23435, 12314, 2003, 1037, 2307, 7705, 2005, 17953, 2361, 102]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('Tensorflow is a great framework for NLP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi kan også bruke tokenizer til å kode en sekvens på en måte som er egnet for å sende til modellen, dvs. inkludert `token_ids`, `input_mask`-felt, osv. Vi kan også spesifisere at vi ønsker Tensorflow-tensorer ved å gi argumentet `return_tensors='tf'`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[ 101, 7592, 1010, 2045,  102]], dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[0, 0, 0, 0, 0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[1, 1, 1, 1, 1]], dtype=int32)>}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(['Hello, there'],return_tensors='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I vårt tilfelle vil vi bruke den forhåndstrente BERT-modellen kalt `bert-base-uncased`. *Uncased* indikerer at modellen er ikke-følsom for store og små bokstaver.\n",
    "\n",
    "Når vi trener modellen, må vi gi en tokenisert sekvens som input, og derfor vil vi designe en databehandlingspipeline. Siden `tokenizer.encode` er en Python-funksjon, vil vi bruke samme tilnærming som i forrige enhet ved å kalle den med `py_function`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(x):\n",
    "    return tokenizer.encode(x.numpy().decode('utf-8'),return_tensors='tf',padding='max_length',max_length=MAX_SEQ_LEN,truncation=True)[0]\n",
    "\n",
    "def process_fn(x):\n",
    "    s = x['title']+' '+x['description']\n",
    "    e = tf.py_function(process,inp=[s],Tout=(tf.int32))\n",
    "    e.set_shape(MAX_SEQ_LEN)\n",
    "    return e,x['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nå kan vi laste inn den faktiske modellen ved å bruke `BertForSequenceClassification`-pakken. Dette sikrer at modellen vår allerede har en nødvendig arkitektur for klassifisering, inkludert den endelige klassifikatoren. Du vil se en advarsel som sier at vektene til den endelige klassifikatoren ikke er initialisert, og at modellen vil kreve forhåndstrening - det er helt greit, fordi det er akkurat det vi skal gjøre!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = transformers.TFBertForSequenceClassification.from_pretrained(bert_model,num_labels=4,output_attentions=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  109482240 \n",
      "_________________________________________________________________\n",
      "dropout_75 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  3076      \n",
      "=================================================================\n",
      "Total params: 109,485,316\n",
      "Trainable params: 109,485,316\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Som du kan se fra `summary()`, inneholder modellen nesten 110 millioner parametere! Antakeligvis, hvis vi ønsker en enkel klassifiseringsoppgave på et relativt lite datasett, ønsker vi ikke å trene BERT-baselaget:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  109482240 \n",
      "_________________________________________________________________\n",
      "dropout_75 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  3076      \n",
      "=================================================================\n",
      "Total params: 109,485,316\n",
      "Trainable params: 3,076\n",
      "Non-trainable params: 109,482,240\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.layers[0].trainable = False\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nå er vi klare til å begynne treningen!\n",
    "\n",
    "> **Note**: Å trene en fullskala BERT-modell kan være svært tidkrevende! Derfor vil vi kun trene den for de første 32 batchene. Dette er bare for å vise hvordan modelltreningen settes opp. Hvis du er interessert i å prøve fullskala trening - fjern bare parameterne `steps_per_epoch` og `validation_steps`, og gjør deg klar til å vente!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 142s 4s/step - loss: 1.3896 - acc: 0.2500 - val_loss: 1.3863 - val_acc: 0.2480\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f1d40a4b6a0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile('adam','sparse_categorical_crossentropy',['acc'])\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "model.fit(ds_train.map(process_fn).batch(32),validation_data=ds_test.map(process_fn).batch(32),steps_per_epoch=32,validation_steps=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hvis du øker antall iterasjoner og venter lenge nok, og trener i flere epoker, kan du forvente at BERT-klassifisering gir oss den beste nøyaktigheten! Dette er fordi BERT allerede forstår språkets struktur ganske godt, og vi trenger bare å finjustere den endelige klassifiseringen. Men siden BERT er en stor modell, tar hele treningsprosessen lang tid og krever betydelig datakraft! (GPU, og helst mer enn én).\n",
    "\n",
    "> **Note:** I vårt eksempel har vi brukt en av de minste forhåndstrente BERT-modellene. Det finnes større modeller som sannsynligvis vil gi bedre resultater.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viktige punkter\n",
    "\n",
    "I denne enheten har vi sett på svært moderne modellarkitekturer basert på **transformere**. Vi har brukt dem til vår oppgave med tekstklassifisering, men på samme måte kan BERT-modeller brukes til enhetsuttrekk, spørsmål-svar og andre NLP-oppgaver.\n",
    "\n",
    "Transformer-modeller representerer dagens toppnivå innen NLP, og i de fleste tilfeller bør de være den første løsningen du eksperimenterer med når du implementerer skreddersydde NLP-løsninger. Likevel er det svært viktig å forstå de grunnleggende prinsippene bak rekurrente nevrale nettverk som ble diskutert i denne modulen, dersom du ønsker å bygge avanserte nevrale modeller.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Ansvarsfraskrivelse**:  \nDette dokumentet er oversatt ved hjelp av AI-oversettelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selv om vi streber etter nøyaktighet, vær oppmerksom på at automatiske oversettelser kan inneholde feil eller unøyaktigheter. Det originale dokumentet på sitt opprinnelige språk bør anses som den autoritative kilden. For kritisk informasjon anbefales profesjonell menneskelig oversettelse. Vi er ikke ansvarlige for misforståelser eller feiltolkninger som oppstår ved bruk av denne oversettelsen.\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0cb620c6d4b9f7a635928804c26cf22403d89d98d79684e4529119355ee6d5a5"
  },
  "kernelspec": {
   "display_name": "py38_tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "coopTranslator": {
   "original_hash": "ab59c532409774988ab875f2260e8e53",
   "translation_date": "2025-08-28T17:37:17+00:00",
   "source_file": "lessons/5-NLP/18-Transformers/TransformersTF.ipynb",
   "language_code": "no"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}