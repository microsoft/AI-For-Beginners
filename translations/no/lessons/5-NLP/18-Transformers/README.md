# Oppmerksomhetsmekanismer og Transformere

## [Quiz f칮r forelesning](https://ff-quizzes.netlify.app/en/ai/quiz/35)

Et av de viktigste problemene innen NLP-feltet er **maskinoversettelse**, en essensiell oppgave som ligger til grunn for verkt칮y som Google Translate. I denne delen vil vi fokusere p친 maskinoversettelse, eller mer generelt, p친 enhver *sekvens-til-sekvens*-oppgave (som ogs친 kalles **setningstransduksjon**).

Med RNN-er implementeres sekvens-til-sekvens med to rekurrente nettverk, hvor ett nettverk, **enkoderen**, komprimerer en inngangssekvens til en skjult tilstand, mens et annet nettverk, **dekoderen**, utvider denne skjulte tilstanden til et oversatt resultat. Det er et par problemer med denne tiln칝rmingen:

* Den endelige tilstanden til enkodernettverket har vanskelig for 친 huske begynnelsen av en setning, noe som f칮rer til d친rlig modellkvalitet for lange setninger.
* Alle ord i en sekvens har samme innvirkning p친 resultatet. I virkeligheten har imidlertid spesifikke ord i inngangssekvensen ofte st칮rre innvirkning p친 sekvensielle utganger enn andre.

**Oppmerksomhetsmekanismer** gir en m친te 친 vekte den kontekstuelle innvirkningen av hver inngangsvektor p친 hver utgangsprediksjon av RNN. Dette implementeres ved 친 lage snarveier mellom mellomliggende tilstander i inngangs-RNN og utgangs-RNN. P친 denne m친ten, n친r vi genererer utgangssymbolet y<sub>t</sub>, tar vi hensyn til alle skjulte inngangstilstander h<sub>i</sub>, med forskjellige vektkoeffisienter &alpha;<sub>t,i</sub>.

![Bilde som viser en enkoder/dekoder-modell med et additivt oppmerksomhetslag](../../../../../translated_images/no/encoder-decoder-attention.7a726296894fb567.webp)

> Enkoder-dekoder-modellen med additiv oppmerksomhetsmekanisme i [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf), sitert fra [denne bloggposten](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)

Oppmerksomhetsmatrisen {&alpha;<sub>i,j</sub>} representerer graden av innflytelse visse inngangsord har p친 genereringen av et gitt ord i utgangssekvensen. Nedenfor er et eksempel p친 en slik matrise:

![Bilde som viser et eksempel p친 justering funnet av RNNsearch-50, hentet fra Bahdanau - arviz.org](../../../../../translated_images/no/bahdanau-fig3.09ba2d37f202a6af.webp)

> Figur fra [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) (Fig.3)

Oppmerksomhetsmekanismer er ansvarlige for mye av den n친v칝rende eller nesten n친v칝rende toppmoderne utviklingen innen NLP. 칀 legge til oppmerksomhet 칮ker imidlertid antallet modellparametere betydelig, noe som f칮rte til skaleringsproblemer med RNN-er. En viktig begrensning ved skalering av RNN-er er at den rekursive naturen til modellene gj칮r det utfordrende 친 batch-prosessere og parallellisere trening. I en RNN m친 hvert element i en sekvens behandles i sekvensiell rekkef칮lge, noe som betyr at det ikke enkelt kan parallelliseres.

![Enkoder Dekoder med Oppmerksomhet](../../../../../lessons/5-NLP/18-Transformers/images/EncDecAttention.gif)

> Figur fra [Googles Blogg](https://research.googleblog.com/2016/09/a-neural-network-for-machine.html)

Adopsjonen av oppmerksomhetsmekanismer kombinert med denne begrensningen f칮rte til opprettelsen av de n친 toppmoderne transformermodellene vi kjenner og bruker i dag, som BERT og Open-GPT3.

## Transformermodeller

En av hovedideene bak transformere er 친 unng친 den sekvensielle naturen til RNN-er og lage en modell som kan parallelliseres under trening. Dette oppn친s ved 친 implementere to ideer:

* posisjonskoding
* bruk av selvoppmerksomhetsmekanisme for 친 fange m칮nstre i stedet for RNN-er (eller CNN-er) (det er derfor artikkelen som introduserer transformere heter *[Attention is all you need](https://arxiv.org/abs/1706.03762)*)

### Posisjonskoding/Embedding

Ideen med posisjonskoding er som f칮lger:
1. N친r man bruker RNN-er, representeres den relative posisjonen til tokenene av antall steg, og trenger derfor ikke 친 bli eksplisitt representert.
2. N친r vi derimot bytter til oppmerksomhet, m친 vi vite de relative posisjonene til tokenene i en sekvens.
3. For 친 f친 posisjonskoding, utvider vi v친r sekvens av token med en sekvens av tokenposisjoner i sekvensen (dvs. en sekvens av tall 0,1, ...).
4. Vi blander deretter tokenposisjonen med en token-embedding-vektor. For 친 transformere posisjonen (heltall) til en vektor, kan vi bruke forskjellige tiln칝rminger:

* Trenbar embedding, lik token-embedding. Dette er tiln칝rmingen vi vurderer her. Vi bruker embedding-lag p친 b친de tokenene og deres posisjoner, noe som resulterer i embedding-vektorer med samme dimensjoner, som vi deretter legger sammen.
* Fast posisjonskodingsfunksjon, som foresl친tt i den originale artikkelen.

<img src="../../../../../translated_images/no/pos-embedding.e41ce9b6cf6078af.webp" width="50%"/>

> Bilde av forfatteren

Resultatet vi f친r med posisjonsembedding inkluderer b친de det originale tokenet og dets posisjon i en sekvens.

### Multi-Head Selvoppmerksomhet

Deretter m친 vi fange noen m칮nstre i v친r sekvens. For 친 gj칮re dette bruker transformere en **selvoppmerksomhetsmekanisme**, som i hovedsak er oppmerksomhet anvendt p친 samme sekvens som inngang og utgang. 칀 bruke selvoppmerksomhet lar oss ta hensyn til **kontekst** i setningen og se hvilke ord som er relaterte. For eksempel lar det oss se hvilke ord som refereres til av korreferanser, som *det*, og ogs친 ta konteksten i betraktning:

![](../../../../../translated_images/no/CoreferenceResolution.861924d6d384a7d6.webp)

> Bilde fra [Google Blog](https://research.googleblog.com/2017/08/transformer-novel-neural-network.html)

I transformere bruker vi **Multi-Head Attention** for 친 gi nettverket kraften til 친 fange flere forskjellige typer avhengigheter, f.eks. langvarige vs. kortvarige ordrelasjoner, korreferanse vs. noe annet, osv.

[TensorFlow Notebook](TransformersTF.ipynb) inneholder flere detaljer om implementeringen av transformer-lag.

### Enkoder-Dekoder Oppmerksomhet

I transformere brukes oppmerksomhet p친 to steder:

* For 친 fange m칮nstre i inngangsteksten ved hjelp av selvoppmerksomhet.
* For 친 utf칮re sekvensoversettelse - det er oppmerksomhetslaget mellom enkoder og dekoder.

Enkoder-dekoder oppmerksomhet er veldig lik oppmerksomhetsmekanismen som brukes i RNN-er, som beskrevet i begynnelsen av denne delen. Denne animerte diagrammet forklarer rollen til enkoder-dekoder oppmerksomhet.

![Animerte GIF som viser hvordan evalueringene utf칮res i transformermodeller.](../../../../../lessons/5-NLP/18-Transformers/images/transformer-animated-explanation.gif)

Siden hver inngangsposisjon kartlegges uavhengig til hver utgangsposisjon, kan transformere parallellisere bedre enn RNN-er, noe som muliggj칮r mye st칮rre og mer uttrykksfulle spr친kmodeller. Hver oppmerksomhetshode kan brukes til 친 l칝re forskjellige relasjoner mellom ord som forbedrer nedstr칮ms oppgaver innen naturlig spr친kbehandling.

## BERT

**BERT** (Bidirectional Encoder Representations from Transformers) er et veldig stort flerlags transformernettverk med 12 lag for *BERT-base*, og 24 for *BERT-large*. Modellen er f칮rst forh친ndstrent p친 en stor tekstkorpus (Wikipedia + b칮ker) ved hjelp av usupervisert trening (predikere maskerte ord i en setning). Under forh친ndstreningen absorberer modellen betydelige niv친er av spr친kforst친else som deretter kan utnyttes med andre datasett ved hjelp av finjustering. Denne prosessen kalles **transfer learning**.

![bilde fra http://jalammar.github.io/illustrated-bert/](../../../../../translated_images/no/jalammarBERT-language-modeling-masked-lm.34f113ea5fec4362.webp)

> Bilde [kilde](http://jalammar.github.io/illustrated-bert/)

## 九꽲잺 칒velser: Transformere

Fortsett l칝ringen i f칮lgende notatb칮ker:

* [Transformere i PyTorch](TransformersPyTorch.ipynb)
* [Transformere i TensorFlow](TransformersTF.ipynb)

## Konklusjon

I denne leksjonen l칝rte du om Transformere og Oppmerksomhetsmekanismer, alle essensielle verkt칮y i NLP-verkt칮ykassen. Det finnes mange varianter av transformerarkitekturer, inkludert BERT, DistilBERT, BigBird, OpenGPT3 og flere som kan finjusteres. [HuggingFace-pakken](https://github.com/huggingface/) gir et bibliotek for trening av mange av disse arkitekturene med b친de PyTorch og TensorFlow.

## 游 Utfordring

## [Quiz etter forelesning](https://ff-quizzes.netlify.app/en/ai/quiz/36)

## Gjennomgang og Selvstudium

* [Bloggpost](https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/), som forklarer den klassiske [Attention is all you need](https://arxiv.org/abs/1706.03762)-artikkelen om transformere.
* [En serie bloggposter](https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452) om transformere, som forklarer arkitekturen i detalj.

## [Oppgave](assignment.md)

---

