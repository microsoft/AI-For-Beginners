# Innebygginger

## [Quiz f√∏r forelesning](https://ff-quizzes.netlify.app/en/ai/quiz/27)

N√•r vi trente klassifikatorer basert p√• BoW eller TF/IDF, jobbet vi med h√∏y-dimensjonale bag-of-words-vektorer med lengde `vocab_size`, og vi konverterte eksplisitt fra lav-dimensjonale posisjonsrepresentasjonsvektorer til sparsomme √©n-hot-representasjoner. Denne √©n-hot-representasjonen er imidlertid ikke minneeffektiv. I tillegg behandles hvert ord uavhengig av hverandre, dvs. √©n-hot-kodede vektorer uttrykker ingen semantisk likhet mellom ord.

Ideen med **innebygging** er √• representere ord med lavere-dimensjonale tette vektorer, som p√• en eller annen m√•te reflekterer den semantiske betydningen av et ord. Vi vil senere diskutere hvordan man bygger meningsfulle ordinnebygginger, men for n√• kan vi bare tenke p√• innebygginger som en m√•te √• redusere dimensjonaliteten til en ordvektor.

S√•, innebyggingslaget vil ta et ord som input og produsere en output-vektor med spesifisert `embedding_size`. P√• en m√•te er det veldig likt et `Linear`-lag, men i stedet for √• ta en √©n-hot-kodet vektor, vil det kunne ta et ordnummer som input, slik at vi kan unng√• √• lage store √©n-hot-kodede vektorer.

Ved √• bruke et innebyggingslag som f√∏rste lag i v√•rt klassifikatornettverk, kan vi bytte fra en bag-of-words til **embedding bag**-modell, hvor vi f√∏rst konverterer hvert ord i teksten v√•r til tilsvarende innebygging, og deretter beregner en aggregatfunksjon over alle disse innebyggingene, som `sum`, `average` eller `max`.  

![Bilde som viser en innebyggingsklassifikator for fem sekvensord.](../../../../../translated_images/no/embedding-classifier-example.b77f021a7ee67eee.webp)

> Bilde av forfatteren

## ‚úçÔ∏è √òvelser: Innebygginger

Fortsett l√¶ringen i f√∏lgende notatb√∏ker:
* [Innebygginger med PyTorch](EmbeddingsPyTorch.ipynb)
* [Innebygginger med TensorFlow](EmbeddingsTF.ipynb)

## Semantiske innebygginger: Word2Vec

Mens innebyggingslaget l√¶rte √• kartlegge ord til vektorrepresentasjon, hadde denne representasjonen imidlertid ikke n√∏dvendigvis mye semantisk mening. Det ville v√¶rt fint √• l√¶re en vektorrepresentasjon slik at lignende ord eller synonymer tilsvarer vektorer som er n√¶r hverandre i henhold til en eller annen vektordistanse (f.eks. Euklidsk distanse).

For √• oppn√• dette m√• vi forh√•ndstrene innebyggingsmodellen v√•r p√• en stor samling tekst p√• en spesifikk m√•te. En m√•te √• trene semantiske innebygginger p√• kalles [Word2Vec](https://en.wikipedia.org/wiki/Word2vec). Det er basert p√• to hovedarkitekturer som brukes til √• produsere en distribuert representasjon av ord:

 - **Continuous bag-of-words** (CBoW) ‚Äî i denne arkitekturen trener vi modellen til √• forutsi et ord fra den omkringliggende konteksten. Gitt ngrammet $(W_{-2},W_{-1},W_0,W_1,W_2)$, er m√•let for modellen √• forutsi $W_0$ fra $(W_{-2},W_{-1},W_1,W_2)$.
 - **Continuous skip-gram** er motsatt av CBoW. Modellen bruker det omkringliggende vinduet av kontekstord til √• forutsi det n√•v√¶rende ordet.

CBoW er raskere, mens skip-gram er tregere, men gj√∏r en bedre jobb med √• representere sjeldne ord.

![Bilde som viser b√•de CBoW- og Skip-Gram-algoritmer for √• konvertere ord til vektorer.](../../../../../translated_images/no/example-algorithms-for-converting-words-to-vectors.fbe9207a726922f6.webp)

> Bilde fra [denne artikkelen](https://arxiv.org/pdf/1301.3781.pdf)

Forh√•ndstrente Word2Vec-innebygginger (samt andre lignende modeller, som GloVe) kan ogs√• brukes i stedet for innebyggingslag i nevrale nettverk. Imidlertid m√• vi h√•ndtere vokabularer, fordi vokabularet som ble brukt til √• forh√•ndstrene Word2Vec/GloVe sannsynligvis vil avvike fra vokabularet i v√•r tekstkorpus. Ta en titt p√• notatb√∏kene ovenfor for √• se hvordan dette problemet kan l√∏ses.

## Kontekstuelle innebygginger

En viktig begrensning med tradisjonelle forh√•ndstrente innebyggingsrepresentasjoner som Word2Vec er problemet med ordsanse-diskriminering. Mens forh√•ndstrente innebygginger kan fange noe av betydningen av ord i kontekst, er hver mulig betydning av et ord kodet inn i den samme innebyggingen. Dette kan skape problemer i nedstr√∏msmodeller, siden mange ord, som ordet 'play', har forskjellige betydninger avhengig av konteksten de brukes i.

For eksempel har ordet 'play' i disse to forskjellige setningene ganske forskjellige betydninger:

- Jeg dro p√• en **forestilling** p√• teateret.
- John vil **leke** med vennene sine.

De forh√•ndstrente innebyggingene ovenfor representerer begge disse betydningene av ordet 'play' i den samme innebyggingen. For √• overvinne denne begrensningen m√• vi bygge innebygginger basert p√• **spr√•kmodellen**, som er trent p√• en stor tekstkorpus og *vet* hvordan ord kan settes sammen i forskjellige kontekster. Diskusjon om kontekstuelle innebygginger er utenfor rammen for denne oppl√¶ringen, men vi vil komme tilbake til dem n√•r vi snakker om spr√•kmodeller senere i kurset.

## Konklusjon

I denne leksjonen oppdaget du hvordan du kan bygge og bruke innebyggingslag i TensorFlow og Pytorch for bedre √• reflektere den semantiske betydningen av ord.

## üöÄ Utfordring

Word2Vec har blitt brukt til noen interessante applikasjoner, inkludert generering av sangtekster og poesi. Ta en titt p√• [denne artikkelen](https://www.politetype.com/blog/word2vec-color-poems) som viser hvordan forfatteren brukte Word2Vec til √• generere poesi. Se ogs√• [denne videoen av Dan Shiffmann](https://www.youtube.com/watch?v=LSS_bos_TPI&ab_channel=TheCodingTrain) for √• oppdage en annen forklaring p√• denne teknikken. Pr√∏v deretter √• bruke disse teknikkene p√• din egen tekstkorpus, kanskje hentet fra Kaggle.

## [Quiz etter forelesning](https://ff-quizzes.netlify.app/en/ai/quiz/28)

## Gjennomgang & Selvstudium

Les gjennom denne artikkelen om Word2Vec: [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf)

## [Oppgave: Notatb√∏ker](assignment.md)

---

