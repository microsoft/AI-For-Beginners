# Representere tekst som tensorer

## [Quiz f칮r forelesning](https://ff-quizzes.netlify.app/en/ai/quiz/25)

## Tekstklassifisering

I den f칮rste delen av denne seksjonen vil vi fokusere p친 oppgaven **tekstklassifisering**. Vi skal bruke [AG News](https://www.kaggle.com/amananandrai/ag-news-classification-dataset)-datasettet, som inneholder nyhetsartikler som f칮lgende:

* Kategori: Sci/Tech
* Tittel: Ky. Company Wins Grant to Study Peptides (AP)
* Kropp: AP - Et selskap grunnlagt av en kjemiforsker ved University of Louisville vant et stipend for 친 utvikle...

M친let v친rt vil v칝re 친 klassifisere nyhetsartikkelen i en av kategoriene basert p친 teksten.

## Representere tekst

Hvis vi 칮nsker 친 l칮se oppgaver innen Natural Language Processing (NLP) med nevrale nettverk, trenger vi en m친te 친 representere tekst som tensorer. Datamaskiner representerer allerede teksttegn som tall som kartlegger til fonter p친 skjermen din ved hjelp av kodinger som ASCII eller UTF-8.

<img alt="Bilde som viser diagram som kartlegger et tegn til ASCII- og bin칝rrepresentasjon" src="../../../../../translated_images/no/ascii-character-map.18ed6aa7f3b0a7ff.webp" width="50%"/>

> [Bildekilde](https://www.seobility.net/en/wiki/ASCII)

Som mennesker forst친r vi hva hver bokstav **representerer**, og hvordan alle tegnene kommer sammen for 친 danne ordene i en setning. Datamaskiner har imidlertid ikke en slik forst친else av seg selv, og det nevrale nettverket m친 l칝re betydningen under trening.

Derfor kan vi bruke forskjellige tiln칝rminger n친r vi representerer tekst:

* **Tegn-niv친 representasjon**, der vi representerer tekst ved 친 behandle hvert tegn som et tall. Gitt at vi har *C* forskjellige tegn i tekstkorpuset v친rt, vil ordet *Hello* bli representert av en 5x*C* tensor. Hver bokstav vil tilsvare en tensor-kolonne i en one-hot encoding.
* **Ord-niv친 representasjon**, der vi lager et **vokabular** av alle ord i teksten v친r, og deretter representerer ord ved hjelp av one-hot encoding. Denne tiln칝rmingen er p친 en m친te bedre, fordi hver bokstav i seg selv ikke har mye mening, og ved 친 bruke h칮yere niv친 semantiske konsepter - ord - forenkler vi oppgaven for det nevrale nettverket. Men gitt den store ordbokst칮rrelsen, m친 vi h친ndtere h칮y-dimensjonale sparsomme tensorer.

Uansett representasjon, m친 vi f칮rst konvertere teksten til en sekvens av **tokens**, der 칠n token er enten et tegn, et ord, eller noen ganger til og med en del av et ord. Deretter konverterer vi tokenet til et tall, vanligvis ved hjelp av **vokabular**, og dette tallet kan mates inn i et nevralt nettverk ved hjelp av one-hot encoding.

## N-Grams

I naturlig spr친k kan den presise betydningen av ord bare bestemmes i kontekst. For eksempel er betydningene av *nevralt nettverk* og *fiskegarn* helt forskjellige. En av m친tene 친 ta dette i betraktning p친 er 친 bygge modellen v친r p친 par av ord, og betrakte ordpar som separate vokabular-tokens. P친 denne m친ten vil setningen *I like to go fishing* bli representert av f칮lgende sekvens av tokens: *I like*, *like to*, *to go*, *go fishing*. Problemet med denne tiln칝rmingen er at ordbokst칮rrelsen 칮ker betydelig, og kombinasjoner som *go fishing* og *go shopping* presenteres av forskjellige tokens, som ikke deler noen semantisk likhet til tross for samme verb.

I noen tilfeller kan vi vurdere 친 bruke tri-grams -- kombinasjoner av tre ord -- ogs친. Dermed kalles denne tiln칝rmingen ofte **n-grams**. Det gir ogs친 mening 친 bruke n-grams med tegn-niv친 representasjon, der n-grams grovt sett vil tilsvare forskjellige stavelser.

## Bag-of-Words og TF/IDF

N친r vi l칮ser oppgaver som tekstklassifisering, m친 vi kunne representere tekst med 칠n vektor av fast st칮rrelse, som vi vil bruke som input til den endelige tette klassifisereren. En av de enkleste m친tene 친 gj칮re dette p친 er 친 kombinere alle individuelle ordrepresentasjoner, f.eks. ved 친 legge dem sammen. Hvis vi legger sammen one-hot encodingene av hvert ord, ender vi opp med en frekvensvektor som viser hvor mange ganger hvert ord vises i teksten. En slik representasjon av tekst kalles **bag of words** (BoW).

<img src="../../../../../translated_images/no/bow.3811869cff59368d.webp" width="90%"/>

> Bilde av forfatteren

En BoW representerer i hovedsak hvilke ord som vises i teksten og i hvilke mengder, noe som faktisk kan v칝re en god indikasjon p친 hva teksten handler om. For eksempel vil en nyhetsartikkel om politikk sannsynligvis inneholde ord som *president* og *land*, mens en vitenskapelig publikasjon vil ha noe som *kollider*, *oppdaget*, osv. Dermed kan ordfrekvenser i mange tilfeller v칝re en god indikator p친 tekstinnhold.

Problemet med BoW er at visse vanlige ord, som *og*, *er*, osv. vises i de fleste tekster, og de har h칮yest frekvenser, noe som maskerer ut ordene som virkelig er viktige. Vi kan redusere viktigheten av disse ordene ved 친 ta hensyn til frekvensen som ord forekommer med i hele dokumentkolleksjonen. Dette er hovedideen bak TF/IDF-tiln칝rmingen, som er dekket mer detaljert i notatb칮kene som er vedlagt denne leksjonen.

Ingen av disse tiln칝rmingene kan imidlertid fullt ut ta hensyn til **semantikken** i teksten. Vi trenger mer kraftige nevrale nettverksmodeller for 친 gj칮re dette, som vi vil diskutere senere i denne seksjonen.

## 九꽲잺 칒velser: Tekstrepresentasjon

Fortsett l칝ringen din i f칮lgende notatb칮ker:

* [Tekstrepresentasjon med PyTorch](TextRepresentationPyTorch.ipynb)
* [Tekstrepresentasjon med TensorFlow](TextRepresentationTF.ipynb)

## Konklusjon

S친 langt har vi studert teknikker som kan legge frekvensvekt til forskjellige ord. De er imidlertid ikke i stand til 친 representere mening eller rekkef칮lge. Som den ber칮mte lingvisten J. R. Firth sa i 1935, "Den fullstendige betydningen av et ord er alltid kontekstuelt, og ingen studie av mening utenfor kontekst kan tas seri칮st." Senere i kurset vil vi l칝re hvordan vi kan fange kontekstuell informasjon fra tekst ved hjelp av spr친kmodellering.

## 游 Utfordring

Pr칮v noen andre 칮velser ved hjelp av bag-of-words og forskjellige datamodeller. Du kan bli inspirert av denne [konkurransen p친 Kaggle](https://www.kaggle.com/competitions/word2vec-nlp-tutorial/overview/part-1-for-beginners-bag-of-words)

## [Quiz etter forelesning](https://ff-quizzes.netlify.app/en/ai/quiz/26)

## Gjennomgang og selvstudium

칒v p친 ferdighetene dine med tekstinnbygging og bag-of-words-teknikker p친 [Microsoft Learn](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-pytorch/?WT.mc_id=academic-77998-cacaste)

## [Oppgave: Notatb칮ker](assignment.md)

---

