# Korduvad Neuraalv√µrgud

## [Eelloengu viktoriin](https://ff-quizzes.netlify.app/en/ai/quiz/31)

Eelnevates osades oleme kasutanud tekstide rikkalikke semantilisi esitusi ja lihtsat lineaarset klassifikaatorit, mis p√µhineb sisenditel. Selline arhitektuur suudab tabada lause s√µnade koondatud t√§hendust, kuid ei arvesta s√µnade **j√§rjekorda**, kuna sisendite koondamine eemaldab selle informatsiooni algtekstist. Kuna need mudelid ei suuda modelleerida s√µnade j√§rjestust, ei ole nad v√µimelised lahendama keerukamaid v√µi mitmet√§henduslikke √ºlesandeid, nagu tekstigeneratsioon v√µi k√ºsimustele vastamine.

Tekstij√§rjestuse t√§henduse tabamiseks peame kasutama teistsugust neuraalv√µrgu arhitektuuri, mida nimetatakse **korduvaks neuraalv√µrguks** ehk RNN-iks. RNN-is edastame oma lause l√§bi v√µrgu √ºhe s√ºmboli kaupa, ja v√µrk genereerib mingi **oleku**, mille edastame v√µrku uuesti koos j√§rgmise s√ºmboliga.

![RNN](../../../../../translated_images/et/rnn.27f5c29c53d727b5.webp)

> Pilt autori poolt

Arvestades sisendj√§rjestust X<sub>0</sub>,...,X<sub>n</sub>, loob RNN j√§rjestuse neuraalv√µrgu plokkidest ja treenib seda j√§rjestust otsast l√µpuni tagasileviku meetodil. Iga v√µrguplokk v√µtab sisendiks paari (X<sub>i</sub>,S<sub>i</sub>) ja genereerib tulemuseks S<sub>i+1</sub>. L√µplik olek S<sub>n</sub> v√µi (v√§ljund Y<sub>n</sub>) edastatakse lineaarsele klassifikaatorile, et saada tulemus. K√µik v√µrguplokid jagavad samu kaalusid ja neid treenitakse otsast l√µpuni √ºhe tagasileviku k√§igus.

Kuna olekuvektorid S<sub>0</sub>,...,S<sub>n</sub> edastatakse l√§bi v√µrgu, suudab see √µppida s√µnade j√§rjestusevahelisi s√µltuvusi. N√§iteks, kui s√µna *mitte* ilmub kuskil j√§rjestuses, v√µib v√µrk √µppida teatud elemente olekuvektoris eitama, mis viib eitamiseni.

> ‚úÖ Kuna k√µik RNN plokkide kaalusid √ºlaloleval pildil jagatakse, v√µib sama pilti kujutada √ºhe plokina (paremal), millel on korduv tagasisideahel, mis edastab v√µrgu v√§ljundoleku tagasi sisendisse.

## RNN-raku anatoomia

Vaatame, kuidas lihtne RNN-rakk on √ºles ehitatud. See v√µtab sisendiks eelneva oleku S<sub>i-1</sub> ja praeguse s√ºmboli X<sub>i</sub>, ning peab genereerima v√§ljundoleku S<sub>i</sub> (ja m√µnikord huvitab meid ka m√µni muu v√§ljund Y<sub>i</sub>, nagu generatiivsete v√µrkude puhul).

Lihtsal RNN-rakul on kaks kaalusid sisaldavat maatriksit: √ºks teisendab sisends√ºmbolit (nimetame seda W-ks) ja teine teisendab sisendolekut (H). Sellisel juhul arvutatakse v√µrgu v√§ljund valemiga &sigma;(W&times;X<sub>i</sub>+H&times;S<sub>i-1</sub>+b), kus &sigma; on aktivatsioonifunktsioon ja b on t√§iendav nihe.

<img alt="RNN-raku anatoomia" src="../../../../../translated_images/et/rnn-anatomy.79ee3f3920b3294b.webp" width="50%"/>

> Pilt autori poolt

Paljudel juhtudel edastatakse sisends√ºmbolid enne RNN-i sisestamist l√§bi sisendite v√§hendamise kihi, et v√§hendada dimensioonilisust. Sellisel juhul, kui sisendvektorite dimensioon on *emb_size* ja olekuvektor on *hid_size*, siis W suurus on *emb_size*&times;*hid_size* ja H suurus on *hid_size*&times;*hid_size*.

## Pikaajaline l√ºhim√§lu (LSTM)

√úks klassikaliste RNN-ide peamisi probleeme on nn **kustuvate gradientide** probleem. Kuna RNN-e treenitakse otsast l√µpuni √ºhe tagasileviku k√§igus, on neil raskusi vea edastamisega v√µrgu esimestesse kihtidesse, mist√µttu ei suuda v√µrk √µppida kaugemate s√ºmbolite vahelisi seoseid. √úks viis selle probleemi v√§ltimiseks on **eksplitsiitne oleku haldamine**, kasutades nn **v√§ravaid**. On kaks tuntud arhitektuuri: **Pikaajaline l√ºhim√§lu** (LSTM) ja **V√§ravaga relee√ºksus** (GRU).

![Pilt, mis n√§itab pikaajalise l√ºhim√§lu raku n√§idet](../../../../../lessons/5-NLP/16-RNN/images/long-short-term-memory-cell.svg)

> Pildi allikas TBD

LSTM-v√µrk on √ºles ehitatud sarnaselt RNN-ile, kuid seal edastatakse kihilt kihile kaks olekut: tegelik olek C ja peidetud vektor H. Igas √ºksuses √ºhendatakse peidetud vektor H<sub>i</sub> sisendiga X<sub>i</sub>, ja need kontrollivad, mis juhtub olekuga C **v√§ravate** kaudu. Iga v√§rav on sigmoid-aktivatsiooniga neuraalv√µrk (v√§ljund vahemikus [0,1]), mida v√µib m√µelda bitimaskina, kui seda korrutatakse olekuvektoriga. J√§rgnevad v√§ravad (vasakult paremale √ºlaloleval pildil):

* **Unustamisv√§rav** v√µtab peidetud vektori ja m√§√§rab, millised komponendid vektorist C tuleb unustada ja millised edasi anda.
* **Sisendv√§rav** v√µtab osa informatsiooni sisendist ja peidetud vektorist ning lisab selle olekusse.
* **V√§ljundv√§rav** teisendab oleku lineaarse kihi kaudu *tanh*-aktivatsiooniga, seej√§rel valib m√µned selle komponendid peidetud vektori H<sub>i</sub> abil, et genereerida uus olek C<sub>i+1</sub>.

Oleku C komponente v√µib m√µelda kui lippe, mida saab sisse ja v√§lja l√ºlitada. N√§iteks, kui kohtame j√§rjestuses nime *Alice*, v√µime eeldada, et see viitab naissoost tegelasele, ja t√µsta olekus lipu, et meil on naissoost nimis√µna lauses. Kui edaspidi kohtame fraasi *ja Tom*, t√µstame lipu, et meil on mitmuse nimis√µna. Seega olekut manipuleerides saame v√§idetavalt j√§lgida lause osade grammatilisi omadusi.

> ‚úÖ Suurep√§rane ressurss LSTM-i sisemuse m√µistmiseks on Christopher Olahi artikkel [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/).

## Kahepoolne ja mitmekihiline RNN

Oleme arutanud korduvaid v√µrke, mis t√∂√∂tavad √ºhes suunas, j√§rjestuse algusest l√µpuni. See tundub loomulik, kuna see sarnaneb viisiga, kuidas me loeme ja kuulame k√µnet. Kuid kuna paljudel praktilistel juhtudel on meil juhuslik juurdep√§√§s sisendj√§rjestusele, v√µib olla m√µistlik k√§ivitada korduv arvutus m√µlemas suunas. Selliseid v√µrke nimetatakse **kahepoolseteks** RNN-ideks. Kahepoolse v√µrgu puhul vajame kahte peidetud oleku vektorit, √ºks iga suuna jaoks.

Korduv v√µrk, olgu see √ºhesuunaline v√µi kahepoolne, tabab teatud mustreid j√§rjestuses ja suudab neid salvestada olekuvektorisse v√µi edastada v√§ljundisse. Nagu konvolutsiooniv√µrkude puhul, saame ehitada teise korduva kihi esimese peale, et tabada k√µrgema taseme mustreid ja ehitada madalama taseme mustritest, mida esimene kiht eraldas. See viib meid **mitmekihilise RNN-i** m√µisteni, mis koosneb kahest v√µi enamast korduvast v√µrgust, kus eelmise kihi v√§ljund edastatakse j√§rgmisele kihile sisendiks.

![Pilt, mis n√§itab mitmekihilist pikaajalise l√ºhim√§lu RNN-i](../../../../../translated_images/et/multi-layer-lstm.dd975e29bb2a59fe.webp)

*Pilt [sellest suurep√§rasest postitusest](https://towardsdatascience.com/from-a-lstm-cell-to-a-multilayer-lstm-network-with-pytorch-2899eb5696f3) autorilt Fernando L√≥pez*

## ‚úçÔ∏è Harjutused: Sisendid

J√§tka √µppimist j√§rgmistes m√§rkmikes:

* [RNN-id PyTorchiga](RNNPyTorch.ipynb)
* [RNN-id TensorFlowga](RNNTF.ipynb)

## Kokkuv√µte

Selles osas n√§gime, et RNN-e saab kasutada j√§rjestuste klassifitseerimiseks, kuid tegelikult suudavad nad lahendada palju rohkem √ºlesandeid, nagu tekstigeneratsioon, masint√µlge ja palju muud. Vaatame neid √ºlesandeid j√§rgmises osas.

## üöÄ V√§ljakutse

Loe l√§bi m√µningaid materjale LSTM-ide kohta ja m√µtle nende rakendustele:

- [Grid Long Short-Term Memory](https://arxiv.org/pdf/1507.01526v1.pdf)
- [Show, Attend and Tell: Neural Image Caption
Generation with Visual Attention](https://arxiv.org/pdf/1502.03044v2.pdf)

## [J√§relloengu viktoriin](https://ff-quizzes.netlify.app/en/ai/quiz/32)

## √úlevaade ja iseseisev √µppimine

- [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) autorilt Christopher Olah.

## [√úlesanne: M√§rkmikud](assignment.md)

---

**Lahti√ºtlus**:  
See dokument on t√µlgitud AI t√µlketeenuse [Co-op Translator](https://github.com/Azure/co-op-translator) abil. Kuigi p√º√ºame tagada t√§psust, palume arvestada, et automaatsed t√µlked v√µivad sisaldada vigu v√µi ebat√§psusi. Algne dokument selle algses keeles tuleks pidada autoriteetseks allikaks. Olulise teabe puhul soovitame kasutada professionaalset inimt√µlget. Me ei vastuta selle t√µlke kasutamisest tulenevate arusaamatuste v√µi valesti t√µlgenduste eest.