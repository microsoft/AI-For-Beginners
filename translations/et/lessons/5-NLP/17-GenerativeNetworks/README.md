# Generatiivsed v√µrgud

## [Eelloengu viktoriin](https://ff-quizzes.netlify.app/en/ai/quiz/33)

Korduvad n√§rviv√µrgud (RNN-id) ja nende v√§ratiga rakud, nagu Long Short Term Memory Cells (LSTM-id) ja Gated Recurrent Units (GRU-d), pakuvad mehhanismi keele modelleerimiseks, kuna nad suudavad √µppida s√µnade j√§rjestust ja ennustada j√§rgmise s√µna j√§rjestuses. See v√µimaldab kasutada RNN-e **generatiivseteks √ºlesanneteks**, nagu tavaline tekstigeneratsioon, masint√µlge ja isegi pildiallkirjade loomine.

> ‚úÖ M√µtle k√µikidele kordadele, kui oled kasu saanud generatiivsetest √ºlesannetest, n√§iteks teksti t√§iendamisest kirjutamise ajal. Uuri oma lemmikrakendusi, et n√§ha, kas nad kasutasid RNN-e.

RNN arhitektuuris, mida k√§sitlesime eelmises √ºksuses, genereeris iga RNN √ºksus j√§rgmise varjatud oleku v√§ljundina. Kuid me saame lisada igale korduv√ºksusele veel √ºhe v√§ljundi, mis v√µimaldab meil genereerida **j√§rjestuse** (mis on sama pikk kui algne j√§rjestus). Lisaks saame kasutada RNN √ºksusi, mis ei v√µta igal sammul sisendit, vaid v√µtavad ainult algse oleku vektori ja genereerivad seej√§rel v√§ljundite j√§rjestuse.

See v√µimaldab erinevaid n√§rviv√µrgu arhitektuure, mida on n√§idatud alloleval pildil:

![Pilt, mis n√§itab korduvate n√§rviv√µrkude levinud mustreid.](../../../../../translated_images/et/unreasonable-effectiveness-of-rnn.541ead816778f42d.webp)

> Pilt blogipostitusest [Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) autorilt [Andrej Karpaty](http://karpathy.github.io/)

* **√úks-√ºhele** on traditsiooniline n√§rviv√µrk √ºhe sisendi ja √ºhe v√§ljundiga
* **√úks-paljudele** on generatiivne arhitektuur, mis v√µtab √ºhe sisendv√§√§rtuse ja genereerib v√§ljundv√§√§rtuste j√§rjestuse. N√§iteks, kui tahame treenida **pildiallkirjade loomise** v√µrku, mis genereeriks pildi tekstilise kirjelduse, saame sisendiks pildi, edastame selle l√§bi CNN-i, et saada varjatud olek, ja seej√§rel genereerib korduv ahel allkirja s√µna-s√µnalt.
* **Palju-√ºhele** vastab RNN arhitektuuridele, mida kirjeldasime eelmises √ºksuses, n√§iteks tekstiklassifikatsioon.
* **Palju-paljudele**, v√µi **j√§rjestus-j√§rjestusele**, vastab √ºlesannetele nagu **masint√µlge**, kus esimene RNN kogub kogu sisendj√§rjestuse info varjatud olekusse ja teine RNN ahel lahtirullib selle oleku v√§ljundj√§rjestuseks.

Selles √ºksuses keskendume lihtsatele generatiivsetele mudelitele, mis aitavad meil teksti genereerida. Lihtsuse huvides kasutame t√§hem√§rgi tasemel tokeniseerimist.

Treename selle RNN-i teksti genereerimiseks samm-sammult. Igal sammul v√µtame t√§hem√§rkide j√§rjestuse pikkusega `nchars` ja palume v√µrgul genereerida j√§rgmise v√§ljundt√§hem√§rgi iga sisendt√§hem√§rgi jaoks:

![Pilt, mis n√§itab RNN-i n√§idet s√µna 'HELLO' genereerimisel.](../../../../../translated_images/et/rnn-generate.56c54afb52f9781d.webp)

Teksti genereerimisel (j√§reldamisel) alustame m√µne **alguspunktiga**, mis edastatakse RNN rakkude kaudu, et genereerida selle vaheolek, ja seej√§rel algab genereerimine sellest olekust. Genereerime √ºhe t√§hem√§rgi korraga ja edastame oleku ja genereeritud t√§hem√§rgi j√§rgmisele RNN rakule, et genereerida j√§rgmine, kuni oleme genereerinud piisavalt t√§hem√§rke.

<img src="../../../../../translated_images/et/rnn-generate-inf.5168dc65e0370eea.webp" width="60%"/>

> Pilt autorilt

## ‚úçÔ∏è Harjutused: Generatiivsed v√µrgud

J√§tka √µppimist j√§rgmistes m√§rkmikes:

* [Generatiivsed v√µrgud PyTorchiga](GenerativePyTorch.ipynb)
* [Generatiivsed v√µrgud TensorFlowga](GenerativeTF.ipynb)

## Pehme teksti genereerimine ja temperatuur

Iga RNN raku v√§ljundiks on t√§hem√§rkide t√µen√§osusjaotus. Kui v√µtame alati t√§hem√§rgi, millel on k√µrgeim t√µen√§osus, kui j√§rgmise t√§hem√§rgi genereerimisel, v√µib tekst sageli muutuda "ts√ºkliliseks", kordudes samade t√§hem√§rkide j√§rjestuste vahel, nagu selles n√§ites:

```
today of the second the company and a second the company ...
```

Kuid kui vaatame j√§rgmise t√§hem√§rgi t√µen√§osusjaotust, v√µib juhtuda, et m√µne k√µrgeima t√µen√§osusega t√§hem√§rgi vahe ei ole suur, n√§iteks √ºhel t√§hem√§rgil v√µib olla t√µen√§osus 0.2, teisel - 0.19 jne. N√§iteks, kui otsime j√§rgmist t√§hem√§rki j√§rjestuses '*play*', v√µib j√§rgmine t√§hem√§rk olla v√µrdselt kas t√ºhik v√µi **e** (nagu s√µnas *player*).

See viib meid j√§relduseni, et alati ei ole "√µiglane" valida t√§hem√§rki, millel on k√µrgem t√µen√§osus, sest teise k√µrgeima valimine v√µib siiski viia t√§hendusliku tekstini. M√µistlikum on **valida** t√§hem√§rke t√µen√§osusjaotusest, mille annab v√µrgu v√§ljund. Samuti saame kasutada parameetrit **temperatuur**, mis tasandab t√µen√§osusjaotust, kui tahame lisada rohkem juhuslikkust, v√µi muudab selle j√§rsemaks, kui tahame rohkem kinni pidada k√µrgeima t√µen√§osusega t√§hem√§rkidest.

Uuri, kuidas see pehme teksti genereerimine on rakendatud √ºlaltoodud m√§rkmikes.

## Kokkuv√µte

Kuigi teksti genereerimine v√µib olla kasulik iseenesest, on peamised eelised seotud v√µimega genereerida teksti RNN-idega m√µnest algsest tunnusvektorist. N√§iteks kasutatakse teksti genereerimist osana masint√µlkest (j√§rjestus-j√§rjestusele, sel juhul kasutatakse *kodeerija* olekuvektorit, et genereerida v√µi *dekodeerida* t√µlgitud s√µnum), v√µi pildi tekstilise kirjelduse genereerimisel (sel juhul p√§rineb tunnusvektor CNN ekstraktorist).

## üöÄ V√§ljakutse

V√µta m√µned Microsoft Learni √µppetunnid sellel teemal

* Teksti genereerimine [PyTorchiga](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-pytorch/6-generative-networks/?WT.mc_id=academic-77998-cacaste)/[TensorFlowga](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-tensorflow/5-generative-networks/?WT.mc_id=academic-77998-cacaste)

## [J√§relloengu viktoriin](https://ff-quizzes.netlify.app/en/ai/quiz/34)

## √úlevaade ja iseseisev √µppimine

Siin on m√µned artiklid, et laiendada oma teadmisi

* Erinevad l√§henemised teksti genereerimisele Markovi ahela, LSTM-i ja GPT-2-ga: [blogipostitus](https://towardsdatascience.com/text-generation-gpt-2-lstm-markov-chain-9ea371820e1e)
* Teksti genereerimise n√§idis [Kerase dokumentatsioonis](https://keras.io/examples/generative/lstm_character_level_text_generation/)

## [√úlesanne](lab/README.md)

Oleme n√§inud, kuidas genereerida teksti t√§hem√§rk-t√§hem√§rgi kaupa. Laboris uurid s√µna tasemel teksti genereerimist.

---

**Lahti√ºtlus**:  
See dokument on t√µlgitud AI t√µlketeenuse [Co-op Translator](https://github.com/Azure/co-op-translator) abil. Kuigi p√º√ºame tagada t√§psust, palume arvestada, et automaatsed t√µlked v√µivad sisaldada vigu v√µi ebat√§psusi. Algne dokument selle algses keeles tuleks pidada autoriteetseks allikaks. Olulise teabe puhul soovitame kasutada professionaalset inimt√µlget. Me ei vastuta selle t√µlke kasutamisest tulenevate arusaamatuste v√µi valesti t√µlgenduste eest.