{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sisestused\n",
    "\n",
    "Eelmises näites töötasime kõrgedimensionaalsete sõnakottide vektoritega, mille pikkus oli `vocab_size`, ja teisendasime madaladimensionaalsed positsiooniesituse vektorid otseselt hõredaks ühekuumaks esitusviisiks. See ühekuum esitusviis ei ole mälusäästlik. Lisaks käsitletakse iga sõna üksteisest sõltumatult, mistõttu ühekuum kodeeritud vektorid ei väljenda sõnade semantilisi sarnasusi.\n",
    "\n",
    "Selles osas jätkame **News AG** andmestiku uurimist. Alustuseks laadime andmed ja võtame mõned definitsioonid eelmisest osast.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "\n",
    "ds_train, ds_test = tfds.load('ag_news_subset').values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mis on embedding?\n",
    "\n",
    "**Embedding** tähendab sõnade esitamist madalama dimensiooniga tihedate vektoritega, mis peegeldavad sõna semantilist tähendust. Hiljem arutame, kuidas luua tähenduslikke sõnaembeddingeid, kuid praegu mõtleme embedding'ist lihtsalt kui viisist vähendada sõnavektori dimensioonilisust.\n",
    "\n",
    "Embedding-kiht võtab sisendiks sõna ja annab väljundiks vektori, mille suurus on määratud `embedding_size`. Mingis mõttes on see väga sarnane `Dense`-kihile, kuid selle asemel, et võtta sisendiks ühekuumkoodiga vektor, suudab see kasutada sõna numbrit.\n",
    "\n",
    "Kasutades embedding-kihti meie võrgu esimeseks kihiks, saame liikuda sõnakottide mudelilt **embedding-koti** mudelile, kus esmalt teisendame iga sõna tekstis vastavaks embedding'iks ja seejärel arvutame nende embedding'ite üle mingi koondfunktsiooni, näiteks `sum`, `average` või `max`.\n",
    "\n",
    "![Pilt, mis näitab embedding-klassiﬁkaatorit viie järjestikuse sõna jaoks.](../../../../../translated_images/et/embedding-classifier-example.b77f021a7ee67eee.webp)\n",
    "\n",
    "Meie klassifitseeriva närvivõrgu kihid on järgmised:\n",
    "\n",
    "* `TextVectorization` kiht, mis võtab sisendiks stringi ja annab väljundiks tokenite numbrite tensor. Määrame mõistliku sõnavara suuruse `vocab_size` ja ignoreerime harvemini kasutatavaid sõnu. Sisendi kuju on 1 ja väljundi kuju on $n$, kuna tulemuseks saame $n$ tokenit, millest igaüks sisaldab numbreid vahemikus 0 kuni `vocab_size`.\n",
    "* `Embedding` kiht, mis võtab $n$ numbrit ja vähendab iga numbri tihedaks vektoriks kindla pikkusega (näiteks 100 meie näites). Seega muudetakse sisendi tensor kujuga $n$ tensoriks kujuga $n\\times 100$.\n",
    "* Koondamise kiht, mis arvutab selle tensori keskmise esimese telje järgi, st arvutab kõigi $n$ sisendtensori keskmise, mis vastavad erinevatele sõnadele. Selle kihi rakendamiseks kasutame `Lambda`-kihti ja anname sellele funktsiooni keskmise arvutamiseks. Väljundil on kuju 100 ja see on kogu sisendjärjestuse numbriline esitus.\n",
    "* Lõplik `Dense` lineaarne klassifitseerija.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization (TextVec  (None, None)             0         \n",
      " torization)                                                     \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, None, 100)         3000000   \n",
      "                                                                 \n",
      " lambda (Lambda)             (None, 100)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4)                 404       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,000,404\n",
      "Trainable params: 3,000,404\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 30000\n",
    "batch_size = 128\n",
    "\n",
    "vectorizer = keras.layers.experimental.preprocessing.TextVectorization(max_tokens=vocab_size,input_shape=(1,))\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    vectorizer,    \n",
    "    keras.layers.Embedding(vocab_size,100),\n",
    "    keras.layers.Lambda(lambda x: tf.reduce_mean(x,axis=1)),\n",
    "    keras.layers.Dense(4, activation='softmax')\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`summary` väljatrükis, **output shape** veerus, tähistab esimene tensorimõõde `None` minibatch'i suurust ja teine mõõde tähistab tokenite järjestuse pikkust. Kõik minibatch'is olevad tokenite järjestused on erineva pikkusega. Järgmises osas arutame, kuidas sellega toime tulla.\n",
    "\n",
    "Nüüd treenime võrku:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training vectorizer\n",
      "938/938 [==============================] - 20s 20ms/step - loss: 0.7891 - acc: 0.8155 - val_loss: 0.4470 - val_acc: 0.8642\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22255515100>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_text(x):\n",
    "    return x['title']+' '+x['description']\n",
    "\n",
    "def tupelize(x):\n",
    "    return (extract_text(x),x['label'])\n",
    "\n",
    "print(\"Training vectorizer\")\n",
    "vectorizer.adapt(ds_train.take(500).map(extract_text))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'])\n",
    "model.fit(ds_train.map(tupelize).batch(batch_size),validation_data=ds_test.map(tupelize).batch(batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "> **Märkus**: me loome vektoreerija andmete alamhulga põhjal. Seda tehakse protsessi kiirendamiseks, kuid see võib viia olukorrani, kus kõik tekstis olevad tokenid ei ole sõnavaras esindatud. Sellisel juhul need tokenid jäetakse tähelepanuta, mis võib põhjustada veidi madalamat täpsust. Kuid päriselus annab tekstialamhulk sageli hea sõnavara hinnangu.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Töötamine muutuvate järjestuse suurustega\n",
    "\n",
    "Vaatame, kuidas treenimine minibatchides toimub. Eelnevas näites on sisendtensoril dimensioon 1 ja me kasutame 128-pikkuseid minibatche, nii et tensori tegelik suurus on $128 \\times 1$. Kuid iga lause tokenite arv on erinev. Kui rakendame `TextVectorization` kihti ühele sisendile, on tagastatud tokenite arv erinev, sõltuvalt sellest, kuidas tekst on tokeniseeritud:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 1 45], shape=(2,), dtype=int64)\n",
      "tf.Tensor([ 112 1271    1    3 1747  158], shape=(6,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer('Hello, world!'))\n",
    "print(vectorizer('I am glad to meet you!'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kuid kui rakendame vektoreerijat mitmele järjestusele, peab see tootma ristkülikukujulise tensori, seega täidab see kasutamata elemendid PAD-tokniga (mis meie puhul on null):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 6), dtype=int64, numpy=\n",
       "array([[   1,   45,    0,    0,    0,    0],\n",
       "       [ 112, 1271,    1,    3, 1747,  158]], dtype=int64)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer(['Hello, world!','I am glad to meet you!'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Siin näeme sisestusi:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 1.53059261e-02,  6.80514947e-02,  3.14026810e-02, ...,\n",
       "         -8.92002955e-02,  1.52911525e-04, -5.65562584e-02],\n",
       "        [ 2.57456154e-01,  2.79364467e-01, -2.03605562e-01, ...,\n",
       "         -2.07474351e-01,  8.31158683e-02, -2.03911960e-01],\n",
       "        [ 3.98201384e-02, -8.03454965e-03,  2.39790026e-02, ...,\n",
       "         -7.18549127e-04,  2.66963355e-02, -4.30646613e-02],\n",
       "        [ 3.98201384e-02, -8.03454965e-03,  2.39790026e-02, ...,\n",
       "         -7.18549127e-04,  2.66963355e-02, -4.30646613e-02],\n",
       "        [ 3.98201384e-02, -8.03454965e-03,  2.39790026e-02, ...,\n",
       "         -7.18549127e-04,  2.66963355e-02, -4.30646613e-02],\n",
       "        [ 3.98201384e-02, -8.03454965e-03,  2.39790026e-02, ...,\n",
       "         -7.18549127e-04,  2.66963355e-02, -4.30646613e-02]],\n",
       "\n",
       "       [[ 1.89674050e-01,  2.61548996e-01, -3.67433839e-02, ...,\n",
       "         -2.07366899e-01, -1.05442435e-01, -2.36952081e-01],\n",
       "        [ 6.16133213e-02,  1.80511594e-01,  9.77298319e-02, ...,\n",
       "         -5.46628237e-02, -1.07340455e-01, -1.06589928e-01],\n",
       "        [ 1.53059261e-02,  6.80514947e-02,  3.14026810e-02, ...,\n",
       "         -8.92002955e-02,  1.52911525e-04, -5.65562584e-02],\n",
       "        [-4.84890305e-02, -8.41715634e-02,  1.51529670e-01, ...,\n",
       "          1.28192469e-01, -7.77286515e-02,  1.26041949e-01],\n",
       "        [-4.17212099e-02, -5.60694858e-02,  4.08860669e-02, ...,\n",
       "          8.70475471e-02,  8.92383084e-02,  1.67974353e-01],\n",
       "        [ 2.85779923e-01,  4.57767487e-01,  4.52292450e-02, ...,\n",
       "         -1.97419018e-01, -2.04659685e-01, -2.79758364e-01]]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[1](vectorizer(['Hello, world!','I am glad to meet you!'])).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Märkus**: Täitematerjali hulga vähendamiseks on mõnel juhul mõistlik järjestada kõik andmekogumi järjestused kasvava pikkuse järjekorras (täpsemalt, vastavalt tokenite arvule). See tagab, et iga minibatch sisaldab sarnase pikkusega järjestusi.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantilised esitlused: Word2Vec\n",
    "\n",
    "Eelmises näites õppis sisendkiht sõnu vektoriteks teisendama, kuid need vektorid ei kandnud semantilist tähendust. Oleks kasulik õppida vektorite esitusviis, kus sarnased sõnad või sünonüümid vastavad vektoritele, mis on üksteisele lähedal mingi vektori kauguse (näiteks eukleidiline kaugus) järgi.\n",
    "\n",
    "Selleks peame oma sisendmudeli eelnevalt treenima suure tekstikogu peal, kasutades tehnikat nagu [Word2Vec](https://en.wikipedia.org/wiki/Word2vec). See põhineb kahel peamisel arhitektuuril, mida kasutatakse sõnade hajutatud esituse loomiseks:\n",
    "\n",
    " - **Järjepidev sõnakott** (Continuous bag-of-words, CBoW), kus mudelit treenitakse ennustama sõna ümbritseva konteksti põhjal. Arvestades ngrammi $(W_{-2},W_{-1},W_0,W_1,W_2)$, on mudeli eesmärk ennustada $W_0$ $(W_{-2},W_{-1},W_1,W_2)$ põhjal.\n",
    " - **Järjepidev vahelejätu-gramm** (Continuous skip-gram) on CBoW vastand. Mudel kasutab ümbritsevat kontekstiakent, et ennustada praegust sõna.\n",
    "\n",
    "CBoW on kiirem, samas kui vahelejätu-gramm on aeglasem, kuid esindab haruldasi sõnu paremini.\n",
    "\n",
    "![Pilt, mis näitab nii CBoW kui ka Skip-Gram algoritme sõnade vektoriteks teisendamiseks.](../../../../../translated_images/et/example-algorithms-for-converting-words-to-vectors.fbe9207a726922f6.webp)\n",
    "\n",
    "Et katsetada Word2Vec sisendit, mis on eelnevalt treenitud Google News andmestiku peal, saame kasutada **gensim** teeki. Allpool otsime sõnu, mis on kõige sarnasemad sõnale 'neural'.\n",
    "\n",
    "> **Märkus:** Kui loote esimest korda sõnavektoreid, võib nende allalaadimine võtta aega!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "w2v = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neuronal -> 0.7804799675941467\n",
      "neurons -> 0.7326500415802002\n",
      "neural_circuits -> 0.7252851724624634\n",
      "neuron -> 0.7174385190010071\n",
      "cortical -> 0.6941086649894714\n",
      "brain_circuitry -> 0.6923246383666992\n",
      "synaptic -> 0.6699118614196777\n",
      "neural_circuitry -> 0.6638563275337219\n",
      "neurochemical -> 0.6555314064025879\n",
      "neuronal_activity -> 0.6531826257705688\n"
     ]
    }
   ],
   "source": [
    "for w,p in w2v.most_similar('neural'):\n",
    "    print(f\"{w} -> {p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saame samuti sõnast vektori sisestuse välja võtta, et kasutada seda klassifitseerimismudeli treenimisel. Sisestusel on 300 komponenti, kuid siin näitame selguse huvides ainult vektori esimesi 20 komponenti:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01226807,  0.06225586,  0.10693359,  0.05810547,  0.23828125,\n",
       "        0.03686523,  0.05151367, -0.20703125,  0.01989746,  0.10058594,\n",
       "       -0.03759766, -0.1015625 , -0.15820312, -0.08105469, -0.0390625 ,\n",
       "       -0.05053711,  0.16015625,  0.2578125 ,  0.10058594, -0.25976562],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v['play'][:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suurepärane asi semantiliste sisendite juures on see, et saate vektorkodeeringut manipuleerida semantika põhjal. Näiteks võime paluda leida sõna, mille vektorrepresentatsioon on võimalikult lähedal sõnadele *kuningas* ja *naine*, ning võimalikult kaugel sõnast *mees*:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('queen', 0.7118192911148071)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar(positive=['king','woman'],negative=['man'])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Ülaltoodud näide kasutab mõningast sisemist GenSymi maagiat, kuid aluseks olev loogika on tegelikult üsna lihtne. Huvitav asi sisendvektorite juures on see, et saate sisendvektoritel teha tavalisi vektorioperatsioone, mis peegeldavad sõnade **tähenduste** operatsioone. Ülaltoodud näidet saab väljendada vektorioperatsioonide abil: arvutame vektori, mis vastab **KUNINGAS-MEES+NAINE** (operatsioonid `+` ja `-` tehakse vastavate sõnade vektorrepresentatsioonidel), ja seejärel leiame sõnastikust sellele vektorile kõige lähedasema sõna:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'queen'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the vector corresponding to kind-man+woman\n",
    "qvec = w2v['king']-1.7*w2v['man']+1.7*w2v['woman']\n",
    "# find the index of the closest embedding vector \n",
    "d = np.sum((w2v.vectors-qvec)**2,axis=1)\n",
    "min_idx = np.argmin(d)\n",
    "# find the corresponding word\n",
    "w2v.index_to_key[min_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NOTE**: Me pidime lisama väikese koefitsiendi *mees* ja *naine* vektoritele - proovi need eemaldada, et näha, mis juhtub.\n",
    "\n",
    "Kõige lähedasema vektori leidmiseks kasutame TensorFlow tööriistu, et arvutada kauguste vektor meie vektori ja kõigi sõnavara vektorite vahel, ning seejärel leiame minimaalse sõna indeksi, kasutades `argmin`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kuigi Word2Vec tundub suurepärane viis sõna semantika väljendamiseks, on sel mitmeid puudusi, sealhulgas järgmised:\n",
    "\n",
    "* Nii CBoW kui ka skip-gram mudelid on **ennustavad sisestused**, mis arvestavad ainult lokaalset konteksti. Word2Vec ei kasuta ära globaalset konteksti.\n",
    "* Word2Vec ei arvesta sõna **morfoloogiat**, st seda, et sõna tähendus võib sõltuda erinevatest sõna osadest, nagu näiteks tüvest.\n",
    "\n",
    "**FastText** püüab ületada teist piirangut ja täiendab Word2Vec-i, õppides vektoriesitusi iga sõna ja sõnas leiduvate tähemärkide n-grammide jaoks. Esituste väärtused keskmistatakse igal treeningusammul üheks vektoriks. Kuigi see lisab eeltreeningule palju täiendavat arvutustööd, võimaldab see sõna sisestustel kodeerida alam-sõna teavet.\n",
    "\n",
    "Teine meetod, **GloVe**, kasutab sõna sisestuste jaoks teistsugust lähenemist, mis põhineb sõna-konteksti maatriksi faktorisatsioonil. Kõigepealt koostab see suure maatriksi, mis loendab sõnade esinemiste arvu erinevates kontekstides, ja seejärel püüab esitada seda maatriksit madalamates dimensioonides viisil, mis minimeerib rekonstruktsiooni kaotuse.\n",
    "\n",
    "Gensim teek toetab neid sõna sisestusi ning saate nendega katsetada, muutes ülaltoodud mudeli laadimise koodi.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eelõpetatud sisendvektorite kasutamine Kerases\n",
    "\n",
    "Me saame ülaltoodud näidet muuta nii, et täidame oma sisendkihis oleva maatriksi semantiliste sisendvektoritega, nagu Word2Vec. Eelõpetatud sisendvektori ja tekstikorpuse sõnavarad ei pruugi kokku langeda, seega peame valima ühe. Siin uurime kahte võimalikku varianti: tokeniseerija sõnavara kasutamine ja Word2Vec sisendvektorite sõnavara kasutamine.\n",
    "\n",
    "### Tokeniseerija sõnavara kasutamine\n",
    "\n",
    "Tokeniseerija sõnavara kasutamisel on osadel sõnavara sõnadel vastavad Word2Vec sisendvektorid, kuid osad võivad puududa. Arvestades, et meie sõnavara suurus on `vocab_size` ja Word2Vec sisendvektori pikkus on `embed_size`, esitatakse sisendkiht kaalu maatriksina kujuga `vocab_size`$\\times$`embed_size`. Täidame selle maatriksi, läbides sõnavara:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding size: 300\n",
      "Populating matrix, this will take some time...Done, found 4551 words, 784 words missing\n"
     ]
    }
   ],
   "source": [
    "embed_size = len(w2v.get_vector('hello'))\n",
    "print(f'Embedding size: {embed_size}')\n",
    "\n",
    "vocab = vectorizer.get_vocabulary()\n",
    "W = np.zeros((vocab_size,embed_size))\n",
    "print('Populating matrix, this will take some time...',end='')\n",
    "found, not_found = 0,0\n",
    "for i,w in enumerate(vocab):\n",
    "    try:\n",
    "        W[i] = w2v.get_vector(w)\n",
    "        found+=1\n",
    "    except:\n",
    "        # W[i] = np.random.normal(0.0,0.3,size=(embed_size,))\n",
    "        not_found+=1\n",
    "\n",
    "print(f\"Done, found {found} words, {not_found} words missing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sõnade jaoks, mis ei ole Word2Vec sõnavaras, võime need kas jätta nullideks või genereerida juhusliku vektori.\n",
    "\n",
    "Nüüd saame määratleda sisestuskihi eeltreenitud kaaludega:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = keras.layers.Embedding(vocab_size,embed_size,weights=[W],trainable=False)\n",
    "model = keras.models.Sequential([\n",
    "    vectorizer, emb,\n",
    "    keras.layers.Lambda(lambda x: tf.reduce_mean(x,axis=1)),\n",
    "    keras.layers.Dense(4, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nüüd treenime oma mudelit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 10s 10ms/step - loss: 1.1075 - acc: 0.7822 - val_loss: 0.9134 - val_acc: 0.8175\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2220226ef10>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'])\n",
    "model.fit(ds_train.map(tupelize).batch(batch_size),\n",
    "          validation_data=ds_test.map(tupelize).batch(batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note**: Pange tähele, et määrame `trainable=False`, kui loome `Embedding`, mis tähendab, et me ei treeni Embedding-kihi uuesti. See võib põhjustada veidi madalama täpsuse, kuid kiirendab treenimisprotsessi.\n",
    "\n",
    "### Embedding-sõnavara kasutamine\n",
    "\n",
    "Eelneva lähenemise üks probleem on see, et TextVectorization ja Embedding kasutavad erinevaid sõnavarasid. Selle probleemi lahendamiseks saame kasutada ühte järgmistest lahendustest:\n",
    "* Treenida Word2Vec mudel uuesti meie sõnavara põhjal.\n",
    "* Laadida meie andmestik, kasutades eelnevalt treenitud Word2Vec mudeli sõnavara. Sõnavara, mida kasutatakse andmestiku laadimiseks, saab määrata laadimise ajal.\n",
    "\n",
    "Viimane lähenemine tundub lihtsam, seega rakendame seda. Kõigepealt loome `TextVectorization` kihi määratud sõnavaraga, mis on võetud Word2Vec embedding'utest:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(w2v.vocab.keys())\n",
    "vectorizer = keras.layers.experimental.preprocessing.TextVectorization(input_shape=(1,))\n",
    "vectorizer.set_vocabulary(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gensimi sõnaembeddingsi teek sisaldab mugavat funktsiooni `get_keras_embeddings`, mis loob automaatselt vastava Keras embeddingsi kihi teie jaoks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "938/938 [==============================] - 20s 14ms/step - loss: 1.3377 - acc: 0.4978 - val_loss: 1.2995 - val_acc: 0.5647\n",
      "Epoch 2/5\n",
      "938/938 [==============================] - 10s 10ms/step - loss: 1.2587 - acc: 0.5722 - val_loss: 1.2339 - val_acc: 0.5842\n",
      "Epoch 3/5\n",
      "938/938 [==============================] - 10s 10ms/step - loss: 1.1980 - acc: 0.5884 - val_loss: 1.1826 - val_acc: 0.5954\n",
      "Epoch 4/5\n",
      "938/938 [==============================] - 12s 13ms/step - loss: 1.1503 - acc: 0.6002 - val_loss: 1.1417 - val_acc: 0.6018\n",
      "Epoch 5/5\n",
      "938/938 [==============================] - 11s 12ms/step - loss: 1.1120 - acc: 0.6097 - val_loss: 1.1083 - val_acc: 0.6104\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2220ccb81c0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    vectorizer, \n",
    "    w2v.get_keras_embedding(train_embeddings=False),\n",
    "    keras.layers.Lambda(lambda x: tf.reduce_mean(x,axis=1)),\n",
    "    keras.layers.Dense(4, activation='softmax')\n",
    "])\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'])\n",
    "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128),epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Üks põhjusi, miks me ei näe suuremat täpsust, on see, et mõned sõnad meie andmestikust puuduvad eeltreenitud GloVe sõnavarast ja seetõttu neid sisuliselt eiratakse. Selle ületamiseks saame treenida omaenda sõnaembeddingsid, mis põhinevad meie andmestikul.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kontekstuaalsed sisendvektorid\n",
    "\n",
    "Üks traditsiooniliste eeltreenitud sisendvektorite, nagu Word2Vec, peamisi piiranguid on see, et kuigi need suudavad tabada sõna mingit tähendust, ei suuda need eristada erinevaid tähendusi. See võib põhjustada probleeme järgnevatel mudelitel.\n",
    "\n",
    "Näiteks sõnal 'play' (mängima/etendus) on erinev tähendus nendes kahes lauses:\n",
    "- Ma käisin teatris **etendust** vaatamas.\n",
    "- John tahab **mängida** oma sõpradega.\n",
    "\n",
    "Eeltreenitud sisendvektorid, millest me rääkisime, esindavad mõlemat tähendust sõnast 'play' sama sisendvektoriga. Selle piirangu ületamiseks peame looma sisendvektorid, mis põhinevad **keelemudelil**, mis on treenitud suurel tekstikorpusel ja *teab*, kuidas sõnu saab erinevates kontekstides kokku panna. Kontekstuaalsete sisendvektorite arutelu jääb selle õpetuse ulatusest välja, kuid me tuleme nende juurde tagasi, kui räägime keelemudelitest järgmises osas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Lahtiütlus**:  \nSee dokument on tõlgitud AI tõlketeenuse [Co-op Translator](https://github.com/Azure/co-op-translator) abil. Kuigi püüame tagada täpsust, palume arvestada, et automaatsed tõlked võivad sisaldada vigu või ebatäpsusi. Algne dokument selle algses keeles tuleks pidada autoriteetseks allikaks. Olulise teabe puhul soovitame kasutada professionaalset inimtõlget. Me ei vastuta selle tõlke kasutamisest tulenevate arusaamatuste või valesti tõlgenduste eest.\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0cb620c6d4b9f7a635928804c26cf22403d89d98d79684e4529119355ee6d5a5"
  },
  "kernel_info": {
   "name": "conda-env-py37_tensorflow-py"
  },
  "kernelspec": {
   "display_name": "py37_tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "coopTranslator": {
   "original_hash": "b859482be7f61d1eadc2c6a2720a37e4",
   "translation_date": "2025-10-11T12:43:45+00:00",
   "source_file": "lessons/5-NLP/14-Embeddings/EmbeddingsTF.ipynb",
   "language_code": "et"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}