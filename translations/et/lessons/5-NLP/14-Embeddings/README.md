# Sisestused

## [Eelloengu viktoriin](https://ff-quizzes.netlify.app/en/ai/quiz/27)

Kui treenisime klassifitseerijaid, mis p√µhinesid BoW-l v√µi TF/IDF-il, t√∂√∂tasime k√µrgedimensiooniliste s√µnakottide vektoritega, mille pikkus oli `vocab_size`, ja teisendasime madaladimensioonilised positsioonirepresentatsiooni vektorid selges√µnaliselt h√µredateks √ºhekuumadeks representatsioonideks. See √ºhekuum representatsioon ei ole aga m√§lus√§√§stlik. Lisaks k√§sitletakse iga s√µna √ºksteisest s√µltumatult, st √ºhekuumad kodeeritud vektorid ei v√§ljenda s√µnade semantilist sarnasust.

**Sisestuse** idee seisneb selles, et s√µnu esitatakse madaladimensiooniliste tihedate vektoritega, mis mingil moel peegeldavad s√µna semantilist t√§hendust. Hiljem arutame, kuidas luua t√§henduslikke s√µnasisestusi, kuid praegu m√µtleme sisestustest kui viisist v√§hendada s√µnavektori dimensionaalsust.

Sisestuskihi √ºlesanne on v√µtta s√µna sisendina ja anda v√§ljundvektor kindlaksm√§√§ratud `embedding_size`-ga. Mingis m√µttes on see v√§ga sarnane `Linear` kihiga, kuid selle asemel, et v√µtta √ºhekuum kodeeritud vektor, suudab see v√µtta s√µna numbri sisendina, v√µimaldades meil v√§ltida suurte √ºhekuumade vektorite loomist.

Kasutades sisestuskihti meie klassifitseerimisv√µrgu esimese kihina, saame liikuda s√µnakotilt **sisestuskoti** mudelile, kus esmalt teisendame iga s√µna meie tekstis vastavaks sisestuseks ja seej√§rel arvutame nende sisestuste √ºle mingi koondfunktsiooni, nagu `sum`, `average` v√µi `max`.  

![Pilt, mis n√§itab sisestuskihi klassifitseerijat viie j√§rjestikuse s√µna jaoks.](../../../../../translated_images/et/embedding-classifier-example.b77f021a7ee67eee.webp)

> Pilt autori poolt

## ‚úçÔ∏è Harjutused: Sisestused

J√§tka √µppimist j√§rgmistes m√§rkmikes:
* [Sisestused PyTorchiga](EmbeddingsPyTorch.ipynb)
* [Sisestused TensorFlowga](EmbeddingsTF.ipynb)

## Semantilised sisestused: Word2Vec

Kuigi sisestuskiht √µppis s√µnu vektorrepresentatsiooniks teisendama, ei pruugi see representatsioon tingimata omada palju semantilist t√§hendust. Oleks tore √µppida vektorrepresentatsiooni, kus sarnased s√µnad v√µi s√ºnon√º√ºmid vastavad vektoritele, mis on √ºksteisele l√§hedal mingi vektorkauguse (nt Eukleidese kauguse) m√µttes.

Selleks peame oma sisestusmudeli eelnevalt treenima suure tekstikogu peal spetsiifilisel viisil. √úks viis semantiliste sisestuste treenimiseks on [Word2Vec](https://en.wikipedia.org/wiki/Word2vec). See p√µhineb kahel peamisel arhitektuuril, mida kasutatakse s√µnade hajutatud representatsiooni loomiseks:

 - **J√§tkuv s√µnakott** (CBoW) ‚Äî selles arhitektuuris treenime mudelit ennustama s√µna √ºmbritseva konteksti p√µhjal. Arvestades ngrammi $(W_{-2},W_{-1},W_0,W_1,W_2)$, on mudeli eesm√§rk ennustada $W_0$ $(W_{-2},W_{-1},W_1,W_2)$ p√µhjal.
 - **J√§tkuv h√ºppegramm** on CBoW vastand. Mudel kasutab √ºmbritsevat kontekstiakent, et ennustada praegust s√µna.

CBoW on kiirem, samas kui h√ºppegramm on aeglasem, kuid teeb paremat t√∂√∂d harvaesinevate s√µnade esindamisel.

![Pilt, mis n√§itab nii CBoW kui ka h√ºppegrammi algoritme s√µnade vektoriteks teisendamiseks.](../../../../../translated_images/et/example-algorithms-for-converting-words-to-vectors.fbe9207a726922f6.webp)

> Pilt [sellest artiklist](https://arxiv.org/pdf/1301.3781.pdf)

Word2Vec eelnevalt treenitud sisestusi (samuti teisi sarnaseid mudeleid, nagu GloVe) saab kasutada sisestuskihi asemel n√§rviv√µrkudes. Siiski peame tegelema s√µnavaradega, kuna Word2Vec/GloVe eeltreenimiseks kasutatud s√µnavara erineb t√µen√§oliselt meie tekstikorpuse s√µnavarast. Vaata √ºlaltoodud m√§rkmikke, et n√§ha, kuidas seda probleemi lahendada.

## Kontekstuaalsed sisestused

Traditsiooniliste eeltreenitud sisestusrepresentatsioonide, nagu Word2Vec, √ºks peamisi piiranguid on s√µna t√§henduse eristamise probleem. Kuigi eeltreenitud sisestused suudavad tabada osa s√µnade t√§hendusest kontekstis, kodeeritakse iga s√µna v√µimalik t√§hendus samasse sisestusse. See v√µib p√µhjustada probleeme j√§rgnevatel mudelitel, kuna paljudel s√µnadel, nagu n√§iteks s√µnal 'play', on erinevad t√§hendused s√µltuvalt kontekstist, milles neid kasutatakse.

N√§iteks s√µnal 'play' on nendes kahes erinevas lauses √ºsna erinev t√§hendus:

- Ma k√§isin teatris **etendust** vaatamas.
- John tahab oma s√µpradega **m√§ngida**.

Eeltreenitud sisestused esindavad m√µlemat t√§hendust s√µnast 'play' samas sisestuses. Selle piirangu √ºletamiseks peame looma sisestusi, mis p√µhinevad **keelemudelil**, mis on treenitud suure tekstikorpuse peal ja *teab*, kuidas s√µnu saab erinevates kontekstides kokku panna. Kontekstuaalsete sisestuste arutamine j√§√§b selle √µpetuse ulatusest v√§lja, kuid tuleme nende juurde tagasi, kui r√§√§gime keelemudelitest hiljem kursuse jooksul.

## Kokkuv√µte

Selles √µppetunnis avastasite, kuidas luua ja kasutada sisestuskihte TensorFlow ja Pytorch abil, et paremini peegeldada s√µnade semantilisi t√§hendusi.

## üöÄ V√§ljakutse

Word2Vec-i on kasutatud huvitavate rakenduste jaoks, sealhulgas laulutekstide ja luule genereerimiseks. Vaata [seda artiklit](https://www.politetype.com/blog/word2vec-color-poems), mis selgitab, kuidas autor kasutas Word2Vec-i luule loomiseks. Vaata ka [seda videot Dan Shiffmannilt](https://www.youtube.com/watch?v=LSS_bos_TPI&ab_channel=TheCodingTrain), et avastada selle tehnika teistsugust selgitust. Seej√§rel proovi rakendada neid tehnikaid oma tekstikorpusele, v√µib-olla Kaggle'ist p√§rit andmetele.

## [J√§relloengu viktoriin](https://ff-quizzes.netlify.app/en/ai/quiz/28)

## √úlevaade ja iseseisev √µppimine

Loe l√§bi see artikkel Word2Vec-i kohta: [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf)

## [√úlesanne: M√§rkmikud](assignment.md)

---

**Lahti√ºtlus**:  
See dokument on t√µlgitud AI t√µlketeenuse [Co-op Translator](https://github.com/Azure/co-op-translator) abil. Kuigi p√º√ºame tagada t√§psust, palume arvestada, et automaatsed t√µlked v√µivad sisaldada vigu v√µi ebat√§psusi. Algne dokument selle algses keeles tuleks pidada autoriteetseks allikaks. Olulise teabe puhul soovitame kasutada professionaalset inimt√µlget. Me ei vastuta selle t√µlke kasutamisest tulenevate arusaamatuste v√µi valesti t√µlgenduste eest.