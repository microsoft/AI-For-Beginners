# Neuraalv√µrkude raamistikud

Nagu me juba √µppinud oleme, on neuraalv√µrkude t√µhusaks treenimiseks vaja teha kahte asja:

* T√∂√∂tada tensoritega, n√§iteks korrutada, liita ja arvutada funktsioone nagu sigmoid v√µi softmax
* Arvutada k√µigi avaldiste gradiendid, et teostada gradientide languse optimeerimist

## [Eelloengu viktoriin](https://ff-quizzes.netlify.app/en/ai/quiz/9)

Kuigi `numpy` teek suudab t√§ita esimest osa, vajame mehhanismi gradientide arvutamiseks. [Meie raamistikus](../04-OwnFramework/OwnFramework.ipynb), mille me eelmises osas v√§lja t√∂√∂tasime, pidime k√§sitsi programmeerima k√µik tuletisfunktsioonid `backward` meetodis, mis teostab tagasipropagatsiooni. Ideaalis peaks raamistik v√µimaldama arvutada gradienti *mis tahes avaldise* jaoks, mida suudame defineerida.

Teine oluline aspekt on v√µimalus teostada arvutusi GPU-l v√µi m√µnel muul spetsialiseeritud arvutus√ºksusel, n√§iteks [TPU](https://en.wikipedia.org/wiki/Tensor_Processing_Unit). S√ºgavate neuraalv√µrkude treenimine n√µuab *v√§ga palju* arvutusi, ja nende arvutuste paralleelne teostamine GPU-l on v√§ga oluline.

> ‚úÖ Termin 'paralleelne teostamine' t√§hendab arvutuste jaotamist mitme seadme vahel.

Praegu on kaks k√µige populaarsemat neuraalv√µrkude raamistikku: [TensorFlow](http://TensorFlow.org) ja [PyTorch](https://pytorch.org/). M√µlemad pakuvad madala taseme API-d tensoritega t√∂√∂tamiseks nii CPU-l kui GPU-l. Madala taseme API peale on olemas ka k√µrgema taseme API, mida nimetatakse vastavalt [Keras](https://keras.io/) ja [PyTorch Lightning](https://pytorchlightning.ai/).

Madala taseme API | [TensorFlow](http://TensorFlow.org) | [PyTorch](https://pytorch.org/)
------------------|-------------------------------------|--------------------------------
K√µrgema taseme API| [Keras](https://keras.io/) | [PyTorch Lightning](https://pytorchlightning.ai/)

**Madala taseme API-d** m√µlemas raamistikus v√µimaldavad luua nn **arvutusgraafikuid**. See graafik m√§√§ratleb, kuidas arvutada v√§ljundit (tavaliselt kaotuse funktsiooni) antud sisendparameetritega ja seda saab GPU-le arvutamiseks saata, kui GPU on saadaval. Graafiku diferentseerimiseks ja gradientide arvutamiseks on olemas funktsioonid, mida saab seej√§rel kasutada mudeli parameetrite optimeerimiseks.

**K√µrgema taseme API-d** k√§sitlevad neuraalv√µrke peamiselt kui **kihtide j√§rjestust**, mis muudab enamiku neuraalv√µrkude konstrueerimise palju lihtsamaks. Mudeli treenimine n√µuab tavaliselt andmete ettevalmistamist ja seej√§rel `fit` funktsiooni kutsumist, et t√∂√∂ √§ra teha.

K√µrgema taseme API v√µimaldab t√º√ºpilisi neuraalv√µrke v√§ga kiiresti konstrueerida, muretsemata paljude detailide p√§rast. Samal ajal pakub madala taseme API palju rohkem kontrolli treenimisprotsessi √ºle, mist√µttu kasutatakse seda palju teadust√∂√∂s, kui tegeletakse uute neuraalv√µrkude arhitektuuridega.

Samuti on oluline m√µista, et m√µlemat API-d saab koos kasutada, n√§iteks saate madala taseme API abil v√§lja t√∂√∂tada oma v√µrgu kihi arhitektuuri ja seej√§rel kasutada seda suuremas v√µrgus, mis on konstrueeritud ja treenitud k√µrgema taseme API abil. V√µi saate defineerida v√µrgu k√µrgema taseme API abil kihtide j√§rjestusena ja seej√§rel kasutada oma madala taseme treenimists√ºklit optimeerimise teostamiseks. M√µlemad API-d kasutavad samu p√µhilisi aluskontseptsioone ja on loodud h√§sti koos t√∂√∂tama.

## √ïppimine

Selles kursuses pakume enamikku sisust nii PyTorchile kui TensorFlowle. Saate valida oma eelistatud raamistikku ja l√§bida ainult vastavad m√§rkmikud. Kui te pole kindel, millist raamistikku valida, lugege internetis arutelusid teemal **PyTorch vs. TensorFlow**. Samuti v√µite m√µlemat raamistikku uurida, et paremini aru saada.

V√µimaluse korral kasutame lihtsuse huvides k√µrgema taseme API-sid. Kuid usume, et on oluline m√µista, kuidas neuraalv√µrgud t√∂√∂tavad algtasemel, seega alustame madala taseme API ja tensoritega t√∂√∂tamisest. Kui aga soovite kiiresti edasi liikuda ja mitte kulutada palju aega nende detailide √µppimisele, v√µite need vahele j√§tta ja minna otse k√µrgema taseme API m√§rkmike juurde.

## ‚úçÔ∏è Harjutused: Raamistikud

J√§tkake √µppimist j√§rgmistes m√§rkmikes:

Madala taseme API | [TensorFlow+Keras m√§rkmik](IntroKerasTF.ipynb) | [PyTorch](IntroPyTorch.ipynb)
------------------|-------------------------------------|--------------------------------
K√µrgema taseme API| [Keras](IntroKeras.ipynb) | *PyTorch Lightning*

P√§rast raamistikest arusaamist vaatame √ºle √ºleliigse sobitamise (overfitting) m√µiste.

# √úleliigne sobitamine

√úleliigne sobitamine on masin√µppes √§√§rmiselt oluline m√µiste ja on v√§ga t√§htis sellest √µigesti aru saada!

Vaatleme j√§rgmist probleemi, kus tuleb ligikaudselt m√§√§rata 5 punkti (graafikutel t√§histatud `x`-ga):

![lineaarne](../../../../../translated_images/et/overfit1.f24b71c6f652e59e.webp) | ![√ºleliigne sobitamine](../../../../../translated_images/et/overfit2.131f5800ae10ca5e.webp)
-------------------------|--------------------------
**Lineaarne mudel, 2 parameetrit** | **Mitte-lineaarne mudel, 7 parameetrit**
Treeningu viga = 5.3 | Treeningu viga = 0
Valideerimise viga = 5.1 | Valideerimise viga = 20

* Vasakul n√§eme head sirgjoonelist ligikaudset m√§√§ratlust. Kuna parameetrite arv on piisav, saab mudel punktide jaotuse idee √µigesti k√§tte.
* Paremal on mudel liiga v√µimas. Kuna meil on ainult 5 punkti ja mudelil on 7 parameetrit, saab see kohanduda nii, et l√§bib k√µik punktid, muutes treeningu vea nulliks. Kuid see takistab mudelil m√µista andmete √µiget mustrit, mist√µttu valideerimise viga on v√§ga suur.

On v√§ga oluline leida √µige tasakaal mudeli rikkuse (parameetrite arv) ja treeningn√§idiste arvu vahel.

## Miks √ºleliigne sobitamine tekib

  * Ebapiisav treeningandmete hulk
  * Liiga v√µimas mudel
  * Liiga palju m√ºra sisendandmetes

## Kuidas tuvastada √ºleliigset sobitamist

Nagu √ºlaltoodud graafikult n√§ha, saab √ºleliigset sobitamist tuvastada v√§ga madala treeningu vea ja k√µrge valideerimise vea j√§rgi. Tavaliselt n√§eme treenimise ajal, kuidas treeningu ja valideerimise vead hakkavad m√µlemad v√§henema, kuid mingil hetkel valideerimise viga v√µib l√µpetada v√§henemise ja hakata kasvama. See on m√§rk √ºleliigsest sobitamisest ja indikaator, et treenimine tuleks t√µen√§oliselt l√µpetada (v√µi v√§hemalt mudelist hetkeseis salvestada).

![√ºleliigne sobitamine](../../../../../translated_images/et/Overfitting.408ad91cd90b4371.webp)

## Kuidas v√§ltida √ºleliigset sobitamist

Kui n√§ete, et √ºleliigne sobitamine toimub, saate teha j√§rgmist:

 * Suurendage treeningandmete hulka
 * V√§hendage mudeli keerukust
 * Kasutage m√µnda [regulaarimistehnikat](../../4-ComputerVision/08-TransferLearning/TrainingTricks.md), n√§iteks [Dropout](../../4-ComputerVision/08-TransferLearning/TrainingTricks.md#Dropout), mida k√§sitleme hiljem.

## √úleliigne sobitamine ja Bias-Variance kompromiss

√úleliigne sobitamine on tegelikult statistikas tuntud √ºldisema probleemi, mida nimetatakse [Bias-Variance kompromissiks](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff), juhtum. Kui kaalume oma mudeli vigade v√µimalikke allikaid, n√§eme kahte t√º√ºpi vigu:

* **Bias-vead** tekivad siis, kui meie algoritm ei suuda treeningandmete vahelisi seoseid √µigesti tabada. See v√µib tuleneda sellest, et meie mudel pole piisavalt v√µimas (**alaliigne sobitamine**).
* **Variance-vead**, mis tekivad siis, kui mudel ligikaudustab sisendandmete m√ºra, mitte t√§henduslikku seost (**√ºleliigne sobitamine**).

Treeningu ajal bias-vead v√§henevad (kuna meie mudel √µpib andmeid ligikaudselt m√§√§ratlema) ja variance-vead suurenevad. On oluline treenimine l√µpetada - kas k√§sitsi (kui tuvastame √ºleliigse sobitamise) v√µi automaatselt (regulaarimise abil) - et v√§ltida √ºleliigset sobitamist.

## Kokkuv√µte

Selles √µppetunnis √µppisite erinevusi kahe k√µige populaarsema AI-raamistiku, TensorFlow ja PyTorch, erinevate API-de vahel. Lisaks √µppisite v√§ga olulist teemat, √ºleliigset sobitamist.

## üöÄ V√§ljakutse

Kaasaolevates m√§rkmikes leiate '√ºlesanded' allosas; t√∂√∂tage m√§rkmikud l√§bi ja t√§itke √ºlesanded.

## [J√§relloengu viktoriin](https://ff-quizzes.netlify.app/en/ai/quiz/10)

## √úlevaade ja iseseisev √µppimine

Tehke uurimist√∂√∂d j√§rgmiste teemade kohta:

- TensorFlow
- PyTorch
- √úleliigne sobitamine

Esitage endale j√§rgmised k√ºsimused:

- Mis vahe on TensorFlow ja PyTorch vahel?
- Mis vahe on √ºleliigsel sobitamisel ja alaliigsel sobitamisel?

## [√úlesanne](lab/README.md)

Selles laboris palutakse teil lahendada kaks klassifitseerimisprobleemi, kasutades √ºhe- ja mitmekihilisi t√§ielikult √ºhendatud v√µrke PyTorch v√µi TensorFlow abil.

* [Juhised](lab/README.md)
* [M√§rkmik](lab/LabFrameworks.ipynb)

---

**Lahti√ºtlus**:  
See dokument on t√µlgitud AI t√µlketeenuse [Co-op Translator](https://github.com/Azure/co-op-translator) abil. Kuigi p√º√ºame tagada t√§psust, palume arvestada, et automaatsed t√µlked v√µivad sisaldada vigu v√µi ebat√§psusi. Algne dokument selle algses keeles tuleks pidada autoriteetseks allikaks. Olulise teabe puhul soovitame kasutada professionaalset inimt√µlget. Me ei vastuta selle t√µlke kasutamisest tulenevate arusaamatuste v√µi valesti t√µlgenduste eest.