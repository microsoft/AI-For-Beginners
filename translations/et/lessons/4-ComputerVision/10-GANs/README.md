# Generatiivsed vastandlikud v√µrgud

Eelmises osas √µppisime **generatiivsete mudelite** kohta: mudelid, mis suudavad luua uusi pilte, mis sarnanevad treeningandmestikus olevatele. VAE oli hea n√§ide generatiivsest mudelist.

## [Eelloengu viktoriin](https://ff-quizzes.netlify.app/en/ai/quiz/19)

Kui aga proovime luua midagi t√µeliselt t√§henduslikku, n√§iteks maali m√µistlikus resolutsioonis, siis VAE-ga treenimine ei pruugi h√§sti √µnnestuda. Selleks kasutusjuhtumiks peaksime √µppima tundma teist arhitektuuri, mis on spetsiaalselt suunatud generatiivsetele mudelitele - **Generatiivsed vastandlikud v√µrgud** ehk GAN-id.

GAN-i peamine idee on kasutada kahte n√§rviv√µrku, mis treenivad √ºksteise vastu:

<img src="../../../../../translated_images/et/gan_architecture.8f3a5ab62b8d5d69.webp" width="70%"/>

> Pilt: [Dmitry Soshnikov](http://soshnikov.com)

> ‚úÖ V√§ike s√µnavara:
> * **Generaator** on v√µrk, mis v√µtab sisendiks juhusliku vektori ja genereerib tulemuseks pildi.
> * **Diskrimineerija** on v√µrk, mis v√µtab sisendiks pildi ja peaks otsustama, kas tegemist on p√§ris pildiga (treeningandmestikust) v√µi generaatori loodud pildiga. Sisuliselt on see pildiklassifikaator.

### Diskrimineerija

Diskrimineerija arhitektuur ei erine tavalisest pildiklassifikatsiooni v√µrgust. Lihtsaimal juhul v√µib see olla t√§ielikult √ºhendatud klassifikaator, kuid t√µen√§oliselt kasutatakse [konvolutsiooniv√µrku](../07-ConvNets/README.md).

> ‚úÖ Konvolutsiooniv√µrkudel p√µhinevat GAN-i nimetatakse [DCGAN-iks](https://arxiv.org/pdf/1511.06434.pdf)

CNN-diskrimineerija koosneb j√§rgmistest kihtidest: mitu konvolutsiooni+√ºhendamist (ruumilise suuruse v√§hendamiseks) ja √ºks v√µi mitu t√§ielikult √ºhendatud kihti, et saada "tunnuste vektor", ning l√µpuks binaarne klassifikaator.

> ‚úÖ '√úhendamine' (pooling) selles kontekstis on tehnika, mis v√§hendab pildi suurust. "√úhendamiskihid v√§hendavad andmete dimensioone, kombineerides √ºhe kihi neuroniklastrite v√§ljundid √ºheks neuroniks j√§rgmises kihis." - [allikas](https://wikipedia.org/wiki/Convolutional_neural_network#Pooling_layers)

### Generaator

Generaator on veidi keerulisem. Seda v√µib pidada p√∂√∂ratud diskrimineerijaks. Alustades latentvektorist (tunnuste vektori asemel), on sellel t√§ielikult √ºhendatud kiht, mis teisendab selle vajalikku suurusesse/kuju, millele j√§rgnevad dekonvolutsioonid+suurendamine. See on sarnane [autokodeerija](../09-Autoencoders/README.md) *dekooder* osale.

> ‚úÖ Kuna konvolutsioonikiht rakendatakse lineaarse filtrina, mis liigub √ºle pildi, on dekonvolutsioon sisuliselt sarnane konvolutsiooniga ja seda saab rakendada sama kihi loogikaga.

<img src="../../../../../translated_images/et/gan_arch_detail.46b95fd366f8e543.webp" width="70%"/>

> Pilt: [Dmitry Soshnikov](http://soshnikov.com)

### GAN-i treenimine

GAN-e nimetatakse **vastandlikeks**, kuna generaatori ja diskrimineerija vahel toimub pidev konkurents. Selle konkurentsi k√§igus paranevad m√µlemad, generaator ja diskrimineerija, ning v√µrk √µpib looma j√§rjest paremaid pilte.

Treening toimub kahes etapis:

* **Diskrimineerija treenimine**. See √ºlesanne on √ºsna lihtne: genereerime generaatori abil pildipartii, m√§rgistades need 0-ga (mis t√§histab v√µltsitud pilti), ja v√µtame treeningandmestikust pildipartii (m√§rgisega 1, p√§ris pilt). Saame *diskrimineerija kaotuse* ja teostame tagasilevikut.
* **Generaatori treenimine**. See on veidi keerulisem, kuna me ei tea generaatori oodatud v√§ljundit otseselt. V√µtame kogu GAN-v√µrgu, mis koosneb generaatorist ja diskrimineerijast, anname sellele juhuslikud vektorid ja eeldame, et tulemus on 1 (mis vastab p√§ris piltidele). Seej√§rel k√ºlmutame diskrimineerija parameetrid (me ei soovi seda selles etapis treenida) ja teostame tagasilevikut.

Selle protsessi k√§igus ei lange generaatori ja diskrimineerija kaotused m√§rkimisv√§√§rselt. Ideaalis peaksid need v√µnkuma, mis n√§itab, et m√µlemad v√µrgud parandavad oma j√µudlust.

## ‚úçÔ∏è Harjutused: GAN-id

* [GAN-i m√§rkmik TensorFlow/Keras-is](GANTF.ipynb)
* [GAN-i m√§rkmik PyTorch-is](GANPyTorch.ipynb)

### Probleemid GAN-i treenimisel

GAN-e peetakse eriti keeruliseks treenida. Siin on m√µned probleemid:

* **Re≈æiimi kokkuvarisemine**. See t√§hendab, et generaator √µpib looma √ºhte edukat pilti, mis petab diskrimineerijat, kuid ei loo erinevaid pilte.
* **Tundlikkus h√ºperparameetritele**. Sageli v√µib n√§ha, et GAN ei konvergeeri √ºldse, ja siis √§kitselt √µnnestub konvergents √µppem√§√§ra v√§hendamisega.
* **Tasakaalu hoidmine** generaatori ja diskrimineerija vahel. Paljudel juhtudel v√µib diskrimineerija kaotus kiiresti nulli langeda, mis takistab generaatoril edasist treenimist. Selle √ºletamiseks v√µib proovida m√§√§rata generaatorile ja diskrimineerijale erinevaid √µppem√§√§rasid v√µi j√§tta diskrimineerija treenimine vahele, kui kaotus on juba liiga madal.
* **K√µrge resolutsiooni treenimine**. Sama probleem nagu autokodeerijatega, see probleem tekib, kuna liiga paljude konvolutsiooniv√µrgu kihtide rekonstrueerimine p√µhjustab artefakte. Seda probleemi lahendatakse tavaliselt nn **progressiivse kasvuga**, kus esimesed kihid treenitakse madala resolutsiooniga piltidel ja seej√§rel kihid "avatakse" v√µi lisatakse. Teine lahendus oleks lisada lisakonnektorid kihtide vahel ja treenida mitut resolutsiooni korraga - vaata selle kohta [Multi-Scale Gradient GANs artiklit](https://arxiv.org/abs/1903.06048).

## Stiiliedastus

GAN-id on suurep√§rane viis kunstiliste piltide loomiseks. Teine huvitav tehnika on nn **stiiliedastus**, mis v√µtab √ºhe **sisu pildi** ja joonistab selle √ºmber teises stiilis, rakendades filtreid **stiilipildist**.

Kuidas see t√∂√∂tab:
* Alustame juhusliku m√ºrapildiga (v√µi sisu pildiga, kuid m√µistmise huvides on lihtsam alustada juhuslikust m√ºrast)
* Meie eesm√§rk on luua selline pilt, mis oleks l√§hedane nii sisu pildile kui ka stiilipildile. Seda m√§√§ravad kaks kaotusfunktsiooni:
   - **Sisu kaotus** arvutatakse CNN-i poolt teatud kihtides praeguse pildi ja sisu pildi vaheliste tunnuste p√µhjal
   - **Stiili kaotus** arvutatakse praeguse pildi ja stiilipildi vahel nutikalt Gram-maatriksite abil (rohkem detaile [n√§idism√§rkmikus](StyleTransfer.ipynb))
* Pildi sujuvamaks muutmiseks ja m√ºra eemaldamiseks lisame ka **variatsiooni kaotuse**, mis arvutab naaberpikslite keskmise kauguse
* Peamine optimeerimists√ºkkel kohandab praegust pilti, kasutades gradientide langust (v√µi m√µnda muud optimeerimisalgoritmi), et minimeerida kogukaotust, mis on k√µigi kolme kaotuse kaalutud summa.

## ‚úçÔ∏è N√§ide: [Stiiliedastus](StyleTransfer.ipynb)

## [J√§relloengu viktoriin](https://ff-quizzes.netlify.app/en/ai/quiz/20)

## Kokkuv√µte

Selles √µppetunnis √µppisite GAN-e ja nende treenimist. Samuti √µppisite tundma eriprobleeme, millega seda t√º√ºpi n√§rviv√µrk v√µib kokku puutuda, ja m√µningaid strateegiaid nende √ºletamiseks.

## üöÄ V√§ljakutse

K√§ivitage [Stiiliedastuse m√§rkmik](StyleTransfer.ipynb), kasutades oma pilte.

## √úlevaade ja iseseisev √µppimine

Lisateabe saamiseks lugege GAN-ide kohta j√§rgmistest ressurssidest:

* Marco Pasini, [10 √µppetundi, mida √µppisin GAN-e treenides √ºhe aasta jooksul](https://towardsdatascience.com/10-lessons-i-learned-training-generative-adversarial-networks-gans-for-a-year-c9071159628)
* [StyleGAN](https://en.wikipedia.org/wiki/StyleGAN), *de facto* GAN arhitektuur, mida kaaluda
* [Generatiivse kunsti loomine GAN-idega Azure ML-is](https://soshnikov.com/scienceart/creating-generative-art-using-gan-on-azureml/)

## √úlesanne

Vaadake uuesti √ºhte kahest selle √µppetunniga seotud m√§rkmikust ja treenige GAN-i oma piltidega. Mida suudate luua?

---

**Lahti√ºtlus**:  
See dokument on t√µlgitud AI t√µlketeenuse [Co-op Translator](https://github.com/Azure/co-op-translator) abil. Kuigi p√º√ºame tagada t√§psust, palume arvestada, et automaatsed t√µlked v√µivad sisaldada vigu v√µi ebat√§psusi. Algne dokument selle algses keeles tuleks pidada autoriteetseks allikaks. Olulise teabe puhul soovitame kasutada professionaalset inimt√µlget. Me ei vastuta selle t√µlke kasutamisest tulenevate arusaamatuste v√µi valesti t√µlgenduste eest.