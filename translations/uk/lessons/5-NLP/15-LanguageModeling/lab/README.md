# Навчання моделі Skip-Gram

Лабораторна робота з [AI for Beginners Curriculum](https://github.com/microsoft/ai-for-beginners).

## Завдання

У цій лабораторній роботі ми пропонуємо вам навчити модель Word2Vec, використовуючи техніку Skip-Gram. Навчіть мережу з вбудовуванням для прогнозування сусідніх слів у Skip-Gram вікні шириною $N$ токенів. Ви можете використати [код із цього уроку](../../../../../../lessons/5-NLP/15-LanguageModeling/CBoW-TF.ipynb) і трохи його змінити.

## Датасет

Ви можете використовувати будь-яку книгу. Багато безкоштовних текстів можна знайти на [Project Gutenberg](https://www.gutenberg.org/), наприклад, ось пряме посилання на [Алісу в Країні Чудес](https://www.gutenberg.org/files/11/11-0.txt)) Льюїса Керролла. Або ви можете використати п'єси Шекспіра, які можна отримати за допомогою наступного коду:

```python
path_to_file = tf.keras.utils.get_file(
   'shakespeare.txt', 
   'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')
text = open(path_to_file, 'rb').read().decode(encoding='utf-8')
```

## Досліджуйте!

Якщо у вас є час і бажання глибше зануритися в тему, спробуйте дослідити кілька речей:

* Як розмір вбудовування впливає на результати?
* Як різні стилі тексту впливають на результат?
* Візьміть кілька дуже різних типів слів і їхніх синонімів, отримайте їхні векторні представлення, застосуйте PCA для зменшення розмірності до 2, і побудуйте їх у 2D-просторі. Чи бачите ви якісь закономірності?

**Відмова від відповідальності**:  
Цей документ було перекладено за допомогою сервісу автоматичного перекладу [Co-op Translator](https://github.com/Azure/co-op-translator). Хоча ми прагнемо до точності, звертаємо вашу увагу, що автоматичні переклади можуть містити помилки або неточності. Оригінальний документ на його рідній мові слід вважати авторитетним джерелом. Для критично важливої інформації рекомендується професійний людський переклад. Ми не несемо відповідальності за будь-які непорозуміння або неправильні тлумачення, що виникли внаслідок використання цього перекладу.