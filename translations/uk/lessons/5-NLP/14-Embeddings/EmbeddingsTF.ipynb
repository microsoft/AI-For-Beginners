{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вбудовування\n",
    "\n",
    "У нашому попередньому прикладі ми працювали з високорозмірними векторами \"мішків слів\" довжиною `vocab_size` і явно перетворювали низькорозмірні вектори позиційного представлення у розріджене однохотне представлення. Це однохотне представлення не є ефективним з точки зору пам'яті. Крім того, кожне слово розглядається незалежно одне від одного, тому однохотні вектори не виражають семантичних подібностей між словами.\n",
    "\n",
    "У цьому розділі ми продовжимо досліджувати набір даних **News AG**. Для початку завантажимо дані та отримаємо деякі визначення з попереднього розділу.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "\n",
    "ds_train, ds_test = tfds.load('ag_news_subset').values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Що таке вбудовування?\n",
    "\n",
    "Ідея **вбудовування** полягає в тому, щоб представляти слова за допомогою низьковимірних щільних векторів, які відображають семантичне значення слова. Пізніше ми обговоримо, як створювати змістовні вбудовування слів, але наразі розглянемо вбудовування як спосіб зменшення розмірності вектора слова.\n",
    "\n",
    "Отже, шар вбудовування приймає слово як вхід і створює вихідний вектор заданого розміру `embedding_size`. У певному сенсі, це дуже схоже на шар `Dense`, але замість того, щоб приймати вхідний вектор у вигляді one-hot кодування, він може приймати номер слова.\n",
    "\n",
    "Використовуючи шар вбудовування як перший шар у нашій мережі, ми можемо перейти від моделі \"мішка слів\" до моделі **мішка вбудовувань**, де спочатку кожне слово в тексті перетворюється на відповідне вбудовування, а потім обчислюється певна агрегатна функція для всіх цих вбудовувань, наприклад, `sum`, `average` або `max`.\n",
    "\n",
    "![Зображення, що показує класифікатор з вбудовуванням для п’яти послідовних слів.](../../../../../translated_images/uk/embedding-classifier-example.b77f021a7ee67eee.webp)\n",
    "\n",
    "Наша нейронна мережа класифікатора складається з таких шарів:\n",
    "\n",
    "* Шар `TextVectorization`, який приймає рядок як вхід і створює тензор номерів токенів. Ми визначимо розумний розмір словника `vocab_size` і ігноруватимемо менш уживані слова. Вхідна форма буде 1, а вихідна форма — $n$, оскільки ми отримаємо $n$ токенів у результаті, кожен із яких містить числа від 0 до `vocab_size`.\n",
    "* Шар `Embedding`, який приймає $n$ чисел і зменшує кожне число до щільного вектора заданої довжини (у нашому прикладі — 100). Таким чином, вхідний тензор форми $n$ буде перетворений на тензор $n\\times 100$.\n",
    "* Шар агрегації, який обчислює середнє значення цього тензора вздовж першої осі, тобто обчислює середнє значення всіх $n$ вхідних тензорів, що відповідають різним словам. Для реалізації цього шару ми використаємо шар `Lambda` і передамо в нього функцію для обчислення середнього значення. Вихідна форма буде 100, і це буде числове представлення всієї вхідної послідовності.\n",
    "* Завершальний лінійний класифікатор `Dense`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization (TextVec  (None, None)             0         \n",
      " torization)                                                     \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, None, 100)         3000000   \n",
      "                                                                 \n",
      " lambda (Lambda)             (None, 100)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4)                 404       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,000,404\n",
      "Trainable params: 3,000,404\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 30000\n",
    "batch_size = 128\n",
    "\n",
    "vectorizer = keras.layers.experimental.preprocessing.TextVectorization(max_tokens=vocab_size,input_shape=(1,))\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    vectorizer,    \n",
    "    keras.layers.Embedding(vocab_size,100),\n",
    "    keras.layers.Lambda(lambda x: tf.reduce_mean(x,axis=1)),\n",
    "    keras.layers.Dense(4, activation='softmax')\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У `summary`-виводі, у колонці **output shape**, перший вимір тензора `None` відповідає розміру мініпакету, а другий — довжині послідовності токенів. Усі послідовності токенів у мініпакеті мають різну довжину. Ми обговоримо, як із цим впоратися, у наступному розділі.\n",
    "\n",
    "А тепер давайте навчимо мережу:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training vectorizer\n",
      "938/938 [==============================] - 20s 20ms/step - loss: 0.7891 - acc: 0.8155 - val_loss: 0.4470 - val_acc: 0.8642\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22255515100>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_text(x):\n",
    "    return x['title']+' '+x['description']\n",
    "\n",
    "def tupelize(x):\n",
    "    return (extract_text(x),x['label'])\n",
    "\n",
    "print(\"Training vectorizer\")\n",
    "vectorizer.adapt(ds_train.take(500).map(extract_text))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'])\n",
    "model.fit(ds_train.map(tupelize).batch(batch_size),validation_data=ds_test.map(tupelize).batch(batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "> **Примітка**: ми створюємо векторизатор на основі підмножини даних. Це робиться для прискорення процесу, і це може призвести до ситуації, коли не всі токени з нашого тексту присутні у словнику. У такому випадку ці токени будуть ігноровані, що може призвести до трохи нижчої точності. Однак у реальному житті підмножина тексту часто дає хорошу оцінку словника.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Робота зі змінними розмірами послідовностей\n",
    "\n",
    "Давайте розберемося, як відбувається навчання в мініпакетах. У наведеному вище прикладі вхідний тензор має розмірність 1, і ми використовуємо мініпакети довжиною 128, тому фактичний розмір тензора становить $128 \\times 1$. Однак кількість токенів у кожному реченні різна. Якщо ми застосуємо шар `TextVectorization` до одного входу, кількість токенів, що повертається, буде різною, залежно від того, як текст токенізується:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 1 45], shape=(2,), dtype=int64)\n",
      "tf.Tensor([ 112 1271    1    3 1747  158], shape=(6,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer('Hello, world!'))\n",
    "print(vectorizer('I am glad to meet you!'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Однак, коли ми застосовуємо векторизатор до кількох послідовностей, він має створити тензор прямокутної форми, тому заповнює невикористані елементи токеном PAD (який у нашому випадку дорівнює нулю):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 6), dtype=int64, numpy=\n",
       "array([[   1,   45,    0,    0,    0,    0],\n",
       "       [ 112, 1271,    1,    3, 1747,  158]], dtype=int64)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer(['Hello, world!','I am glad to meet you!'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ось ми можемо побачити вбудовування:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 1.53059261e-02,  6.80514947e-02,  3.14026810e-02, ...,\n",
       "         -8.92002955e-02,  1.52911525e-04, -5.65562584e-02],\n",
       "        [ 2.57456154e-01,  2.79364467e-01, -2.03605562e-01, ...,\n",
       "         -2.07474351e-01,  8.31158683e-02, -2.03911960e-01],\n",
       "        [ 3.98201384e-02, -8.03454965e-03,  2.39790026e-02, ...,\n",
       "         -7.18549127e-04,  2.66963355e-02, -4.30646613e-02],\n",
       "        [ 3.98201384e-02, -8.03454965e-03,  2.39790026e-02, ...,\n",
       "         -7.18549127e-04,  2.66963355e-02, -4.30646613e-02],\n",
       "        [ 3.98201384e-02, -8.03454965e-03,  2.39790026e-02, ...,\n",
       "         -7.18549127e-04,  2.66963355e-02, -4.30646613e-02],\n",
       "        [ 3.98201384e-02, -8.03454965e-03,  2.39790026e-02, ...,\n",
       "         -7.18549127e-04,  2.66963355e-02, -4.30646613e-02]],\n",
       "\n",
       "       [[ 1.89674050e-01,  2.61548996e-01, -3.67433839e-02, ...,\n",
       "         -2.07366899e-01, -1.05442435e-01, -2.36952081e-01],\n",
       "        [ 6.16133213e-02,  1.80511594e-01,  9.77298319e-02, ...,\n",
       "         -5.46628237e-02, -1.07340455e-01, -1.06589928e-01],\n",
       "        [ 1.53059261e-02,  6.80514947e-02,  3.14026810e-02, ...,\n",
       "         -8.92002955e-02,  1.52911525e-04, -5.65562584e-02],\n",
       "        [-4.84890305e-02, -8.41715634e-02,  1.51529670e-01, ...,\n",
       "          1.28192469e-01, -7.77286515e-02,  1.26041949e-01],\n",
       "        [-4.17212099e-02, -5.60694858e-02,  4.08860669e-02, ...,\n",
       "          8.70475471e-02,  8.92383084e-02,  1.67974353e-01],\n",
       "        [ 2.85779923e-01,  4.57767487e-01,  4.52292450e-02, ...,\n",
       "         -1.97419018e-01, -2.04659685e-01, -2.79758364e-01]]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[1](vectorizer(['Hello, world!','I am glad to meet you!'])).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Примітка**: Щоб мінімізувати кількість заповнення, в деяких випадках має сенс сортувати всі послідовності в наборі даних у порядку зростання довжини (або, точніше, кількості токенів). Це забезпечить, що кожна мініпартія містить послідовності схожої довжини.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Семантичні вбудовування: Word2Vec\n",
    "\n",
    "У нашому попередньому прикладі шар вбудовування навчався відображати слова у векторні представлення, проте ці представлення не мали семантичного значення. Було б добре навчити векторне представлення таким чином, щоб схожі слова або синоніми відповідали векторами, які близькі один до одного за певною векторною відстанню (наприклад, евклідовою відстанню).\n",
    "\n",
    "Для цього нам потрібно попередньо навчити модель вбудовування на великій колекції текстів, використовуючи техніку, таку як [Word2Vec](https://en.wikipedia.org/wiki/Word2vec). Вона базується на двох основних архітектурах, які використовуються для створення розподіленого представлення слів:\n",
    "\n",
    " - **Неперервний мішок слів** (CBoW), де ми навчаємо модель передбачати слово за навколишнім контекстом. Дано n-грам $(W_{-2},W_{-1},W_0,W_1,W_2)$, мета моделі — передбачити $W_0$ за $(W_{-2},W_{-1},W_1,W_2)$.\n",
    " - **Неперервний скіп-грам** є протилежністю CBoW. Модель використовує навколишнє вікно контекстних слів для передбачення поточного слова.\n",
    "\n",
    "CBoW працює швидше, тоді як скіп-грам повільніший, але краще представляє рідковживані слова.\n",
    "\n",
    "![Зображення, що показує алгоритми CBoW і Skip-Gram для перетворення слів у вектори.](../../../../../translated_images/uk/example-algorithms-for-converting-words-to-vectors.fbe9207a726922f6.webp)\n",
    "\n",
    "Щоб експериментувати з вбудовуванням Word2Vec, попередньо навченим на наборі даних Google News, ми можемо використовувати бібліотеку **gensim**. Нижче наведено приклад пошуку слів, найбільш схожих до 'neural'.\n",
    "\n",
    "> **Примітка:** Коли ви вперше створюєте векторні представлення слів, їх завантаження може зайняти деякий час!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "w2v = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neuronal -> 0.7804799675941467\n",
      "neurons -> 0.7326500415802002\n",
      "neural_circuits -> 0.7252851724624634\n",
      "neuron -> 0.7174385190010071\n",
      "cortical -> 0.6941086649894714\n",
      "brain_circuitry -> 0.6923246383666992\n",
      "synaptic -> 0.6699118614196777\n",
      "neural_circuitry -> 0.6638563275337219\n",
      "neurochemical -> 0.6555314064025879\n",
      "neuronal_activity -> 0.6531826257705688\n"
     ]
    }
   ],
   "source": [
    "for w,p in w2v.most_similar('neural'):\n",
    "    print(f\"{w} -> {p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ми також можемо витягти векторне вбудовування зі слова, щоб використовувати його для навчання моделі класифікації. Вбудовування має 300 компонентів, але тут ми показуємо лише перші 20 компонентів вектора для ясності:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01226807,  0.06225586,  0.10693359,  0.05810547,  0.23828125,\n",
       "        0.03686523,  0.05151367, -0.20703125,  0.01989746,  0.10058594,\n",
       "       -0.03759766, -0.1015625 , -0.15820312, -0.08105469, -0.0390625 ,\n",
       "       -0.05053711,  0.16015625,  0.2578125 ,  0.10058594, -0.25976562],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v['play'][:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чудова річ у семантичних вбудовуваннях полягає в тому, що ви можете маніпулювати векторним кодуванням на основі семантики. Наприклад, ми можемо попросити знайти слово, чия векторна репрезентація є якомога ближчою до слів *король* і *жінка*, і якомога далі від слова *чоловік*:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('queen', 0.7118192911148071)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar(positive=['king','woman'],negative=['man'])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Приклад вище використовує деяку внутрішню магію GenSym, але основна логіка насправді досить проста. Цікава річ про вбудовування полягає в тому, що ви можете виконувати звичайні операції з векторами на векторах вбудовування, і це буде відображати операції над **значеннями** слів. Приклад вище можна виразити в термінах векторних операцій: ми обчислюємо вектор, що відповідає **KING-MAN+WOMAN** (операції `+` і `-` виконуються на векторних представленнях відповідних слів), а потім знаходимо найближче слово в словнику до цього вектора:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'queen'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the vector corresponding to kind-man+woman\n",
    "qvec = w2v['king']-1.7*w2v['man']+1.7*w2v['woman']\n",
    "# find the index of the closest embedding vector \n",
    "d = np.sum((w2v.vectors-qvec)**2,axis=1)\n",
    "min_idx = np.argmin(d)\n",
    "# find the corresponding word\n",
    "w2v.index_to_key[min_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NOTE**: Ми додали невеликі коефіцієнти до векторів *man* і *woman* - спробуйте їх прибрати, щоб побачити, що станеться.\n",
    "\n",
    "Щоб знайти найближчий вектор, ми використовуємо механізми TensorFlow для обчислення вектора відстаней між нашим вектором і всіма векторами у словнику, а потім знаходимо індекс мінімального слова за допомогою `argmin`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Хоча Word2Vec здається чудовим способом вираження семантики слів, він має багато недоліків, зокрема такі:\n",
    "\n",
    "* Моделі CBoW і skip-gram є **прогнозуючими векторами**, і вони враховують лише локальний контекст. Word2Vec не використовує глобальний контекст.\n",
    "* Word2Vec не враховує **морфологію** слів, тобто той факт, що значення слова може залежати від різних частин слова, таких як корінь.\n",
    "\n",
    "**FastText** намагається подолати друге обмеження і розширює можливості Word2Vec, навчаючи векторні представлення для кожного слова та n-грам символів, які зустрічаються в кожному слові. Значення цих представлень потім усереднюються в один вектор на кожному етапі навчання. Хоча це додає багато додаткових обчислень до попереднього навчання, це дозволяє векторним представленням слів кодувати інформацію про частини слова.\n",
    "\n",
    "Інший метод, **GloVe**, використовує інший підхід до векторних представлень слів, заснований на факторизації матриці \"слово-контекст\". Спочатку він створює велику матрицю, яка рахує кількість появ слова в різних контекстах, а потім намагається представити цю матрицю в нижчих вимірах таким чином, щоб мінімізувати втрати при реконструкції.\n",
    "\n",
    "Бібліотека gensim підтримує ці векторні представлення слів, і ви можете експериментувати з ними, змінюючи код завантаження моделі вище.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Використання попередньо навчених векторів у Keras\n",
    "\n",
    "Ми можемо змінити наведений вище приклад, щоб заповнити матрицю в нашому шарі векторизації семантичними векторами, такими як Word2Vec. Словники попередньо навчених векторів і текстового корпусу, ймовірно, не співпадатимуть, тому нам потрібно обрати один. Тут ми розглянемо два можливі варіанти: використання словника токенізатора та використання словника з Word2Vec.\n",
    "\n",
    "### Використання словника токенізатора\n",
    "\n",
    "При використанні словника токенізатора деякі слова зі словника матимуть відповідні вектори Word2Vec, а деякі будуть відсутні. Враховуючи, що розмір нашого словника дорівнює `vocab_size`, а довжина векторів Word2Vec — `embed_size`, шар векторизації буде представлений матрицею ваг розміром `vocab_size`$\\times$`embed_size`. Ми заповнимо цю матрицю, проходячи через словник:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding size: 300\n",
      "Populating matrix, this will take some time...Done, found 4551 words, 784 words missing\n"
     ]
    }
   ],
   "source": [
    "embed_size = len(w2v.get_vector('hello'))\n",
    "print(f'Embedding size: {embed_size}')\n",
    "\n",
    "vocab = vectorizer.get_vocabulary()\n",
    "W = np.zeros((vocab_size,embed_size))\n",
    "print('Populating matrix, this will take some time...',end='')\n",
    "found, not_found = 0,0\n",
    "for i,w in enumerate(vocab):\n",
    "    try:\n",
    "        W[i] = w2v.get_vector(w)\n",
    "        found+=1\n",
    "    except:\n",
    "        # W[i] = np.random.normal(0.0,0.3,size=(embed_size,))\n",
    "        not_found+=1\n",
    "\n",
    "print(f\"Done, found {found} words, {not_found} words missing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для слів, які відсутні у словнику Word2Vec, ми можемо залишити їх як нулі або згенерувати випадковий вектор.\n",
    "\n",
    "Тепер ми можемо визначити шар вбудовування з попередньо навченими вагами:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = keras.layers.Embedding(vocab_size,embed_size,weights=[W],trainable=False)\n",
    "model = keras.models.Sequential([\n",
    "    vectorizer, emb,\n",
    "    keras.layers.Lambda(lambda x: tf.reduce_mean(x,axis=1)),\n",
    "    keras.layers.Dense(4, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 10s 10ms/step - loss: 1.1075 - acc: 0.7822 - val_loss: 0.9134 - val_acc: 0.8175\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2220226ef10>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'])\n",
    "model.fit(ds_train.map(tupelize).batch(batch_size),\n",
    "          validation_data=ds_test.map(tupelize).batch(batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Примітка**: Зверніть увагу, що ми встановили `trainable=False` під час створення `Embedding`, що означає, що ми не перенавчаємо шар Embedding. Це може трохи знизити точність, але прискорює навчання.\n",
    "\n",
    "### Використання словника ембедингів\n",
    "\n",
    "Одна з проблем попереднього підходу полягає в тому, що словники, які використовуються в TextVectorization і Embedding, відрізняються. Щоб вирішити цю проблему, ми можемо скористатися одним із наступних рішень:\n",
    "* Перенавчити модель Word2Vec на нашому словнику.\n",
    "* Завантажити наш набір даних із використанням словника з попередньо навченої моделі Word2Vec. Словники, які використовуються для завантаження набору даних, можна вказати під час завантаження.\n",
    "\n",
    "Другий підхід здається простішим, тому давайте реалізуємо його. Спочатку ми створимо шар `TextVectorization` із вказаним словником, взятим із ембедингів Word2Vec:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(w2v.vocab.keys())\n",
    "vectorizer = keras.layers.experimental.preprocessing.TextVectorization(input_shape=(1,))\n",
    "vectorizer.set_vocabulary(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Бібліотека векторних представлень слів gensim містить зручну функцію `get_keras_embeddings`, яка автоматично створить відповідний шар векторних представлень Keras для вас.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "938/938 [==============================] - 20s 14ms/step - loss: 1.3377 - acc: 0.4978 - val_loss: 1.2995 - val_acc: 0.5647\n",
      "Epoch 2/5\n",
      "938/938 [==============================] - 10s 10ms/step - loss: 1.2587 - acc: 0.5722 - val_loss: 1.2339 - val_acc: 0.5842\n",
      "Epoch 3/5\n",
      "938/938 [==============================] - 10s 10ms/step - loss: 1.1980 - acc: 0.5884 - val_loss: 1.1826 - val_acc: 0.5954\n",
      "Epoch 4/5\n",
      "938/938 [==============================] - 12s 13ms/step - loss: 1.1503 - acc: 0.6002 - val_loss: 1.1417 - val_acc: 0.6018\n",
      "Epoch 5/5\n",
      "938/938 [==============================] - 11s 12ms/step - loss: 1.1120 - acc: 0.6097 - val_loss: 1.1083 - val_acc: 0.6104\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2220ccb81c0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    vectorizer, \n",
    "    w2v.get_keras_embedding(train_embeddings=False),\n",
    "    keras.layers.Lambda(lambda x: tf.reduce_mean(x,axis=1)),\n",
    "    keras.layers.Dense(4, activation='softmax')\n",
    "])\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'])\n",
    "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128),epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Однією з причин, чому ми не спостерігаємо вищої точності, є те, що деякі слова з нашого набору даних відсутні у попередньо натренованому словнику GloVe, і тому вони фактично ігноруються. Щоб вирішити це, ми можемо натренувати власні векторні представлення на основі нашого набору даних.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Контекстуальні вбудовування\n",
    "\n",
    "Одним із ключових обмежень традиційних попередньо навчених представлень вбудовувань, таких як Word2Vec, є те, що, хоча вони можуть передавати певне значення слова, вони не здатні розрізняти різні значення. Це може спричиняти проблеми в моделях, які використовуються далі.\n",
    "\n",
    "Наприклад, слово «play» має різні значення в цих двох реченнях:\n",
    "- Я ходив на **виставу** в театрі.\n",
    "- Джон хоче **гратися** зі своїми друзями.\n",
    "\n",
    "Попередньо навчені вбудовування, про які ми говорили, представляють обидва значення слова «play» в одному вбудовуванні. Щоб подолати це обмеження, нам потрібно створювати вбудовування на основі **мовної моделі**, яка тренується на великому корпусі тексту і *знає*, як слова можуть поєднуватися в різних контекстах. Обговорення контекстуальних вбудовувань виходить за рамки цього уроку, але ми повернемося до них, коли будемо говорити про мовні моделі в наступному розділі.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Відмова від відповідальності**:  \nЦей документ був перекладений за допомогою сервісу автоматичного перекладу [Co-op Translator](https://github.com/Azure/co-op-translator). Хоча ми прагнемо до точності, будь ласка, майте на увазі, що автоматичні переклади можуть містити помилки або неточності. Оригінальний документ на його рідній мові слід вважати авторитетним джерелом. Для критичної інформації рекомендується професійний людський переклад. Ми не несемо відповідальності за будь-які непорозуміння або неправильні тлумачення, що виникають внаслідок використання цього перекладу.\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0cb620c6d4b9f7a635928804c26cf22403d89d98d79684e4529119355ee6d5a5"
  },
  "kernel_info": {
   "name": "conda-env-py37_tensorflow-py"
  },
  "kernelspec": {
   "display_name": "py37_tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "coopTranslator": {
   "original_hash": "b859482be7f61d1eadc2c6a2720a37e4",
   "translation_date": "2025-08-30T10:34:40+00:00",
   "source_file": "lessons/5-NLP/14-Embeddings/EmbeddingsTF.ipynb",
   "language_code": "uk"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}