# Aprendizaje Profundo por Refuerzo

El aprendizaje por refuerzo (RL) se considera uno de los paradigmas b치sicos del aprendizaje autom치tico, junto al aprendizaje supervisado y no supervisado. Mientras que en el aprendizaje supervisado nos basamos en un conjunto de datos con resultados conocidos, el RL se basa en **aprender haciendo**. Por ejemplo, cuando vemos un videojuego por primera vez, comenzamos a jugar, incluso sin conocer las reglas, y pronto somos capaces de mejorar nuestras habilidades simplemente a trav칠s del proceso de jugar y ajustar nuestro comportamiento.

## [Cuestionario previo a la clase](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/122)

Para realizar RL, necesitamos:

* Un **entorno** o **simulador** que establezca las reglas del juego. Debemos poder ejecutar experimentos en el simulador y observar los resultados.
* Una **funci칩n de recompensa**, que indique cu치n exitoso fue nuestro experimento. En el caso de aprender a jugar un videojuego, la recompensa ser칤a nuestra puntuaci칩n final.

Bas치ndonos en la funci칩n de recompensa, deber칤amos poder ajustar nuestro comportamiento y mejorar nuestras habilidades, de modo que la pr칩xima vez juguemos mejor. La principal diferencia entre otros tipos de aprendizaje autom치tico y el RL es que en el RL t칤picamente no sabemos si ganamos o perdemos hasta que terminamos el juego. Por lo tanto, no podemos decir si un determinado movimiento por s칤 solo es bueno o no; solo recibimos una recompensa al final del juego.

Durante el RL, normalmente realizamos muchos experimentos. Durante cada experimento, necesitamos equilibrar entre seguir la estrategia 칩ptima que hemos aprendido hasta ahora (**explotaci칩n**) y explorar nuevos estados posibles (**exploraci칩n**).

## OpenAI Gym

Una gran herramienta para RL es el [OpenAI Gym](https://gym.openai.com/) - un **entorno de simulaci칩n**, que puede simular muchos entornos diferentes, desde juegos de Atari hasta la f칤sica detr치s del equilibrio de un palo. Es uno de los entornos de simulaci칩n m치s populares para entrenar algoritmos de aprendizaje por refuerzo, y es mantenido por [OpenAI](https://openai.com/).

> **Nota**: Puedes ver todos los entornos disponibles en OpenAI Gym [aqu칤](https://gym.openai.com/envs/#classic_control).

## Equilibrio de CartPole

Probablemente todos hayan visto dispositivos de equilibrio modernos como el *Segway* o *Gyroscooters*. Son capaces de equilibrarse autom치ticamente ajustando sus ruedas en respuesta a una se침al de un aceler칩metro o giroscopio. En esta secci칩n, aprenderemos c칩mo resolver un problema similar: equilibrar un palo. Es similar a una situaci칩n en la que un artista de circo necesita equilibrar un palo en su mano, pero este equilibrio de palo solo ocurre en 1D.

Una versi칩n simplificada del equilibrio se conoce como el problema de **CartPole**. En el mundo de cartpole, tenemos un deslizador horizontal que puede moverse hacia la izquierda o hacia la derecha, y el objetivo es equilibrar un palo vertical en la parte superior del deslizador mientras se mueve.

<img alt="un cartpole" src="images/cartpole.png" width="200"/>

Para crear y usar este entorno, necesitamos un par de l칤neas de c칩digo en Python:

```python
import gym
env = gym.make("CartPole-v1")

env.reset()
done = False
total_reward = 0
while not done:
   env.render()
   action = env.action_space.sample()
   observaton, reward, done, info = env.step(action)
   total_reward += reward

print(f"Total reward: {total_reward}")
```

Cada entorno se puede acceder exactamente de la misma manera:
* `env.reset` starts a new experiment
* `env.step` realiza un paso de simulaci칩n. Recibe una **acci칩n** del **espacio de acciones**, y devuelve una **observaci칩n** (del espacio de observaciones), as칤 como una recompensa y una bandera de terminaci칩n.

En el ejemplo anterior, realizamos una acci칩n aleatoria en cada paso, por lo que la vida del experimento es muy corta:

![cartpole sin equilibrio](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-nobalance.gif)

El objetivo de un algoritmo de RL es entrenar un modelo - la llamada **pol칤tica**  - que devolver치 la acci칩n en respuesta a un estado dado. Tambi칠n podemos considerar la pol칤tica como probabil칤stica, es decir, para cualquier estado *s* y acci칩n *a* devolver치 la probabilidad (*a*|*s*) de que debamos tomar *a* en el estado *s*.

## Algoritmo de Gradientes de Pol칤tica

La forma m치s obvia de modelar una pol칤tica es creando una red neuronal que tome estados como entrada y devuelva acciones correspondientes (o m치s bien las probabilidades de todas las acciones). En cierto sentido, ser칤a similar a una tarea de clasificaci칩n normal, con una gran diferencia: no sabemos de antemano qu칠 acciones debemos tomar en cada uno de los pasos.

La idea aqu칤 es estimar esas probabilidades. Construimos un vector de **recompensas acumulativas** que muestra nuestra recompensa total en cada paso del experimento. Tambi칠n aplicamos **descuento de recompensa** multiplicando las recompensas anteriores por alg칰n coeficiente 풥=0.99, con el fin de disminuir el papel de las recompensas anteriores. Luego, reforzamos esos pasos a lo largo del camino del experimento que generan mayores recompensas.

> Aprende m치s sobre el algoritmo de Gradiente de Pol칤tica y m칤ralo en acci칩n en el [cuaderno de ejemplo](../../../../../lessons/6-Other/22-DeepRL/CartPole-RL-TF.ipynb).

## Algoritmo Actor-Critic

Una versi칩n mejorada del enfoque de Gradientes de Pol칤tica se llama **Actor-Critic**. La idea principal detr치s de esto es que la red neuronal se entrenar칤a para devolver dos cosas:

* La pol칤tica, que determina qu칠 acci칩n tomar. Esta parte se llama **actor**.
* La estimaci칩n de la recompensa total que podemos esperar obtener en este estado - esta parte se llama **cr칤tico**.

En cierto sentido, esta arquitectura se asemeja a un [GAN](../../4-ComputerVision/10-GANs/README.md), donde tenemos dos redes que se entrenan entre s칤. En el modelo actor-cr칤tico, el actor propone la acci칩n que necesitamos tomar, y el cr칤tico intenta ser cr칤tico y estimar el resultado. Sin embargo, nuestro objetivo es entrenar esas redes de manera conjunta.

Dado que conocemos tanto las recompensas acumulativas reales como los resultados devueltos por el cr칤tico durante el experimento, es relativamente f치cil construir una funci칩n de p칠rdida que minimice la diferencia entre ellas. Eso nos dar칤a la **p칠rdida del cr칤tico**. Podemos calcular la **p칠rdida del actor** utilizando el mismo enfoque que en el algoritmo de gradiente de pol칤tica.

Despu칠s de ejecutar uno de estos algoritmos, podemos esperar que nuestro CartPole se comporte as칤:

![un cartpole en equilibrio](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-balance.gif)

## 九꽲잺 Ejercicios: Gradientes de Pol칤tica y RL Actor-Critic

Contin칰a tu aprendizaje en los siguientes cuadernos:

* [RL en TensorFlow](../../../../../lessons/6-Other/22-DeepRL/CartPole-RL-TF.ipynb)
* [RL en PyTorch](../../../../../lessons/6-Other/22-DeepRL/CartPole-RL-PyTorch.ipynb)

## Otras Tareas de RL

El Aprendizaje por Refuerzo hoy en d칤a es un campo de investigaci칩n de r치pido crecimiento. Algunos de los ejemplos interesantes de aprendizaje por refuerzo son:

* Ense침ar a una computadora a jugar **Juegos de Atari**. La parte desafiante de este problema es que no tenemos un estado simple representado como un vector, sino m치s bien una captura de pantalla, y necesitamos usar la CNN para convertir esta imagen de pantalla en un vector de caracter칤sticas, o extraer informaci칩n de recompensa. Los juegos de Atari est치n disponibles en el Gym.
* Ense침ar a una computadora a jugar juegos de mesa, como Ajedrez y Go. Recientemente, programas de vanguardia como **Alpha Zero** fueron entrenados desde cero por dos agentes jugando entre s칤 y mejorando en cada paso.
* En la industria, el RL se utiliza para crear sistemas de control a partir de simulaciones. Un servicio llamado [Bonsai](https://azure.microsoft.com/services/project-bonsai/?WT.mc_id=academic-77998-cacaste) est치 dise침ado espec칤ficamente para eso.

## Conclusi칩n

Ahora hemos aprendido c칩mo entrenar agentes para lograr buenos resultados simplemente proporcion치ndoles una funci칩n de recompensa que define el estado deseado del juego y d치ndoles la oportunidad de explorar inteligentemente el espacio de b칰squeda. Hemos probado con 칠xito dos algoritmos y logrado un buen resultado en un per칤odo de tiempo relativamente corto. Sin embargo, este es solo el comienzo de tu viaje en RL, y definitivamente deber칤as considerar tomar un curso separado si deseas profundizar m치s.

## 游 Desaf칤o

Explora las aplicaciones enumeradas en la secci칩n 'Otras Tareas de RL' y trata de implementar una.

## [Cuestionario posterior a la clase](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/222)

## Revisi칩n y Autoestudio

Aprende m치s sobre el aprendizaje por refuerzo cl치sico en nuestro [Curr칤culo de Aprendizaje Autom치tico para Principiantes](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/README.md).

Mira [este gran video](https://www.youtube.com/watch?v=qv6UVOQ0F44) que habla sobre c칩mo una computadora puede aprender a jugar Super Mario.

## Tarea: [Entrenar un Mountain Car](lab/README.md)

Tu objetivo durante esta tarea ser칤a entrenar un entorno diferente de Gym - [Mountain Car](https://www.gymlibrary.ml/environments/classic_control/mountain_car/).

**Descargo de responsabilidad**:  
Este documento ha sido traducido utilizando servicios de traducci칩n autom치tica basados en inteligencia artificial. Aunque nos esforzamos por lograr precisi칩n, tenga en cuenta que las traducciones automatizadas pueden contener errores o inexactitudes. El documento original en su idioma nativo debe considerarse la fuente autorizada. Para informaci칩n cr칤tica, se recomienda una traducci칩n profesional humana. No nos hacemos responsables de malentendidos o malas interpretaciones que surjan del uso de esta traducci칩n.