# Redes generativas

## [Cuestionario previo a la clase](https://ff-quizzes.netlify.app/en/ai/quiz/33)

Las Redes Neuronales Recurrentes (RNNs) y sus variantes con celdas de compuerta, como las Long Short Term Memory Cells (LSTMs) y las Gated Recurrent Units (GRUs), proporcionan un mecanismo para el modelado del lenguaje, ya que pueden aprender el orden de las palabras y ofrecer predicciones para la siguiente palabra en una secuencia. Esto nos permite usar RNNs para tareas **generativas**, como la generaci√≥n de texto ordinario, la traducci√≥n autom√°tica e incluso la generaci√≥n de descripciones de im√°genes.

> ‚úÖ Piensa en todas las veces que te has beneficiado de tareas generativas, como la autocompletaci√≥n de texto mientras escribes. Investiga tus aplicaciones favoritas para ver si utilizan RNNs.

En la arquitectura de RNN que discutimos en la unidad anterior, cada unidad RNN produc√≠a el siguiente estado oculto como salida. Sin embargo, tambi√©n podemos agregar otra salida a cada unidad recurrente, lo que nos permitir√≠a generar una **secuencia** (que tiene la misma longitud que la secuencia original). Adem√°s, podemos usar unidades RNN que no aceptan una entrada en cada paso, sino que toman un vector de estado inicial y luego producen una secuencia de salidas.

Esto permite diferentes arquitecturas neuronales, como se muestra en la imagen a continuaci√≥n:

![Imagen que muestra patrones comunes de redes neuronales recurrentes.](../../../../../translated_images/es/unreasonable-effectiveness-of-rnn.541ead816778f42d.webp)

> Imagen del blog [Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) por [Andrej Karpaty](http://karpathy.github.io/)

* **Uno a uno** es una red neuronal tradicional con una entrada y una salida.
* **Uno a muchos** es una arquitectura generativa que acepta un valor de entrada y genera una secuencia de valores de salida. Por ejemplo, si queremos entrenar una red de **descripci√≥n de im√°genes** que produzca una descripci√≥n textual de una imagen, podemos tomar una imagen como entrada, pasarla por una CNN para obtener su estado oculto y luego usar una cadena recurrente para generar la descripci√≥n palabra por palabra.
* **Muchos a uno** corresponde a las arquitecturas RNN que describimos en la unidad anterior, como la clasificaci√≥n de texto.
* **Muchos a muchos**, o **secuencia a secuencia**, corresponde a tareas como la **traducci√≥n autom√°tica**, donde primero una RNN recopila toda la informaci√≥n de la secuencia de entrada en el estado oculto, y otra cadena RNN despliega este estado en la secuencia de salida.

En esta unidad, nos enfocaremos en modelos generativos simples que nos ayuden a generar texto. Para simplificar, utilizaremos tokenizaci√≥n a nivel de caracteres.

Entrenaremos esta RNN para generar texto paso a paso. En cada paso, tomaremos una secuencia de caracteres de longitud `nchars` y pediremos a la red que genere el siguiente car√°cter de salida para cada car√°cter de entrada:

![Imagen que muestra un ejemplo de generaci√≥n de la palabra 'HELLO' con una RNN.](../../../../../translated_images/es/rnn-generate.56c54afb52f9781d.webp)

Cuando generamos texto (durante la inferencia), comenzamos con un **prompt**, que se pasa por las celdas RNN para generar su estado intermedio, y luego comienza la generaci√≥n desde este estado. Generamos un car√°cter a la vez y pasamos el estado y el car√°cter generado a otra celda RNN para generar el siguiente, hasta que generemos suficientes caracteres.

<img src="../../../../../translated_images/es/rnn-generate-inf.5168dc65e0370eea.webp" width="60%"/>

> Imagen del autor

## ‚úçÔ∏è Ejercicios: Redes generativas

Contin√∫a tu aprendizaje en los siguientes notebooks:

* [Redes generativas con PyTorch](GenerativePyTorch.ipynb)
* [Redes generativas con TensorFlow](GenerativeTF.ipynb)

## Generaci√≥n de texto suave y temperatura

La salida de cada celda RNN es una distribuci√≥n de probabilidad de caracteres. Si siempre tomamos el car√°cter con la mayor probabilidad como el siguiente car√°cter en el texto generado, el texto puede volverse "c√≠clico", repitiendo las mismas secuencias de caracteres una y otra vez, como en este ejemplo:

```
today of the second the company and a second the company ...
```

Sin embargo, si observamos la distribuci√≥n de probabilidad para el siguiente car√°cter, podr√≠a ser que la diferencia entre las probabilidades m√°s altas no sea muy grande, por ejemplo, un car√°cter puede tener una probabilidad de 0.2 y otro de 0.19, etc. Por ejemplo, al buscar el siguiente car√°cter en la secuencia '*play*', el siguiente car√°cter podr√≠a ser igualmente un espacio o **e** (como en la palabra *player*).

Esto nos lleva a la conclusi√≥n de que no siempre es "justo" seleccionar el car√°cter con mayor probabilidad, ya que elegir el segundo m√°s alto a√∫n podr√≠a llevarnos a un texto significativo. Es m√°s sabio **muestrear** caracteres de la distribuci√≥n de probabilidad dada por la salida de la red. Tambi√©n podemos usar un par√°metro, **temperatura**, que suaviza la distribuci√≥n de probabilidad si queremos agregar m√°s aleatoriedad, o la hace m√°s pronunciada si queremos ce√±irnos m√°s a los caracteres de mayor probabilidad.

Explora c√≥mo se implementa esta generaci√≥n de texto suave en los notebooks vinculados anteriormente.

## Conclusi√≥n

Aunque la generaci√≥n de texto puede ser √∫til por s√≠ misma, los mayores beneficios provienen de la capacidad de generar texto utilizando RNNs a partir de alg√∫n vector de caracter√≠sticas inicial. Por ejemplo, la generaci√≥n de texto se utiliza como parte de la traducci√≥n autom√°tica (secuencia a secuencia, en este caso el vector de estado del *encoder* se utiliza para generar o *decodificar* el mensaje traducido), o para generar descripciones textuales de una imagen (en cuyo caso el vector de caracter√≠sticas provendr√≠a de un extractor CNN).

## üöÄ Desaf√≠o

Toma algunas lecciones en Microsoft Learn sobre este tema:

* Generaci√≥n de texto con [PyTorch](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-pytorch/6-generative-networks/?WT.mc_id=academic-77998-cacaste)/[TensorFlow](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-tensorflow/5-generative-networks/?WT.mc_id=academic-77998-cacaste)

## [Cuestionario posterior a la clase](https://ff-quizzes.netlify.app/en/ai/quiz/34)

## Revisi√≥n y autoestudio

Aqu√≠ tienes algunos art√≠culos para ampliar tu conocimiento:

* Diferentes enfoques para la generaci√≥n de texto con Markov Chain, LSTM y GPT-2: [blog post](https://towardsdatascience.com/text-generation-gpt-2-lstm-markov-chain-9ea371820e1e)
* Ejemplo de generaci√≥n de texto en la [documentaci√≥n de Keras](https://keras.io/examples/generative/lstm_character_level_text_generation/)

## [Asignaci√≥n](lab/README.md)

Hemos visto c√≥mo generar texto car√°cter por car√°cter. En el laboratorio, explorar√°s la generaci√≥n de texto a nivel de palabras.

---

