# Mecanismos de Atenci贸n y Transformadores

## [Cuestionario previo a la clase](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/118)

Uno de los problemas m谩s importantes en el dominio del PLN es la **traducci贸n autom谩tica**, una tarea esencial que subyace a herramientas como Google Translate. En esta secci贸n, nos enfocaremos en la traducci贸n autom谩tica, o, de manera m谩s general, en cualquier tarea de *secuencia a secuencia* (que tambi茅n se llama **transducci贸n de oraciones**).

Con las RNN, la secuencia a secuencia se implementa mediante dos redes recurrentes, donde una red, el **codificador**, colapsa una secuencia de entrada en un estado oculto, mientras que otra red, el **decodificador**, despliega este estado oculto en un resultado traducido. Hay un par de problemas con este enfoque:

* El estado final de la red del codificador tiene dificultades para recordar el comienzo de una oraci贸n, lo que provoca una mala calidad del modelo para oraciones largas.
* Todas las palabras en una secuencia tienen el mismo impacto en el resultado. Sin embargo, en la realidad, ciertas palabras en la secuencia de entrada a menudo tienen m谩s impacto en las salidas secuenciales que otras.

**Los Mecanismos de Atenci贸n** proporcionan un medio para ponderar el impacto contextual de cada vector de entrada en cada predicci贸n de salida de la RNN. La forma en que se implementa es creando atajos entre los estados intermedios de la RNN de entrada y la RNN de salida. De esta manera, al generar el s铆mbolo de salida y<sub>t</sub>, tomaremos en cuenta todos los estados ocultos de entrada h<sub>i</sub>, con diferentes coeficientes de peso 伪<sub>t,i</sub>.

![Imagen que muestra un modelo de codificador/decodificador con una capa de atenci贸n aditiva](../../../../../translated_images/encoder-decoder-attention.7a726296894fb567aa2898c94b17b3289087f6705c11907df8301df9e5eeb3de.es.png)

> El modelo codificador-decodificador con mecanismo de atenci贸n aditiva en [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf), citado de [esta publicaci贸n de blog](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)

La matriz de atenci贸n {伪<sub>i,j</sub>} representar铆a el grado en que ciertas palabras de entrada juegan un papel en la generaci贸n de una palabra dada en la secuencia de salida. A continuaci贸n se muestra un ejemplo de tal matriz:

![Imagen que muestra una alineaci贸n de muestra encontrada por RNNsearch-50, tomada de Bahdanau - arviz.org](../../../../../translated_images/bahdanau-fig3.09ba2d37f202a6af11de6c82d2d197830ba5f4528d9ea430eb65fd3a75065973.es.png)

> Figura de [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) (Fig.3)

Los mecanismos de atenci贸n son responsables de gran parte del estado actual o casi actual de la t茅cnica en PLN. Sin embargo, a帽adir atenci贸n aumenta considerablemente el n煤mero de par谩metros del modelo, lo que llev贸 a problemas de escalabilidad con las RNN. Una restricci贸n clave de la escalabilidad de las RNN es que la naturaleza recurrente de los modelos hace que sea un desaf铆o agrupar y paralelizar el entrenamiento. En una RNN, cada elemento de una secuencia necesita ser procesado en orden secuencial, lo que significa que no se puede paralelizar f谩cilmente.

![Codificador Decodificador con Atenci贸n](../../../../../lessons/5-NLP/18-Transformers/images/EncDecAttention.gif)

> Figura de [Google's Blog](https://research.googleblog.com/2016/09/a-neural-network-for-machine.html)

La adopci贸n de mecanismos de atenci贸n combinados con esta restricci贸n llev贸 a la creaci贸n de los ahora modelos transformadores de 煤ltima generaci贸n que conocemos y utilizamos hoy, como BERT y Open-GPT3.

## Modelos Transformadores

Una de las ideas principales detr谩s de los transformadores es evitar la naturaleza secuencial de las RNN y crear un modelo que sea paralelizable durante el entrenamiento. Esto se logra implementando dos ideas:

* codificaci贸n posicional
* uso de un mecanismo de autoatenci贸n para capturar patrones en lugar de RNNs (o CNNs) (por eso el art铆culo que introduce los transformadores se llama *[Attention is all you need](https://arxiv.org/abs/1706.03762)*)

### Codificaci贸n/Embebido Posicional

La idea de la codificaci贸n posicional es la siguiente. 
1. Al usar RNNs, la posici贸n relativa de los tokens est谩 representada por el n煤mero de pasos, y por lo tanto no necesita ser representada expl铆citamente. 
2. Sin embargo, una vez que cambiamos a atenci贸n, necesitamos conocer las posiciones relativas de los tokens dentro de una secuencia. 
3. Para obtener la codificaci贸n posicional, aumentamos nuestra secuencia de tokens con una secuencia de posiciones de tokens en la secuencia (es decir, una secuencia de n煤meros 0,1, ...).
4. Luego mezclamos la posici贸n del token con un vector de embebido del token. Para transformar la posici贸n (entero) en un vector, podemos usar diferentes enfoques:

* Embebido entrenable, similar al embebido de tokens. Este es el enfoque que consideramos aqu铆. Aplicamos capas de embebido sobre ambos, los tokens y sus posiciones, resultando en vectores de embebido de las mismas dimensiones, que luego sumamos.
* Funci贸n de codificaci贸n de posici贸n fija, como se propuso en el art铆culo original.

<img src="images/pos-embedding.png" width="50%"/>

> Imagen del autor

El resultado que obtenemos con el embebido posicional integra tanto el token original como su posici贸n dentro de una secuencia.

### Autoatenci贸n Multi-Cabeza

A continuaci贸n, necesitamos capturar algunos patrones dentro de nuestra secuencia. Para hacer esto, los transformadores utilizan un mecanismo de **autoatenci贸n**, que es esencialmente atenci贸n aplicada a la misma secuencia como entrada y salida. Aplicar autoatenci贸n nos permite tener en cuenta el **contexto** dentro de la oraci贸n y ver qu茅 palabras est谩n interrelacionadas. Por ejemplo, nos permite ver qu茅 palabras son referidas por co-referencias, como *ello*, y tambi茅n tener en cuenta el contexto:

![](../../../../../translated_images/CoreferenceResolution.861924d6d384a7d68d8d0039d06a71a151f18a796b8b1330239d3590bd4947eb.es.png)

> Imagen del [Blog de Google](https://research.googleblog.com/2017/08/transformer-novel-neural-network.html)

En los transformadores, utilizamos **Atenci贸n Multi-Cabeza** para darle al modelo la capacidad de capturar varios tipos diferentes de dependencias, por ejemplo, relaciones de palabras a largo plazo frente a corto plazo, co-referencia frente a algo m谩s, etc.

[Notebook de TensorFlow](../../../../../lessons/5-NLP/18-Transformers/TransformersTF.ipynb) contiene m谩s detalles sobre la implementaci贸n de las capas transformadoras.

### Atenci贸n Codificador-Decodificador

En los transformadores, la atenci贸n se utiliza en dos lugares:

* Para capturar patrones dentro del texto de entrada utilizando autoatenci贸n
* Para realizar traducci贸n de secuencias - es la capa de atenci贸n entre el codificador y el decodificador.

La atenci贸n codificador-decodificador es muy similar al mecanismo de atenci贸n utilizado en las RNN, como se describi贸 al principio de esta secci贸n. Este diagrama animado explica el papel de la atenci贸n codificador-decodificador.

![GIF animado que muestra c贸mo se realizan las evaluaciones en modelos transformadores.](../../../../../lessons/5-NLP/18-Transformers/images/transformer-animated-explanation.gif)

Dado que cada posici贸n de entrada se mapea independientemente a cada posici贸n de salida, los transformadores pueden paralelizarse mejor que las RNN, lo que permite modelos de lenguaje mucho m谩s grandes y expresivos. Cada cabeza de atenci贸n puede ser utilizada para aprender diferentes relaciones entre palabras que mejoran las tareas de Procesamiento de Lenguaje Natural posteriores.

## BERT

**BERT** (Representaciones de Codificador Bidireccional de Transformadores) es una red transformadora de m煤ltiples capas muy grande con 12 capas para *BERT-base*, y 24 para *BERT-large*. El modelo se preentrena primero en un gran corpus de datos textuales (WikiPedia + libros) utilizando entrenamiento no supervisado (prediciendo palabras enmascaradas en una oraci贸n). Durante el preentrenamiento, el modelo absorbe niveles significativos de comprensi贸n del lenguaje que luego pueden ser aprovechados con otros conjuntos de datos utilizando ajuste fino. Este proceso se llama **aprendizaje por transferencia**.

![imagen de http://jalammar.github.io/illustrated-bert/](../../../../../translated_images/jalammarBERT-language-modeling-masked-lm.34f113ea5fec4362e39ee4381aab7cad06b5465a0b5f053a0f2aa05fbe14e746.es.png)

> Imagen [fuente](http://jalammar.github.io/illustrated-bert/)

## 锔 Ejercicios: Transformadores

Contin煤a tu aprendizaje en los siguientes cuadernos:

* [Transformadores en PyTorch](../../../../../lessons/5-NLP/18-Transformers/TransformersPyTorch.ipynb)
* [Transformadores en TensorFlow](../../../../../lessons/5-NLP/18-Transformers/TransformersTF.ipynb)

## Conclusi贸n

En esta lecci贸n aprendiste sobre Transformadores y Mecanismos de Atenci贸n, herramientas esenciales en la caja de herramientas del PLN. Hay muchas variaciones de arquitecturas de Transformadores, incluyendo BERT, DistilBERT, BigBird, OpenGPT3 y m谩s que pueden ser ajustadas. El [paquete HuggingFace](https://github.com/huggingface/) proporciona un repositorio para entrenar muchas de estas arquitecturas tanto con PyTorch como con TensorFlow.

##  Desaf铆o

## [Cuestionario posterior a la clase](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/218)

## Revisi贸n y Autoestudio

* [Publicaci贸n de blog](https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/), que explica el cl谩sico art铆culo [Attention is all you need](https://arxiv.org/abs/1706.03762) sobre transformadores.
* [Una serie de publicaciones de blog](https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452) sobre transformadores, que explican la arquitectura en detalle.

## [Asignaci贸n](assignment.md)

**Descargo de responsabilidad**:  
Este documento ha sido traducido utilizando servicios de traducci贸n autom谩tica basados en inteligencia artificial. Si bien nos esforzamos por lograr precisi贸n, tenga en cuenta que las traducciones automatizadas pueden contener errores o inexactitudes. El documento original en su idioma nativo debe considerarse la fuente autorizada. Para informaci贸n cr铆tica, se recomienda una traducci贸n profesional realizada por humanos. No nos hacemos responsables de malentendidos o malas interpretaciones que surjan del uso de esta traducci贸n.