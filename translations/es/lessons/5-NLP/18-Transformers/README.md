<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "7e617f0b8de85a43957a853aba09bfeb",
  "translation_date": "2025-08-24T10:17:30+00:00",
  "source_file": "lessons/5-NLP/18-Transformers/README.md",
  "language_code": "es"
}
-->
# Mecanismos de Atenci贸n y Transformadores

## [Cuestionario previo a la clase](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/118)

Uno de los problemas m谩s importantes en el 谩mbito de NLP es la **traducci贸n autom谩tica**, una tarea esencial que sustenta herramientas como Google Translate. En esta secci贸n, nos centraremos en la traducci贸n autom谩tica o, m谩s generalmente, en cualquier tarea de *secuencia a secuencia* (tambi茅n llamada **transducci贸n de oraciones**).

Con las RNNs, la tarea de secuencia a secuencia se implementa mediante dos redes recurrentes, donde una red, el **codificador**, comprime una secuencia de entrada en un estado oculto, mientras que otra red, el **decodificador**, despliega este estado oculto en un resultado traducido. Hay algunos problemas con este enfoque:

* El estado final de la red codificadora tiene dificultades para recordar el inicio de una oraci贸n, lo que provoca una baja calidad del modelo para oraciones largas.
* Todas las palabras en una secuencia tienen el mismo impacto en el resultado. Sin embargo, en la realidad, palabras espec铆ficas en la secuencia de entrada suelen tener m谩s impacto en las salidas secuenciales que otras.

Los **mecanismos de atenci贸n** proporcionan un medio para ponderar el impacto contextual de cada vector de entrada en cada predicci贸n de salida de la RNN. La forma en que se implementa es creando atajos entre los estados intermedios de la RNN de entrada y la RNN de salida. De esta manera, al generar el s铆mbolo de salida y<sub>t</sub>, tomaremos en cuenta todos los estados ocultos de entrada h<sub>i</sub>, con diferentes coeficientes de peso 伪<sub>t,i</sub>.

![Imagen que muestra un modelo codificador/decodificador con una capa de atenci贸n aditiva](../../../../../lessons/5-NLP/18-Transformers/images/encoder-decoder-attention.png)

> El modelo codificador-decodificador con mecanismo de atenci贸n aditiva en [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf), citado de [este blog](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)

La matriz de atenci贸n {伪<sub>i,j</sub>} representar铆a el grado en que ciertas palabras de entrada influyen en la generaci贸n de una palabra dada en la secuencia de salida. A continuaci贸n, se muestra un ejemplo de dicha matriz:

![Imagen que muestra una alineaci贸n de muestra encontrada por RNNsearch-50, tomada de Bahdanau - arviz.org](../../../../../lessons/5-NLP/18-Transformers/images/bahdanau-fig3.png)

> Figura de [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) (Fig.3)

Los mecanismos de atenci贸n son responsables de gran parte del estado del arte actual o cercano al actual en NLP. Sin embargo, agregar atenci贸n aumenta significativamente el n煤mero de par谩metros del modelo, lo que llev贸 a problemas de escalabilidad con las RNNs. Una restricci贸n clave para escalar las RNNs es que la naturaleza recurrente de los modelos dificulta el procesamiento en lotes y la paralelizaci贸n del entrenamiento. En una RNN, cada elemento de una secuencia necesita ser procesado en orden secuencial, lo que significa que no se puede paralelizar f谩cilmente.

![Codificador-Decodificador con Atenci贸n](../../../../../lessons/5-NLP/18-Transformers/images/EncDecAttention.gif)

> Figura del [Blog de Google](https://research.googleblog.com/2016/09/a-neural-network-for-machine.html)

La adopci贸n de mecanismos de atenci贸n combinada con esta restricci贸n llev贸 a la creaci贸n de los modelos transformadores, ahora estado del arte, que conocemos y usamos hoy en d铆a, como BERT y Open-GPT3.

## Modelos Transformadores

Una de las ideas principales detr谩s de los transformadores es evitar la naturaleza secuencial de las RNNs y crear un modelo que sea paralelizable durante el entrenamiento. Esto se logra implementando dos ideas:

* codificaci贸n posicional
* uso del mecanismo de auto-atenci贸n para capturar patrones en lugar de RNNs (o CNNs) (por eso el art铆culo que introduce los transformadores se llama *[Attention is all you need](https://arxiv.org/abs/1706.03762)*)

### Codificaci贸n/Embebido Posicional

La idea de la codificaci贸n posicional es la siguiente:  
1. Al usar RNNs, la posici贸n relativa de los tokens est谩 representada por el n煤mero de pasos, y por lo tanto no necesita ser representada expl铆citamente.  
2. Sin embargo, una vez que cambiamos a atenci贸n, necesitamos conocer las posiciones relativas de los tokens dentro de una secuencia.  
3. Para obtener la codificaci贸n posicional, ampliamos nuestra secuencia de tokens con una secuencia de posiciones de tokens en la secuencia (es decir, una secuencia de n煤meros 0,1, ...).  
4. Luego mezclamos la posici贸n del token con un vector de embebido del token. Para transformar la posici贸n (entero) en un vector, podemos usar diferentes enfoques:

* Embebido entrenable, similar al embebido de tokens. Este es el enfoque que consideramos aqu铆. Aplicamos capas de embebido tanto a los tokens como a sus posiciones, lo que da como resultado vectores de embebido de las mismas dimensiones, que luego sumamos.
* Funci贸n de codificaci贸n posicional fija, como se propone en el art铆culo original.

<img src="images/pos-embedding.png" width="50%"/>

> Imagen del autor

El resultado que obtenemos con el embebido posicional incluye tanto el token original como su posici贸n dentro de una secuencia.

### Auto-Atenci贸n Multi-Cabezal

A continuaci贸n, necesitamos capturar algunos patrones dentro de nuestra secuencia. Para hacerlo, los transformadores utilizan un mecanismo de **auto-atenci贸n**, que es esencialmente atenci贸n aplicada a la misma secuencia como entrada y salida. Aplicar auto-atenci贸n nos permite tomar en cuenta el **contexto** dentro de la oraci贸n y ver qu茅 palabras est谩n interrelacionadas. Por ejemplo, nos permite ver qu茅 palabras son referidas por correferencias, como *it*, y tambi茅n tomar el contexto en cuenta:

![](../../../../../lessons/5-NLP/18-Transformers/images/CoreferenceResolution.png)

> Imagen del [Blog de Google](https://research.googleblog.com/2017/08/transformer-novel-neural-network.html)

En los transformadores, usamos **atenci贸n multi-cabezal** para darle a la red la capacidad de capturar varios tipos diferentes de dependencias, por ejemplo, relaciones de palabras a largo plazo frente a corto plazo, correferencia frente a algo m谩s, etc.

[Notebook de TensorFlow](../../../../../lessons/5-NLP/18-Transformers/TransformersTF.ipynb) contiene m谩s detalles sobre la implementaci贸n de capas de transformadores.

### Atenci贸n Codificador-Decodificador

En los transformadores, la atenci贸n se utiliza en dos lugares:

* Para capturar patrones dentro del texto de entrada utilizando auto-atenci贸n.
* Para realizar la traducci贸n de secuencias: es la capa de atenci贸n entre el codificador y el decodificador.

La atenci贸n codificador-decodificador es muy similar al mecanismo de atenci贸n utilizado en las RNNs, como se describi贸 al inicio de esta secci贸n. Este diagrama animado explica el papel de la atenci贸n codificador-decodificador.

![GIF animado que muestra c贸mo se realizan las evaluaciones en los modelos transformadores.](../../../../../lessons/5-NLP/18-Transformers/images/transformer-animated-explanation.gif)

Dado que cada posici贸n de entrada se mapea independientemente a cada posici贸n de salida, los transformadores pueden paralelizar mejor que las RNNs, lo que permite modelos de lenguaje mucho m谩s grandes y expresivos. Cada cabeza de atenci贸n puede usarse para aprender diferentes relaciones entre palabras que mejoran las tareas de procesamiento de lenguaje natural.

## BERT

**BERT** (Representaciones de Codificador Bidireccional de Transformadores) es una red transformadora muy grande con m煤ltiples capas: 12 capas para *BERT-base* y 24 para *BERT-large*. El modelo se preentrena primero en un gran corpus de datos de texto (Wikipedia + libros) utilizando entrenamiento no supervisado (predicci贸n de palabras enmascaradas en una oraci贸n). Durante el preentrenamiento, el modelo absorbe niveles significativos de comprensi贸n del lenguaje que luego pueden aprovecharse con otros conjuntos de datos mediante ajuste fino. Este proceso se llama **aprendizaje por transferencia**.

![imagen de http://jalammar.github.io/illustrated-bert/](../../../../../lessons/5-NLP/18-Transformers/images/jalammarBERT-language-modeling-masked-lm.png)

> Imagen [fuente](http://jalammar.github.io/illustrated-bert/)

## 锔 Ejercicios: Transformadores

Contin煤a tu aprendizaje en los siguientes notebooks:

* [Transformadores en PyTorch](../../../../../lessons/5-NLP/18-Transformers/TransformersPyTorch.ipynb)
* [Transformadores en TensorFlow](../../../../../lessons/5-NLP/18-Transformers/TransformersTF.ipynb)

## Conclusi贸n

En esta lecci贸n aprendiste sobre Transformadores y Mecanismos de Atenci贸n, herramientas esenciales en el conjunto de herramientas de NLP. Existen muchas variaciones de arquitecturas de transformadores, incluyendo BERT, DistilBERT, BigBird, OpenGPT3 y m谩s, que pueden ajustarse. El paquete [HuggingFace](https://github.com/huggingface/) proporciona un repositorio para entrenar muchas de estas arquitecturas con PyTorch y TensorFlow.

##  Desaf铆o

## [Cuestionario posterior a la clase](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/218)

## Revisi贸n y Estudio Aut贸nomo

* [Publicaci贸n de blog](https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/), que explica el cl谩sico art铆culo [Attention is all you need](https://arxiv.org/abs/1706.03762) sobre transformadores.
* [Una serie de publicaciones de blog](https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452) sobre transformadores, que explican la arquitectura en detalle.

## [Tarea](assignment.md)

**Descargo de responsabilidad**:  
Este documento ha sido traducido utilizando el servicio de traducci贸n autom谩tica [Co-op Translator](https://github.com/Azure/co-op-translator). Si bien nos esforzamos por lograr precisi贸n, tenga en cuenta que las traducciones autom谩ticas pueden contener errores o imprecisiones. El documento original en su idioma nativo debe considerarse como la fuente autorizada. Para informaci贸n cr铆tica, se recomienda una traducci贸n profesional realizada por humanos. No nos hacemos responsables de malentendidos o interpretaciones err贸neas que puedan surgir del uso de esta traducci贸n.