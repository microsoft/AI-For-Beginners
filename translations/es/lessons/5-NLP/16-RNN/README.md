# Redes Neuronales Recurrentes

## [Cuestionario previo a la clase](https://ff-quizzes.netlify.app/en/ai/quiz/31)

En secciones anteriores, hemos estado utilizando representaciones sem√°nticas enriquecidas de texto y un clasificador lineal simple sobre las incrustaciones. Lo que hace esta arquitectura es capturar el significado agregado de las palabras en una oraci√≥n, pero no toma en cuenta el **orden** de las palabras, ya que la operaci√≥n de agregaci√≥n sobre las incrustaciones elimina esta informaci√≥n del texto original. Debido a que estos modelos no pueden modelar el orden de las palabras, no pueden resolver tareas m√°s complejas o ambiguas como la generaci√≥n de texto o la respuesta a preguntas.

Para capturar el significado de una secuencia de texto, necesitamos usar otra arquitectura de red neuronal, llamada **red neuronal recurrente**, o RNN. En una RNN, pasamos nuestra oraci√≥n a trav√©s de la red un s√≠mbolo a la vez, y la red produce un **estado**, que luego pasamos nuevamente a la red junto con el siguiente s√≠mbolo.

![RNN](../../../../../translated_images/es/rnn.27f5c29c53d727b5.webp)

> Imagen del autor

Dada la secuencia de entrada de tokens X<sub>0</sub>,...,X<sub>n</sub>, la RNN crea una secuencia de bloques de red neuronal y entrena esta secuencia de extremo a extremo utilizando retropropagaci√≥n. Cada bloque de red toma un par (X<sub>i</sub>,S<sub>i</sub>) como entrada y produce S<sub>i+1</sub> como resultado. El estado final S<sub>n</sub> o (salida Y<sub>n</sub>) se pasa a un clasificador lineal para producir el resultado. Todos los bloques de red comparten los mismos pesos y se entrenan de extremo a extremo utilizando una sola pasada de retropropagaci√≥n.

Debido a que los vectores de estado S<sub>0</sub>,...,S<sub>n</sub> se pasan a trav√©s de la red, esta es capaz de aprender las dependencias secuenciales entre palabras. Por ejemplo, cuando la palabra *no* aparece en alg√∫n lugar de la secuencia, puede aprender a negar ciertos elementos dentro del vector de estado, resultando en una negaci√≥n.

> ‚úÖ Dado que los pesos de todos los bloques de RNN en la imagen anterior son compartidos, la misma imagen puede representarse como un solo bloque (a la derecha) con un bucle de retroalimentaci√≥n recurrente, que pasa el estado de salida de la red nuevamente a la entrada.

## Anatom√≠a de una C√©lula RNN

Veamos c√≥mo est√° organizada una c√©lula RNN simple. Acepta el estado anterior S<sub>i-1</sub> y el s√≠mbolo actual X<sub>i</sub> como entradas, y debe producir el estado de salida S<sub>i</sub> (y, a veces, tambi√©n estamos interesados en alguna otra salida Y<sub>i</sub>, como en el caso de redes generativas).

Una c√©lula RNN simple tiene dos matrices de pesos internas: una transforma un s√≠mbolo de entrada (llam√©mosla W) y otra transforma un estado de entrada (H). En este caso, la salida de la red se calcula como &sigma;(W&times;X<sub>i</sub>+H&times;S<sub>i-1</sub>+b), donde &sigma; es la funci√≥n de activaci√≥n y b es un sesgo adicional.

<img alt="Anatom√≠a de una c√©lula RNN" src="../../../../../translated_images/es/rnn-anatomy.79ee3f3920b3294b.webp" width="50%"/>

> Imagen del autor

En muchos casos, los tokens de entrada se pasan a trav√©s de la capa de incrustaci√≥n antes de ingresar a la RNN para reducir la dimensionalidad. En este caso, si la dimensi√≥n de los vectores de entrada es *emb_size*, y el vector de estado es *hid_size*, el tama√±o de W es *emb_size*&times;*hid_size*, y el tama√±o de H es *hid_size*&times;*hid_size*.

## Memoria a Largo y Corto Plazo (LSTM)

Uno de los principales problemas de las RNN cl√°sicas es el llamado problema de **gradientes que se desvanecen**. Debido a que las RNN se entrenan de extremo a extremo en una sola pasada de retropropagaci√≥n, tienen dificultades para propagar el error a las primeras capas de la red, y por lo tanto, la red no puede aprender relaciones entre tokens distantes. Una de las formas de evitar este problema es introducir **gesti√≥n expl√≠cita del estado** utilizando los llamados **puertas**. Hay dos arquitecturas bien conocidas de este tipo: **Memoria a Largo y Corto Plazo** (LSTM) y **Unidad de Relevo con Puerta** (GRU).

![Imagen mostrando un ejemplo de c√©lula de memoria a largo y corto plazo](../../../../../lessons/5-NLP/16-RNN/images/long-short-term-memory-cell.svg)

> Fuente de la imagen por determinar

La red LSTM est√° organizada de manera similar a la RNN, pero hay dos estados que se pasan de capa a capa: el estado real C y el vector oculto H. En cada unidad, el vector oculto H<sub>i</sub> se concatena con la entrada X<sub>i</sub>, y controlan lo que sucede con el estado C a trav√©s de **puertas**. Cada puerta es una red neuronal con activaci√≥n sigmoide (salida en el rango [0,1]), que puede considerarse como una m√°scara de bits cuando se multiplica por el vector de estado. Las puertas son las siguientes (de izquierda a derecha en la imagen anterior):

* La **puerta de olvido** toma un vector oculto y determina qu√© componentes del vector C necesitamos olvidar y cu√°les pasar.
* La **puerta de entrada** toma informaci√≥n de los vectores de entrada y ocultos e inserta esta informaci√≥n en el estado.
* La **puerta de salida** transforma el estado a trav√©s de una capa lineal con activaci√≥n *tanh*, luego selecciona algunos de sus componentes utilizando un vector oculto H<sub>i</sub> para producir un nuevo estado C<sub>i+1</sub>.

Los componentes del estado C pueden considerarse como algunas banderas que pueden activarse o desactivarse. Por ejemplo, cuando encontramos un nombre como *Alice* en la secuencia, podr√≠amos asumir que se refiere a un personaje femenino y activar la bandera en el estado que indica que tenemos un sustantivo femenino en la oraci√≥n. Cuando m√°s adelante encontramos frases como *y Tom*, activaremos la bandera que indica que tenemos un sustantivo plural. As√≠, manipulando el estado, podemos supuestamente realizar un seguimiento de las propiedades gramaticales de las partes de la oraci√≥n.

> ‚úÖ Un excelente recurso para entender los detalles internos de LSTM es este gran art√≠culo [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) de Christopher Olah.

## RNN Bidireccionales y Multicapa

Hemos discutido redes recurrentes que operan en una direcci√≥n, desde el inicio de una secuencia hasta el final. Esto parece natural, ya que se asemeja a la forma en que leemos y escuchamos el habla. Sin embargo, dado que en muchos casos pr√°cticos tenemos acceso aleatorio a la secuencia de entrada, podr√≠a tener sentido realizar c√°lculos recurrentes en ambas direcciones. Estas redes se llaman **RNN bidireccionales**. Al trabajar con una red bidireccional, necesitar√≠amos dos vectores de estado oculto, uno para cada direcci√≥n.

Una red recurrente, ya sea unidireccional o bidireccional, captura ciertos patrones dentro de una secuencia y puede almacenarlos en un vector de estado o pasarlos a la salida. Al igual que con las redes convolucionales, podemos construir otra capa recurrente sobre la primera para capturar patrones de nivel superior y construir a partir de los patrones de bajo nivel extra√≠dos por la primera capa. Esto nos lleva a la noci√≥n de una **RNN multicapa**, que consiste en dos o m√°s redes recurrentes, donde la salida de la capa anterior se pasa a la siguiente capa como entrada.

![Imagen mostrando una RNN multicapa de memoria a largo y corto plazo](../../../../../translated_images/es/multi-layer-lstm.dd975e29bb2a59fe.webp)

*Imagen tomada de [este maravilloso art√≠culo](https://towardsdatascience.com/from-a-lstm-cell-to-a-multilayer-lstm-network-with-pytorch-2899eb5696f3) de Fernando L√≥pez*

## ‚úçÔ∏è Ejercicios: Incrustaciones

Contin√∫a tu aprendizaje en los siguientes cuadernos:

* [RNNs con PyTorch](RNNPyTorch.ipynb)
* [RNNs con TensorFlow](RNNTF.ipynb)

## Conclusi√≥n

En esta unidad, hemos visto que las RNN pueden usarse para la clasificaci√≥n de secuencias, pero de hecho, pueden manejar muchas m√°s tareas, como generaci√≥n de texto, traducci√≥n autom√°tica y m√°s. Consideraremos esas tareas en la pr√≥xima unidad.

## üöÄ Desaf√≠o

Lee algo de literatura sobre LSTMs y considera sus aplicaciones:

- [Grid Long Short-Term Memory](https://arxiv.org/pdf/1507.01526v1.pdf)
- [Show, Attend and Tell: Neural Image Caption
Generation with Visual Attention](https://arxiv.org/pdf/1502.03044v2.pdf)

## [Cuestionario posterior a la clase](https://ff-quizzes.netlify.app/en/ai/quiz/32)

## Revisi√≥n y Estudio Aut√≥nomo

- [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) de Christopher Olah.

## [Asignaci√≥n: Cuadernos](assignment.md)

---

