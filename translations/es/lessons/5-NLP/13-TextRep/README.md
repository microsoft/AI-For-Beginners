# Representaci칩n de Texto como Tensores

## [Cuestionario previo a la clase](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/113)

## Clasificaci칩n de Texto

A lo largo de la primera parte de esta secci칩n, nos centraremos en la tarea de **clasificaci칩n de texto**. Utilizaremos el conjunto de datos [AG News](https://www.kaggle.com/amananandrai/ag-news-classification-dataset), que contiene art칤culos de noticias como los siguientes:

* Categor칤a: Sci/Tech
* T칤tulo: La empresa de Ky. gana una subvenci칩n para estudiar p칠ptidos (AP)
* Cuerpo: AP - Una empresa fundada por un investigador en qu칤mica en la Universidad de Louisville gan칩 una subvenci칩n para desarrollar...

Nuestro objetivo ser치 clasificar el art칤culo de noticias en una de las categor칤as basadas en el texto.

## Representaci칩n del texto

Si queremos resolver tareas de Procesamiento de Lenguaje Natural (NLP) con redes neuronales, necesitamos alguna forma de representar el texto como tensores. Las computadoras ya representan caracteres textuales como n칰meros que se asignan a fuentes en tu pantalla utilizando codificaciones como ASCII o UTF-8.

<img alt="Imagen que muestra un diagrama que mapea un car치cter a una representaci칩n ASCII y binaria" src="images/ascii-character-map.png" width="50%"/>

> [Fuente de la imagen](https://www.seobility.net/en/wiki/ASCII)

Como humanos, entendemos lo que cada letra **representa**, y c칩mo todos los caracteres se combinan para formar las palabras de una oraci칩n. Sin embargo, las computadoras por s칤 solas no tienen tal comprensi칩n, y la red neuronal tiene que aprender el significado durante el entrenamiento.

Por lo tanto, podemos usar diferentes enfoques al representar el texto:

* **Representaci칩n a nivel de car치cter**, cuando representamos el texto tratando cada car치cter como un n칰mero. Dado que tenemos *C* caracteres diferentes en nuestro corpus de texto, la palabra *Hola* se representar칤a mediante un tensor de 5x*C*. Cada letra corresponder칤a a una columna del tensor en codificaci칩n one-hot.
* **Representaci칩n a nivel de palabra**, en la que creamos un **vocabulario** de todas las palabras en nuestro texto, y luego representamos las palabras utilizando codificaci칩n one-hot. Este enfoque es algo mejor, porque cada letra por s칤 sola no tiene mucho significado, y as칤, al usar conceptos sem치nticos de mayor nivel - palabras - simplificamos la tarea para la red neuronal. Sin embargo, dado el gran tama침o del diccionario, necesitamos lidiar con tensores dispersos de alta dimensi칩n.

Independientemente de la representaci칩n, primero necesitamos convertir el texto en una secuencia de **tokens**, donde un token puede ser un car치cter, una palabra, o a veces incluso parte de una palabra. Luego, convertimos el token en un n칰mero, t칤picamente usando un **vocabulario**, y este n칰mero puede ser alimentado a una red neuronal utilizando codificaci칩n one-hot.

## N-Grams

En el lenguaje natural, el significado preciso de las palabras solo puede determinarse en contexto. Por ejemplo, los significados de *red neuronal* y *red de pesca* son completamente diferentes. Una de las formas de tener esto en cuenta es construir nuestro modelo sobre pares de palabras, considerando los pares de palabras como tokens de vocabulario separados. De esta manera, la oraci칩n *Me gusta ir a pescar* se representar치 mediante la siguiente secuencia de tokens: *Me gusta*, *gusta ir*, *ir a*, *a pescar*. El problema con este enfoque es que el tama침o del diccionario crece significativamente, y combinaciones como *ir a pescar* y *ir de compras* se presentan con diferentes tokens, que no comparten ninguna similitud sem치ntica a pesar de tener el mismo verbo.  

En algunos casos, tambi칠n podemos considerar el uso de tri-gramas -- combinaciones de tres palabras --. As칤, el enfoque se denomina a menudo **n-grams**. Adem치s, tiene sentido usar n-grams con representaci칩n a nivel de car치cter, en cuyo caso los n-grams corresponder치n aproximadamente a diferentes s칤labas.

## Bolsa de Palabras y TF/IDF

Al resolver tareas como la clasificaci칩n de texto, necesitamos poder representar el texto mediante un vector de tama침o fijo, que utilizaremos como entrada para el clasificador denso final. Una de las formas m치s simples de hacerlo es combinar todas las representaciones de palabras individuales, por ejemplo, sum치ndolas. Si sumamos las codificaciones one-hot de cada palabra, terminaremos con un vector de frecuencias, que muestra cu치ntas veces aparece cada palabra dentro del texto. Tal representaci칩n del texto se llama **bolsa de palabras** (BoW).

<img src="images/bow.png" width="90%"/>

> Imagen del autor

Un BoW representa esencialmente qu칠 palabras aparecen en el texto y en qu칠 cantidades, lo que puede ser una buena indicaci칩n de lo que trata el texto. Por ejemplo, un art칤culo de noticias sobre pol칤tica es probable que contenga palabras como *presidente* y *pa칤s*, mientras que una publicaci칩n cient칤fica tendr칤a algo como *colisionador*, *descubierto*, etc. As칤, las frecuencias de palabras pueden ser en muchos casos un buen indicador del contenido del texto.

El problema con BoW es que ciertas palabras comunes, como *y*, *es*, etc., aparecen en la mayor칤a de los textos, y tienen las frecuencias m치s altas, ocultando las palabras que son realmente importantes. Podemos reducir la importancia de esas palabras teniendo en cuenta la frecuencia con la que ocurren en toda la colecci칩n de documentos. Esta es la idea principal detr치s del enfoque TF/IDF, que se cubre con m치s detalle en los cuadernos adjuntos a esta lecci칩n.

Sin embargo, ninguno de estos enfoques puede tener en cuenta completamente la **sem치ntica** del texto. Necesitamos modelos de redes neuronales m치s poderosos para hacer esto, que discutiremos m치s adelante en esta secci칩n.

## 九꽲잺 Ejercicios: Representaci칩n de Texto

Contin칰a tu aprendizaje en los siguientes cuadernos:

* [Representaci칩n de Texto con PyTorch](../../../../../lessons/5-NLP/13-TextRep/TextRepresentationPyTorch.ipynb)
* [Representaci칩n de Texto con TensorFlow](../../../../../lessons/5-NLP/13-TextRep/TextRepresentationTF.ipynb)

## Conclusi칩n

Hasta ahora, hemos estudiado t칠cnicas que pueden agregar peso de frecuencia a diferentes palabras. Sin embargo, no son capaces de representar el significado o el orden. Como dijo el famoso ling칲ista J. R. Firth en 1935, "El significado completo de una palabra siempre es contextual, y ning칰n estudio de significado separado del contexto puede tomarse en serio." Aprenderemos m치s adelante en el curso c칩mo capturar la informaci칩n contextual del texto utilizando modelado de lenguaje.

## 游 Desaf칤o

Intenta algunos otros ejercicios utilizando bolsa de palabras y diferentes modelos de datos. Podr칤as inspirarte en esta [competencia en Kaggle](https://www.kaggle.com/competitions/word2vec-nlp-tutorial/overview/part-1-for-beginners-bag-of-words)

## [Cuestionario posterior a la clase](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/213)

## Revisi칩n y Autoestudio

Practica tus habilidades con t칠cnicas de embeddings de texto y bolsa de palabras en [Microsoft Learn](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-pytorch/?WT.mc_id=academic-77998-cacaste)

## [Asignaci칩n: Cuadernos](assignment.md)

**Descargo de responsabilidad**:  
Este documento ha sido traducido utilizando servicios de traducci칩n autom치tica basados en inteligencia artificial. Aunque nos esforzamos por lograr precisi칩n, tenga en cuenta que las traducciones autom치ticas pueden contener errores o inexactitudes. El documento original en su idioma nativo debe considerarse la fuente autorizada. Para informaci칩n cr칤tica, se recomienda la traducci칩n profesional por parte de un humano. No nos hacemos responsables de malentendidos o malas interpretaciones que surjan del uso de esta traducci칩n.