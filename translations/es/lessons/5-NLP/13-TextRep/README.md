# Representar texto como tensores

## [Cuestionario previo a la clase](https://ff-quizzes.netlify.app/en/ai/quiz/25)

## Clasificaci칩n de texto

En la primera parte de esta secci칩n, nos enfocaremos en la tarea de **clasificaci칩n de texto**. Utilizaremos el conjunto de datos [AG News](https://www.kaggle.com/amananandrai/ag-news-classification-dataset), que contiene art칤culos de noticias como el siguiente:

* Categor칤a: Ciencia/Tecnolog칤a  
* T칤tulo: Empresa de Ky. gana subvenci칩n para estudiar p칠ptidos (AP)  
* Cuerpo: AP - Una empresa fundada por un investigador de qu칤mica de la Universidad de Louisville gan칩 una subvenci칩n para desarrollar...

Nuestro objetivo ser치 clasificar el art칤culo de noticias en una de las categor칤as bas치ndonos en el texto.

## Representar texto

Si queremos resolver tareas de Procesamiento de Lenguaje Natural (NLP) con redes neuronales, necesitamos una forma de representar el texto como tensores. Las computadoras ya representan caracteres textuales como n칰meros que se asignan a fuentes en tu pantalla utilizando codificaciones como ASCII o UTF-8.

<img alt="Imagen que muestra un diagrama que mapea un car치cter a una representaci칩n ASCII y binaria" src="../../../../../translated_images/es/ascii-character-map.18ed6aa7f3b0a7ff.webp" width="50%"/>

> [Fuente de la imagen](https://www.seobility.net/en/wiki/ASCII)

Como humanos, entendemos lo que cada letra **representa** y c칩mo todos los caracteres se combinan para formar las palabras de una oraci칩n. Sin embargo, las computadoras por s칤 solas no tienen tal comprensi칩n, y la red neuronal debe aprender el significado durante el entrenamiento.

Por lo tanto, podemos usar diferentes enfoques para representar texto:

* **Representaci칩n a nivel de caracteres**, donde representamos el texto tratando cada car치cter como un n칰mero. Dado que tenemos *C* caracteres diferentes en nuestro corpus de texto, la palabra *Hello* se representar칤a por un tensor de 5x*C*. Cada letra corresponder칤a a una columna del tensor en codificaci칩n one-hot.  
* **Representaci칩n a nivel de palabras**, en la que creamos un **vocabulario** de todas las palabras en nuestro texto y luego representamos las palabras utilizando codificaci칩n one-hot. Este enfoque es algo mejor, porque cada letra por s칤 sola no tiene mucho significado, y al usar conceptos sem치nticos de nivel superior - palabras - simplificamos la tarea para la red neuronal. Sin embargo, dado el tama침o del diccionario, necesitamos manejar tensores dispersos de alta dimensi칩n.

Independientemente de la representaci칩n, primero necesitamos convertir el texto en una secuencia de **tokens**, siendo un token un car치cter, una palabra o incluso parte de una palabra. Luego, convertimos el token en un n칰mero, t칤picamente usando un **vocabulario**, y este n칰mero puede ser alimentado a una red neuronal utilizando codificaci칩n one-hot.

## N-Gramas

En el lenguaje natural, el significado preciso de las palabras solo puede determinarse en contexto. Por ejemplo, los significados de *red neuronal* y *red de pesca* son completamente diferentes. Una de las formas de tener esto en cuenta es construir nuestro modelo basado en pares de palabras, considerando los pares de palabras como tokens separados del vocabulario. De esta manera, la oraci칩n *Me gusta ir a pescar* se representar치 por la siguiente secuencia de tokens: *Me gusta*, *gusta ir*, *ir a*, *a pescar*. El problema con este enfoque es que el tama침o del diccionario crece significativamente, y combinaciones como *a pescar* y *a comprar* se presentan como tokens diferentes, que no comparten ninguna similitud sem치ntica a pesar del mismo verbo.

En algunos casos, podemos considerar usar tri-gramas, combinaciones de tres palabras. Por lo tanto, este enfoque se denomina **n-gramas**. Tambi칠n tiene sentido usar n-gramas con representaci칩n a nivel de caracteres, en cuyo caso los n-gramas corresponder치n aproximadamente a diferentes s칤labas.

## Bag-of-Words y TF/IDF

Cuando resolvemos tareas como la clasificaci칩n de texto, necesitamos poder representar el texto mediante un vector de tama침o fijo, que utilizaremos como entrada para el clasificador denso final. Una de las formas m치s simples de hacerlo es combinar todas las representaciones individuales de palabras, por ejemplo, sum치ndolas. Si sumamos las codificaciones one-hot de cada palabra, terminaremos con un vector de frecuencias que muestra cu치ntas veces aparece cada palabra dentro del texto. Tal representaci칩n del texto se llama **bag of words** (BoW).

<img src="../../../../../translated_images/es/bow.3811869cff59368d.webp" width="90%"/>

> Imagen del autor

Un BoW esencialmente representa qu칠 palabras aparecen en el texto y en qu칠 cantidades, lo que puede ser una buena indicaci칩n de qu칠 trata el texto. Por ejemplo, un art칤culo de noticias sobre pol칤tica probablemente contenga palabras como *presidente* y *pa칤s*, mientras que una publicaci칩n cient칤fica podr칤a tener algo como *colisionador*, *descubierto*, etc. Por lo tanto, las frecuencias de palabras pueden ser en muchos casos un buen indicador del contenido del texto.

El problema con BoW es que ciertas palabras comunes, como *y*, *es*, etc., aparecen en la mayor칤a de los textos y tienen las frecuencias m치s altas, ocultando las palabras que realmente son importantes. Podemos reducir la importancia de esas palabras teniendo en cuenta la frecuencia con la que ocurren en toda la colecci칩n de documentos. Esta es la idea principal detr치s del enfoque TF/IDF, que se cubre en m치s detalle en los cuadernos adjuntos a esta lecci칩n.

Sin embargo, ninguno de estos enfoques puede tener en cuenta completamente la **sem치ntica** del texto. Necesitamos modelos de redes neuronales m치s poderosos para hacerlo, los cuales discutiremos m치s adelante en esta secci칩n.

## 九꽲잺 Ejercicios: Representaci칩n de texto

Contin칰a tu aprendizaje en los siguientes cuadernos:

* [Representaci칩n de texto con PyTorch](TextRepresentationPyTorch.ipynb)  
* [Representaci칩n de texto con TensorFlow](TextRepresentationTF.ipynb)  

## Conclusi칩n

Hasta ahora, hemos estudiado t칠cnicas que pueden agregar peso de frecuencia a diferentes palabras. Sin embargo, estas no son capaces de representar el significado o el orden. Como dijo el famoso ling칲ista J. R. Firth en 1935: "El significado completo de una palabra siempre es contextual, y ning칰n estudio de significado fuera del contexto puede tomarse en serio". M치s adelante en el curso aprenderemos c칩mo capturar informaci칩n contextual del texto utilizando modelos de lenguaje.

## 游 Desaf칤o

Prueba algunos otros ejercicios utilizando bag-of-words y diferentes modelos de datos. Podr칤as inspirarte en esta [competencia en Kaggle](https://www.kaggle.com/competitions/word2vec-nlp-tutorial/overview/part-1-for-beginners-bag-of-words)

## [Cuestionario posterior a la clase](https://ff-quizzes.netlify.app/en/ai/quiz/26)

## Repaso y autoestudio

Practica tus habilidades con t칠cnicas de embeddings de texto y bag-of-words en [Microsoft Learn](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-pytorch/?WT.mc_id=academic-77998-cacaste)

## [Asignaci칩n: Cuadernos](assignment.md)

---

