# Redes Pre-entrenadas y Aprendizaje por Transferencia

Entrenar redes neuronales convolucionales (CNNs) puede llevar mucho tiempo y requiere una gran cantidad de datos. Sin embargo, gran parte del tiempo se invierte en aprender los mejores filtros de bajo nivel que una red puede usar para extraer patrones de las im√°genes. Surge una pregunta natural: ¬øpodemos usar una red neuronal entrenada en un conjunto de datos y adaptarla para clasificar im√°genes diferentes sin necesidad de un proceso completo de entrenamiento?

## [Cuestionario previo a la clase](https://ff-quizzes.netlify.app/en/ai/quiz/15)

Este enfoque se llama **aprendizaje por transferencia**, porque transferimos conocimiento de un modelo de red neuronal a otro. En el aprendizaje por transferencia, normalmente comenzamos con un modelo pre-entrenado, que ha sido entrenado en un gran conjunto de datos de im√°genes, como **ImageNet**. Estos modelos ya son buenos extrayendo diferentes caracter√≠sticas de im√°genes gen√©ricas, y en muchos casos, simplemente construir un clasificador sobre esas caracter√≠sticas extra√≠das puede dar buenos resultados.

> ‚úÖ El aprendizaje por transferencia es un t√©rmino que tambi√©n se encuentra en otros campos acad√©micos, como la Educaci√≥n. Se refiere al proceso de tomar conocimiento de un dominio y aplicarlo a otro.

## Modelos Pre-entrenados como Extractores de Caracter√≠sticas

Las redes convolucionales que hemos mencionado en la secci√≥n anterior contienen varias capas, cada una de las cuales est√° dise√±ada para extraer caracter√≠sticas de la imagen, comenzando con combinaciones de p√≠xeles de bajo nivel (como l√≠neas horizontales/verticales o trazos), hasta combinaciones de caracter√≠sticas de nivel superior, correspondientes a cosas como el ojo de una llama. Si entrenamos una CNN en un conjunto de datos suficientemente grande de im√°genes gen√©ricas y diversas, la red deber√≠a aprender a extraer esas caracter√≠sticas comunes.

Tanto Keras como PyTorch contienen funciones para cargar f√°cilmente pesos de redes neuronales pre-entrenadas para algunas arquitecturas comunes, la mayor√≠a de las cuales fueron entrenadas con im√°genes de ImageNet. Las m√°s utilizadas se describen en la p√°gina [Arquitecturas de CNN](../07-ConvNets/CNN_Architectures.md) de la lecci√≥n anterior. En particular, podr√≠as considerar usar una de las siguientes:

* **VGG-16/VGG-19**, que son modelos relativamente simples pero que a√∫n ofrecen buena precisi√≥n. Usar VGG como primer intento es una buena opci√≥n para ver c√≥mo funciona el aprendizaje por transferencia.
* **ResNet**, una familia de modelos propuesta por Microsoft Research en 2015. Tienen m√°s capas y, por lo tanto, requieren m√°s recursos.
* **MobileNet**, una familia de modelos con tama√±o reducido, adecuados para dispositivos m√≥viles. √ösalos si tienes recursos limitados y puedes sacrificar un poco de precisi√≥n.

Aqu√≠ hay caracter√≠sticas extra√≠das de una imagen de un gato por la red VGG-16:

![Caracter√≠sticas extra√≠das por VGG-16](../../../../../translated_images/es/features.6291f9c7ba3a0b95.webp)

## Conjunto de Datos de Gatos vs. Perros

En este ejemplo, utilizaremos un conjunto de datos de [Gatos y Perros](https://www.microsoft.com/download/details.aspx?id=54765&WT.mc_id=academic-77998-cacaste), que est√° muy cerca de un escenario real de clasificaci√≥n de im√°genes.

## ‚úçÔ∏è Ejercicio: Aprendizaje por Transferencia

Veamos el aprendizaje por transferencia en acci√≥n en los notebooks correspondientes:

* [Transfer Learning - PyTorch](TransferLearningPyTorch.ipynb)
* [Transfer Learning - TensorFlow](TransferLearningTF.ipynb)

## Visualizando un Gato Adversarial

Una red neuronal pre-entrenada contiene diferentes patrones en su *cerebro*, incluyendo nociones de **gato ideal** (as√≠ como perro ideal, cebra ideal, etc.). Ser√≠a interesante de alguna manera **visualizar esta imagen**. Sin embargo, no es sencillo, porque los patrones est√°n distribuidos por todos los pesos de la red y organizados en una estructura jer√°rquica.

Un enfoque que podemos tomar es comenzar con una imagen aleatoria y luego usar la t√©cnica de **optimizaci√≥n por descenso de gradiente** para ajustar esa imagen de tal manera que la red comience a pensar que es un gato.

![Bucle de Optimizaci√≥n de Imagen](../../../../../translated_images/es/ideal-cat-loop.999fbb8ff306e044.webp)

Sin embargo, si hacemos esto, obtendremos algo muy similar a un ruido aleatorio. Esto se debe a que *hay muchas formas de hacer que la red piense que la imagen de entrada es un gato*, incluyendo algunas que no tienen sentido visualmente. Aunque esas im√°genes contienen muchos patrones t√≠picos de un gato, no hay nada que las restrinja a ser visualmente distintivas.

Para mejorar el resultado, podemos agregar otro t√©rmino a la funci√≥n de p√©rdida, llamado **p√©rdida de variaci√≥n**. Es una m√©trica que muestra cu√°n similares son los p√≠xeles vecinos de la imagen. Minimizar la p√©rdida de variaci√≥n hace que la imagen sea m√°s suave y elimina el ruido, revelando patrones m√°s atractivos visualmente. Aqu√≠ hay un ejemplo de im√°genes "ideales" que son clasificadas como gato y como cebra con alta probabilidad:

![Gato Ideal](../../../../../translated_images/es/ideal-cat.203dd4597643d6b0.webp) | ![Cebra Ideal](../../../../../translated_images/es/ideal-zebra.7f70e8b54ee15a7a.webp)
-----|-----
 *Gato Ideal* | *Cebra Ideal*

Un enfoque similar puede usarse para realizar los llamados **ataques adversariales** en una red neuronal. Supongamos que queremos enga√±ar a una red neuronal y hacer que un perro parezca un gato. Si tomamos la imagen de un perro, que es reconocida por la red como un perro, podemos ajustarla un poco usando optimizaci√≥n por descenso de gradiente hasta que la red comience a clasificarla como un gato:

![Imagen de un Perro](../../../../../translated_images/es/original-dog.8f68a67d2fe0911f.webp) | ![Imagen de un perro clasificada como un gato](../../../../../translated_images/es/adversarial-dog.d9fc7773b0142b89.webp)
-----|-----
*Imagen original de un perro* | *Imagen de un perro clasificada como un gato*

Consulta el c√≥digo para reproducir los resultados anteriores en el siguiente notebook:

* [Ideal y Gato Adversarial - TensorFlow](AdversarialCat_TF.ipynb)

## Conclusi√≥n

Usando el aprendizaje por transferencia, puedes armar r√°pidamente un clasificador para una tarea de clasificaci√≥n de objetos personalizada y lograr alta precisi√≥n. Puedes ver que las tareas m√°s complejas que estamos resolviendo ahora requieren mayor poder computacional y no pueden resolverse f√°cilmente en la CPU. En la pr√≥xima unidad, intentaremos usar una implementaci√≥n m√°s ligera para entrenar el mismo modelo utilizando menos recursos computacionales, lo que resulta en una precisi√≥n ligeramente menor.

## üöÄ Desaf√≠o

En los notebooks acompa√±antes, hay notas al final sobre c√≥mo el conocimiento transferido funciona mejor con datos de entrenamiento algo similares (quiz√°s un nuevo tipo de animal). Haz experimentos con tipos de im√°genes completamente nuevos para ver qu√© tan bien o mal funcionan tus modelos de conocimiento transferido.

## [Cuestionario posterior a la clase](https://ff-quizzes.netlify.app/en/ai/quiz/16)

## Revisi√≥n y Autoestudio

Lee [TrainingTricks.md](TrainingTricks.md) para profundizar tu conocimiento sobre otras formas de entrenar tus modelos.

## [Asignaci√≥n](lab/README.md)

En este laboratorio, utilizaremos el conjunto de datos real [Oxford-IIIT](https://www.robots.ox.ac.uk/~vgg/data/pets/) de mascotas con 35 razas de gatos y perros, y construiremos un clasificador de aprendizaje por transferencia.

---

