<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "d85c8b08f6d1b48fd7f35b99f93c1138",
  "translation_date": "2025-08-24T09:16:52+00:00",
  "source_file": "lessons/4-ComputerVision/11-ObjectDetection/README.md",
  "language_code": "es"
}
-->
# Detecci√≥n de Objetos

Los modelos de clasificaci√≥n de im√°genes que hemos visto hasta ahora toman una imagen y producen un resultado categ√≥rico, como la clase 'n√∫mero' en un problema de MNIST. Sin embargo, en muchos casos no solo queremos saber que una imagen contiene objetos, sino que tambi√©n queremos determinar su ubicaci√≥n precisa. Esto es exactamente el objetivo de la **detecci√≥n de objetos**.

## [Cuestionario previo a la lecci√≥n](https://ff-quizzes.netlify.app/en/ai/quiz/21)

![Detecci√≥n de Objetos](../../../../../lessons/4-ComputerVision/11-ObjectDetection/images/Screen_Shot_2016-11-17_at_11.14.54_AM.png)

> Imagen de [sitio web de YOLO v2](https://pjreddie.com/darknet/yolov2/)

## Un Enfoque Ingenuo para la Detecci√≥n de Objetos

Supongamos que queremos encontrar un gato en una imagen. Un enfoque muy ingenuo para la detecci√≥n de objetos ser√≠a el siguiente:

1. Dividir la imagen en varias secciones o mosaicos.
2. Ejecutar un modelo de clasificaci√≥n de im√°genes en cada mosaico.
3. Aquellos mosaicos que resulten en una activaci√≥n suficientemente alta pueden considerarse como que contienen el objeto en cuesti√≥n.

![Detecci√≥n Ingenua](../../../../../lessons/4-ComputerVision/11-ObjectDetection/images/naive-detection.png)

> *Imagen del [Cuaderno de Ejercicios](../../../../../lessons/4-ComputerVision/11-ObjectDetection/ObjectDetection-TF.ipynb)*

Sin embargo, este enfoque est√° lejos de ser ideal, ya que solo permite al algoritmo localizar la caja delimitadora del objeto de manera muy imprecisa. Para una ubicaci√≥n m√°s precisa, necesitamos ejecutar alg√∫n tipo de **regresi√≥n** para predecir las coordenadas de las cajas delimitadoras, y para ello necesitamos conjuntos de datos espec√≠ficos.

## Regresi√≥n para la Detecci√≥n de Objetos

[Esta publicaci√≥n de blog](https://towardsdatascience.com/object-detection-with-neural-networks-a4e2c46b4491) ofrece una excelente introducci√≥n a la detecci√≥n de formas.

## Conjuntos de Datos para la Detecci√≥n de Objetos

Es posible que encuentres los siguientes conjuntos de datos para esta tarea:

* [PASCAL VOC](http://host.robots.ox.ac.uk/pascal/VOC/) - 20 clases
* [COCO](http://cocodataset.org/#home) - Objetos Comunes en Contexto. 80 clases, cajas delimitadoras y m√°scaras de segmentaci√≥n

![COCO](../../../../../lessons/4-ComputerVision/11-ObjectDetection/images/coco-examples.jpg)

## M√©tricas de Detecci√≥n de Objetos

### Intersecci√≥n sobre Uni√≥n

Mientras que para la clasificaci√≥n de im√°genes es f√°cil medir qu√© tan bien funciona el algoritmo, para la detecci√≥n de objetos necesitamos medir tanto la correcci√≥n de la clase como la precisi√≥n de la ubicaci√≥n inferida de la caja delimitadora. Para esto √∫ltimo, usamos la llamada **Intersecci√≥n sobre Uni√≥n** (IoU), que mide qu√© tan bien se superponen dos cajas (o dos √°reas arbitrarias).

![IoU](../../../../../lessons/4-ComputerVision/11-ObjectDetection/images/iou_equation.png)

> *Figura 2 de [esta excelente publicaci√≥n sobre IoU](https://pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/)*

La idea es simple: dividimos el √°rea de intersecci√≥n entre dos figuras por el √°rea de su uni√≥n. Para dos √°reas id√©nticas, IoU ser√≠a 1, mientras que para √°reas completamente disjuntas ser√° 0. En otros casos, variar√° entre 0 y 1. Normalmente solo consideramos aquellas cajas delimitadoras para las que IoU supera un cierto valor.

### Precisi√≥n Promedio

Supongamos que queremos medir qu√© tan bien se reconoce una clase de objetos $C$ dada. Para medirlo, usamos la m√©trica de **Precisi√≥n Promedio**, que se calcula de la siguiente manera:

1. Consideramos la curva de Precisi√≥n-Recall, que muestra la precisi√≥n dependiendo de un valor umbral de detecci√≥n (de 0 a 1).
2. Dependiendo del umbral, obtendremos m√°s o menos objetos detectados en la imagen, y diferentes valores de precisi√≥n y recall.
3. La curva se ver√° as√≠:

<img src="https://github.com/shwars/NeuroWorkshop/raw/master/images/ObjDetectionPrecisionRecall.png"/>

> *Imagen de [NeuroWorkshop](http://github.com/shwars/NeuroWorkshop)*

La Precisi√≥n Promedio para una clase $C$ dada es el √°rea bajo esta curva. M√°s precisamente, el eje de Recall se divide t√≠picamente en 10 partes, y la Precisi√≥n se promedia en todos esos puntos:

$$
AP = {1\over11}\sum_{i=0}^{10}\mbox{Precision}(\mbox{Recall}={i\over10})
$$

### AP e IoU

Solo consideraremos aquellas detecciones para las que IoU est√© por encima de un cierto valor. Por ejemplo, en el conjunto de datos PASCAL VOC t√≠picamente se asume $\mbox{IoU Threshold} = 0.5$, mientras que en COCO AP se mide para diferentes valores de $\mbox{IoU Threshold}$.

<img src="https://github.com/shwars/NeuroWorkshop/raw/master/images/ObjDetectionPrecisionRecallIoU.png"/>

> *Imagen de [NeuroWorkshop](http://github.com/shwars/NeuroWorkshop)*

### Precisi√≥n Promedio Media - mAP

La m√©trica principal para la Detecci√≥n de Objetos se llama **Precisi√≥n Promedio Media**, o **mAP**. Es el valor de la Precisi√≥n Promedio, promediado entre todas las clases de objetos, y a veces tambi√©n sobre $\mbox{IoU Threshold}$. El proceso de c√°lculo de **mAP** se describe en detalle
[en esta publicaci√≥n de blog](https://medium.com/@timothycarlen/understanding-the-map-evaluation-metric-for-object-detection-a07fe6962cf3), y tambi√©n [aqu√≠ con ejemplos de c√≥digo](https://gist.github.com/tarlen5/008809c3decf19313de216b9208f3734).

## Diferentes Enfoques para la Detecci√≥n de Objetos

Existen dos grandes clases de algoritmos de detecci√≥n de objetos:

* **Redes de Propuesta de Regiones** (R-CNN, Fast R-CNN, Faster R-CNN). La idea principal es generar **Regiones de Inter√©s** (ROI) y ejecutar una CNN sobre ellas, buscando la activaci√≥n m√°xima. Es un poco similar al enfoque ingenuo, con la excepci√≥n de que las ROI se generan de una manera m√°s inteligente. Una de las principales desventajas de estos m√©todos es que son lentos, ya que se necesitan muchos pases del clasificador CNN sobre la imagen.
* M√©todos de **una sola pasada** (YOLO, SSD, RetinaNet). En estas arquitecturas dise√±amos la red para predecir tanto las clases como las ROI en un solo pase.

### R-CNN: CNN Basada en Regiones

[R-CNN](http://islab.ulsan.ac.kr/files/announcement/513/rcnn_pami.pdf) utiliza [Selective Search](http://www.huppelen.nl/publications/selectiveSearchDraft.pdf) para generar una estructura jer√°rquica de regiones ROI, que luego se pasan por extractores de caracter√≠sticas CNN y clasificadores SVM para determinar la clase del objeto, y regresi√≥n lineal para determinar las coordenadas de la *caja delimitadora*. [Art√≠culo oficial](https://arxiv.org/pdf/1506.01497v1.pdf)

![RCNN](../../../../../lessons/4-ComputerVision/11-ObjectDetection/images/rcnn1.png)

> *Imagen de van de Sande et al. ICCV‚Äô11*

![RCNN-1](../../../../../lessons/4-ComputerVision/11-ObjectDetection/images/rcnn2.png)

> *Im√°genes de [este blog](https://towardsdatascience.com/r-cnn-fast-r-cnn-faster-r-cnn-yolo-object-detection-algorithms-36d53571365e)*

### F-RCNN - Fast R-CNN

Este enfoque es similar a R-CNN, pero las regiones se definen despu√©s de que se han aplicado las capas de convoluci√≥n.

![FRCNN](../../../../../lessons/4-ComputerVision/11-ObjectDetection/images/f-rcnn.png)

> Imagen del [Art√≠culo Oficial](https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Girshick_Fast_R-CNN_ICCV_2015_paper.pdf), [arXiv](https://arxiv.org/pdf/1504.08083.pdf), 2015

### Faster R-CNN

La idea principal de este enfoque es usar una red neuronal para predecir las ROI, la llamada *Red de Propuesta de Regiones*. [Art√≠culo](https://arxiv.org/pdf/1506.01497.pdf), 2016

![FasterRCNN](../../../../../lessons/4-ComputerVision/11-ObjectDetection/images/faster-rcnn.png)

> Imagen del [art√≠culo oficial](https://arxiv.org/pdf/1506.01497.pdf)

### R-FCN: Red Completamente Convolucional Basada en Regiones

Este algoritmo es incluso m√°s r√°pido que Faster R-CNN. La idea principal es la siguiente:

1. Extraemos caracter√≠sticas usando ResNet-101.
2. Las caracter√≠sticas se procesan mediante un **Mapa de Puntuaci√≥n Sensible a la Posici√≥n**. Cada objeto de las clases $C$ se divide en regiones de $k\times k$, y entrenamos para predecir partes de los objetos.
3. Para cada parte de las regiones $k\times k$, todas las redes votan por las clases de objetos, y se selecciona la clase de objeto con el m√°ximo voto.

![r-fcn image](../../../../../lessons/4-ComputerVision/11-ObjectDetection/images/r-fcn.png)

> Imagen del [art√≠culo oficial](https://arxiv.org/abs/1605.06409)

### YOLO - You Only Look Once

YOLO es un algoritmo de una sola pasada en tiempo real. La idea principal es la siguiente:

 * La imagen se divide en regiones de $S\times S$.
 * Para cada regi√≥n, **CNN** predice $n$ posibles objetos, las coordenadas de la *caja delimitadora* y la *confianza* = *probabilidad* * IoU.

 ![YOLO](../../../../../lessons/4-ComputerVision/11-ObjectDetection/images/yolo.png)

> Imagen del [art√≠culo oficial](https://arxiv.org/abs/1506.02640)

### Otros Algoritmos

* RetinaNet: [art√≠culo oficial](https://arxiv.org/abs/1708.02002)
   - [Implementaci√≥n en PyTorch en Torchvision](https://pytorch.org/vision/stable/_modules/torchvision/models/detection/retinanet.html)
   - [Implementaci√≥n en Keras](https://github.com/fizyr/keras-retinanet)
   - [Detecci√≥n de Objetos con RetinaNet](https://keras.io/examples/vision/retinanet/) en ejemplos de Keras
* SSD (Single Shot Detector): [art√≠culo oficial](https://arxiv.org/abs/1512.02325)

## ‚úçÔ∏è Ejercicios: Detecci√≥n de Objetos

Contin√∫a tu aprendizaje en el siguiente cuaderno:

[ObjectDetection.ipynb](../../../../../lessons/4-ComputerVision/11-ObjectDetection/ObjectDetection.ipynb)

## Conclusi√≥n

En esta lecci√≥n hiciste un recorrido r√°pido por las diversas formas en que se puede lograr la detecci√≥n de objetos.

## üöÄ Desaf√≠o

Lee estos art√≠culos y cuadernos sobre YOLO y pru√©balos por ti mismo:

* [Buen art√≠culo](https://www.analyticsvidhya.com/blog/2018/12/practical-guide-object-detection-yolo-framewor-python/) que describe YOLO
 * [Sitio oficial](https://pjreddie.com/darknet/yolo/)
 * Yolo: [Implementaci√≥n en Keras](https://github.com/experiencor/keras-yolo2), [cuaderno paso a paso](https://github.com/experiencor/basic-yolo-keras/blob/master/Yolo%20Step-by-Step.ipynb)
 * Yolo v2: [Implementaci√≥n en Keras](https://github.com/experiencor/keras-yolo2), [cuaderno paso a paso](https://github.com/experiencor/keras-yolo2/blob/master/Yolo%20Step-by-Step.ipynb)

## [Cuestionario posterior a la lecci√≥n](https://ff-quizzes.netlify.app/en/ai/quiz/22)

## Revisi√≥n y Autoestudio

* [Detecci√≥n de Objetos](https://tjmachinelearning.com/lectures/1718/obj/) por Nikhil Sardana
* [Una buena comparaci√≥n de algoritmos de detecci√≥n de objetos](https://lilianweng.github.io/lil-log/2018/12/27/object-detection-part-4.html)
* [Revisi√≥n de Algoritmos de Aprendizaje Profundo para la Detecci√≥n de Objetos](https://medium.com/comet-app/review-of-deep-learning-algorithms-for-object-detection-c1f3d437b852)
* [Introducci√≥n paso a paso a los algoritmos b√°sicos de detecci√≥n de objetos](https://www.analyticsvidhya.com/blog/2018/10/a-step-by-step-introduction-to-the-basic-object-detection-algorithms-part-1/)
* [Implementaci√≥n de Faster R-CNN en Python para la Detecci√≥n de Objetos](https://www.analyticsvidhya.com/blog/2018/11/implementation-faster-r-cnn-python-object-detection/)

## [Asignaci√≥n: Detecci√≥n de Objetos](lab/README.md)

**Descargo de responsabilidad**:  
Este documento ha sido traducido utilizando el servicio de traducci√≥n autom√°tica [Co-op Translator](https://github.com/Azure/co-op-translator). Si bien nos esforzamos por lograr precisi√≥n, tenga en cuenta que las traducciones autom√°ticas pueden contener errores o imprecisiones. El documento original en su idioma nativo debe considerarse como la fuente autorizada. Para informaci√≥n cr√≠tica, se recomienda una traducci√≥n profesional realizada por humanos. No nos hacemos responsables de malentendidos o interpretaciones err√≥neas que puedan surgir del uso de esta traducci√≥n.