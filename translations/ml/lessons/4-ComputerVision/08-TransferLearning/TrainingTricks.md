# ഡീപ് ലേണിംഗ് പരിശീലന തന്ത്രങ്ങൾ

ന്യൂറൽ നെറ്റ്വർക്കുകൾ കൂടുതൽ ആഴത്തിൽ പോകുമ്പോൾ, അവയുടെ പരിശീലന പ്രക്രിയ കൂടുതൽ വെല്ലുവിളികളായിരിക്കും. പ്രധാന പ്രശ്നങ്ങളിൽ ഒന്നാണ്所谓 [വാനിഷിംഗ് ഗ്രാഡിയന്റുകൾ](https://en.wikipedia.org/wiki/Vanishing_gradient_problem) അല്ലെങ്കിൽ [എക്സ്പ്ലോഡിംഗ് ഗ്രാഡിയന്റുകൾ](https://deepai.org/machine-learning-glossary-and-terms/exploding-gradient-problem#:~:text=Exploding%20gradients%20are%20a%20problem,updates%20are%20small%20and%20controlled.) എന്ന പ്രശ്നം. [ഈ പോസ്റ്റ്](https://towardsdatascience.com/the-vanishing-exploding-gradient-problem-in-deep-neural-networks-191358470c11) ഈ പ്രശ്നങ്ങളെ കുറിച്ച് നല്ല പരിചയം നൽകുന്നു.

ഡീപ് നെറ്റ്വർക്കുകൾ കൂടുതൽ ഫലപ്രദമായി പരിശീലിപ്പിക്കാൻ ചില സാങ്കേതിക വിദ്യകൾ ഉപയോഗിക്കാം.

## മൂല്യങ്ങൾ യുക്തമായ പരിധിയിൽ സൂക്ഷിക്കുക

സംഖ്യാത്മക കണക്കുകൂട്ടലുകൾ കൂടുതൽ സ്ഥിരതയുള്ളതാക്കാൻ, നമ്മുടെ ന്യൂറൽ നെറ്റ്വർക്കിലെ എല്ലാ മൂല്യങ്ങളും യുക്തമായ സ്കെയിലിൽ (സാധാരണയായി [-1..1] അല്ലെങ്കിൽ [0..1]) ഉണ്ടാകണമെന്ന് ഉറപ്പാക്കണം. ഇത് കർശനമായ ആവശ്യകതയല്ല, പക്ഷേ ഫ്ലോട്ടിംഗ് പോയിന്റ് കണക്കുകൂട്ടലുകളുടെ സ്വഭാവം മൂലം വ്യത്യസ്ത വലുപ്പത്തിലുള്ള മൂല്യങ്ങൾ കൃത്യമായി ഒരുമിച്ച് കൈകാര്യം ചെയ്യാൻ കഴിയില്ല. ഉദാഹരണത്തിന്, 10<sup>-10</sup>നും 10<sup>10</sup>നും കൂട്ടിച്ചേർക്കുമ്പോൾ, സാധാരണയായി 10<sup>10</sup> ലഭിക്കും, കാരണം ചെറിയ മൂല്യം വലിയതിന്റെ ഓർഡറിലേക്ക് "പരിവർത്തനം" ചെയ്യപ്പെടുകയും mantissa നഷ്ടപ്പെടുകയും ചെയ്യും.

അധികഭാഗം ആക്ടിവേഷൻ ഫംഗ്ഷനുകൾ [-1..1] പരിധിയിലുണ്ടായുള്ള നോണ്ലിനിയാരിറ്റികൾ ഉള്ളതിനാൽ, എല്ലാ ഇൻപുട്ട് ഡാറ്റയും [-1..1] അല്ലെങ്കിൽ [0..1] പരിധിയിലേക്ക് സ്കെയിൽ ചെയ്യുന്നത് യുക്തിയുള്ളതാണ്.

## പ്രാരംഭ ഭാരങ്ങളുടെ ഇൻഷിയലൈസേഷൻ

സിദ്ധാന്തപരമായി, നെറ്റ്വർക്ക് ലെയറുകൾ കടന്നുപോകുമ്പോൾ മൂല്യങ്ങൾ ഒരേ പരിധിയിലായിരിക്കണം. അതിനാൽ, ഭാരങ്ങൾ ഇങ്ങനെ ഇൻഷിയലൈസ് ചെയ്യുന്നത് പ്രധാനമാണ്, മൂല്യങ്ങളുടെ വിതരണത്തെ സംരക്ഷിക്കാൻ.

സാധാരണ വിതരണമായ **N(0,1)** നല്ല ആശയമല്ല, കാരണം *n* ഇൻപുട്ടുകൾ ഉണ്ടെങ്കിൽ, ഔട്ട്പുട്ടിന്റെ സ്റ്റാൻഡേർഡ് ഡിവിയേഷൻ *n* ആയിരിക്കും, മൂല്യങ്ങൾ [0..1] പരിധി വിട്ട് പോകാൻ സാധ്യതയുണ്ട്.

താഴെ പറയുന്ന ഇൻഷിയലൈസേഷനുകൾ സാധാരണ ഉപയോഗിക്കുന്നു:

 * യൂണിഫോം വിതരണം -- `uniform`
 * **N(0,1/n)** -- `gaussian`
 * **N(0,1/&radic;n_in)** - ഇൻപുട്ടുകൾക്ക് സീറോ മീൻ, സ്റ്റാൻഡേർഡ് ഡിവിയേഷൻ 1 ആണെങ്കിൽ, അതേ മീൻ/സ്റ്റാൻഡേർഡ് ഡിവിയേഷൻ നിലനിർത്താൻ ഉറപ്പാക്കുന്നു
 * **N(0,&radic;2/(n_in+n_out))** --所谓 **Xavier initialization** (`glorot`), ഇത് ഫോർവേഡ്, ബാക്ക്‌വേഡ് പ്രൊപ്പഗേഷനിൽ സിഗ്നലുകൾ പരിധിയിലാക്കാൻ സഹായിക്കുന്നു

## ബാച്ച് നോർമലൈസേഷൻ

ശരിയായ ഭാര ഇൻഷിയലൈസേഷനും ഉണ്ടായിട്ടും, പരിശീലനത്തിനിടെ ഭാരങ്ങൾ അനിയന്ത്രിതമായി വലുതോ ചെറുതോ ആകാം, ഇത് സിഗ്നലുകൾ ശരിയായ പരിധി വിട്ട് പോകാൻ ഇടയാക്കും. സിഗ്നലുകൾ തിരിച്ചെത്തിക്കാൻ **നോർമലൈസേഷൻ** സാങ്കേതിക വിദ്യകൾ ഉപയോഗിക്കാം. പലതരം നോർമലൈസേഷനുകൾ ഉണ്ട് (Weight normalization, Layer Normalization), എന്നാൽ ഏറ്റവും സാധാരണമായി ഉപയോഗിക്കുന്നത് ബാച്ച് നോർമലൈസേഷനാണ്.

**ബാച്ച് നോർമലൈസേഷൻ** എന്ന ആശയം, മിനിബാച്ചിലെ എല്ലാ മൂല്യങ്ങളും പരിഗണിച്ച്, അവയുടെ അടിസ്ഥാനത്തിൽ (അർത്ഥം കുറയ്ക്കുകയും സ്റ്റാൻഡേർഡ് ഡിവിയേഷൻ കൊണ്ട് വിഭജിക്കുകയും ചെയ്യുക) നോർമലൈസ് ചെയ്യുക എന്നതാണ്. ഇത് ഒരു നെറ്റ്വർക്ക് ലെയർ ആയി നടപ്പിലാക്കുന്നു, ഭാരങ്ങൾ പ്രയോഗിച്ചതിന് ശേഷം, ആക്ടിവേഷൻ ഫംഗ്ഷൻ മുൻപ് ഈ നോർമലൈസേഷൻ നടത്തുന്നു. ഇതിന്റെ ഫലമായി, ഉയർന്ന അന്തിമ കൃത്യതയും വേഗത്തിലുള്ള പരിശീലനവും സാധ്യമാണ്.

ഇവിടെ ബാച്ച് നോർമലൈസേഷനിലെ [അസൽ പേപ്പർ](https://arxiv.org/pdf/1502.03167.pdf), [വിക്കിപീഡിയയിലെ വിശദീകരണം](https://en.wikipedia.org/wiki/Batch_normalization), കൂടാതെ [ഒരു നല്ല പരിചയപ്പെടുത്തൽ ബ്ലോഗ് പോസ്റ്റ്](https://towardsdatascience.com/batch-normalization-in-3-levels-of-understanding-14c2da90a338) (റഷ്യൻ ഭാഷയിൽ [ഇവിടെ](https://habrahabr.ru/post/309302/)) കാണാം.

## ഡ്രോപ്പ്ഔട്ട്

**ഡ്രോപ്പ്ഔട്ട്** ഒരു രസകരമായ സാങ്കേതിക വിദ്യയാണ്, പരിശീലനത്തിനിടെ ഒരു നിശ്ചിത ശതമാനം റാൻഡം ന്യൂറോണുകൾ നീക്കം ചെയ്യുന്നത്. ഇത് ഒരു ലെയർ ആയി നടപ്പിലാക്കുന്നു, ഒരു പാരാമീറ്റർ (നീക്കം ചെയ്യേണ്ട ന്യൂറോണുകളുടെ ശതമാനം, സാധാരണയായി 10%-50%) ഉപയോഗിച്ച്, പരിശീലന സമയത്ത് ഇൻപുട്ട് വെക്ടറിലെ റാൻഡം ഘടകങ്ങൾ സീറോ ആക്കുന്നു, തുടർന്ന് അടുത്ത ലെയറിലേക്ക് അയയ്ക്കുന്നു.

ഇത് ഒരു വിചിത്രമായ ആശയമായിരിക്കാം, പക്ഷേ [`Dropout.ipynb`](Dropout.ipynb) നോട്ട്‌ബുക്കിൽ MNIST ഡിജിറ്റ് ക്ലാസിഫയറിന്റെ പരിശീലനത്തിൽ ഡ്രോപ്പ്ഔട്ടിന്റെ ഫലങ്ങൾ കാണാം. ഇത് പരിശീലനം വേഗത്തിലാക്കുകയും കുറവ് എപ്പോക്കുകളിൽ ഉയർന്ന കൃത്യത നേടാൻ സഹായിക്കുകയും ചെയ്യുന്നു.

ഈ ഫലം പലവിധം വിശദീകരിക്കാം:

 * ഇത് മോഡലിന് ഒരു റാൻഡം ഷോക്കിംഗ് ഫാക്ടറായി കണക്കാക്കാം, ഇത് ഓപ്റ്റിമൈസേഷൻ ലോക്കൽ മിനിമത്തിൽ നിന്ന് പുറത്തെടുക്കുന്നു
 * ഇത് *അപ്രത്യക്ഷ മോഡൽ ശരാശരി* ആയി കണക്കാക്കാം, കാരണം ഡ്രോപ്പ്ഔട്ടിൽ നാം ചെറിയ വ്യത്യാസമുള്ള മോഡലുകൾ പരിശീലിപ്പിക്കുന്നതാണ്

> *ചിലർ പറയുന്നു, മദ്യപിച്ച ഒരാൾ എന്തെങ്കിലും പഠിക്കാൻ ശ്രമിക്കുമ്പോൾ, അവൻ അടുത്ത ദിവസം അതു കൂടുതൽ നന്നായി ഓർക്കും, കാരണം ചില ന്യൂറോണുകൾ malfunction ചെയ്യുന്നതുകൊണ്ട് മസ്തിഷ്കം അർത്ഥം പിടികൂടാൻ കൂടുതൽ ശ്രമിക്കുന്നു. ഇത് ശരിയാണോ എന്ന് ഞങ്ങൾ പരീക്ഷിച്ചിട്ടില്ല.*

## ഓവർഫിറ്റിംഗ് തടയൽ

ഡീപ് ലേണിംഗിലെ വളരെ പ്രധാനപ്പെട്ട ഒരു ഭാഗമാണ് [ഓവർഫിറ്റിംഗ്](../../3-NeuralNetworks/05-Frameworks/Overfitting.md) തടയാൻ കഴിയുക. വളരെ ശക്തമായ ന്യൂറൽ നെറ്റ്വർക്ക് മോഡൽ ഉപയോഗിക്കാൻ ആഗ്രഹിച്ചാലും, മോഡൽ പാരാമീറ്ററുകളുടെ എണ്ണം പരിശീലന സാമ്പിളുകളുടെ എണ്ണത്തോടൊപ്പം ബാലൻസ് ചെയ്യണം.

> മുമ്പ് പരിചയപ്പെടുത്തിയ [ഓവർഫിറ്റിംഗ്](../../3-NeuralNetworks/05-Frameworks/Overfitting.md) ആശയം നിങ്ങൾ മനസ്സിലാക്കിയത് ഉറപ്പാക്കുക!

ഓവർഫിറ്റിംഗ് തടയാനുള്ള ചില മാർഗങ്ങൾ:

 * എർലി സ്റ്റോപ്പിംഗ് -- വാലിഡേഷൻ സെറ്റിലെ പിശക് നിരന്തരം നിരീക്ഷിച്ച്, വാലിഡേഷൻ പിശക് വർദ്ധിക്കുമ്പോൾ പരിശീലനം നിർത്തുക
 * എക്സ്പ്ലിസിറ്റ് വെയ്റ്റ് ഡികേ / റെഗുലറൈസേഷൻ -- ഭാരങ്ങളുടെ വലിയ മൂല്യങ്ങൾക്ക് നഷ്ടഫല ഫംഗ്ഷനിൽ അധിക ശിക്ഷ നൽകുക, ഇത് മോഡൽ അനിയന്ത്രിതമായ ഫലങ്ങൾ നൽകുന്നത് തടയുന്നു
 * മോഡൽ ശരാശരി -- പല മോഡലുകളും പരിശീലിപ്പിച്ച് ഫലങ്ങൾ ശരാശരി ചെയ്യുക. ഇത് വ്യത്യാസം കുറയ്ക്കാൻ സഹായിക്കുന്നു
 * ഡ്രോപ്പ്ഔട്ട് (അപ്രത്യക്ഷ മോഡൽ ശരാശരി)

## ഓപ്റ്റിമൈസറുകൾ / പരിശീലന ആൽഗോരിതങ്ങൾ

പരിശീലനത്തിലെ മറ്റൊരു പ്രധാന ഘടകം നല്ല പരിശീലന ആൽഗോരിതം തിരഞ്ഞെടുക്കലാണ്. പരമ്പരാഗത **ഗ്രാഡിയന്റ് ഡിസെന്റ്** ഒരു യുക്തമായ തിരഞ്ഞെടുപ്പാണ്, പക്ഷേ ചിലപ്പോൾ ഇത് വളരെ മന്ദഗതിയിലായിരിക്കാം അല്ലെങ്കിൽ മറ്റ് പ്രശ്നങ്ങൾ ഉണ്ടാകാം.

ഡീപ് ലേണിംഗിൽ, നാം **സ്റ്റോക്കാസ്റ്റിക് ഗ്രാഡിയന്റ് ഡിസെന്റ്** (SGD) ഉപയോഗിക്കുന്നു, ഇത് ട്രെയിനിംഗ് സെറ്റിൽ നിന്നുള്ള മിനിബാച്ചുകൾക്ക് പ്രയോഗിക്കുന്ന ഗ്രാഡിയന്റ് ഡിസെന്റാണ്. ഭാരങ്ങൾ ഈ ഫോർമുല ഉപയോഗിച്ച് ക്രമീകരിക്കുന്നു:

w<sup>t+1</sup> = w<sup>t</sup> - &eta;&nabla;&lagran;

### മൊമെന്റം

**മൊമെന്റം SGD**-യിൽ, നാം മുൻപത്തെ ഘട്ടങ്ങളിൽ നിന്നുള്ള ഗ്രാഡിയന്റിന്റെ ഒരു ഭാഗം സൂക്ഷിക്കുന്നു. ഇത് inertia ഉപയോഗിച്ച് എവിടെയോ നീങ്ങുമ്പോൾ, വ്യത്യസ്ത ദിശയിൽ ഒരു പഞ്ച് കിട്ടിയാലും, ട്രാജക്ടറി ഉടൻ മാറാതെ ആദ്യ ചലനത്തിന്റെ ഒരു ഭാഗം നിലനിർത്തുന്നതുപോലെയാണ്. ഇവിടെ മറ്റൊരു വെക്ടർ v പരിചയപ്പെടുത്തുന്നു, ഇത് *വേഗം* പ്രതിനിധീകരിക്കുന്നു:

* v<sup>t+1</sup> = &gamma; v<sup>t</sup> - &eta;&nabla;&lagran;
* w<sup>t+1</sup> = w<sup>t</sup>+v<sup>t+1</sup>

ഇവിടെ &gamma; പാരാമീറ്റർ inertia എത്രമാത്രം പരിഗണിക്കണമെന്ന് സൂചിപ്പിക്കുന്നു: &gamma;=0 എന്നത് പരമ്പരാഗത SGD-നാണ്; &gamma;=1 ശുദ്ധമായ ചലന സമവാക്യമാണ്.

### Adam, Adagrad, മുതലായവ

ഓരോ ലെയറിലും നാം സിഗ്നലുകൾ ഒരു മാട്രിക്സ് W<sub>i</sub> ഉപയോഗിച്ച് ഗുണിക്കുന്നു, ||W<sub>i</sub>|| അനുസരിച്ച്, ഗ്രാഡിയന്റ് കുറയുകയും 0-നോട് അടുത്ത് പോകുകയും, അല്ലെങ്കിൽ അനന്തമായി ഉയരുകയും ചെയ്യാം. ഇത് Exploding/Vanishing Gradients പ്രശ്നത്തിന്റെ സാരാംശമാണ്.

ഈ പ്രശ്നത്തിന് ഒരു പരിഹാരമായി, ഗ്രാഡിയന്റിന്റെ ദിശ മാത്രം ഉപയോഗിച്ച്, അതിന്റെ ആബ്സല്യൂട്ട് മൂല്യം അവഗണിക്കുക എന്നതാണ്, അഥവാ

w<sup>t+1</sup> = w<sup>t</sup> - &eta;(&nabla;&lagran;/||&nabla;&lagran;||), ഇവിടെ ||&nabla;&lagran;|| = &radic;&sum;(&nabla;&lagran;)<sup>2</sup>

ഈ ആൽഗോരിതം **Adagrad** എന്നാണ് അറിയപ്പെടുന്നത്. ഇതേ ആശയം ഉപയോഗിക്കുന്ന മറ്റ് ആൽഗോരിതങ്ങൾ: **RMSProp**, **Adam**

> **Adam** പല ആപ്ലിക്കേഷനുകൾക്കും വളരെ ഫലപ്രദമായ ആൽഗോരിതമായി കണക്കാക്കപ്പെടുന്നു, അതിനാൽ ഏത് ഉപയോഗിക്കണമെന്ന് ഉറപ്പില്ലെങ്കിൽ Adam ഉപയോഗിക്കുക.

### ഗ്രാഡിയന്റ് ക്ലിപ്പിംഗ്

ഗ്രാഡിയന്റ് ക്ലിപ്പിംഗ് മുകളിൽ പറഞ്ഞ ആശയത്തിന്റെ വിപുലീകരണമാണ്. ||&nabla;&lagran;|| &le; &theta; ആണെങ്കിൽ, നാം യഥാർത്ഥ ഗ്രാഡിയന്റ് ഉപയോഗിച്ച് ഭാരങ്ങൾ അപ്ഡേറ്റ് ചെയ്യുന്നു, എന്നാൽ ||&nabla;&lagran;|| > &theta; ആണെങ്കിൽ, ഗ്രാഡിയന്റിനെ അതിന്റെ നോർമിൽ വിഭജിക്കുന്നു. ഇവിടെ &theta; ഒരു പാരാമീറ്ററാണ്, സാധാരണയായി &theta;=1 അല്ലെങ്കിൽ &theta;=10 എടുക്കാം.

### ലേണിംഗ് റേറ്റ് ഡികേ

പരിശീലന വിജയത്തിന് ലേണിംഗ് റേറ്റ് പാരാമീറ്റർ &eta; വളരെ പ്രധാനമാണ്. വലിയ &eta; മൂല്യങ്ങൾ വേഗത്തിലുള്ള പരിശീലനത്തിന് കാരണമാകുമെന്ന് നാം കരുതുന്നു, ഇത് പരിശീലനത്തിന്റെ തുടക്കത്തിൽ സാധാരണ ആഗ്രഹിക്കുന്നതാണ്, പിന്നീട് ചെറിയ &eta; മൂല്യം നെറ്റ്വർക്ക് ഫൈൻ-ട്യൂൺ ചെയ്യാൻ സഹായിക്കും. അതിനാൽ, പലപ്പോഴും പരിശീലനത്തിനിടെ &eta; കുറയ്ക്കാൻ ആഗ്രഹിക്കുന്നു.

ഇത് ഓരോ എപ്പോക്കിനും ശേഷം &eta;-യെ ഒരു സംഖ്യ (ഉദാ. 0.98) കൊണ്ട് ഗുണിക്കുന്നതിലൂടെ, അല്ലെങ്കിൽ കൂടുതൽ സങ്കീർണ്ണമായ **learning rate schedule** ഉപയോഗിച്ച് ചെയ്യാം.

## വ്യത്യസ്ത നെറ്റ്വർക്ക് ആർക്കിടെക്ചറുകൾ

നിങ്ങളുടെ പ്രശ്നത്തിന് അനുയോജ്യമായ നെറ്റ്വർക്ക് ആർക്കിടെക്ചർ തിരഞ്ഞെടുക്കുന്നത് ബുദ്ധിമുട്ടുള്ള കാര്യം ആകാം. സാധാരണയായി, നമുക്ക് പ്രത്യേക ടാസ്കിനോ അതിനോട് സമാനമായ ടാസ്കിനോ ഫലപ്രദമായതായി തെളിയിച്ച ആർക്കിടെക്ചർ തിരഞ്ഞെടുക്കും. കംപ്യൂട്ടർ വിഷൻ ആവശ്യങ്ങൾക്ക് [ഒരു നല്ല അവലോകനം](https://www.topbots.com/a-brief-history-of-neural-network-architectures/) ഇവിടെ കാണാം.

> നമുക്ക് ഉള്ള പരിശീലന സാമ്പിളുകളുടെ എണ്ണത്തിന് അനുയോജ്യമായ ശക്തിയുള്ള ആർക്കിടെക്ചർ തിരഞ്ഞെടുക്കുന്നത് പ്രധാനമാണ്. വളരെ ശക്തമായ മോഡൽ തിരഞ്ഞെടുക്കുന്നത് [ഓവർഫിറ്റിംഗ്](../../3-NeuralNetworks/05-Frameworks/Overfitting.md) ഉണ്ടാക്കാം.

മറ്റൊരു നല്ല മാർഗം ആവശ്യമായ സങ്കീർണ്ണതയ്ക്ക് സ്വയം ക്രമീകരിക്കുന്ന ആർക്കിടെക്ചർ ഉപയോഗിക്കുകയാണ്. ചില അളവിൽ, **ResNet** ആർക്കിടെക്ചറും **Inception** ആർക്കിടെക്ചറും സ്വയം ക്രമീകരിക്കുന്നവയാണ്. [കമ്പ്യൂട്ടർ വിഷൻ ആർക്കിടെക്ചറുകൾക്കുറിച്ച് കൂടുതൽ](../07-ConvNets/CNN_Architectures.md)

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**അസൂയാ**:  
ഈ രേഖ AI വിവർത്തന സേവനം [Co-op Translator](https://github.com/Azure/co-op-translator) ഉപയോഗിച്ച് വിവർത്തനം ചെയ്തതാണ്. നാം കൃത്യതയ്ക്ക് ശ്രമിച്ചിട്ടുണ്ടെങ്കിലും, സ്വയം പ്രവർത്തിക്കുന്ന വിവർത്തനങ്ങളിൽ പിശകുകൾ അല്ലെങ്കിൽ തെറ്റുകൾ ഉണ്ടാകാമെന്ന് ദയവായി ശ്രദ്ധിക്കുക. അതിന്റെ മാതൃഭാഷയിലുള്ള യഥാർത്ഥ രേഖ അധികാരപരമായ ഉറവിടമായി കണക്കാക്കണം. നിർണായക വിവരങ്ങൾക്ക്, പ്രൊഫഷണൽ മനുഷ്യ വിവർത്തനം ശുപാർശ ചെയ്യപ്പെടുന്നു. ഈ വിവർത്തനം ഉപയോഗിക്കുന്നതിൽ നിന്നുണ്ടാകുന്ന ഏതെങ്കിലും തെറ്റിദ്ധാരണകൾക്കോ തെറ്റായ വ്യാഖ്യാനങ്ങൾക്കോ ഞങ്ങൾ ഉത്തരവാദികളല്ല.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->