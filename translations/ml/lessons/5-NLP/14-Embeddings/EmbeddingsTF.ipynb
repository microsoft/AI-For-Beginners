{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## എംബെഡിംഗുകൾ\n",
    "\n",
    "മുൻ ഉദാഹരണത്തിൽ, നാം `vocab_size` നീളമുള്ള ഉയർന്ന-ഡൈമെൻഷണൽ ബാഗ്-ഓഫ്-വേർഡ് വെക്ടറുകളിൽ പ്രവർത്തിച്ചിരുന്നു, കൂടാതെ കുറഞ്ഞ-ഡൈമെൻഷണൽ സ്ഥാനപരമായ പ്രതിനിധാന വെക്ടറുകളെ സ്പാർസ് വൺ-ഹോട്ട് പ്രതിനിധാനത്തിലേക്ക് വ്യക്തമായി മാറ്റിയിരുന്നു. ഈ വൺ-ഹോട്ട് പ്രതിനിധാനം മെമ്മറി കാര്യക്ഷമമല്ല. കൂടാതെ, ഓരോ വാക്കും പരസ്പരം സ്വതന്ത്രമായി പരിഗണിക്കപ്പെടുന്നു, അതിനാൽ വൺ-ഹോട്ട് എൻകോഡുചെയ്ത വെക്ടറുകൾ വാക്കുകൾക്കിടയിലെ അർത്ഥപരമായ സമാനതകൾ പ്രകടിപ്പിക്കുന്നില്ല.\n",
    "\n",
    "ഈ യൂണിറ്റിൽ, നാം **News AG** ഡാറ്റാസെറ്റിനെ തുടർന്നു പരിശോധിക്കും. തുടങ്ങാൻ, ഡാറ്റ ലോഡ് ചെയ്ത് മുൻ യൂണിറ്റിൽ നിന്നുള്ള ചില നിർവചനങ്ങൾ നേടാം.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "\n",
    "ds_train, ds_test = tfds.load('ag_news_subset').values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### എമ്പെഡ്ഡിംഗ് എന്താണ്?\n",
    "\n",
    "**എമ്പെഡ്ഡിംഗ്** എന്ന ആശയം ഒരു വാക്കിന്റെ അർത്ഥപരമായ അർത്ഥം പ്രതിഫലിപ്പിക്കുന്ന കുറവ്-ഡൈമെൻഷണൽ സാന്ദ്ര വെക്ടറുകൾ ഉപയോഗിച്ച് വാക്കുകളെ പ്രതിനിധീകരിക്കുന്നതാണ്. നാം പിന്നീട് അർത്ഥപൂർണ്ണമായ വാക്ക് എമ്പെഡ്ഡിംഗുകൾ എങ്ങനെ നിർമ്മിക്കാമെന്ന് ചർച്ച ചെയ്യും, പക്ഷേ ഇപ്പോൾ എമ്പെഡ്ഡിംഗുകളെ ഒരു വാക്ക് വെക്ടറിന്റെ ഡൈമെൻഷണാലിറ്റി കുറയ്ക്കാനുള്ള ഒരു മാർഗമായി മാത്രം കരുതാം.\n",
    "\n",
    "അതിനാൽ, ഒരു എമ്പെഡ്ഡിംഗ് ലെയർ ഒരു വാക്ക് ഇൻപുട്ടായി സ്വീകരിച്ച്, നിർദ്ദിഷ്ടമായ `embedding_size` ഉള്ള ഔട്ട്പുട്ട് വെക്ടർ ഉത്പാദിപ്പിക്കുന്നു. ഒരു അർത്ഥത്തിൽ, ഇത് ഒരു `Dense` ലെയറിനോട് വളരെ സമാനമാണ്, പക്ഷേ ഒരു വൺ-ഹോട്ട് എൻകോഡഡ് വെക്ടർ സ്വീകരിക്കുന്നതിന് പകരം, ഇത് ഒരു വാക്കിന്റെ നമ്പർ സ്വീകരിക്കാൻ കഴിയും.\n",
    "\n",
    "നമ്മുടെ നെറ്റ്‌വർക്കിലെ ആദ്യ ലെയറായി ഒരു എമ്പെഡ്ഡിംഗ് ലെയർ ഉപയോഗിച്ച്, നാം ബാഗ്-ഓഫ്-വേർഡ്സ് മോഡലിൽ നിന്ന് **എമ്പെഡ്ഡിംഗ് ബാഗ്** മോഡലിലേക്ക് മാറാം, ഇവിടെ ആദ്യം നമ്മുടെ ടെക്സ്റ്റിലെ ഓരോ വാക്കും അനുയോജ്യമായ എമ്പെഡ്ഡിങ്ങിലേക്ക് മാറ്റുകയും, പിന്നീട് ആ എമ്പെഡ്ഡിങ്ങുകളുടെ മേൽ `sum`, `average` അല്ലെങ്കിൽ `max` പോലുള്ള ഒരു സംഗ്രഹ ഫംഗ്ഷൻ കണക്കാക്കുകയും ചെയ്യും.\n",
    "\n",
    "![അഞ്ച് സീക്വൻസ് വാക്കുകൾക്കുള്ള ഒരു എമ്പെഡ്ഡിംഗ് ക്ലാസിഫയർ കാണിക്കുന്ന ചിത്രം.](../../../../../translated_images/ml/embedding-classifier-example.b77f021a7ee67eee.webp)\n",
    "\n",
    "നമ്മുടെ ക്ലാസിഫയർ ന്യൂറൽ നെറ്റ്‌വർക്ക് താഴെപ്പറയുന്ന ലെയറുകൾ ഉൾക്കൊള്ളുന്നു:\n",
    "\n",
    "* `TextVectorization` ലെയർ, ഇത് ഒരു സ്ട്രിംഗ് ഇൻപുട്ടായി സ്വീകരിച്ച്, ടോക്കൺ നമ്പറുകളുടെ ടെൻസർ ഉത്പാദിപ്പിക്കുന്നു. നാം ചില യുക്തിപരമായ വാക്ക് സമാഹാര വലുപ്പം `vocab_size` നിർദ്ദേശിക്കുകയും, കുറവായി ഉപയോഗിക്കുന്ന വാക്കുകൾ അവഗണിക്കുകയും ചെയ്യും. ഇൻപുട്ട് ആകൃതി 1 ആയിരിക്കും, ഔട്ട്പുട്ട് ആകൃതി $n$ ആയിരിക്കും, കാരണം നാം $n$ ടോക്കണുകൾ ലഭിക്കും, ഓരോന്നിലും 0 മുതൽ `vocab_size` വരെ നമ്പറുകൾ അടങ്ങിയിരിക്കും.\n",
    "* `Embedding` ലെയർ, ഇത് $n$ നമ്പറുകൾ സ്വീകരിച്ച്, ഓരോ നമ്പറിനെയും ഒരു നിശ്ചിത നീളമുള്ള (നമ്മുടെ ഉദാഹരണത്തിൽ 100) സാന്ദ്ര വെക്ടറിലേക്ക് കുറയ്ക്കുന്നു. അതിനാൽ, $n$ ആകൃതിയിലുള്ള ഇൻപുട്ട് ടെൻസർ $n \\times 100$ ആകൃതിയിലുള്ള ടെൻസറായി മാറും.\n",
    "* ആഗ്രീഗേഷൻ ലെയർ, ഇത് ഈ ടെൻസറിന്റെ ആദ്യ ആക്സിസിൽ ശരാശരി കണക്കാക്കുന്നു, അഥവാ വ്യത്യസ്ത വാക്കുകളുമായി ബന്ധപ്പെട്ട എല്ലാ $n$ ഇൻപുട്ട് ടെൻസറുകളുടെ ശരാശരി കണക്കാക്കും. ഈ ലെയർ നടപ്പിലാക്കാൻ, നാം `Lambda` ലെയർ ഉപയോഗിച്ച് ശരാശരി കണക്കാക്കുന്ന ഫംഗ്ഷൻ പാസ്സ് ചെയ്യും. ഔട്ട്പുട്ട് ആകൃതി 100 ആയിരിക്കും, ഇത് മുഴുവൻ ഇൻപുട്ട് സീക്വൻസിന്റെ സംഖ്യാത്മക പ്രതിനിധാനം ആയിരിക്കും.\n",
    "* അന്തിമ `Dense` ലീനിയർ ക്ലാസിഫയർ.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization (TextVec  (None, None)             0         \n",
      " torization)                                                     \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, None, 100)         3000000   \n",
      "                                                                 \n",
      " lambda (Lambda)             (None, 100)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4)                 404       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,000,404\n",
      "Trainable params: 3,000,404\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 30000\n",
    "batch_size = 128\n",
    "\n",
    "vectorizer = keras.layers.experimental.preprocessing.TextVectorization(max_tokens=vocab_size,input_shape=(1,))\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    vectorizer,    \n",
    "    keras.layers.Embedding(vocab_size,100),\n",
    "    keras.layers.Lambda(lambda x: tf.reduce_mean(x,axis=1)),\n",
    "    keras.layers.Dense(4, activation='softmax')\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`summary` പ്രിന്റ് ഔട്ടിൽ, **output shape** കോളത്തിൽ, ആദ്യത്തെ ടെൻസർ ഡൈമെൻഷൻ `None` മിനിബാച്ച് സൈസിനാണ്, രണ്ടാമത്തേത് ടോക്കൺ സീക്വൻസിന്റെ നീളിനാണ്. മിനിബാച്ചിലെ എല്ലാ ടോക്കൺ സീക്വൻസുകൾക്കും വ്യത്യസ്ത നീളുകൾ ഉണ്ട്. ഇതിനെ എങ്ങനെ കൈകാര്യം ചെയ്യാമെന്ന് അടുത്ത ഭാഗത്ത് ചർച്ച ചെയ്യും.\n",
    "\n",
    "ഇപ്പോൾ നെറ്റ്‌വർക്ക് ട്രെയിൻ ചെയ്യാം:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training vectorizer\n",
      "938/938 [==============================] - 20s 20ms/step - loss: 0.7891 - acc: 0.8155 - val_loss: 0.4470 - val_acc: 0.8642\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22255515100>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_text(x):\n",
    "    return x['title']+' '+x['description']\n",
    "\n",
    "def tupelize(x):\n",
    "    return (extract_text(x),x['label'])\n",
    "\n",
    "print(\"Training vectorizer\")\n",
    "vectorizer.adapt(ds_train.take(500).map(extract_text))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'])\n",
    "model.fit(ds_train.map(tupelize).batch(batch_size),validation_data=ds_test.map(tupelize).batch(batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "> **കുറിപ്പ്** നാം ഡാറ്റയുടെ ഒരു ഉപസമൂഹത്തെ അടിസ്ഥാനമാക്കി വെക്ടറൈസർ നിർമ്മിക്കുകയാണ്. പ്രക്രിയ വേഗത്തിലാക്കുന്നതിനായി ഇത് ചെയ്യപ്പെടുന്നു, അതിനാൽ നമ്മുടെ ടെക്സ്റ്റിലെ എല്ലാ ടോക്കണുകളും വാക്കസഞ്ചയത്തിൽ ഉണ്ടാകാതിരിക്കാം. ഈ സാഹചര്യത്തിൽ, ആ ടോക്കണുകൾ അവഗണിക്കപ്പെടും, ഇത് അല്പം കുറവ് കൃത്യതയ്ക്ക് കാരണമാകാം. എന്നിരുന്നാലും, യാഥാർത്ഥ്യത്തിൽ ടെക്സ്റ്റിന്റെ ഒരു ഉപസമൂഹം സാധാരണയായി നല്ല വാക്കസഞ്ചയ അളവുകൂട്ടലിന് സഹായിക്കുന്നു.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### വ്യത്യസ്ത സീക്വൻസ് വലുപ്പങ്ങൾ കൈകാര്യം ചെയ്യൽ\n",
    "\n",
    "മിനിബാച്ചുകളിൽ പരിശീലനം എങ്ങനെ നടക്കുന്നു എന്ന് നമുക്ക് മനസിലാക്കാം. മുകളിൽ നൽകിയ ഉദാഹരണത്തിൽ, ഇൻപുട്ട് ടെൻസറിന്റെ ഡൈമെൻഷൻ 1 ആണ്, കൂടാതെ 128-ലോംഗ് മിനിബാച്ചുകൾ ഉപയോഗിക്കുന്നു, അതിനാൽ ടെൻസറിന്റെ യഥാർത്ഥ വലുപ്പം $128 \\times 1$ ആണ്. എന്നാൽ, ഓരോ വാചകത്തിലെ ടോക്കണുകളുടെ എണ്ണം വ്യത്യസ്തമാണ്. `TextVectorization` ലെയർ ഒരു സിംഗിൾ ഇൻപുട്ടിൽ പ്രയോഗിച്ചാൽ, ടോക്കണുകളുടെ എണ്ണം ടെക്സ്റ്റ് എങ്ങനെ ടോക്കൺ ചെയ്യപ്പെടുന്നു എന്നതിനെ ആശ്രയിച്ച് വ്യത്യസ്തമായിരിക്കും:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 1 45], shape=(2,), dtype=int64)\n",
      "tf.Tensor([ 112 1271    1    3 1747  158], shape=(6,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer('Hello, world!'))\n",
    "print(vectorizer('I am glad to meet you!'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "എങ്കിലും, നാം വെക്ടറൈസർ പല സീക്വൻസുകളിലും പ്രയോഗിക്കുമ്പോൾ, അത് ഒരു ചതുരാകൃതിയിലുള്ള ടെൻസർ ഉത്പാദിപ്പിക്കണം, അതിനാൽ ഉപയോഗിക്കാത്ത ഘടകങ്ങൾ PAD ടോക്കൺ (നമ്മുടെ കേസിൽ ഇത് പൂജ്യം) ഉപയോഗിച്ച് പൂരിപ്പിക്കുന്നു:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 6), dtype=int64, numpy=\n",
       "array([[   1,   45,    0,    0,    0,    0],\n",
       "       [ 112, 1271,    1,    3, 1747,  158]], dtype=int64)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer(['Hello, world!','I am glad to meet you!'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ഇവിടെ നാം എംബെഡിംഗുകൾ കാണാം:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 1.53059261e-02,  6.80514947e-02,  3.14026810e-02, ...,\n",
       "         -8.92002955e-02,  1.52911525e-04, -5.65562584e-02],\n",
       "        [ 2.57456154e-01,  2.79364467e-01, -2.03605562e-01, ...,\n",
       "         -2.07474351e-01,  8.31158683e-02, -2.03911960e-01],\n",
       "        [ 3.98201384e-02, -8.03454965e-03,  2.39790026e-02, ...,\n",
       "         -7.18549127e-04,  2.66963355e-02, -4.30646613e-02],\n",
       "        [ 3.98201384e-02, -8.03454965e-03,  2.39790026e-02, ...,\n",
       "         -7.18549127e-04,  2.66963355e-02, -4.30646613e-02],\n",
       "        [ 3.98201384e-02, -8.03454965e-03,  2.39790026e-02, ...,\n",
       "         -7.18549127e-04,  2.66963355e-02, -4.30646613e-02],\n",
       "        [ 3.98201384e-02, -8.03454965e-03,  2.39790026e-02, ...,\n",
       "         -7.18549127e-04,  2.66963355e-02, -4.30646613e-02]],\n",
       "\n",
       "       [[ 1.89674050e-01,  2.61548996e-01, -3.67433839e-02, ...,\n",
       "         -2.07366899e-01, -1.05442435e-01, -2.36952081e-01],\n",
       "        [ 6.16133213e-02,  1.80511594e-01,  9.77298319e-02, ...,\n",
       "         -5.46628237e-02, -1.07340455e-01, -1.06589928e-01],\n",
       "        [ 1.53059261e-02,  6.80514947e-02,  3.14026810e-02, ...,\n",
       "         -8.92002955e-02,  1.52911525e-04, -5.65562584e-02],\n",
       "        [-4.84890305e-02, -8.41715634e-02,  1.51529670e-01, ...,\n",
       "          1.28192469e-01, -7.77286515e-02,  1.26041949e-01],\n",
       "        [-4.17212099e-02, -5.60694858e-02,  4.08860669e-02, ...,\n",
       "          8.70475471e-02,  8.92383084e-02,  1.67974353e-01],\n",
       "        [ 2.85779923e-01,  4.57767487e-01,  4.52292450e-02, ...,\n",
       "         -1.97419018e-01, -2.04659685e-01, -2.79758364e-01]]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[1](vectorizer(['Hello, world!','I am glad to meet you!'])).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **കുറിപ്പ്**: പാഡിംഗിന്റെ അളവ് കുറയ്ക്കാൻ, ചില സാഹചര്യങ്ങളിൽ ഡാറ്റാസെറ്റിലെ എല്ലാ സീക്വൻസുകളും വർദ്ധിക്കുന്ന നീളത്തിന്റെ (അഥവാ, കൂടുതൽ കൃത്യമായി, ടോക്കണുകളുടെ എണ്ണം) ക്രമത്തിൽ ക്രമീകരിക്കുന്നത് ഉചിതമാണ്. ഇതിലൂടെ ഓരോ മിനിബാച്ചിലും സമാന നീളമുള്ള സീക്വൻസുകൾ ഉൾപ്പെടുന്നതായി ഉറപ്പാക്കാം.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## സിമാന്റിക് എംബെഡിംഗുകൾ: Word2Vec\n",
    "\n",
    "മുൻ ഉദാഹരണത്തിൽ, എംബെഡിംഗ് ലെയർ വാക്കുകളെ വെക്ടർ പ്രതിനിധാനങ്ങളായി മാപ്പ് ചെയ്യാൻ പഠിച്ചു, എന്നാൽ ഈ പ്രതിനിധാനങ്ങൾക്ക് സിമാന്റിക് അർത്ഥം ഉണ്ടായിരുന്നില്ല. സമാനമായ വാക്കുകൾ അല്ലെങ്കിൽ പദസമാനാർത്ഥങ്ങൾ ചില വെക്ടർ ദൂരങ്ങളിൽ (ഉദാഹരണത്തിന് യൂക്ലിഡിയൻ ദൂരം) അടുത്തുള്ള വെക്ടറുകളായി പ്രതിനിധാനം ചെയ്യപ്പെടുന്ന ഒരു വെക്ടർ പ്രതിനിധാനം പഠിക്കാനാകും നല്ലത്.\n",
    "\n",
    "അത് ചെയ്യാൻ, വലിയ ഒരു ടെക്സ്റ്റ് ശേഖരത്തിൽ നമ്മുടെ എംബെഡിംഗ് മോഡൽ പ്രീട്രെയിൻ ചെയ്യേണ്ടതുണ്ട്, [Word2Vec](https://en.wikipedia.org/wiki/Word2vec) പോലുള്ള സാങ്കേതിക വിദ്യ ഉപയോഗിച്ച്. വാക്കുകളുടെ വിതരണ പ്രതിനിധാനം സൃഷ്ടിക്കാൻ ഉപയോഗിക്കുന്ന രണ്ട് പ്രധാന ആർക്കിടെക്ചറുകൾ ഇതാണ്:\n",
    "\n",
    " - **കണ്ടിന്യൂവസ് ബാഗ്-ഓഫ്-വേർഡ്സ്** (CBoW), ഇവിടെ മോഡൽ ചുറ്റുപാടിലുള്ള കോൺടെക്സ്റ്റിൽ നിന്നൊരു വാക്ക് പ്രവചിക്കാൻ പരിശീലിപ്പിക്കുന്നു. ngram $(W_{-2},W_{-1},W_0,W_1,W_2)$ നൽകിയാൽ, മോഡലിന്റെ ലക്ഷ്യം $(W_{-2},W_{-1},W_1,W_2)$-ൽ നിന്നു $W_0$ പ്രവചിക്കുക എന്നതാണ്.\n",
    " - **കണ്ടിന്യൂവസ് സ്കിപ്പ്-ഗ്രാം** CBoW-യുടെ വിപരീതമാണ്. മോഡൽ നിലവിലെ വാക്ക് പ്രവചിക്കാൻ ചുറ്റുപാടിലുള്ള കോൺടെക്സ്റ്റ് വാക്കുകളുടെ വിൻഡോ ഉപയോഗിക്കുന്നു.\n",
    "\n",
    "CBoW വേഗത്തിലാണ്, സ്കിപ്പ്-ഗ്രാം മന്ദഗതിയിലാണ്, എന്നാൽ അപൂർവമായ വാക്കുകൾ പ്രതിനിധാനം ചെയ്യുന്നതിൽ മികച്ചതാണ്.\n",
    "\n",
    "![വാക്കുകൾ വെക്ടറുകളായി മാറ്റാൻ CBoWയും സ്കിപ്പ്-ഗ്രാം ആൽഗോരിതങ്ങളും കാണിക്കുന്ന ചിത്രം.](../../../../../translated_images/ml/example-algorithms-for-converting-words-to-vectors.fbe9207a726922f6.webp)\n",
    "\n",
    "Google News ഡാറ്റാസെറ്റിൽ പ്രീട്രെയിൻ ചെയ്ത Word2Vec എംബെഡിംഗ് പരീക്ഷിക്കാൻ, **gensim** ലൈബ്രറി ഉപയോഗിക്കാം. താഴെ 'neural' എന്ന വാക്കിനോട് ഏറ്റവും സമാനമായ വാക്കുകൾ കാണിക്കുന്നു.\n",
    "\n",
    "> **Note:** വാക്ക് വെക്ടറുകൾ ആദ്യമായി സൃഷ്ടിക്കുമ്പോൾ, ഡൗൺലോഡ് ചെയ്യാൻ കുറച്ച് സമയം എടുക്കാം!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "w2v = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neuronal -> 0.7804799675941467\n",
      "neurons -> 0.7326500415802002\n",
      "neural_circuits -> 0.7252851724624634\n",
      "neuron -> 0.7174385190010071\n",
      "cortical -> 0.6941086649894714\n",
      "brain_circuitry -> 0.6923246383666992\n",
      "synaptic -> 0.6699118614196777\n",
      "neural_circuitry -> 0.6638563275337219\n",
      "neurochemical -> 0.6555314064025879\n",
      "neuronal_activity -> 0.6531826257705688\n"
     ]
    }
   ],
   "source": [
    "for w,p in w2v.most_similar('neural'):\n",
    "    print(f\"{w} -> {p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "നാം വാക്കിൽ നിന്നുള്ള വെക്ടർ എംബെഡ്ഡിംഗ് കൂടി എടുക്കാം, ക്ലാസിഫിക്കേഷൻ മോഡൽ പരിശീലനത്തിൽ ഉപയോഗിക്കാൻ. എംബെഡ്ഡിംഗിന് 300 ഘടകങ്ങളുണ്ട്, പക്ഷേ ഇവിടെ വ്യക്തതയ്ക്കായി വെക്ടറിന്റെ ആദ്യ 20 ഘടകങ്ങൾ മാത്രമാണ് കാണിക്കുന്നത്:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01226807,  0.06225586,  0.10693359,  0.05810547,  0.23828125,\n",
       "        0.03686523,  0.05151367, -0.20703125,  0.01989746,  0.10058594,\n",
       "       -0.03759766, -0.1015625 , -0.15820312, -0.08105469, -0.0390625 ,\n",
       "       -0.05053711,  0.16015625,  0.2578125 ,  0.10058594, -0.25976562],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v['play'][:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "സെമാന്റിക് എംബെഡിംഗുകളുടെ വലിയ ഗുണം എന്തെന്നാൽ, സെമാന്റിക്സിന്റെ അടിസ്ഥാനത്തിൽ വെക്ടർ എൻകോഡിംഗ് മാനിപ്പുലേറ്റ് ചെയ്യാൻ കഴിയുന്നതാണ്. ഉദാഹരണത്തിന്, *king* എന്ന വാക്കിനും *woman* എന്ന വാക്കിനും όσο അടുത്തുള്ള വെക്ടർ പ്രതിനിധാനം ഉള്ള ഒരു വാക്ക് കണ്ടെത്താൻ, കൂടാതെ *man* എന്ന വാക്കിൽ നിന്ന് όσο ദൂരമുള്ള ഒരു വാക്ക് കണ്ടെത്താൻ ഞങ്ങൾ ചോദിക്കാം:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('queen', 0.7118192911148071)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar(positive=['king','woman'],negative=['man'])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "മുകളിൽ നൽകിയ ഉദാഹരണം ചില ആന്തരിക GenSym മാജിക് ഉപയോഗിച്ചാണ്, പക്ഷേ അടിസ്ഥാന തർക്കം വളരെ ലളിതമാണ്. embeddings-ന്റെ ഒരു രസകരമായ കാര്യം എന്തെന്നാൽ, embedding വെക്ടറുകളിൽ സാധാരണ വെക്ടർ പ്രവർത്തനങ്ങൾ നടത്താൻ കഴിയും, അത് വാക്കുകളുടെ **അർത്ഥങ്ങളിൽ** പ്രവർത്തനങ്ങൾ പ്രതിഫലിപ്പിക്കും. മുകളിൽ നൽകിയ ഉദാഹരണം വെക്ടർ പ്രവർത്തനങ്ങളുടെ രൂപത്തിൽ这样可以表达：നാം **KING-MAN+WOMAN** എന്ന വെക്ടർ കണക്കാക്കുന്നു (പ്രവർത്തനങ്ങൾ `+` ഉം `-` ഉം ബന്ധപ്പെട്ട വാക്കുകളുടെ വെക്ടർ പ്രതിനിധാനങ്ങളിൽ നടത്തപ്പെടുന്നു), തുടർന്ന് ആ വെക്ടറിനോട് ഏറ്റവും അടുത്ത വാക്ക് നിഘണ്ടുവിൽ കണ്ടെത്തുന്നു:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'queen'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the vector corresponding to kind-man+woman\n",
    "qvec = w2v['king']-1.7*w2v['man']+1.7*w2v['woman']\n",
    "# find the index of the closest embedding vector \n",
    "d = np.sum((w2v.vectors-qvec)**2,axis=1)\n",
    "min_idx = np.argmin(d)\n",
    "# find the corresponding word\n",
    "w2v.index_to_key[min_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NOTE**: *man* and *woman* വെക്ടറുകളിൽ ചെറിയ കോഫിഷ്യന്റുകൾ ചേർക്കേണ്ടിവന്നു - അവ നീക്കംചെയ്താൽ എന്ത് സംഭവിക്കുന്നുവെന്ന് നോക്കൂ.\n",
    "\n",
    "ഏറ്റവും അടുത്ത വെക്ടർ കണ്ടെത്താൻ, TensorFlow യന്ത്രം ഉപയോഗിച്ച് നമ്മുടെ വെക്ടറും വാക്കുകളുടെ വോകാബുലറിയിലുള്ള എല്ലാ വെക്ടറുകളും തമ്മിലുള്ള ദൂരങ്ങളുടെ വെക്ടർ കണക്കാക്കുന്നു, പിന്നീട് `argmin` ഉപയോഗിച്ച് ഏറ്റവും കുറഞ്ഞ വാക്കിന്റെ ഇൻഡക്സ് കണ്ടെത്തുന്നു.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec വാക്കുകളുടെ അർത്ഥം പ്രകടിപ്പിക്കാൻ മികച്ച മാർഗമായിരുന്നാലും, അതിന് നിരവധി ദോഷങ്ങൾ ഉണ്ട്, അവയിൽ ചിലത് താഴെപ്പറയുന്നവയാണ്:\n",
    "\n",
    "* CBoW ഉം skip-gram മോഡലുകളും **പ്രഡിക്ടീവ് എംബെഡിംഗുകൾ** ആണെന്നും അവയ്ക്ക് പ്രാദേശിക കോൺടെക്സ്റ്റ് മാത്രമേ പരിഗണിക്കുകയുള്ളൂ എന്നും. Word2Vec ആഗോള കോൺടെക്സ്റ്റിന്റെ പ്രയോജനം ഉപയോഗപ്പെടുത്തുന്നില്ല.\n",
    "* Word2Vec വാക്കിന്റെ **മോർഫോളജി** പരിഗണിക്കുന്നില്ല, അതായത് വാക്കിന്റെ അർത്ഥം വാക്കിന്റെ വിവിധ ഭാഗങ്ങളിൽ, ഉദാഹരണത്തിന് റൂട്ട്, ആശ്രയിച്ചിരിക്കാം എന്ന സത്യം പരിഗണിക്കുന്നില്ല.\n",
    "\n",
    "**FastText** രണ്ടാം പരിമിതിയെ മറികടക്കാൻ ശ്രമിക്കുന്നു, Word2Vec-നെ അടിസ്ഥാനമാക്കി ഓരോ വാക്കിനും അതിലെ അക്ഷര n-ഗ്രാമുകൾക്കും വെക്ടർ പ്രതിനിധാനങ്ങൾ പഠിക്കുന്നു. ഓരോ ട്രെയിനിംഗ് ഘട്ടത്തിലും ഈ പ്രതിനിധാനങ്ങളുടെ മൂല്യങ്ങൾ ശരാശരി എടുത്ത് ഒരു വെക്ടറായി മാറ്റുന്നു. ഇതു പ്രീട്രെയിനിംഗിൽ അധിക കണക്കുകൂട്ടലുകൾ കൂട്ടിച്ചേർക്കുന്നുണ്ടെങ്കിലും, വാക്ക് എംബെഡിംഗുകൾക്ക് ഉപവാക്ക് വിവരങ്ങൾ കോഡ് ചെയ്യാൻ സാധ്യമാക്കുന്നു.\n",
    "\n",
    "മറ്റൊരു രീതി, **GloVe**, വാക്ക് കോൺടെക്സ്റ്റ് മാട്രിക്സ് ഫാക്ടറൈസേഷനിൽ അടിസ്ഥാനമാക്കിയുള്ള വ്യത്യസ്ത സമീപനം ഉപയോഗിക്കുന്നു. ആദ്യം, വ്യത്യസ്ത കോൺടെക്സ്റ്റുകളിൽ വാക്കുകളുടെ സംഭവനകളുടെ എണ്ണം എണ്ണുന്ന വലിയ ഒരു മാട്രിക്സ് നിർമ്മിക്കുന്നു, പിന്നീട് ഈ മാട്രിക്സ് കുറവ് ഡൈമെൻഷനുകളിൽ പ്രതിനിധാനം ചെയ്യാൻ ശ്രമിക്കുന്നു, പുനർനിർമ്മാണ നഷ്ടം കുറയ്ക്കുന്ന വിധത്തിൽ.\n",
    "\n",
    "gensim ലൈബ്രറി ആ വാക്ക് എംബെഡിംഗുകൾ പിന്തുണയ്ക്കുന്നു, മുകളിൽ നൽകിയ മോഡൽ ലോഡിംഗ് കോഡ് മാറ്റി അവയുമായി പരീക്ഷണം നടത്താം.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras-ൽ pretrained embeddings ഉപയോഗിക്കൽ\n",
    "\n",
    "മുകളിൽ നൽകിയ ഉദാഹരണം മാറ്റി, embedding ലെയറിൽ semantic embeddings, ഉദാഹരണത്തിന് Word2Vec ഉപയോഗിച്ച് matrix മുൻകൂട്ടി പൂരിപ്പിക്കാം. pretrained embedding-ന്റെ vocabularyയും ടെക്സ്റ്റ് കോർപ്പസിന്റെ vocabularyയും സാധാരണയായി പൊരുത്തപ്പെടില്ല, അതിനാൽ ഒരു vocabulary തിരഞ്ഞെടുക്കേണ്ടതുണ്ട്. ഇവിടെ രണ്ട് സാധ്യതകൾ പരിശോധിക്കുന്നു: tokenizer vocabulary ഉപയോഗിക്കൽ, Word2Vec embeddings-ന്റെ vocabulary ഉപയോഗിക്കൽ.\n",
    "\n",
    "### Tokenizer vocabulary ഉപയോഗിക്കൽ\n",
    "\n",
    "Tokenizer vocabulary ഉപയോഗിക്കുമ്പോൾ, vocabularyയിലെ ചില വാക്കുകൾക്ക് Word2Vec embeddings ഉണ്ടാകും, ചിലത് ഇല്ലാതിരിക്കും. നമ്മുടെ vocabulary വലുപ്പം `vocab_size` ആണെന്നും Word2Vec embedding വെക്ടർ നീളം `embed_size` ആണെന്നും കരുതുമ്പോൾ, embedding ലെയർ ഒരു വെയ്റ്റ് മാട്രിക്സ് ആയി പ്രതിനിധീകരിക്കും, അതിന്റെ ആകൃതി `vocab_size`$\\times$`embed_size` ആണ്. vocabulary വഴി കടന്നുപോകിയാണ് ഈ മാട്രിക്സ് പൂരിപ്പിക്കുന്നത്:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding size: 300\n",
      "Populating matrix, this will take some time...Done, found 4551 words, 784 words missing\n"
     ]
    }
   ],
   "source": [
    "embed_size = len(w2v.get_vector('hello'))\n",
    "print(f'Embedding size: {embed_size}')\n",
    "\n",
    "vocab = vectorizer.get_vocabulary()\n",
    "W = np.zeros((vocab_size,embed_size))\n",
    "print('Populating matrix, this will take some time...',end='')\n",
    "found, not_found = 0,0\n",
    "for i,w in enumerate(vocab):\n",
    "    try:\n",
    "        W[i] = w2v.get_vector(w)\n",
    "        found+=1\n",
    "    except:\n",
    "        # W[i] = np.random.normal(0.0,0.3,size=(embed_size,))\n",
    "        not_found+=1\n",
    "\n",
    "print(f\"Done, found {found} words, {not_found} words missing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec വാക്ക് കോശത്തിൽ ഇല്ലാത്ത വാക്കുകൾക്ക്, നാം അവയെ പൂജ്യം വാല്യുക്കളായി വയ്ക്കാമോ, അല്ലെങ്കിൽ ഒരു യാദൃച്ഛിക വെക്ടർ സൃഷ്ടിക്കാമോ.\n",
    "\n",
    "ഇപ്പോൾ നാം pretrained ഭാരങ്ങളോടുകൂടിയ ഒരു embedding ലെയർ നിർവചിക്കാം:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = keras.layers.Embedding(vocab_size,embed_size,weights=[W],trainable=False)\n",
    "model = keras.models.Sequential([\n",
    "    vectorizer, emb,\n",
    "    keras.layers.Lambda(lambda x: tf.reduce_mean(x,axis=1)),\n",
    "    keras.layers.Dense(4, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ഇപ്പോൾ നമുക്ക് നമ്മുടെ മോഡൽ പരിശീലിപ്പിക്കാം.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 10s 10ms/step - loss: 1.1075 - acc: 0.7822 - val_loss: 0.9134 - val_acc: 0.8175\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2220226ef10>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'])\n",
    "model.fit(ds_train.map(tupelize).batch(batch_size),\n",
    "          validation_data=ds_test.map(tupelize).batch(batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **കുറിപ്പ്**: `Embedding` സൃഷ്ടിക്കുമ്പോൾ `trainable=False` എന്ന് നമുക്ക് സജ്ജമാക്കിയിട്ടുണ്ട്, അതായത് Embedding ലെയർ വീണ്ടും പരിശീലിപ്പിക്കുന്നില്ല. ഇതിന്റെ ഫലമായി കൃത്യത കുറവാകാം, പക്ഷേ പരിശീലനം വേഗത്തിലാക്കും.\n",
    "\n",
    "### embedding വാക്ക് സമാഹാരം ഉപയോഗിക്കൽ\n",
    "\n",
    "മുൻപത്തെ സമീപനത്തിലെ ഒരു പ്രശ്നം TextVectorization-ലും Embedding-ലും ഉപയോഗിക്കുന്ന വാക്ക് സമാഹാരങ്ങൾ വ്യത്യസ്തമാണെന്നതാണ്. ഈ പ്രശ്നം മറികടക്കാൻ, താഴെ പറയുന്ന പരിഹാരങ്ങളിൽ ഒന്നോ അതിലധികമോ ഉപയോഗിക്കാം:\n",
    "* Word2Vec മോഡൽ നമ്മുടെ വാക്ക് സമാഹാരത്തിൽ വീണ്ടും പരിശീലിപ്പിക്കുക.\n",
    "* pretrained Word2Vec മോഡലിൽ നിന്നുള്ള വാക്ക് സമാഹാരം ഉപയോഗിച്ച് നമ്മുടെ ഡാറ്റാസെറ്റ് ലോഡ് ചെയ്യുക. ഡാറ്റാസെറ്റ് ലോഡുചെയ്യുമ്പോൾ ഉപയോഗിക്കുന്ന വാക്ക് സമാഹാരം വ്യക്തമാക്കാം.\n",
    "\n",
    "ഇരണ്ടാമത്തെ സമീപനം എളുപ്പമാണ്, അതിനാൽ അത് നടപ്പിലാക്കാം. ആദ്യം, Word2Vec embeddings-ൽ നിന്നുള്ള നിശ്ചിത വാക്ക് സമാഹാരം ഉപയോഗിച്ച് `TextVectorization` ലെയർ സൃഷ്ടിക്കാം:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(w2v.vocab.keys())\n",
    "vectorizer = keras.layers.experimental.preprocessing.TextVectorization(input_shape=(1,))\n",
    "vectorizer.set_vocabulary(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gensim വാക്ക് എംബെഡ്ഡിംഗ്സ് ലൈബ്രറിയിൽ ഒരു സൗകര്യപ്രദമായ ഫംഗ്ഷൻ ഉണ്ട്, `get_keras_embeddings`, ഇത് സ്വയം അനുയോജ്യമായ Keras എംബെഡ്ഡിംഗ്സ് ലെയർ സൃഷ്ടിക്കും.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "938/938 [==============================] - 20s 14ms/step - loss: 1.3377 - acc: 0.4978 - val_loss: 1.2995 - val_acc: 0.5647\n",
      "Epoch 2/5\n",
      "938/938 [==============================] - 10s 10ms/step - loss: 1.2587 - acc: 0.5722 - val_loss: 1.2339 - val_acc: 0.5842\n",
      "Epoch 3/5\n",
      "938/938 [==============================] - 10s 10ms/step - loss: 1.1980 - acc: 0.5884 - val_loss: 1.1826 - val_acc: 0.5954\n",
      "Epoch 4/5\n",
      "938/938 [==============================] - 12s 13ms/step - loss: 1.1503 - acc: 0.6002 - val_loss: 1.1417 - val_acc: 0.6018\n",
      "Epoch 5/5\n",
      "938/938 [==============================] - 11s 12ms/step - loss: 1.1120 - acc: 0.6097 - val_loss: 1.1083 - val_acc: 0.6104\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2220ccb81c0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    vectorizer, \n",
    "    w2v.get_keras_embedding(train_embeddings=False),\n",
    "    keras.layers.Lambda(lambda x: tf.reduce_mean(x,axis=1)),\n",
    "    keras.layers.Dense(4, activation='softmax')\n",
    "])\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'])\n",
    "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128),epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "നാം ഉയർന്ന കൃത്യത കാണാത്തതിന്റെ ഒരു കാരണമാണ് നമ്മുടെ ഡാറ്റാസെറ്റിലെ ചില വാക്കുകൾ പ്രീട്രെയിൻ ചെയ്ത GloVe വാക്കസഞ്ചാരത്തിൽ ഇല്ലാതിരിക്കുക, അതിനാൽ അവ അടിസ്ഥാനപരമായി അവഗണിക്കപ്പെടുന്നു. ഇതിനെ മറികടക്കാൻ, നാം നമ്മുടെ ഡാറ്റാസെറ്റിനെ അടിസ്ഥാനമാക്കി നമ്മുടെ സ്വന്തം എംബെഡിംഗുകൾ പരിശീലിപ്പിക്കാം.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## സാന്ദർഭിക എംബെഡിംഗുകൾ\n",
    "\n",
    "Word2Vec പോലുള്ള പരമ്പരാഗത പ്രീട്രെയിൻഡ് എംബെഡിംഗ് പ്രതിനിധാനങ്ങളുടെ ഒരു പ്രധാന പരിമിതിയാണ്, അവ ഒരു വാക്കിന്റെ ചില അർത്ഥങ്ങൾ പിടിച്ചുപറ്റാൻ കഴിയുന്നുവെങ്കിലും, വ്യത്യസ്ത അർത്ഥങ്ങൾ തമ്മിൽ വ്യത്യാസപ്പെടുത്താൻ കഴിയാത്തത്. ഇത് ഡൗൺസ്ട്രീം മോഡലുകളിൽ പ്രശ്നങ്ങൾ സൃഷ്ടിക്കാം.\n",
    "\n",
    "ഉദാഹരണത്തിന്, 'play' എന്ന വാക്കിന് ഈ രണ്ട് വ്യത്യസ്ത വാക്യങ്ങളിൽ വ്യത്യസ്ത അർത്ഥങ്ങളുണ്ട്:\n",
    "- ഞാൻ തിയേറ്ററിൽ ഒരു **play** കാണാൻ പോയി.\n",
    "- ജോൺ തന്റെ സുഹൃത്തുക്കളുമായി **play** ചെയ്യാൻ ആഗ്രഹിക്കുന്നു.\n",
    "\n",
    "നാം ചർച്ച ചെയ്ത പ്രീട്രെയിൻഡ് എംബെഡിംഗുകൾ 'play' എന്ന വാക്കിന്റെ രണ്ട് അർത്ഥങ്ങളും ഒരേ എംബെഡിംഗിൽ പ്രതിനിധാനം ചെയ്യുന്നു. ഈ പരിമിതിയെ മറികടക്കാൻ, വലിയ ടെക്സ്റ്റ് കോർപ്പസിൽ പരിശീലിപ്പിച്ച **ഭാഷാ മോഡലിന്റെ** അടിസ്ഥാനത്തിൽ എംബെഡിംഗുകൾ നിർമ്മിക്കേണ്ടതുണ്ട്, ഇത് വാക്കുകൾ വ്യത്യസ്ത സാന്ദർഭങ്ങളിൽ എങ്ങനെ ചേർക്കാമെന്ന് *അറിയുന്നു*. സാന്ദർഭിക എംബെഡിംഗുകൾ ചർച്ച ചെയ്യുന്നത് ഈ ട്യൂട്ടോറിയലിന്റെ പരിധിക്ക് പുറത്താണ്, പക്ഷേ അടുത്ത യൂണിറ്റിൽ ഭാഷാ മോഡലുകൾക്കുറിച്ച് സംസാരിക്കുമ്പോൾ അവയെക്കുറിച്ച് വീണ്ടും വരാം.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n<!-- CO-OP TRANSLATOR DISCLAIMER START -->\n**അസൂയാ**:  \nഈ രേഖ AI വിവർത്തന സേവനം [Co-op Translator](https://github.com/Azure/co-op-translator) ഉപയോഗിച്ച് വിവർത്തനം ചെയ്തതാണ്. നാം കൃത്യതയ്ക്ക് ശ്രമിച്ചിട്ടുണ്ടെങ്കിലും, സ്വയം പ്രവർത്തിക്കുന്ന വിവർത്തനങ്ങളിൽ പിശകുകൾ അല്ലെങ്കിൽ തെറ്റുകൾ ഉണ്ടാകാമെന്ന് ദയവായി ശ്രദ്ധിക്കുക. അതിന്റെ മാതൃഭാഷയിലുള്ള യഥാർത്ഥ രേഖയാണ് പ്രാമാണികമായ ഉറവിടം എന്ന് പരിഗണിക്കേണ്ടതാണ്. നിർണായകമായ വിവരങ്ങൾക്ക്, പ്രൊഫഷണൽ മനുഷ്യ വിവർത്തനം ശുപാർശ ചെയ്യപ്പെടുന്നു. ഈ വിവർത്തനം ഉപയോഗിക്കുന്നതിൽ നിന്നുണ്ടാകുന്ന ഏതെങ്കിലും തെറ്റിദ്ധാരണകൾക്കോ തെറ്റായ വ്യാഖ്യാനങ്ങൾക്കോ ഞങ്ങൾ ഉത്തരവാദികളല്ല.\n<!-- CO-OP TRANSLATOR DISCLAIMER END -->\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0cb620c6d4b9f7a635928804c26cf22403d89d98d79684e4529119355ee6d5a5"
  },
  "kernel_info": {
   "name": "conda-env-py37_tensorflow-py"
  },
  "kernelspec": {
   "display_name": "py37_tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "coopTranslator": {
   "original_hash": "b859482be7f61d1eadc2c6a2720a37e4",
   "translation_date": "2025-11-26T01:51:37+00:00",
   "source_file": "lessons/5-NLP/14-Embeddings/EmbeddingsTF.ipynb",
   "language_code": "ml"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}