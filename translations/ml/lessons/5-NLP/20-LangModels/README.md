# മുൻകൂട്ടി പരിശീലിപ്പിച്ച വലിയ ഭാഷാ മോഡലുകൾ

മുൻപ് ചെയ്ത എല്ലാ പ്രവർത്തനങ്ങളിലും, നാം ലേബൽ ചെയ്ത ഡാറ്റാസെറ്റ് ഉപയോഗിച്ച് ഒരു ന്യുറൽ നെറ്റ്‌വർക്ക് ഒരു പ്രത്യേക പ്രവർത്തനം നിർവഹിക്കാൻ പരിശീലിപ്പിച്ചിരുന്നു. BERT പോലുള്ള വലിയ ട്രാൻസ്ഫോർമർ മോഡലുകളുമായി, സ്വയം-പരിശീലന രീതിയിൽ ഭാഷാ മോഡലിംഗ് ഉപയോഗിച്ച് ഒരു ഭാഷാ മോഡൽ നിർമ്മിക്കുന്നു, പിന്നീട് അത് പ്രത്യേക ഡൊമെയ്ൻ-സ്പെസിഫിക് പരിശീലനത്തോടെ പ്രത്യേക ഡൗൺസ്ട്രീം ടാസ്കിനായി പ്രത്യേകമാക്കുന്നു. എന്നാൽ, വലിയ ഭാഷാ മോഡലുകൾ ഡൊമെയ്ൻ-സ്പെസിഫിക് പരിശീലനം ഇല്ലാതെ പല പ്രവർത്തനങ്ങളും പരിഹരിക്കാൻ കഴിവുള്ളതായും തെളിയിച്ചിട്ടുണ്ട്. അത്തരത്തിലുള്ള മോഡലുകളുടെ ഒരു കുടുംബം **GPT** എന്ന് വിളിക്കുന്നു: Generative Pre-Trained Transformer.

## [പഠനത്തിന് മുൻപ് ക്വിസ്](https://ff-quizzes.netlify.app/en/ai/quiz/39)

## ടെക്സ്റ്റ് ജനറേഷൻ ആൻഡ് പെർപ്ലെക്സിറ്റി

ഡൗൺസ്ട്രീം പരിശീലനം ഇല്ലാതെ ഒരു ന്യുറൽ നെറ്റ്‌വർക്ക് സാധാരണ പ്രവർത്തനങ്ങൾ ചെയ്യാൻ കഴിയുമെന്ന് [Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) പേപ്പറിൽ അവതരിപ്പിച്ചിരിക്കുന്നു. പ്രധാന ആശയം, പല മറ്റ് പ്രവർത്തനങ്ങളും **ടെക്സ്റ്റ് ജനറേഷൻ** ഉപയോഗിച്ച് മോഡലാക്കാൻ കഴിയുന്നതാണ്, കാരണം ടെക്സ്റ്റ് മനസ്സിലാക്കുന്നത് അതിനെ ഉത്പാദിപ്പിക്കാൻ കഴിയുന്നതായിരിക്കുകയാണ്. മോഡൽ മനുഷ്യ അറിവ് ഉൾക്കൊള്ളുന്ന വലിയ ടെക്സ്റ്റ് ഡാറ്റയിൽ പരിശീലിപ്പിക്കപ്പെട്ടതിനാൽ, ഇത് വിവിധ വിഷയങ്ങളിൽ അറിവ് നേടുകയും ചെയ്യുന്നു.

> ടെക്സ്റ്റ് മനസ്സിലാക്കുകയും ഉത്പാദിപ്പിക്കാനും കഴിയുന്നത് നമ്മുടെ ചുറ്റുപാടിലുള്ള ലോകത്തെ കുറിച്ച് എന്തെങ്കിലും അറിയുന്നതും ഉൾക്കൊള്ളുന്നു. ആളുകളും വായനയിലൂടെ കൂടുതലായി പഠിക്കുന്നു, GPT നെറ്റ്‌വർക്ക് ഇതിൽ സമാനമാണ്.

ടെക്സ്റ്റ് ജനറേഷൻ നെറ്റ്‌വർക്ക് അടുത്ത വാക്കിന്റെ സാധ്യത പ്രവചിച്ച് പ്രവർത്തിക്കുന്നു $$P(w_N)$$. എന്നാൽ, അടുത്ത വാക്കിന്റെ അനിയന്ത്രിത സാധ്യത ടെക്സ്റ്റ് കോർപ്പസിലെ ആ വാക്കിന്റെ ആവൃത്തി തുല്യമാണ്. GPT നമുക്ക് മുൻവാക്കുകൾ നൽകിയാൽ അടുത്ത വാക്കിന്റെ **ശരിതമായ സാധ്യത** നൽകാൻ കഴിയും: $$P(w_N | w_{n-1}, ..., w_0)$$

> സാധ്യതകളെക്കുറിച്ച് കൂടുതൽ വായിക്കാൻ ഞങ്ങളുടെ [ഡാറ്റാ സയൻസ് ഫോർ ബിഗിനേഴ്സ് പാഠ്യപദ്ധതി](https://github.com/microsoft/Data-Science-For-Beginners/tree/main/1-Introduction/04-stats-and-probability) കാണുക.

ഭാഷാ ജനറേറ്റിംഗ് മോഡലിന്റെ ഗുണമേന്മ **പെർപ്ലെക്സിറ്റി** ഉപയോഗിച്ച് നിർവചിക്കാം. ഇത് ഒരു ആന്തരിക മാനദണ്ഡമാണ്, പ്രത്യേക ടാസ്ക്-സ്പെസിഫിക് ഡാറ്റാസെറ്റ് ഇല്ലാതെ മോഡലിന്റെ ഗുണമേന്മ അളക്കാൻ സഹായിക്കുന്നു. ഇത് *വാക്യത്തിന്റെ സാധ്യത* എന്ന ആശയത്തെ അടിസ്ഥാനമാക്കുന്നു - മോഡൽ യാഥാർത്ഥ്യമാകാൻ സാധ്യതയുള്ള വാക്യങ്ങൾക്ക് ഉയർന്ന സാധ്യത നൽകുന്നു (അഥവാ മോഡൽ അതിൽ **പെർപ്ലെക്സ്ഡ്** അല്ല), കുറവ് അർത്ഥമുള്ള വാക്യങ്ങൾക്ക് (ഉദാ: *Can it does what?*) കുറഞ്ഞ സാധ്യത നൽകുന്നു. യഥാർത്ഥ ടെക്സ്റ്റ് കോർപ്പസിൽ നിന്നുള്ള വാക്യങ്ങൾ മോഡലിന് നൽകുമ്പോൾ, അവയ്ക്ക് ഉയർന്ന സാധ്യതയും കുറഞ്ഞ **പെർപ്ലെക്സിറ്റിയും** പ്രതീക്ഷിക്കാം. ഗണിതപരമായി, ഇത് ടെസ്റ്റ് സെറ്റിന്റെ സാധാരണവത്കൃത വിപരീത സാധ്യതയായി നിർവചിക്കുന്നു:
$$
\mathrm{Perplexity}(W) = \sqrt[N]{1\over P(W_1,...,W_N)}
$$ 

**[Hugging Face-ന്റെ GPT-ശക്തിയുള്ള ടെക്സ്റ്റ് എഡിറ്റർ](https://transformer.huggingface.co/doc/gpt2-large) ഉപയോഗിച്ച് ടെക്സ്റ്റ് ജനറേഷൻ പരീക്ഷിക്കാം**. ഈ എഡിറ്ററിൽ, നിങ്ങൾ നിങ്ങളുടെ ടെക്സ്റ്റ് എഴുതാൻ തുടങ്ങുമ്പോൾ, **[TAB]** അമർത്തുന്നത് നിങ്ങൾക്ക് പല പൂർത്തീകരണ ഓപ്ഷനുകളും നൽകും. അവ വളരെ ചെറുതാണെങ്കിൽ അല്ലെങ്കിൽ നിങ്ങൾക്ക് തൃപ്തി ഇല്ലെങ്കിൽ, വീണ്ടും [TAB] അമർത്തുക, കൂടുതൽ ഓപ്ഷനുകൾ, ഉൾപ്പെടെ കൂടുതൽ ദൈർഘ്യമുള്ള ടെക്സ്റ്റ് ലഭിക്കും.

## GPT ഒരു കുടുംബമാണ്

GPT ഒരു ഏക മോഡൽ അല്ല, മറിച്ച് [OpenAI](https://openai.com) വികസിപ്പിച്ച് പരിശീലിപ്പിച്ച മോഡലുകളുടെ ഒരു ശേഖരമാണ്.

GPT മോഡലുകൾക്കുള്ളിൽ:

| [GPT-2](https://huggingface.co/docs/transformers/model_doc/gpt2#openai-gpt2) | [GPT 3](https://openai.com/research/language-models-are-few-shot-learners) | [GPT-4](https://openai.com/gpt-4) |
| -- | -- | -- |
| 1.5 ബില്യൺ പാരാമീറ്ററുകൾ വരെ ഉള്ള ഭാഷാ മോഡൽ | 175 ബില്യൺ പാരാമീറ്ററുകൾ വരെ ഉള്ള ഭാഷാ മോഡൽ | 100 ട്രില്യൺ പാരാമീറ്ററുകൾ, ഇമേജ്, ടെക്സ്റ്റ് ഇൻപുട്ടുകളും സ്വീകരിച്ച് ടെക്സ്റ്റ് ഔട്ട്പുട്ട് നൽകുന്നു |

GPT-3, GPT-4 മോഡലുകൾ [Microsoft Azure-യുടെ കോഗ്നിറ്റീവ് സർവീസായി](https://azure.microsoft.com/en-us/services/cognitive-services/openai-service/#overview?WT.mc_id=academic-77998-cacaste) ലഭ്യമാണ്, കൂടാതെ [OpenAI API](https://openai.com/api/) ആയി ലഭ്യമാണ്.

## പ്രോംപ്റ്റ് എഞ്ചിനീയറിംഗ്

GPT വലിയ ഡാറ്റാ വോളിയങ്ങളിൽ പരിശീലിപ്പിച്ചിട്ടുള്ളതിനാൽ ഭാഷയും കോഡും മനസ്സിലാക്കുന്നു, അവ ഇൻപുട്ടുകൾ (പ്രോംപ്റ്റുകൾ) ലഭിച്ചാൽ ഔട്ട്പുട്ടുകൾ നൽകുന്നു. പ്രോംപ്റ്റുകൾ GPT-യ്ക്ക് നൽകുന്ന ഇൻപുട്ടുകളോ ക്വെറിയുകളോ ആണ്, അവ മോഡലുകൾക്ക് നിർദ്ദേശങ്ങൾ നൽകുന്നു അടുത്ത് പൂർത്തിയാക്കേണ്ട ടാസ്കുകൾക്കായി. ആഗ്രഹിക്കുന്ന ഫലം ലഭിക്കാൻ ഏറ്റവും ഫലപ്രദമായ പ്രോംപ്റ്റ് വേണം, അതിനായി ശരിയായ വാക്കുകൾ, ഫോർമാറ്റുകൾ, വാചകങ്ങൾ അല്ലെങ്കിൽ ചിഹ്നങ്ങൾ തിരഞ്ഞെടുക്കണം. ഇതാണ് [പ്രോംപ്റ്റ് എഞ്ചിനീയറിംഗ്](https://learn.microsoft.com/en-us/shows/ai-show/the-basics-of-prompt-engineering-with-azure-openai-service?WT.mc_id=academic-77998-bethanycheum).

[ഈ ഡോക്യുമെന്റേഷൻ](https://learn.microsoft.com/en-us/semantic-kernel/prompt-engineering/?WT.mc_id=academic-77998-bethanycheum) പ്രോംപ്റ്റ് എഞ്ചിനീയറിങ്ങിനെക്കുറിച്ച് കൂടുതൽ വിവരങ്ങൾ നൽകുന്നു.

## ✍️ ഉദാഹരണ നോട്ട്‌ബുക്ക്: [OpenAI-GPT-യുമായി കളിക്കുക](GPT-PyTorch.ipynb)

താഴെപ്പറയുന്ന നോട്ട്‌ബുക്കുകളിൽ നിങ്ങളുടെ പഠനം തുടരുക:

* [OpenAI-GPT, Hugging Face Transformers ഉപയോഗിച്ച് ടെക്സ്റ്റ് ജനറേറ്റ് ചെയ്യൽ](GPT-PyTorch.ipynb)

## സമാപനം

പുതിയ പൊതുവായ മുൻകൂട്ടി പരിശീലിപ്പിച്ച ഭാഷാ മോഡലുകൾ ഭാഷാ ഘടന മാത്രമല്ല, പ്രകൃതിഭാഷയുടെ വലിയ അളവ് ഉൾക്കൊള്ളുന്നു. അതിനാൽ, അവ ചില NLP ടാസ്കുകൾ സീറോ-ഷോട്ട് അല്ലെങ്കിൽ ഫ്യൂ-ഷോട്ട് സജ്ജീകരണങ്ങളിൽ ഫലപ്രദമായി പരിഹരിക്കാൻ കഴിയും.

## [പഠനത്തിന് ശേഷം ക്വിസ്](https://ff-quizzes.netlify.app/en/ai/quiz/40)

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**അസൂയാ**:  
ഈ രേഖ AI വിവർത്തന സേവനം [Co-op Translator](https://github.com/Azure/co-op-translator) ഉപയോഗിച്ച് വിവർത്തനം ചെയ്തതാണ്. നാം കൃത്യതയ്ക്ക് ശ്രമിച്ചെങ്കിലും, സ്വയം പ്രവർത്തിക്കുന്ന വിവർത്തനങ്ങളിൽ പിശകുകൾ അല്ലെങ്കിൽ തെറ്റുകൾ ഉണ്ടാകാമെന്ന് ദയവായി ശ്രദ്ധിക്കുക. അതിന്റെ മാതൃഭാഷയിലുള്ള യഥാർത്ഥ രേഖയാണ് പ്രാമാണികമായ ഉറവിടം എന്ന് പരിഗണിക്കേണ്ടതാണ്. നിർണായകമായ വിവരങ്ങൾക്ക്, പ്രൊഫഷണൽ മനുഷ്യ വിവർത്തനം ശുപാർശ ചെയ്യപ്പെടുന്നു. ഈ വിവർത്തനം ഉപയോഗിക്കുന്നതിൽ നിന്നുണ്ടാകുന്ന ഏതെങ്കിലും തെറ്റിദ്ധാരണകൾക്കോ വ്യാഖ്യാനക്കേടുകൾക്കോ ഞങ്ങൾ ഉത്തരവാദികളല്ല.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->