{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# കാർട്ട്പോൾ ബാലൻസിംഗ് ചെയ്യാൻ RL പരിശീലനം\n",
    "\n",
    "ഈ നോട്ട്‌ബുക്ക് [AI for Beginners Curriculum](http://aka.ms/ai-beginners) എന്ന കോഴ്സിന്റെ ഭാഗമാണ്. ഇത് [അധികൃത PyTorch ട്യൂട്ടോറിയൽ](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html)നും [ഈ കാർട്ട്പോൾ PyTorch നടപ്പാക്കലിനും](https://github.com/yc930401/Actor-Critic-pytorch) പ്രചോദനം നൽകിയതാണ്.\n",
    "\n",
    "ഈ ഉദാഹരണത്തിൽ, ഒരു മോഡൽ ഒരു കാർട്ടിൽ മുകളിലായി നില്ക്കുന്ന പോളിനെ ഇടത്തും വലത്തും നീങ്ങുന്ന ഹോരിസോണ്ടൽ സ്കെയിലിൽ ബാലൻസ് ചെയ്യാൻ RL ഉപയോഗിച്ച് പരിശീലിപ്പിക്കും. പോളിനെ സിമുലേറ്റ് ചെയ്യാൻ [OpenAI Gym](https://www.gymlibrary.ml/) പരിസ്ഥിതി ഉപയോഗിക്കും.\n",
    "\n",
    "> **Note**: ഈ പാഠത്തിന്റെ കോഡ് നിങ്ങൾക്ക് ലോക്കലായി (ഉദാ. Visual Studio Code-ൽ) ഓടിക്കാം, അപ്പോൾ സിമുലേഷൻ പുതിയ വിൻഡോയിൽ തുറക്കും. ഓൺലൈനിൽ കോഡ് ഓടിക്കുമ്പോൾ, [ഇവിടെ](https://towardsdatascience.com/rendering-openai-gym-envs-on-binder-and-google-colab-536f99391cc7) വിവരിച്ചിരിക്കുന്നതുപോലെ ചില മാറ്റങ്ങൾ വരുത്തേണ്ടി വരാം.\n",
    "\n",
    "ആദ്യം Gym ഇൻസ്റ്റാൾ ചെയ്തിട്ടുണ്ടെന്ന് ഉറപ്പാക്കാം:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ഇപ്പോൾ നമുക്ക് CartPole പരിസ്ഥിതി സൃഷ്ടിച്ച് അതിൽ എങ്ങനെ പ്രവർത്തിക്കാമെന്ന് നോക്കാം. ഒരു പരിസ്ഥിതിക്ക് താഴെപ്പറയുന്ന ഗുണങ്ങൾ ഉണ്ടാകും:\n",
    "\n",
    "* **Action space** എന്നത് സിമുലേഷന്റെ ഓരോ ഘട്ടത്തിലും നാം ചെയ്യാൻ കഴിയുന്ന പ്രവർത്തനങ്ങളുടെ സമാഹാരമാണ്\n",
    "* **Observation space** എന്നത് നാം നടത്താൻ കഴിയുന്ന നിരീക്ഷണങ്ങളുടെ സ്ഥലം ആണ്\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "print(f\"Action space: {env.action_space}\")\n",
    "print(f\"Observation space: {env.observation_space}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "സിമുലേഷൻ എങ്ങനെ പ്രവർത്തിക്കുന്നു എന്ന് നോക്കാം. താഴെയുള്ള ലൂപ്പ് സിമുലേഷൻ നടത്തുന്നു, `env.step` അവസാനിപ്പിക്കൽ ഫ്ലാഗ് `done` തിരികെ നൽകുന്നത് വരെ. നാം ക്രമരഹിതമായി പ്രവർത്തനങ്ങൾ തിരഞ്ഞെടുക്കും `env.action_space.sample()` ഉപയോഗിച്ച്, അതായത് പരീക്ഷണം വളരെ വേഗം പരാജയപ്പെടാൻ സാധ്യതയുണ്ട് (CartPole പരിസ്ഥിതി CartPole-യുടെ വേഗത, സ്ഥാനം അല്ലെങ്കിൽ കോണം ചില പരിധികൾക്കു പുറത്തായാൽ അവസാനിക്കും).\n",
    "\n",
    "> സിമുലേഷൻ പുതിയ വിൻഡോയിൽ തുറക്കും. നിങ്ങൾക്ക് കോഡ് പലതവണ ഓടിച്ച് അത് എങ്ങനെ പ്രവർത്തിക്കുന്നു എന്ന് കാണാം.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "\n",
    "done = False\n",
    "total_reward = 0\n",
    "while not done:\n",
    "   env.render()\n",
    "   obs, rew, done, info = env.step(env.action_space.sample())\n",
    "   total_reward += rew\n",
    "   print(f\"{obs} -> {rew}\")\n",
    "print(f\"Total reward: {total_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "നിങ്ങൾ ശ്രദ്ധിക്കാം, നിരീക്ഷണങ്ങളിൽ 4 സംഖ്യകൾ അടങ്ങിയിരിക്കുന്നു. അവയാണ്:\n",
    "- കാർട്ടിന്റെ സ്ഥാനം\n",
    "- കാർട്ടിന്റെ വേഗത\n",
    "- പോളിന്റെ കോണം\n",
    "- പോളിന്റെ തിരിവിന്റെ വേഗത\n",
    "\n",
    "`rew` എന്നത് ഓരോ ഘട്ടത്തിലും ലഭിക്കുന്ന റിവാർഡാണ്. CartPole പരിസ്ഥിതിയിൽ ഓരോ സിമുലേഷൻ ഘട്ടത്തിനും നിങ്ങൾക്ക് 1 പോയിന്റ് ലഭിക്കുന്നുവെന്ന് കാണാം, ലക്ഷ്യം ആകെ റിവാർഡ് പരമാവധി ആക്കുകയാണ്, അതായത് CartPole വീഴാതെ തുല്യസ്ഥിതിയിൽ നിലനിൽക്കാൻ കഴിയുന്ന സമയം.\n",
    "\n",
    "റീൻഫോഴ്‌സ്‌മെന്റ് ലേണിങ്ങിന്റെ സമയത്ത്, നമ്മുടെ ലക്ഷ്യം ഒരു **പോളിസി** $\\pi$ പരിശീലിപ്പിക്കുകയാണ്, അത് ഓരോ സ്റ്റേറ്റ് $s$-ക്കും എടുക്കേണ്ട ആക്ഷൻ $a$ പറയുന്നവയാണ്, അതായത് $a = \\pi(s)$.\n",
    "\n",
    "നിങ്ങൾ പ്രൊബബിലിസ്റ്റിക് പരിഹാരമുണ്ടാക്കാൻ ആഗ്രഹിക്കുന്നുവെങ്കിൽ, ഓരോ ആക്ഷനിനും ഒരു പ്രൊബബിലിറ്റി സെറ്റ് നൽകുന്ന രീതിയിലാണ് പോളിസിയെ കാണുന്നത്, അതായത് $\\pi(a|s)$ എന്നത് സ്റ്റേറ്റ് $s$-ൽ ആക്ഷൻ $a$ എടുക്കേണ്ടതിന്റെ സാധ്യതയെ സൂചിപ്പിക്കും.\n",
    "\n",
    "## പോളിസി ഗ്രേഡിയന്റ് മെത്തഡ്\n",
    "\n",
    "ഏറ്റവും ലളിതമായ RL ആൽഗോരിതമായ **പോളിസി ഗ്രേഡിയന്റ്**-ൽ, അടുത്ത ആക്ഷൻ പ്രവചിക്കാൻ ഒരു ന്യൂറൽ നെറ്റ്‌വർക്ക് പരിശീലിപ്പിക്കും.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "num_inputs = 4\n",
    "num_actions = 2\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(num_inputs, 128, bias=False, dtype=torch.float32),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(128, num_actions, bias = False, dtype=torch.float32),\n",
    "    torch.nn.Softmax(dim=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "നാം നിരവധി പരീക്ഷണങ്ങൾ നടത്തിക്കൊണ്ട് നെറ്റ്‌വർക്ക് പരിശീലിപ്പിക്കും, ഓരോ റൺ കഴിഞ്ഞ് നമ്മുടെ നെറ്റ്‌വർക്ക് അപ്ഡേറ്റ് ചെയ്യും. പരീക്ഷണം നടത്തുകയും ഫലങ്ങൾ (അഥവാ **ട്രേസ്**) തിരികെ നൽകുകയും ചെയ്യുന്ന ഒരു ഫംഗ്ഷൻ നിർവചിക്കാം - എല്ലാ സ്റ്റേറ്റുകളും, ആക്ഷനുകളും (അവയുടെ ശുപാർശ ചെയ്ത സാധ്യതകളും), റിവാർഡുകളും:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(max_steps_per_episode = 10000,render=False):    \n",
    "    states, actions, probs, rewards = [],[],[],[]\n",
    "    state = env.reset()\n",
    "    for _ in range(max_steps_per_episode):\n",
    "        if render:\n",
    "            env.render()\n",
    "        action_probs = model(torch.from_numpy(np.expand_dims(state,0)))[0]\n",
    "        action = np.random.choice(num_actions, p=np.squeeze(action_probs.detach().numpy()))\n",
    "        nstate, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            break\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        probs.append(action_probs.detach().numpy())\n",
    "        rewards.append(reward)\n",
    "        state = nstate\n",
    "    return np.vstack(states), np.vstack(actions), np.vstack(probs), np.vstack(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "നിങ്ങൾ ഒരു എപ്പിസോഡ് പരിശീലനമില്ലാത്ത നെറ്റ്‌വർക്കുമായി ഓടിച്ച് ആകെ റിവാർഡ് (അഥവാ എപ്പിസോഡിന്റെ ദൈർഘ്യം) വളരെ കുറവാണെന്ന് കാണാം:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s, a, p, r = run_episode()\n",
    "print(f\"Total reward: {np.sum(r)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "പോളിസി ഗ്രേഡിയന്റ് ആൽഗോറിതത്തിന്റെ ഒരു സങ്കീർണ്ണമായ ഭാഗം **ഡിസ്‌കൗണ്ടഡ് റിവാർഡുകൾ** ഉപയോഗിക്കുകയാണ്. കളിയുടെ ഓരോ ഘട്ടത്തിലും മൊത്തം റിവാർഡുകളുടെ വെക്ടർ കണക്കാക്കുകയാണ് ആശയം, ഈ പ്രക്രിയയിൽ ആദ്യം ലഭിക്കുന്ന റിവാർഡുകൾ ഒരു കോഫിഷ്യന്റ് $gamma$ ഉപയോഗിച്ച് ഡിസ്‌കൗണ്ട് ചെയ്യുന്നു. നമുക്ക് ഇത് പരിശീലനത്തെ ബാധിക്കുന്ന ഭാരമായി ഉപയോഗിക്കേണ്ടതിനാൽ, ലഭിച്ച വെക്ടർ നോർമലൈസ് ചെയ്യുകയും ചെയ്യുന്നു:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 0.0001\n",
    "\n",
    "def discounted_rewards(rewards,gamma=0.99,normalize=True):\n",
    "    ret = []\n",
    "    s = 0\n",
    "    for r in rewards[::-1]:\n",
    "        s = r + gamma * s\n",
    "        ret.insert(0, s)\n",
    "    if normalize:\n",
    "        ret = (ret-np.mean(ret))/(np.std(ret)+eps)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ഇപ്പോൾ നമുക്ക് യഥാർത്ഥ പരിശീലനം തുടങ്ങാം! നാം 300 എപ്പിസോഡുകൾ നടത്തും, ഓരോ എപ്പിസോഡിലും നാം താഴെ പറയുന്ന കാര്യങ്ങൾ ചെയ്യും:\n",
    "\n",
    "1. പരീക്ഷണം നടത്തുകയും ട്രേസ് ശേഖരിക്കുകയും ചെയ്യുക  \n",
    "2. എടുത്ത പ്രവർത്തനങ്ങളും പ്രവചിച്ച സാധ്യതകളും തമ്മിലുള്ള വ്യത്യാസം (`gradients`) കണക്കാക്കുക. വ്യത്യാസം കുറവായിരിക്കും എങ്കിൽ, നാം ശരിയായ പ്രവർത്തനം എടുത്തതായി കൂടുതൽ ഉറപ്പുണ്ടാകും.  \n",
    "3. ഡിസ്കൗണ്ടഡ് റിവാർഡുകൾ കണക്കാക്കി, ഗ്രാഡിയന്റുകൾക്ക് ഡിസ്കൗണ്ടഡ് റിവാർഡുകൾ ഗുണിക്കുക - ഇതിലൂടെ ഉയർന്ന റിവാർഡ് ലഭിച്ച ഘട്ടങ്ങൾ താഴ്ന്ന റിവാർഡ് ലഭിച്ച ഘട്ടങ്ങളെക്കാൾ അന്തിമ ഫലത്തിൽ കൂടുതൽ സ്വാധീനം ചെലുത്തും.  \n",
    "4. നമ്മുടെ ന്യൂറൽ നെറ്റ്‌വർക്കിന്റെ പ്രതീക്ഷിച്ച ലക്ഷ്യ പ്രവർത്തനങ്ങൾ partly റൺ സമയത്ത് പ്രവചിച്ച സാധ്യതകളിൽ നിന്നും partly കണക്കാക്കിയ ഗ്രാഡിയന്റുകളിൽ നിന്നുമാകും. ഗ്രാഡിയന്റുകളും റിവാർഡുകളും എത്രമാത്രം പരിഗണിക്കണമെന്ന് നിശ്ചയിക്കാൻ `alpha` പാരാമീറ്റർ ഉപയോഗിക്കും - ഇതാണ് റീ ഇൻഫോഴ്‌സ്‌മെന്റ് ആൽഗോരിതത്തിന്റെ *learning rate*.  \n",
    "5. അവസാനം, നാം സ്റ്റേറ്റുകളും പ്രതീക്ഷിച്ച പ്രവർത്തനങ്ങളും ഉപയോഗിച്ച് നെറ്റ്‌വർക്ക് പരിശീലിപ്പിച്ച്, ഈ പ്രക്രിയ ആവർത്തിക്കും.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "def train_on_batch(x, y):\n",
    "    x = torch.from_numpy(x)\n",
    "    y = torch.from_numpy(y)\n",
    "    optimizer.zero_grad()\n",
    "    predictions = model(x)\n",
    "    loss = -torch.mean(torch.log(predictions) * y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1e-4\n",
    "\n",
    "history = []\n",
    "for epoch in range(300):\n",
    "    states, actions, probs, rewards = run_episode()\n",
    "    one_hot_actions = np.eye(2)[actions.T][0]\n",
    "    gradients = one_hot_actions-probs\n",
    "    dr = discounted_rewards(rewards)\n",
    "    gradients *= dr\n",
    "    target = alpha*np.vstack([gradients])+probs\n",
    "    train_on_batch(states,target)\n",
    "    history.append(np.sum(rewards))\n",
    "    if epoch%100==0:\n",
    "        print(f\"{epoch} -> {np.sum(rewards)}\")\n",
    "\n",
    "plt.plot(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ഇപ്പോൾ ഫലങ്ങൾ കാണാൻ റെൻഡറിംഗ് സഹിതം എപ്പിസോഡ് ഓടിക്കാം:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = run_episode(render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "നമ്മൾ ഇപ്പോൾ കാണാൻ കഴിയുന്നത് പോൾ വളരെ നന്നായി ബാലൻസ് ചെയ്യാൻ കഴിഞ്ഞു എന്നതാണ്!\n",
    "\n",
    "## ആക്ടർ-ക്രിട്ടിക് മോഡൽ\n",
    "\n",
    "ആക്ടർ-ക്രിട്ടിക് മോഡൽ പോളിസി ഗ്രേഡിയന്റുകളുടെ കൂടുതൽ വികസനമാണ്, ഇതിൽ നാം ഒരു ന്യൂറൽ നെറ്റ്‌വർക്ക് നിർമ്മിച്ച് പോളിസിയും അളവിട്ട റിവാർഡുകളും പഠിപ്പിക്കുന്നു. നെറ്റ്‌വർക്ക് രണ്ട് ഔട്ട്പുട്ടുകൾ ഉണ്ടാകും (അല്ലെങ്കിൽ ഇത് രണ്ട് വേർതിരിച്ച നെറ്റ്‌വർക്കുകളായി കാണാം):\n",
    "* **ആക്ടർ** പോളിസി ഗ്രേഡിയന്റ് മോഡലിൽപോലെ സ്റ്റേറ്റ് പ്രൊബബിലിറ്റി ഡിസ്‌ട്രിബ്യൂഷൻ നൽകിയാണ് എടുക്കേണ്ട ആക്ഷൻ നിർദ്ദേശിക്കുന്നത്\n",
    "* **ക്രിട്ടിക്** ആ ആക്ഷനുകളിൽ നിന്നുണ്ടാകുന്ന റിവാർഡ് എത്രയാകും എന്ന് അളക്കുന്നു. നൽകിയ സ്റ്റേറ്റിൽ ഭാവിയിൽ ലഭിക്കാവുന്ന മൊത്തം അളവിട്ട റിവാർഡുകൾ ഇത് തിരിച്ചുകൊടുക്കും.\n",
    "\n",
    "ഇത്തരമൊരു മോഡൽ നിർവചിക്കാം:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import count\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "lr = 0.0001\n",
    "\n",
    "class Actor(torch.nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Actor, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.linear1 = torch.nn.Linear(self.state_size, 128)\n",
    "        self.linear2 = torch.nn.Linear(128, 256)\n",
    "        self.linear3 = torch.nn.Linear(256, self.action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        output = F.relu(self.linear1(state))\n",
    "        output = F.relu(self.linear2(output))\n",
    "        output = self.linear3(output)\n",
    "        distribution = torch.distributions.Categorical(F.softmax(output, dim=-1))\n",
    "        return distribution\n",
    "\n",
    "\n",
    "class Critic(torch.nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Critic, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.linear1 = torch.nn.Linear(self.state_size, 128)\n",
    "        self.linear2 = torch.nn.Linear(128, 256)\n",
    "        self.linear3 = torch.nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        output = F.relu(self.linear1(state))\n",
    "        output = F.relu(self.linear2(output))\n",
    "        value = self.linear3(output)\n",
    "        return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "നമുക്ക് നമ്മുടെ `discounted_rewards` மற்றும் `run_episode` ഫംഗ്ഷനുകൾ ചെറിയ മാറ്റം വരുത്തേണ്ടതുണ്ട്:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discounted_rewards(next_value, rewards, masks, gamma=0.99):\n",
    "    R = next_value\n",
    "    returns = []\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        R = rewards[step] + gamma * R * masks[step]\n",
    "        returns.insert(0, R)\n",
    "    return returns\n",
    "\n",
    "def run_episode(actor, critic, n_iters):\n",
    "    optimizerA = torch.optim.Adam(actor.parameters())\n",
    "    optimizerC = torch.optim.Adam(critic.parameters())\n",
    "    for iter in range(n_iters):\n",
    "        state = env.reset()\n",
    "        log_probs = []\n",
    "        values = []\n",
    "        rewards = []\n",
    "        masks = []\n",
    "        entropy = 0\n",
    "        env.reset()\n",
    "\n",
    "        for i in count():\n",
    "            env.render()\n",
    "            state = torch.FloatTensor(state).to(device)\n",
    "            dist, value = actor(state), critic(state)\n",
    "\n",
    "            action = dist.sample()\n",
    "            next_state, reward, done, _ = env.step(action.cpu().numpy())\n",
    "\n",
    "            log_prob = dist.log_prob(action).unsqueeze(0)\n",
    "            entropy += dist.entropy().mean()\n",
    "\n",
    "            log_probs.append(log_prob)\n",
    "            values.append(value)\n",
    "            rewards.append(torch.tensor([reward], dtype=torch.float, device=device))\n",
    "            masks.append(torch.tensor([1-done], dtype=torch.float, device=device))\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                print('Iteration: {}, Score: {}'.format(iter, i))\n",
    "                break\n",
    "\n",
    "\n",
    "        next_state = torch.FloatTensor(next_state).to(device)\n",
    "        next_value = critic(next_state)\n",
    "        returns = discounted_rewards(next_value, rewards, masks)\n",
    "\n",
    "        log_probs = torch.cat(log_probs)\n",
    "        returns = torch.cat(returns).detach()\n",
    "        values = torch.cat(values)\n",
    "\n",
    "        advantage = returns - values\n",
    "\n",
    "        actor_loss = -(log_probs * advantage.detach()).mean()\n",
    "        critic_loss = advantage.pow(2).mean()\n",
    "\n",
    "        optimizerA.zero_grad()\n",
    "        optimizerC.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        critic_loss.backward()\n",
    "        optimizerA.step()\n",
    "        optimizerC.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ഇപ്പോൾ നാം പ്രധാന പരിശീലന ലൂപ്പ് നടത്തും. ശരിയായ നഷ്ട ഫംഗ്ഷനുകൾ കണക്കാക്കി നെറ്റ്‌വർക്ക് പാരാമീറ്ററുകൾ അപ്ഡേറ്റ് ചെയ്ത് മാനുവൽ നെറ്റ്‌വർക്ക് പരിശീലന പ്രക്രിയ ഉപയോഗിക്കും:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "actor = Actor(state_size, action_size).to(device)\n",
    "critic = Critic(state_size, action_size).to(device)\n",
    "run_episode(actor, critic, n_iters=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "അവസാനമായി, നമുക്ക് പരിസ്ഥിതിയെ അടയ്ക്കാം.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## പ്രധാനപ്പെട്ട കാര്യങ്ങൾ\n",
    "\n",
    "ഈ ഡെമോയിൽ നാം രണ്ട് RL ആൽഗോരിതങ്ങൾ കണ്ടു: ലളിതമായ പോളിസി ഗ്രേഡിയന്റ്, കൂടാതെ കൂടുതൽ സങ്കീർണ്ണമായ ആക്ടർ-ക്രിട്ടിക്. ആൽഗോരിതങ്ങൾ സ്റ്റേറ്റ്, ആക്ഷൻ, റിവാർഡ് എന്ന аб്സ്ട്രാക്റ്റ് ആശയങ്ങളുമായി പ്രവർത്തിക്കുന്നതായതിനാൽ, അവ വളരെ വ്യത്യസ്തമായ പരിസ്ഥിതികളിൽ പ്രയോഗിക്കാവുന്നതാണ്.\n",
    "\n",
    "റീ ഇൻഫോഴ്‌സ്‌മെന്റ് ലേണിംഗ് നമ്മെ പ്രശ്നം പരിഹരിക്കാൻ ഏറ്റവും നല്ല തന്ത്രം പഠിക്കാനാകും, അവസാന റിവാർഡ് നോക്കിയാൽ മതി. ലേബൽ ചെയ്ത ഡാറ്റാസെറ്റുകൾ ആവശ്യമില്ലാത്തതുകൊണ്ട്, നമ്മുടെ മോഡലുകൾ മെച്ചപ്പെടുത്താൻ സിമുലേഷനുകൾ പലതവണ ആവർത്തിക്കാം. എങ്കിലും, RL-യിൽ ഇപ്പോഴും നിരവധി വെല്ലുവിളികൾ ഉണ്ട്, ഈ രസകരമായ AI മേഖലയിലേക്ക് കൂടുതൽ ശ്രദ്ധ കേന്ദ്രീകരിക്കാൻ നിങ്ങൾ തീരുമാനിച്ചാൽ അവയെക്കുറിച്ച് പഠിക്കാം.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n<!-- CO-OP TRANSLATOR DISCLAIMER START -->\n**അസൂയാ**:  \nഈ രേഖ AI വിവർത്തന സേവനം [Co-op Translator](https://github.com/Azure/co-op-translator) ഉപയോഗിച്ച് വിവർത്തനം ചെയ്തതാണ്. നാം കൃത്യതയ്ക്ക് ശ്രമിച്ചെങ്കിലും, സ്വയം പ്രവർത്തിക്കുന്ന വിവർത്തനങ്ങളിൽ പിശകുകൾ അല്ലെങ്കിൽ തെറ്റുകൾ ഉണ്ടാകാമെന്ന് ദയവായി ശ്രദ്ധിക്കുക. അതിന്റെ മാതൃഭാഷയിലുള്ള യഥാർത്ഥ രേഖ അധികാരപരമായ ഉറവിടമായി കണക്കാക്കണം. നിർണായക വിവരങ്ങൾക്ക്, പ്രൊഫഷണൽ മനുഷ്യ വിവർത്തനം ശുപാർശ ചെയ്യപ്പെടുന്നു. ഈ വിവർത്തനത്തിന്റെ ഉപയോഗത്തിൽ നിന്നുണ്ടാകുന്ന ഏതെങ്കിലും തെറ്റിദ്ധാരണകൾക്കോ തെറ്റായ വ്യാഖ്യാനങ്ങൾക്കോ ഞങ്ങൾ ഉത്തരവാദികളല്ല.\n<!-- CO-OP TRANSLATOR DISCLAIMER END -->\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  },
  "coopTranslator": {
   "original_hash": "04f8d9978cd11281d81dd037cbf6ce20",
   "translation_date": "2025-11-26T02:40:12+00:00",
   "source_file": "lessons/6-Other/22-DeepRL/CartPole-RL-PyTorch.ipynb",
   "language_code": "ml"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}