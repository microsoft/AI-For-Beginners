# Предварително обучени големи езикови модели

Във всички предишни задачи обучавахме невронна мрежа да изпълнява определена задача, използвайки етикетиран набор от данни. С големите трансформаторни модели, като BERT, използваме езиково моделиране в самонаблюдаващ се режим, за да изградим езиков модел, който след това се специализира за конкретна задача чрез допълнително обучение в специфична област. Въпреки това е доказано, че големите езикови модели могат да решават много задачи без НИКАКВО обучение в специфична област. Семейството от модели, способни на това, се нарича **GPT**: Генеративен предварително обучен трансформатор.

## [Тест преди лекцията](https://ff-quizzes.netlify.app/en/ai/quiz/39)

## Генериране на текст и перплексност

Идеята за невронна мрежа, която може да изпълнява общи задачи без допълнително обучение, е представена в статията [Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf). Основната идея е, че много други задачи могат да бъдат моделирани чрез **генериране на текст**, защото разбирането на текста по същество означава способност за неговото създаване. Тъй като моделът е обучен върху огромно количество текст, който обхваща човешкото знание, той става компетентен в широк спектър от теми.

> Разбирането и способността за създаване на текст включва и познания за света около нас. Хората също учат до голяма степен чрез четене, и мрежата GPT е подобна в това отношение.

Мрежите за генериране на текст работят чрез предсказване на вероятността за следващата дума $$P(w_N)$$. Въпреки това, безусловната вероятност за следващата дума е равна на честотата на тази дума в текстовия корпус. GPT може да ни предостави **условна вероятност** за следващата дума, като се вземат предвид предишните: $$P(w_N | w_{n-1}, ..., w_0)$$.

> Можете да научите повече за вероятностите в нашата [Учебна програма за начинаещи в науката за данни](https://github.com/microsoft/Data-Science-For-Beginners/tree/main/1-Introduction/04-stats-and-probability).

Качеството на модела за генериране на текст може да се определи чрез **перплексност**. Това е вътрешна метрика, която ни позволява да измерим качеството на модела без използване на специфичен набор от данни за задачата. Тя се основава на понятието за *вероятност на изречение* - моделът присвоява висока вероятност на изречение, което е вероятно да бъде реално (т.е. моделът не е **объркан** от него), и ниска вероятност на изречения, които имат по-малко смисъл (например *Може ли това да направи какво?*). Когато предоставим на модела изречения от реален текстов корпус, очакваме те да имат висока вероятност и ниска **перплексност**. Математически, тя се дефинира като нормализирана обратна вероятност на тестовия набор:
$$
\mathrm{Perplexity}(W) = \sqrt[N]{1\over P(W_1,...,W_N)}
$$ 

**Можете да експериментирате с генериране на текст, използвайки [GPT-базиран текстов редактор от Hugging Face](https://transformer.huggingface.co/doc/gpt2-large)**. В този редактор започвате да пишете текст, и натискането на **[TAB]** ще ви предложи няколко опции за завършване. Ако те са твърде кратки или не сте доволни от тях - натиснете [TAB] отново, и ще получите повече опции, включително по-дълги текстове.

## GPT е семейство

GPT не е един модел, а по-скоро колекция от модели, разработени и обучени от [OpenAI](https://openai.com).

Под моделите GPT имаме:

| [GPT-2](https://huggingface.co/docs/transformers/model_doc/gpt2#openai-gpt2) | [GPT 3](https://openai.com/research/language-models-are-few-shot-learners) | [GPT-4](https://openai.com/gpt-4) |
| -- | -- | -- |
|Езиков модел с до 1.5 милиарда параметри. | Езиков модел с до 175 милиарда параметри | 100T параметри и приема както изображения, така и текстови входове, а изходът е текст. |

Моделите GPT-3 и GPT-4 са достъпни [като когнитивна услуга от Microsoft Azure](https://azure.microsoft.com/en-us/services/cognitive-services/openai-service/#overview?WT.mc_id=academic-77998-cacaste) и като [OpenAI API](https://openai.com/api/).

## Инженеринг на подсказки

Тъй като GPT е обучен върху огромни обеми данни за разбиране на език и код, той предоставя изходи в отговор на входове (подсказки). Подсказките са входове или заявки към GPT, при които се предоставят инструкции на моделите за задачите, които трябва да бъдат изпълнени. За да получите желания резултат, трябва най-ефективната подсказка, която включва избор на правилните думи, формати, фрази или дори символи. Този подход се нарича [Инженеринг на подсказки](https://learn.microsoft.com/en-us/shows/ai-show/the-basics-of-prompt-engineering-with-azure-openai-service?WT.mc_id=academic-77998-bethanycheum).

[Тази документация](https://learn.microsoft.com/en-us/semantic-kernel/prompt-engineering/?WT.mc_id=academic-77998-bethanycheum) предоставя повече информация за инженеринг на подсказки.

## ✍️ Примерен тетрадка: [Игра с OpenAI-GPT](GPT-PyTorch.ipynb)

Продължете обучението си с следните тетрадки:

* [Генериране на текст с OpenAI-GPT и Hugging Face Transformers](GPT-PyTorch.ipynb)

## Заключение

Новите общи предварително обучени езикови модели не само моделират езиковата структура, но и съдържат огромно количество естествен език. Следователно, те могат ефективно да се използват за решаване на някои задачи в обработката на естествен език в условия на нулеви или малко примери.

## [Тест след лекцията](https://ff-quizzes.netlify.app/en/ai/quiz/40)

---

