{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вграждания\n",
    "\n",
    "В предишния пример работихме с високодименсионални вектори на чанта с думи с дължина `vocab_size`, и изрично преобразувахме нискодименсионалните вектори на позиционно представяне в редки едноточкови представяния. Това едноточково представяне не е ефективно по отношение на паметта. Освен това, всяка дума се третира независимо от другите, така че едноточково кодираните вектори не изразяват семантични прилики между думите.\n",
    "\n",
    "В този модул ще продължим да изследваме набора от данни **News AG**. За начало, нека заредим данните и вземем някои дефиниции от предишния модул.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "\n",
    "ds_train, ds_test = tfds.load('ag_news_subset').values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Какво е embedding?\n",
    "\n",
    "Идеята на **embedding** е да представим думите чрез нискоразмерни плътни вектори, които отразяват семантичното значение на думата. По-късно ще обсъдим как да създадем смислени word embeddings, но засега нека просто мислим за embedding като начин за намаляване на размерността на вектор на дума.\n",
    "\n",
    "Така че, embedding слой приема дума като вход и произвежда изходен вектор със зададен размер `embedding_size`. В известен смисъл, той е много подобен на слой `Dense`, но вместо да приема вектор с one-hot кодиране като вход, той може да приема номер на дума.\n",
    "\n",
    "Като използваме embedding слой като първи слой в нашата мрежа, можем да преминем от модел bag-of-words към модел **embedding bag**, където първо преобразуваме всяка дума в текста в съответния embedding, а след това изчисляваме някаква агрегираща функция върху всички тези embeddings, като например `sum`, `average` или `max`.\n",
    "\n",
    "![Изображение, показващо embedding класификатор за пет последователни думи.](../../../../../translated_images/bg/embedding-classifier-example.b77f021a7ee67eee.webp)\n",
    "\n",
    "Нашата невронна мрежа за класификация се състои от следните слоеве:\n",
    "\n",
    "* Слой `TextVectorization`, който приема низ като вход и произвежда тензор от номера на токени. Ще зададем разумен размер на речника `vocab_size` и ще игнорираме по-рядко използваните думи. Формата на входа ще бъде 1, а формата на изхода ще бъде $n$, тъй като ще получим $n$ токени като резултат, всеки от тях съдържащ числа от 0 до `vocab_size`.\n",
    "* Слой `Embedding`, който приема $n$ числа и намалява всяко число до плътен вектор с дадена дължина (100 в нашия пример). Така входният тензор с форма $n$ ще бъде трансформиран в тензор с форма $n\\times 100$.\n",
    "* Агрегиращ слой, който взема средната стойност на този тензор по първата ос, т.е. ще изчисли средната стойност на всички $n$ входни тензори, съответстващи на различни думи. За да реализираме този слой, ще използваме слой `Lambda` и ще му предадем функцията за изчисляване на средната стойност. Изходът ще има форма 100 и ще бъде численото представяне на цялата входна последователност.\n",
    "* Финален `Dense` линеен класификатор.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization (TextVec  (None, None)             0         \n",
      " torization)                                                     \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, None, 100)         3000000   \n",
      "                                                                 \n",
      " lambda (Lambda)             (None, 100)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4)                 404       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,000,404\n",
      "Trainable params: 3,000,404\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 30000\n",
    "batch_size = 128\n",
    "\n",
    "vectorizer = keras.layers.experimental.preprocessing.TextVectorization(max_tokens=vocab_size,input_shape=(1,))\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    vectorizer,    \n",
    "    keras.layers.Embedding(vocab_size,100),\n",
    "    keras.layers.Lambda(lambda x: tf.reduce_mean(x,axis=1)),\n",
    "    keras.layers.Dense(4, activation='softmax')\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В `summary` таблицата, в колоната **output shape**, първото измерение на тензора `None` съответства на размера на минипартидата, а второто - на дължината на последователността от токени. Всички последователности от токени в минипартидата имат различни дължини. Ще обсъдим как да се справим с това в следващия раздел.\n",
    "\n",
    "Сега нека обучим мрежата:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training vectorizer\n",
      "938/938 [==============================] - 20s 20ms/step - loss: 0.7891 - acc: 0.8155 - val_loss: 0.4470 - val_acc: 0.8642\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22255515100>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_text(x):\n",
    "    return x['title']+' '+x['description']\n",
    "\n",
    "def tupelize(x):\n",
    "    return (extract_text(x),x['label'])\n",
    "\n",
    "print(\"Training vectorizer\")\n",
    "vectorizer.adapt(ds_train.take(500).map(extract_text))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'])\n",
    "model.fit(ds_train.map(tupelize).batch(batch_size),validation_data=ds_test.map(tupelize).batch(batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "> **Забележка**: изграждаме векторизатор, базиран на подмножество от данните. Това се прави с цел ускоряване на процеса и може да доведе до ситуация, в която не всички токени от нашия текст присъстват в речника. В този случай тези токени ще бъдат игнорирани, което може да доведе до леко по-ниска точност. Въпреки това, в реалния живот подмножество от текста често дава добра оценка на речника.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Работа с променливи размери на последователности\n",
    "\n",
    "Нека разберем как се извършва обучението в минипартиди. В горния пример входният тензор има размерност 1 и използваме минипартиди с дължина 128, така че реалният размер на тензора е $128 \\times 1$. Въпреки това, броят на токените във всяко изречение е различен. Ако приложим слоя `TextVectorization` към един вход, броят на върнатите токени ще бъде различен в зависимост от начина, по който текстът е токенизиран:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 1 45], shape=(2,), dtype=int64)\n",
      "tf.Tensor([ 112 1271    1    3 1747  158], shape=(6,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer('Hello, world!'))\n",
    "print(vectorizer('I am glad to meet you!'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Въпреки това, когато приложим векторизатора към няколко последователности, той трябва да произведе тензор с правоъгълна форма, затова запълва неизползваните елементи с токена PAD (който в нашия случай е нула):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 6), dtype=int64, numpy=\n",
       "array([[   1,   45,    0,    0,    0,    0],\n",
       "       [ 112, 1271,    1,    3, 1747,  158]], dtype=int64)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer(['Hello, world!','I am glad to meet you!'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тук можем да видим вгражданията:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 1.53059261e-02,  6.80514947e-02,  3.14026810e-02, ...,\n",
       "         -8.92002955e-02,  1.52911525e-04, -5.65562584e-02],\n",
       "        [ 2.57456154e-01,  2.79364467e-01, -2.03605562e-01, ...,\n",
       "         -2.07474351e-01,  8.31158683e-02, -2.03911960e-01],\n",
       "        [ 3.98201384e-02, -8.03454965e-03,  2.39790026e-02, ...,\n",
       "         -7.18549127e-04,  2.66963355e-02, -4.30646613e-02],\n",
       "        [ 3.98201384e-02, -8.03454965e-03,  2.39790026e-02, ...,\n",
       "         -7.18549127e-04,  2.66963355e-02, -4.30646613e-02],\n",
       "        [ 3.98201384e-02, -8.03454965e-03,  2.39790026e-02, ...,\n",
       "         -7.18549127e-04,  2.66963355e-02, -4.30646613e-02],\n",
       "        [ 3.98201384e-02, -8.03454965e-03,  2.39790026e-02, ...,\n",
       "         -7.18549127e-04,  2.66963355e-02, -4.30646613e-02]],\n",
       "\n",
       "       [[ 1.89674050e-01,  2.61548996e-01, -3.67433839e-02, ...,\n",
       "         -2.07366899e-01, -1.05442435e-01, -2.36952081e-01],\n",
       "        [ 6.16133213e-02,  1.80511594e-01,  9.77298319e-02, ...,\n",
       "         -5.46628237e-02, -1.07340455e-01, -1.06589928e-01],\n",
       "        [ 1.53059261e-02,  6.80514947e-02,  3.14026810e-02, ...,\n",
       "         -8.92002955e-02,  1.52911525e-04, -5.65562584e-02],\n",
       "        [-4.84890305e-02, -8.41715634e-02,  1.51529670e-01, ...,\n",
       "          1.28192469e-01, -7.77286515e-02,  1.26041949e-01],\n",
       "        [-4.17212099e-02, -5.60694858e-02,  4.08860669e-02, ...,\n",
       "          8.70475471e-02,  8.92383084e-02,  1.67974353e-01],\n",
       "        [ 2.85779923e-01,  4.57767487e-01,  4.52292450e-02, ...,\n",
       "         -1.97419018e-01, -2.04659685e-01, -2.79758364e-01]]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[1](vectorizer(['Hello, world!','I am glad to meet you!'])).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Забележка**: За да се минимизира количеството на допълнителните символи, в някои случаи има смисъл да се сортират всички последователности в набора от данни по реда на нарастваща дължина (или, по-точно, брой токени). Това ще гарантира, че всяка минипартида съдържа последователности с подобна дължина.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Семантични вграждания: Word2Vec\n",
    "\n",
    "В предишния пример слоят за вграждане научи как да преобразува думите във векторни представяния, но тези представяния нямаха семантично значение. Би било полезно да се научи векторно представяне, при което подобни думи или синоними съответстват на вектори, които са близо един до друг според някакво векторно разстояние (например евклидово разстояние).\n",
    "\n",
    "За да постигнем това, трябва предварително да обучим модела за вграждане върху голяма колекция от текстове, използвайки техника като [Word2Vec](https://en.wikipedia.org/wiki/Word2vec). Тя се основава на две основни архитектури, които се използват за създаване на разпределено представяне на думите:\n",
    "\n",
    " - **Непрекъсната торба с думи** (CBoW), при която обучаваме модела да предсказва дума въз основа на заобикалящия контекст. Даден е n-грам $(W_{-2},W_{-1},W_0,W_1,W_2)$, като целта на модела е да предскаже $W_0$ от $(W_{-2},W_{-1},W_1,W_2)$.\n",
    " - **Непрекъснат скип-грам** е противоположен на CBoW. Моделът използва заобикалящия прозорец от контекстни думи, за да предскаже текущата дума.\n",
    "\n",
    "CBoW е по-бърз, докато скип-грам е по-бавен, но се справя по-добре с представянето на редки думи.\n",
    "\n",
    "![Изображение, показващо алгоритмите CBoW и Skip-Gram за преобразуване на думи във вектори.](../../../../../translated_images/bg/example-algorithms-for-converting-words-to-vectors.fbe9207a726922f6.webp)\n",
    "\n",
    "За да експериментираме с вграждането Word2Vec, предварително обучено върху набора от данни Google News, можем да използваме библиотеката **gensim**. По-долу намираме думите, които са най-близки до 'neural'.\n",
    "\n",
    "> **Забележка:** Когато за първи път създавате векторите на думите, изтеглянето им може да отнеме известно време!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "w2v = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neuronal -> 0.7804799675941467\n",
      "neurons -> 0.7326500415802002\n",
      "neural_circuits -> 0.7252851724624634\n",
      "neuron -> 0.7174385190010071\n",
      "cortical -> 0.6941086649894714\n",
      "brain_circuitry -> 0.6923246383666992\n",
      "synaptic -> 0.6699118614196777\n",
      "neural_circuitry -> 0.6638563275337219\n",
      "neurochemical -> 0.6555314064025879\n",
      "neuronal_activity -> 0.6531826257705688\n"
     ]
    }
   ],
   "source": [
    "for w,p in w2v.most_similar('neural'):\n",
    "    print(f\"{w} -> {p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можем също така да извлечем векторното вграждане от думата, за да го използваме при обучението на модела за класификация. Вграждането има 300 компонента, но тук показваме само първите 20 компонента на вектора за яснота:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01226807,  0.06225586,  0.10693359,  0.05810547,  0.23828125,\n",
       "        0.03686523,  0.05151367, -0.20703125,  0.01989746,  0.10058594,\n",
       "       -0.03759766, -0.1015625 , -0.15820312, -0.08105469, -0.0390625 ,\n",
       "       -0.05053711,  0.16015625,  0.2578125 ,  0.10058594, -0.25976562],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v['play'][:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Страхотното при семантичните вграждания е, че можете да манипулирате векторното кодиране въз основа на семантиката. Например, можем да поискаме да намерим дума, чиято векторна репрезентация е възможно най-близка до думите *крал* и *жена*, и възможно най-далеч от думата *мъж*:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('queen', 0.7118192911148071)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar(positive=['king','woman'],negative=['man'])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Примерът по-горе използва малко вътрешна магия на GenSym, но основната логика всъщност е доста проста. Интересното при вгражданията е, че можете да извършвате нормални операции с вектори върху векторите на вгражданията, и това би отразило операции върху **значенията** на думите. Примерът по-горе може да бъде изразен чрез операции с вектори: изчисляваме вектора, съответстващ на **KING-MAN+WOMAN** (операциите `+` и `-` се извършват върху векторните представяния на съответните думи), и след това намираме най-близката дума в речника до този вектор:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'queen'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the vector corresponding to kind-man+woman\n",
    "qvec = w2v['king']-1.7*w2v['man']+1.7*w2v['woman']\n",
    "# find the index of the closest embedding vector \n",
    "d = np.sum((w2v.vectors-qvec)**2,axis=1)\n",
    "min_idx = np.argmin(d)\n",
    "# find the corresponding word\n",
    "w2v.index_to_key[min_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NOTE**: Трябваше да добавим малки коефициенти към векторите на *man* и *woman* - опитайте да ги премахнете, за да видите какво ще се случи.\n",
    "\n",
    "За да намерим най-близкия вектор, използваме механизма на TensorFlow, за да изчислим вектор от разстояния между нашия вектор и всички вектори в речника, а след това намираме индекса на минималната дума, използвайки `argmin`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Докато Word2Vec изглежда като чудесен начин за изразяване на семантиката на думите, той има много недостатъци, включително следните:\n",
    "\n",
    "* И двата модела CBoW и skip-gram са **предсказващи вграждания**, и те вземат предвид само локалния контекст. Word2Vec не използва глобалния контекст.\n",
    "* Word2Vec не отчита **морфологията** на думите, т.е. факта, че значението на думата може да зависи от различни части на думата, като корена.\n",
    "\n",
    "**FastText** се опитва да преодолее второто ограничение и надгражда Word2Vec, като учи векторни представяния за всяка дума и n-грамите от символи, които се намират в рамките на всяка дума. Стойностите на тези представяния след това се осредняват в един вектор на всяка стъпка от обучението. Въпреки че това добавя много допълнителни изчисления към предварителното обучение, то позволява вгражданията на думите да кодират информация за поддуми.\n",
    "\n",
    "Друг метод, **GloVe**, използва различен подход към вгражданията на думи, базиран на факторизацията на матрицата на контекста на думите. Първо, той изгражда голяма матрица, която брои броя на срещанията на думите в различни контексти, и след това се опитва да представи тази матрица в по-ниски измерения по начин, който минимизира загубата при реконструкция.\n",
    "\n",
    "Библиотеката gensim поддържа тези вграждания на думи, и можете да експериментирате с тях, като промените кода за зареждане на модела по-горе.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Използване на предварително обучени вграждания в Keras\n",
    "\n",
    "Можем да модифицираме горния пример, за да запълним предварително матрицата в нашия слой за вграждане със семантични вграждания, като Word2Vec. Речниците на предварително обученото вграждане и текстовия корпус вероятно няма да съвпадат, така че трябва да изберем един от тях. Тук разглеждаме двете възможни опции: използване на речника на токенизатора и използване на речника от Word2Vec вгражданията.\n",
    "\n",
    "### Използване на речника на токенизатора\n",
    "\n",
    "Когато използваме речника на токенизатора, някои от думите в речника ще имат съответстващи Word2Vec вграждания, а други ще липсват. Като се има предвид, че размерът на нашия речник е `vocab_size`, а дължината на векторите на Word2Vec вгражданията е `embed_size`, слоят за вграждане ще бъде представен чрез тегловна матрица с форма `vocab_size`$\\times$`embed_size`. Ще запълним тази матрица, като преминем през речника:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding size: 300\n",
      "Populating matrix, this will take some time...Done, found 4551 words, 784 words missing\n"
     ]
    }
   ],
   "source": [
    "embed_size = len(w2v.get_vector('hello'))\n",
    "print(f'Embedding size: {embed_size}')\n",
    "\n",
    "vocab = vectorizer.get_vocabulary()\n",
    "W = np.zeros((vocab_size,embed_size))\n",
    "print('Populating matrix, this will take some time...',end='')\n",
    "found, not_found = 0,0\n",
    "for i,w in enumerate(vocab):\n",
    "    try:\n",
    "        W[i] = w2v.get_vector(w)\n",
    "        found+=1\n",
    "    except:\n",
    "        # W[i] = np.random.normal(0.0,0.3,size=(embed_size,))\n",
    "        not_found+=1\n",
    "\n",
    "print(f\"Done, found {found} words, {not_found} words missing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "За думи, които не присъстват в Word2Vec речника, можем или да ги оставим като нули, или да генерираме случаен вектор.\n",
    "\n",
    "Сега можем да дефинираме слой за вграждане с предварително обучени тегла:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = keras.layers.Embedding(vocab_size,embed_size,weights=[W],trainable=False)\n",
    "model = keras.models.Sequential([\n",
    "    vectorizer, emb,\n",
    "    keras.layers.Lambda(lambda x: tf.reduce_mean(x,axis=1)),\n",
    "    keras.layers.Dense(4, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 10s 10ms/step - loss: 1.1075 - acc: 0.7822 - val_loss: 0.9134 - val_acc: 0.8175\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2220226ef10>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'])\n",
    "model.fit(ds_train.map(tupelize).batch(batch_size),\n",
    "          validation_data=ds_test.map(tupelize).batch(batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note**: Забележете, че задаваме `trainable=False`, когато създаваме `Embedding`, което означава, че няма да обучаваме отново слоя Embedding. Това може да доведе до леко намаляване на точността, но ускорява процеса на обучение.\n",
    "\n",
    "### Използване на речник за вграждане\n",
    "\n",
    "Един проблем с предишния подход е, че речниците, използвани в TextVectorization и Embedding, са различни. За да преодолеем този проблем, можем да използваме едно от следните решения:\n",
    "* Преобучаване на модела Word2Vec върху нашия речник.\n",
    "* Зареждане на нашия набор от данни с речника от предварително обучения модел Word2Vec. Речниците, използвани за зареждане на набора от данни, могат да бъдат зададени по време на зареждането.\n",
    "\n",
    "Вторият подход изглежда по-лесен, затова нека го приложим. На първо място, ще създадем слой `TextVectorization` със зададен речник, взет от вгражданията на Word2Vec:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(w2v.vocab.keys())\n",
    "vectorizer = keras.layers.experimental.preprocessing.TextVectorization(input_shape=(1,))\n",
    "vectorizer.set_vocabulary(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Библиотеката за вграждане на думи gensim съдържа удобна функция, `get_keras_embeddings`, която автоматично ще създаде съответния слой за вграждане на Keras за вас.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "938/938 [==============================] - 20s 14ms/step - loss: 1.3377 - acc: 0.4978 - val_loss: 1.2995 - val_acc: 0.5647\n",
      "Epoch 2/5\n",
      "938/938 [==============================] - 10s 10ms/step - loss: 1.2587 - acc: 0.5722 - val_loss: 1.2339 - val_acc: 0.5842\n",
      "Epoch 3/5\n",
      "938/938 [==============================] - 10s 10ms/step - loss: 1.1980 - acc: 0.5884 - val_loss: 1.1826 - val_acc: 0.5954\n",
      "Epoch 4/5\n",
      "938/938 [==============================] - 12s 13ms/step - loss: 1.1503 - acc: 0.6002 - val_loss: 1.1417 - val_acc: 0.6018\n",
      "Epoch 5/5\n",
      "938/938 [==============================] - 11s 12ms/step - loss: 1.1120 - acc: 0.6097 - val_loss: 1.1083 - val_acc: 0.6104\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2220ccb81c0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    vectorizer, \n",
    "    w2v.get_keras_embedding(train_embeddings=False),\n",
    "    keras.layers.Lambda(lambda x: tf.reduce_mean(x,axis=1)),\n",
    "    keras.layers.Dense(4, activation='softmax')\n",
    "])\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'])\n",
    "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128),epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Една от причините, поради които не наблюдаваме по-висока точност, е, че някои думи от нашия набор от данни липсват в предварително обучената GloVe лексика и следователно те са на практика игнорирани. За да преодолеем това, можем да обучим собствени вграждания, базирани на нашия набор от данни.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Контекстуални вграждания\n",
    "\n",
    "Едно от основните ограничения на традиционните предварително обучени представяния на вграждания като Word2Vec е фактът, че, въпреки че могат да уловят някакво значение на дадена дума, те не могат да различат различните ѝ значения. Това може да създаде проблеми в последващите модели.\n",
    "\n",
    "Например, думата „play“ има различно значение в тези две изречения:\n",
    "- Отидох на **пиеса** в театъра.\n",
    "- Джон иска да **играе** с приятелите си.\n",
    "\n",
    "Предварително обучените вграждания, за които говорихме, представят и двете значения на думата „play“ в едно и също вграждане. За да преодолеем това ограничение, трябва да изградим вграждания, базирани на **езиков модел**, който е обучен върху голям корпус от текст и *знае* как думите могат да се комбинират в различни контексти. Обсъждането на контекстуални вграждания е извън обхвата на този урок, но ще се върнем към тях, когато говорим за езикови модели в следващия модул.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Отказ от отговорност**:  \nТози документ е преведен с помощта на AI услуга за превод [Co-op Translator](https://github.com/Azure/co-op-translator). Въпреки че се стремим към точност, моля, имайте предвид, че автоматизираните преводи може да съдържат грешки или неточности. Оригиналният документ на неговия роден език трябва да се счита за авторитетен източник. За критична информация се препоръчва професионален човешки превод. Ние не носим отговорност за недоразумения или погрешни интерпретации, произтичащи от използването на този превод.\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0cb620c6d4b9f7a635928804c26cf22403d89d98d79684e4529119355ee6d5a5"
  },
  "kernel_info": {
   "name": "conda-env-py37_tensorflow-py"
  },
  "kernelspec": {
   "display_name": "py37_tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "coopTranslator": {
   "original_hash": "b859482be7f61d1eadc2c6a2720a37e4",
   "translation_date": "2025-08-30T01:04:39+00:00",
   "source_file": "lessons/5-NLP/14-Embeddings/EmbeddingsTF.ipynb",
   "language_code": "bg"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}