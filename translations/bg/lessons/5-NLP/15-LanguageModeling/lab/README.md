# Обучение на Skip-Gram модел

Лабораторно упражнение от [AI for Beginners Curriculum](https://github.com/microsoft/ai-for-beginners).

## Задача

В това упражнение ви предизвикваме да обучите Word2Vec модел, използвайки техниката Skip-Gram. Обучете мрежа с вграждания, за да предсказва съседни думи в Skip-Gram прозорец с ширина $N$ токена. Можете да използвате [кода от този урок](../../../../../../lessons/5-NLP/15-LanguageModeling/CBoW-TF.ipynb) и да го модифицирате леко.

## Данните

Можете да използвате всяка книга. Много безплатни текстове можете да намерите в [Project Gutenberg](https://www.gutenberg.org/), например, тук е директен линк към [Алиса в страната на чудесата](https://www.gutenberg.org/files/11/11-0.txt) от Луис Карол. Или можете да използвате пиесите на Шекспир, които можете да получите с помощта на следния код:

```python
path_to_file = tf.keras.utils.get_file(
   'shakespeare.txt', 
   'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')
text = open(path_to_file, 'rb').read().decode(encoding='utf-8')
```

## Изследвайте!

Ако имате време и искате да се задълбочите в темата, опитайте да изследвате няколко неща:

* Как размерът на вграждането влияе на резултатите?
* Как различните стилове на текст влияят на резултата?
* Вземете няколко много различни типа думи и техните синоними, извлечете техните векторни представяния, приложете PCA, за да намалите размерностите до 2, и ги изобразете в 2D пространство. Забелязвате ли някакви модели?

**Отказ от отговорност**:  
Този документ е преведен с помощта на AI услуга за превод [Co-op Translator](https://github.com/Azure/co-op-translator). Въпреки че се стремим към точност, моля, имайте предвид, че автоматизираните преводи може да съдържат грешки или неточности. Оригиналният документ на неговия роден език трябва да се счита за авторитетен източник. За критична информация се препоръчва професионален човешки превод. Ние не носим отговорност за недоразумения или погрешни интерпретации, произтичащи от използването на този превод.