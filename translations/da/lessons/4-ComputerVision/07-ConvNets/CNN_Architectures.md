<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "2f7b97b375358cb51a1e098df306bf73",
  "translation_date": "2025-08-28T15:13:26+00:00",
  "source_file": "lessons/4-ComputerVision/07-ConvNets/CNN_Architectures.md",
  "language_code": "da"
}
-->
# Velkendte CNN-arkitekturer

### VGG-16

VGG-16 er et netv칝rk, der opn친ede 92,7% n칮jagtighed i ImageNet top-5 klassifikation i 2014. Det har f칮lgende lagstruktur:

![ImageNet Layers](../../../../../translated_images/vgg-16-arch1.d901a5583b3a51baeaab3e768567d921e5d54befa46e1e642616c5458c934028.da.jpg)

Som du kan se, f칮lger VGG en traditionel pyramidearkitektur, som er en sekvens af konvolutions- og pooling-lag.

![ImageNet Pyramid](../../../../../translated_images/vgg-16-arch.64ff2137f50dd49fdaa786e3f3a975b3f22615efd13efb19c5d22f12e01451a1.da.jpg)

> Billede fra [Researchgate](https://www.researchgate.net/figure/Vgg16-model-structure-To-get-the-VGG-NIN-model-we-replace-the-2-nd-4-th-6-th-7-th_fig2_335194493)

### ResNet

ResNet er en familie af modeller foresl친et af Microsoft Research i 2015. Hovedid칠en bag ResNet er brugen af **residualblokke**:

<img src="images/resnet-block.png" width="300"/>

> Billede fra [denne artikel](https://arxiv.org/pdf/1512.03385.pdf)

Grunden til at bruge identitets-pass-through er, at laget skal forudsige **forskellen** mellem resultatet af et tidligere lag og outputtet fra residualblokken - deraf navnet *residual*. Disse blokke er meget lettere at tr칝ne, og man kan konstruere netv칝rk med flere hundrede af disse blokke (de mest almindelige varianter er ResNet-52, ResNet-101 og ResNet-152).

Du kan ogs친 t칝nke p친 dette netv칝rk som v칝rende i stand til at tilpasse sin kompleksitet til datas칝ttet. I starten, n친r du begynder at tr칝ne netv칝rket, er v칝gtene sm친, og det meste af signalet g친r gennem identitetslagene. Efterh친nden som tr칝ningen skrider frem, og v칝gtene bliver st칮rre, vokser betydningen af netv칝rkets parametre, og netv칝rket tilpasser sig for at opn친 den n칮dvendige udtrykskraft til korrekt at klassificere tr칝ningsbillederne.

### Google Inception

Google Inception-arkitekturen tager denne id칠 et skridt videre og bygger hvert netv칝rkslag som en kombination af flere forskellige veje:

<img src="images/inception.png" width="400"/>

> Billede fra [Researchgate](https://www.researchgate.net/figure/Inception-module-with-dimension-reductions-left-and-schema-for-Inception-ResNet-v1_fig2_355547454)

Her skal vi fremh칝ve rollen af 1x1-konvolutioner, fordi de ved f칮rste 칮jekast ikke giver mening. Hvorfor skulle vi bruge et 1x1-filter p친 billedet? Men det er vigtigt at huske, at konvolutionsfiltre ogs친 arbejder med flere dybdekanaler (oprindeligt - RGB-farver, i efterf칮lgende lag - kanaler for forskellige filtre), og 1x1-konvolution bruges til at blande disse inputkanaler sammen ved hj칝lp af forskellige tr칝nbare v칝gte. Det kan ogs친 ses som en form for nedsampling (pooling) over kanal-dimensionen.

Her er [en god blogpost](https://medium.com/analytics-vidhya/talented-mr-1x1-comprehensive-look-at-1x1-convolution-in-deep-learning-f6b355825578) om emnet og [den originale artikel](https://arxiv.org/pdf/1312.4400.pdf).

### MobileNet

MobileNet er en familie af modeller med reduceret st칮rrelse, der er velegnede til mobile enheder. Brug dem, hvis du har begr칝nsede ressourcer og kan acceptere en lille reduktion i n칮jagtighed. Hovedid칠en bag dem er den s친kaldte **depthwise separable convolution**, som g칮r det muligt at repr칝sentere konvolutionsfiltre som en sammens칝tning af rumlige konvolutioner og 1x1-konvolution over dybdekanaler. Dette reducerer antallet af parametre betydeligt, hvilket g칮r netv칝rket mindre i st칮rrelse og lettere at tr칝ne med mindre data.

Her er [en god blogpost om MobileNet](https://medium.com/analytics-vidhya/image-classification-with-mobilenet-cc6fbb2cd470).

## Konklusion

I denne enhed har du l칝rt hovedkonceptet bag neurale netv칝rk til computer vision - konvolutionsnetv칝rk. Virkelige arkitekturer, der driver billedklassifikation, objektgenkendelse og endda billedgenerering, er alle baseret p친 CNN'er, blot med flere lag og nogle ekstra tr칝ningstricks.

## 游 Udfordring

I de medf칮lgende notebooks er der noter nederst om, hvordan man opn친r st칮rre n칮jagtighed. Lav nogle eksperimenter for at se, om du kan opn친 h칮jere n칮jagtighed.

## [Quiz efter forel칝sning](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/207)

## Gennemgang & Selvstudie

Selvom CNN'er oftest bruges til opgaver inden for computer vision, er de generelt gode til at udtr칝kke m칮nstre af fast st칮rrelse. For eksempel, hvis vi arbejder med lyd, kan vi ogs친 bruge CNN'er til at finde specifikke m칮nstre i lydsignaler - i s친 fald ville filtrene v칝re 1-dimensionelle (og dette CNN ville kaldes 1D-CNN). Nogle gange bruges ogs친 3D-CNN til at udtr칝kke funktioner i et multidimensionelt rum, s친som visse begivenheder, der forekommer i videoer - CNN kan fange visse m칮nstre af funktioner, der 칝ndrer sig over tid. Lav en gennemgang og selvstudie om andre opgaver, der kan l칮ses med CNN'er.

## [Opgave](lab/README.md)

I denne lab skal du klassificere forskellige katte- og hunderacer. Disse billeder er mere komplekse end MNIST-datas칝ttet, har h칮jere dimensioner, og der er mere end 10 klasser.

---

**Ansvarsfraskrivelse**:  
Dette dokument er blevet oversat ved hj칝lp af AI-overs칝ttelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selvom vi bestr칝ber os p친 n칮jagtighed, skal du v칝re opm칝rksom p친, at automatiserede overs칝ttelser kan indeholde fejl eller un칮jagtigheder. Det originale dokument p친 dets oprindelige sprog b칮r betragtes som den autoritative kilde. For kritisk information anbefales professionel menneskelig overs칝ttelse. Vi p친tager os ikke ansvar for eventuelle misforst친elser eller fejltolkninger, der opst친r som f칮lge af brugen af denne overs칝ttelse.