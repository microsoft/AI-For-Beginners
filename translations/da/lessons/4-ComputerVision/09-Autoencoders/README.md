# Autoencodere

N√•r vi tr√¶ner CNN'er, er en af udfordringerne, at vi har brug for en stor m√¶ngde m√¶rkede data. I tilf√¶lde af billedklassifikation skal vi opdele billeder i forskellige klasser, hvilket kr√¶ver manuelt arbejde.

## [Quiz f√∏r lektionen](https://ff-quizzes.netlify.app/en/ai/quiz/17)

Vi kan dog √∏nske at bruge r√• (um√¶rkede) data til at tr√¶ne CNN-featureekstraktorer, hvilket kaldes **selv-superviseret l√¶ring**. I stedet for labels bruger vi tr√¶ningsbilleder som b√•de netv√¶rksinput og -output. Hovedideen bag en **autoencoder** er, at vi har et **encoder-netv√¶rk**, der konverterer inputbilledet til et **latent rum** (normalt en vektor af mindre st√∏rrelse), og derefter et **decoder-netv√¶rk**, hvis m√•l er at rekonstruere det oprindelige billede.

> ‚úÖ En [autoencoder](https://wikipedia.org/wiki/Autoencoder) er "en type kunstigt neuralt netv√¶rk, der bruges til at l√¶re effektive kodninger af um√¶rkede data."

Da vi tr√¶ner en autoencoder til at fange s√• meget information som muligt fra det oprindelige billede for at opn√• en pr√¶cis rekonstruktion, fors√∏ger netv√¶rket at finde den bedste **indlejring** af inputbilleder for at fange meningen.

![AutoEncoder Diagram](../../../../../translated_images/da/autoencoder_schema.5e6fc9ad98a5eb61.webp)

> Billede fra [Keras blog](https://blog.keras.io/building-autoencoders-in-keras.html)

## Scenarier for brug af autoencodere

Selvom rekonstruktion af oprindelige billeder m√•ske ikke virker nyttigt i sig selv, er der nogle scenarier, hvor autoencodere er s√¶rligt anvendelige:

* **Reducering af billeddimensioner til visualisering** eller **tr√¶ning af billedindlejringer**. Autoencodere giver ofte bedre resultater end PCA, fordi de tager h√∏jde for billedernes rumlige natur og hierarkiske tr√¶k.
* **Denoising**, dvs. fjernelse af st√∏j fra billeder. Da st√∏j indeholder en masse unyttig information, kan autoencoderen ikke passe det hele ind i det relativt lille latente rum og fanger derfor kun de vigtige dele af billedet. N√•r vi tr√¶ner st√∏jfjernerne, starter vi med oprindelige billeder og bruger billeder med kunstigt tilf√∏jet st√∏j som input til autoencoderen.
* **Superopl√∏sning**, dvs. √∏gning af billedopl√∏sning. Vi starter med billeder i h√∏j opl√∏sning og bruger billeder med lavere opl√∏sning som input til autoencoderen.
* **Generative modeller**. N√•r vi har tr√¶net autoencoderen, kan decoder-delen bruges til at skabe nye objekter ud fra tilf√¶ldige latente vektorer.

## Variationsautoencodere (VAE)

Traditionelle autoencodere reducerer inputdataens dimensioner og finder de vigtige tr√¶k ved inputbillederne. Dog giver latente vektorer ofte ikke meget mening. Med andre ord, hvis vi tager MNIST-datas√¶ttet som eksempel, er det ikke nemt at finde ud af, hvilke cifre der svarer til forskellige latente vektorer, fordi n√¶rliggende latente vektorer ikke n√∏dvendigvis svarer til de samme cifre.

For at tr√¶ne *generative* modeller er det derimod bedre at have en forst√•else af det latente rum. Denne id√© f√∏rer os til **variationsautoencoder** (VAE).

En VAE er en autoencoder, der l√¶rer at forudsige en *statistisk fordeling* af de latente parametre, det s√•kaldte **latente fordeling**. For eksempel kan vi √∏nske, at latente vektorer er normalfordelte med en middelv√¶rdi z<sub>mean</sub> og en standardafvigelse z<sub>sigma</sub> (b√•de middelv√¶rdi og standardafvigelse er vektorer med en vis dimensionalitet d). Encoderen i VAE l√¶rer at forudsige disse parametre, og decoder-delen tager en tilf√¶ldig vektor fra denne fordeling for at rekonstruere objektet.

Opsummeret:

 * Fra inputvektoren forudsiger vi `z_mean` og `z_log_sigma` (i stedet for at forudsige standardafvigelsen direkte, forudsiger vi dens logaritme).
 * Vi sampler en vektor `sample` fra fordelingen N(z<sub>mean</sub>,exp(z<sub>log\_sigma</sub>)).
 * Decoderen fors√∏ger at dekode det oprindelige billede ved hj√¶lp af `sample` som inputvektor.

 <img src="../../../../../translated_images/da/vae.464c465a5b6a9e25.webp" width="50%">

> Billede fra [denne blogpost](https://ijdykeman.github.io/ml/2016/12/21/cvae.html) af Isaak Dykeman

Variationsautoencodere bruger en kompleks tab-funktion, der best√•r af to dele:

* **Rekonstruktionstab** er tab-funktionen, der viser, hvor t√¶t det rekonstruerede billede er p√• m√•let (det kan v√¶re Mean Squared Error eller MSE). Det er den samme tab-funktion som i normale autoencodere.
* **KL-tab**, som sikrer, at de latente variabels fordelinger forbliver t√¶t p√• normalfordelingen. Det er baseret p√• begrebet [Kullback-Leibler-divergens](https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained) - en metrik til at estimere, hvor ens to statistiske fordelinger er.

En vigtig fordel ved VAE'er er, at de g√∏r det relativt nemt at generere nye billeder, fordi vi ved, hvilken fordeling vi skal sample latente vektorer fra. For eksempel, hvis vi tr√¶ner en VAE med en 2D latent vektor p√• MNIST, kan vi derefter variere komponenterne i den latente vektor for at f√• forskellige cifre:

<img alt="vaemnist" src="../../../../../translated_images/da/vaemnist.cab9e602dc08dc50.webp" width="50%"/>

> Billede af [Dmitry Soshnikov](http://soshnikov.com)

Bem√¶rk, hvordan billederne flyder ind i hinanden, n√•r vi begynder at tage latente vektorer fra forskellige dele af det latente parameter-rum. Vi kan ogs√• visualisere dette rum i 2D:

<img alt="vaemnist cluster" src="../../../../../translated_images/da/vaemnist-diag.694315f775d5d666.webp" width="50%"/> 

> Billede af [Dmitry Soshnikov](http://soshnikov.com)

## ‚úçÔ∏è √òvelser: Autoencodere

L√¶r mere om autoencodere i disse tilh√∏rende notebooks:

* [Autoencodere i TensorFlow](AutoencodersTF.ipynb)
* [Autoencodere i PyTorch](AutoEncodersPyTorch.ipynb)

## Egenskaber ved autoencodere

* **Dataspecifikke** - de fungerer kun godt med den type billeder, de er blevet tr√¶net p√•. For eksempel, hvis vi tr√¶ner et superopl√∏sningsnetv√¶rk p√• blomster, vil det ikke fungere godt p√• portr√¶tter. Dette skyldes, at netv√¶rket kan producere billeder i h√∏jere opl√∏sning ved at tage fine detaljer fra tr√¶k, der er l√¶rt fra tr√¶ningsdatas√¶ttet.
* **Tabsgivende** - det rekonstruerede billede er ikke det samme som det oprindelige billede. Arten af tabet defineres af den *tab-funktion*, der bruges under tr√¶ningen.
* Fungerer p√• **um√¶rkede data**

## [Quiz efter lektionen](https://ff-quizzes.netlify.app/en/ai/quiz/18)

## Konklusion

I denne lektion l√¶rte du om de forskellige typer autoencodere, der er tilg√¶ngelige for AI-forskeren. Du l√¶rte, hvordan man bygger dem, og hvordan man bruger dem til at rekonstruere billeder. Du l√¶rte ogs√• om VAE og hvordan man bruger det til at generere nye billeder.

## üöÄ Udfordring

I denne lektion l√¶rte du om brugen af autoencodere til billeder. Men de kan ogs√• bruges til musik! Tjek Magenta-projektets [MusicVAE](https://magenta.tensorflow.org/music-vae)-projekt, som bruger autoencodere til at l√¶re at rekonstruere musik. Lav nogle [eksperimenter](https://colab.research.google.com/github/magenta/magenta-demos/blob/master/colab-notebooks/Multitrack_MusicVAE.ipynb) med dette bibliotek for at se, hvad du kan skabe.

## [Quiz efter lektionen](https://ff-quizzes.netlify.app/en/ai/quiz/16)

## Gennemgang & Selvstudie

Til reference kan du l√¶se mere om autoencodere i disse ressourcer:

* [Building Autoencoders in Keras](https://blog.keras.io/building-autoencoders-in-keras.html)
* [Blogpost p√• NeuroHive](https://neurohive.io/ru/osnovy-data-science/variacionnyj-avtojenkoder-vae/)
* [Variational Autoencoders Explained](https://kvfrans.com/variational-autoencoders-explained/)
* [Conditional Variational Autoencoders](https://ijdykeman.github.io/ml/2016/12/21/cvae.html)

## Opgave

I slutningen af [denne notebook med TensorFlow](AutoencodersTF.ipynb) finder du en 'opgave' - brug denne som din opgave.

---

