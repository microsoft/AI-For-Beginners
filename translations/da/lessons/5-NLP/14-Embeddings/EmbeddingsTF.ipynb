{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indlejring\n",
    "\n",
    "I vores tidligere eksempel arbejdede vi med højdimensionelle bag-of-words vektorer med længden `vocab_size`, og vi konverterede eksplicit lavdimensionelle positionsrepræsentationsvektorer til sparsomme one-hot repræsentationer. Denne one-hot repræsentation er ikke hukommelseseffektiv. Derudover behandles hvert ord uafhængigt af hinanden, så one-hot kodede vektorer udtrykker ikke semantiske ligheder mellem ord.\n",
    "\n",
    "I denne enhed vil vi fortsætte med at udforske **News AG**-datasættet. For at starte, lad os indlæse dataene og få nogle definitioner fra den tidligere enhed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "\n",
    "ds_train, ds_test = tfds.load('ag_news_subset').values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hvad er en embedding?\n",
    "\n",
    "Ideen med **embedding** er at repræsentere ord ved hjælp af lavdimensionelle tætte vektorer, der afspejler ordets semantiske betydning. Vi vil senere diskutere, hvordan man bygger meningsfulde word embeddings, men for nu kan vi tænke på embeddings som en måde at reducere dimensionaliteten af en ordvektor.\n",
    "\n",
    "En embedding-lag tager altså et ord som input og producerer en output-vektor med en specificeret `embedding_size`. På en måde minder det meget om et `Dense`-lag, men i stedet for at tage en one-hot-kodet vektor som input, kan det tage et ordnummer.\n",
    "\n",
    "Ved at bruge et embedding-lag som det første lag i vores netværk kan vi skifte fra bag-of-words til en **embedding bag**-model, hvor vi først konverterer hvert ord i vores tekst til den tilsvarende embedding og derefter beregner en aggregeringsfunktion over alle disse embeddings, såsom `sum`, `average` eller `max`.\n",
    "\n",
    "![Billede, der viser en embedding-klassifikator for fem sekvensord.](../../../../../translated_images/da/embedding-classifier-example.b77f021a7ee67eee.webp)\n",
    "\n",
    "Vores klassifikator-neurale netværk består af følgende lag:\n",
    "\n",
    "* `TextVectorization`-lag, som tager en streng som input og producerer en tensor af token-numre. Vi vil specificere en rimelig ordforrådsstørrelse `vocab_size` og ignorere mindre hyppigt anvendte ord. Inputformen vil være 1, og outputformen vil være $n$, da vi får $n$ tokens som resultat, hvor hver af dem indeholder numre fra 0 til `vocab_size`.\n",
    "* `Embedding`-lag, som tager $n$ numre og reducerer hvert nummer til en tæt vektor af en given længde (100 i vores eksempel). Således vil input-tensoren med formen $n$ blive transformeret til en $n\\times 100$ tensor.\n",
    "* Aggregeringslag, som tager gennemsnittet af denne tensor langs den første akse, dvs. det vil beregne gennemsnittet af alle $n$ input-tensorer, der svarer til forskellige ord. For at implementere dette lag vil vi bruge et `Lambda`-lag og give det funktionen til at beregne gennemsnittet. Outputtet vil have formen 100 og vil være den numeriske repræsentation af hele inputsekvensen.\n",
    "* Endeligt `Dense` lineært klassifikationslag.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization (TextVec  (None, None)             0         \n",
      " torization)                                                     \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, None, 100)         3000000   \n",
      "                                                                 \n",
      " lambda (Lambda)             (None, 100)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4)                 404       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,000,404\n",
      "Trainable params: 3,000,404\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 30000\n",
    "batch_size = 128\n",
    "\n",
    "vectorizer = keras.layers.experimental.preprocessing.TextVectorization(max_tokens=vocab_size,input_shape=(1,))\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    vectorizer,    \n",
    "    keras.layers.Embedding(vocab_size,100),\n",
    "    keras.layers.Lambda(lambda x: tf.reduce_mean(x,axis=1)),\n",
    "    keras.layers.Dense(4, activation='softmax')\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I `summary`-udskriften, i **output shape**-kolonnen, svarer den første tensor-dimension `None` til minibatch-størrelsen, og den anden svarer til længden af token-sekvensen. Alle token-sekvenser i minibatchen har forskellige længder. Vi vil diskutere, hvordan man håndterer dette i næste afsnit.\n",
    "\n",
    "Lad os nu træne netværket:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training vectorizer\n",
      "938/938 [==============================] - 20s 20ms/step - loss: 0.7891 - acc: 0.8155 - val_loss: 0.4470 - val_acc: 0.8642\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22255515100>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_text(x):\n",
    "    return x['title']+' '+x['description']\n",
    "\n",
    "def tupelize(x):\n",
    "    return (extract_text(x),x['label'])\n",
    "\n",
    "print(\"Training vectorizer\")\n",
    "vectorizer.adapt(ds_train.take(500).map(extract_text))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'])\n",
    "model.fit(ds_train.map(tupelize).batch(batch_size),validation_data=ds_test.map(tupelize).batch(batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "> **Bemærk** at vi bygger en vektorisator baseret på et delmængde af dataene. Dette gøres for at fremskynde processen, og det kan resultere i en situation, hvor ikke alle tokens fra vores tekst er til stede i ordforrådet. I dette tilfælde vil disse tokens blive ignoreret, hvilket kan føre til en lidt lavere nøjagtighed. Dog giver en delmængde af tekst ofte en god estimering af ordforrådet i virkeligheden.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Håndtering af variabel sekvensstørrelse\n",
    "\n",
    "Lad os forstå, hvordan træning foregår i minibatches. I eksemplet ovenfor har input-tensoren dimension 1, og vi bruger minibatches med en længde på 128, så den faktiske størrelse af tensoren er $128 \\times 1$. Antallet af tokens i hver sætning er dog forskelligt. Hvis vi anvender `TextVectorization`-laget på et enkelt input, er antallet af tokens, der returneres, forskelligt, afhængigt af hvordan teksten er tokeniseret:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 1 45], shape=(2,), dtype=int64)\n",
      "tf.Tensor([ 112 1271    1    3 1747  158], shape=(6,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer('Hello, world!'))\n",
    "print(vectorizer('I am glad to meet you!'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Men når vi anvender vektoriseringen på flere sekvenser, skal den producere en tensor med rektangulær form, så den udfylder ubrugte elementer med PAD-tokenet (som i vores tilfælde er nul):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 6), dtype=int64, numpy=\n",
       "array([[   1,   45,    0,    0,    0,    0],\n",
       "       [ 112, 1271,    1,    3, 1747,  158]], dtype=int64)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer(['Hello, world!','I am glad to meet you!'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Her kan vi se indlejringerne:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 1.53059261e-02,  6.80514947e-02,  3.14026810e-02, ...,\n",
       "         -8.92002955e-02,  1.52911525e-04, -5.65562584e-02],\n",
       "        [ 2.57456154e-01,  2.79364467e-01, -2.03605562e-01, ...,\n",
       "         -2.07474351e-01,  8.31158683e-02, -2.03911960e-01],\n",
       "        [ 3.98201384e-02, -8.03454965e-03,  2.39790026e-02, ...,\n",
       "         -7.18549127e-04,  2.66963355e-02, -4.30646613e-02],\n",
       "        [ 3.98201384e-02, -8.03454965e-03,  2.39790026e-02, ...,\n",
       "         -7.18549127e-04,  2.66963355e-02, -4.30646613e-02],\n",
       "        [ 3.98201384e-02, -8.03454965e-03,  2.39790026e-02, ...,\n",
       "         -7.18549127e-04,  2.66963355e-02, -4.30646613e-02],\n",
       "        [ 3.98201384e-02, -8.03454965e-03,  2.39790026e-02, ...,\n",
       "         -7.18549127e-04,  2.66963355e-02, -4.30646613e-02]],\n",
       "\n",
       "       [[ 1.89674050e-01,  2.61548996e-01, -3.67433839e-02, ...,\n",
       "         -2.07366899e-01, -1.05442435e-01, -2.36952081e-01],\n",
       "        [ 6.16133213e-02,  1.80511594e-01,  9.77298319e-02, ...,\n",
       "         -5.46628237e-02, -1.07340455e-01, -1.06589928e-01],\n",
       "        [ 1.53059261e-02,  6.80514947e-02,  3.14026810e-02, ...,\n",
       "         -8.92002955e-02,  1.52911525e-04, -5.65562584e-02],\n",
       "        [-4.84890305e-02, -8.41715634e-02,  1.51529670e-01, ...,\n",
       "          1.28192469e-01, -7.77286515e-02,  1.26041949e-01],\n",
       "        [-4.17212099e-02, -5.60694858e-02,  4.08860669e-02, ...,\n",
       "          8.70475471e-02,  8.92383084e-02,  1.67974353e-01],\n",
       "        [ 2.85779923e-01,  4.57767487e-01,  4.52292450e-02, ...,\n",
       "         -1.97419018e-01, -2.04659685e-01, -2.79758364e-01]]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[1](vectorizer(['Hello, world!','I am glad to meet you!'])).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Bemærk**: For at minimere mængden af udfyldning kan det i nogle tilfælde give mening at sortere alle sekvenser i datasættet i rækkefølge efter stigende længde (eller mere præcist, antal tokens). Dette vil sikre, at hver minibatch indeholder sekvenser af lignende længde.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantiske indlejringer: Word2Vec\n",
    "\n",
    "I vores tidligere eksempel lærte indlejringslaget at kortlægge ord til vektorrepræsentationer, men disse repræsentationer havde ikke nogen semantisk betydning. Det ville være rart at lære en vektorrepræsentation, hvor lignende ord eller synonymer svarer til vektorer, der ligger tæt på hinanden i forhold til en eller anden vektordistance (for eksempel euklidisk distance).\n",
    "\n",
    "For at opnå dette skal vi fortræne vores indlejringsmodel på en stor samling tekst ved hjælp af en teknik som [Word2Vec](https://en.wikipedia.org/wiki/Word2vec). Den er baseret på to hovedarkitekturer, der bruges til at producere en distribueret repræsentation af ord:\n",
    "\n",
    " - **Continuous bag-of-words** (CBoW), hvor vi træner modellen til at forudsige et ord ud fra den omgivende kontekst. Givet ngrammet $(W_{-2},W_{-1},W_0,W_1,W_2)$ er målet for modellen at forudsige $W_0$ ud fra $(W_{-2},W_{-1},W_1,W_2)$.\n",
    " - **Continuous skip-gram** er det modsatte af CBoW. Modellen bruger det omgivende vindue af kontekstord til at forudsige det aktuelle ord.\n",
    "\n",
    "CBoW er hurtigere, og selvom skip-gram er langsommere, er det bedre til at repræsentere sjældne ord.\n",
    "\n",
    "![Billede, der viser både CBoW- og Skip-Gram-algoritmer til at konvertere ord til vektorer.](../../../../../translated_images/da/example-algorithms-for-converting-words-to-vectors.fbe9207a726922f6.webp)\n",
    "\n",
    "For at eksperimentere med Word2Vec-indlejringen, der er fortrænet på Google News-datasættet, kan vi bruge **gensim**-biblioteket. Nedenfor finder vi de ord, der minder mest om 'neural'.\n",
    "\n",
    "> **Note:** Når du først opretter ordvektorer, kan det tage noget tid at downloade dem!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "w2v = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neuronal -> 0.7804799675941467\n",
      "neurons -> 0.7326500415802002\n",
      "neural_circuits -> 0.7252851724624634\n",
      "neuron -> 0.7174385190010071\n",
      "cortical -> 0.6941086649894714\n",
      "brain_circuitry -> 0.6923246383666992\n",
      "synaptic -> 0.6699118614196777\n",
      "neural_circuitry -> 0.6638563275337219\n",
      "neurochemical -> 0.6555314064025879\n",
      "neuronal_activity -> 0.6531826257705688\n"
     ]
    }
   ],
   "source": [
    "for w,p in w2v.most_similar('neural'):\n",
    "    print(f\"{w} -> {p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi kan også udtrække vektorindlejringen fra ordet, som skal bruges til at træne klassifikationsmodellen. Indlejringen har 300 komponenter, men her viser vi kun de første 20 komponenter af vektoren for klarhed:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01226807,  0.06225586,  0.10693359,  0.05810547,  0.23828125,\n",
       "        0.03686523,  0.05151367, -0.20703125,  0.01989746,  0.10058594,\n",
       "       -0.03759766, -0.1015625 , -0.15820312, -0.08105469, -0.0390625 ,\n",
       "       -0.05053711,  0.16015625,  0.2578125 ,  0.10058594, -0.25976562],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v['play'][:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Det fantastiske ved semantiske indlejringer er, at du kan manipulere vektorindkodningen baseret på semantik. For eksempel kan vi bede om at finde et ord, hvis vektorrepræsentation er så tæt som muligt på ordene *konge* og *kvinde*, og så langt som muligt fra ordet *mand*:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('queen', 0.7118192911148071)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar(positive=['king','woman'],negative=['man'])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Et eksempel ovenfor bruger noget intern GenSym-magi, men den underliggende logik er faktisk ret simpel. En interessant ting ved indlejringer er, at du kan udføre normale vektoroperationer på indlejringsvektorer, og det ville afspejle operationer på ords **betydninger**. Eksemplet ovenfor kan udtrykkes i form af vektoroperationer: vi beregner vektoren svarende til **KONGE-MAND+KVINDE** (operationerne `+` og `-` udføres på vektorrepræsentationer af de tilsvarende ord), og derefter finder vi det nærmeste ord i ordbogen til den vektor:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'queen'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the vector corresponding to kind-man+woman\n",
    "qvec = w2v['king']-1.7*w2v['man']+1.7*w2v['woman']\n",
    "# find the index of the closest embedding vector \n",
    "d = np.sum((w2v.vectors-qvec)**2,axis=1)\n",
    "min_idx = np.argmin(d)\n",
    "# find the corresponding word\n",
    "w2v.index_to_key[min_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NOTE**: Vi var nødt til at tilføje små koefficienter til *man*- og *woman*-vektorerne - prøv at fjerne dem for at se, hvad der sker.\n",
    "\n",
    "For at finde den nærmeste vektor bruger vi TensorFlow-mekanik til at beregne en vektor af afstande mellem vores vektor og alle vektorer i ordforrådet, og derefter finde indekset for det minimale ord ved hjælp af `argmin`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mens Word2Vec virker som en fantastisk måde at udtrykke ords semantik på, har det mange ulemper, herunder følgende:\n",
    "\n",
    "* Både CBoW- og skip-gram-modeller er **forudsigende indlejringer**, og de tager kun lokal kontekst i betragtning. Word2Vec udnytter ikke global kontekst.\n",
    "* Word2Vec tager ikke højde for ords **morfologi**, dvs. det faktum, at ordets betydning kan afhænge af forskellige dele af ordet, såsom roden.\n",
    "\n",
    "**FastText** forsøger at overvinde den anden begrænsning og bygger videre på Word2Vec ved at lære vektorrepræsentationer for hvert ord og de karakter-n-grammer, der findes inden for hvert ord. Værdierne af repræsentationerne gennemsnitliggøres derefter til én vektor ved hver træningsfase. Selvom dette tilføjer en masse ekstra beregning til prætræningen, gør det det muligt for ordindlejringer at kode sub-ord-information.\n",
    "\n",
    "En anden metode, **GloVe**, bruger en anderledes tilgang til ordindlejringer, baseret på faktorisering af ord-kontekst-matricen. Først opbygges en stor matrix, der tæller antallet af ordforekomster i forskellige kontekster, og derefter forsøger den at repræsentere denne matrix i lavere dimensioner på en måde, der minimerer rekonstruktionsfejl.\n",
    "\n",
    "Gensim-biblioteket understøtter disse ordindlejringer, og du kan eksperimentere med dem ved at ændre modelindlæsningskoden ovenfor.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brug af forudtrænede embeddings i Keras\n",
    "\n",
    "Vi kan ændre eksemplet ovenfor for at forudfylde matricen i vores embedding-lag med semantiske embeddings, såsom Word2Vec. Ordforrådene fra den forudtrænede embedding og tekstkorpuset vil sandsynligvis ikke matche, så vi skal vælge ét. Her undersøger vi de to mulige muligheder: at bruge tokenizer-ordforrådet og at bruge ordforrådet fra Word2Vec-embeddings.\n",
    "\n",
    "### Brug af tokenizer-ordforråd\n",
    "\n",
    "Når vi bruger tokenizer-ordforrådet, vil nogle af ordene fra ordforrådet have tilsvarende Word2Vec-embeddings, og nogle vil mangle. Givet at vores ordforrådsstørrelse er `vocab_size`, og længden af Word2Vec embedding-vektoren er `embed_size`, vil embedding-laget blive repræsenteret af en vægtmatrix med formen `vocab_size`$\\times$`embed_size`. Vi vil udfylde denne matrix ved at gennemgå ordforrådet:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding size: 300\n",
      "Populating matrix, this will take some time...Done, found 4551 words, 784 words missing\n"
     ]
    }
   ],
   "source": [
    "embed_size = len(w2v.get_vector('hello'))\n",
    "print(f'Embedding size: {embed_size}')\n",
    "\n",
    "vocab = vectorizer.get_vocabulary()\n",
    "W = np.zeros((vocab_size,embed_size))\n",
    "print('Populating matrix, this will take some time...',end='')\n",
    "found, not_found = 0,0\n",
    "for i,w in enumerate(vocab):\n",
    "    try:\n",
    "        W[i] = w2v.get_vector(w)\n",
    "        found+=1\n",
    "    except:\n",
    "        # W[i] = np.random.normal(0.0,0.3,size=(embed_size,))\n",
    "        not_found+=1\n",
    "\n",
    "print(f\"Done, found {found} words, {not_found} words missing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For ord, der ikke findes i Word2Vec-ordforrådet, kan vi enten lade dem være nul, eller generere en tilfældig vektor.\n",
    "\n",
    "Nu kan vi definere et embedding-lag med fortrænede vægte:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = keras.layers.Embedding(vocab_size,embed_size,weights=[W],trainable=False)\n",
    "model = keras.models.Sequential([\n",
    "    vectorizer, emb,\n",
    "    keras.layers.Lambda(lambda x: tf.reduce_mean(x,axis=1)),\n",
    "    keras.layers.Dense(4, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 10s 10ms/step - loss: 1.1075 - acc: 0.7822 - val_loss: 0.9134 - val_acc: 0.8175\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2220226ef10>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'])\n",
    "model.fit(ds_train.map(tupelize).batch(batch_size),\n",
    "          validation_data=ds_test.map(tupelize).batch(batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Bemærk**: Bemærk, at vi sætter `trainable=False`, når vi opretter `Embedding`, hvilket betyder, at vi ikke genoplærer Embedding-laget. Dette kan medføre, at nøjagtigheden bliver en smule lavere, men det gør træningen hurtigere.\n",
    "\n",
    "### Brug af embedding-ordforråd\n",
    "\n",
    "Et problem med den tidligere tilgang er, at de ordforslag, der bruges i TextVectorization og Embedding, er forskellige. For at løse dette problem kan vi bruge en af følgende løsninger:\n",
    "* Genoplære Word2Vec-modellen på vores ordforslag.\n",
    "* Indlæse vores datasæt med ordforslaget fra den fortrænede Word2Vec-model. Ordforslag, der bruges til at indlæse datasættet, kan specificeres under indlæsningen.\n",
    "\n",
    "Den sidstnævnte tilgang virker nemmere, så lad os implementere den. Først og fremmest vil vi oprette et `TextVectorization`-lag med det specificerede ordforslag, taget fra Word2Vec-embeddings:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(w2v.vocab.keys())\n",
    "vectorizer = keras.layers.experimental.preprocessing.TextVectorization(input_shape=(1,))\n",
    "vectorizer.set_vocabulary(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Biblioteket gensim word embeddings indeholder en praktisk funktion, `get_keras_embeddings`, som automatisk vil oprette det tilsvarende Keras embeddings-lag for dig.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "938/938 [==============================] - 20s 14ms/step - loss: 1.3377 - acc: 0.4978 - val_loss: 1.2995 - val_acc: 0.5647\n",
      "Epoch 2/5\n",
      "938/938 [==============================] - 10s 10ms/step - loss: 1.2587 - acc: 0.5722 - val_loss: 1.2339 - val_acc: 0.5842\n",
      "Epoch 3/5\n",
      "938/938 [==============================] - 10s 10ms/step - loss: 1.1980 - acc: 0.5884 - val_loss: 1.1826 - val_acc: 0.5954\n",
      "Epoch 4/5\n",
      "938/938 [==============================] - 12s 13ms/step - loss: 1.1503 - acc: 0.6002 - val_loss: 1.1417 - val_acc: 0.6018\n",
      "Epoch 5/5\n",
      "938/938 [==============================] - 11s 12ms/step - loss: 1.1120 - acc: 0.6097 - val_loss: 1.1083 - val_acc: 0.6104\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2220ccb81c0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    vectorizer, \n",
    "    w2v.get_keras_embedding(train_embeddings=False),\n",
    "    keras.layers.Lambda(lambda x: tf.reduce_mean(x,axis=1)),\n",
    "    keras.layers.Dense(4, activation='softmax')\n",
    "])\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'])\n",
    "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128),epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En af grundene til, at vi ikke ser højere nøjagtighed, er fordi nogle ord fra vores datasæt mangler i den fortrænede GloVe-ordforråd, og derfor bliver de i bund og grund ignoreret. For at overvinde dette kan vi træne vores egne indlejringer baseret på vores datasæt.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kontekstuelle indlejringer\n",
    "\n",
    "En vigtig begrænsning ved traditionelle forudtrænede indlejringsrepræsentationer som Word2Vec er, at selvom de kan fange en vis betydning af et ord, kan de ikke skelne mellem forskellige betydninger. Dette kan skabe problemer i efterfølgende modeller.\n",
    "\n",
    "For eksempel har ordet 'play' forskellige betydninger i disse to sætninger:\n",
    "- Jeg var til en **forestilling** på teatret.\n",
    "- John vil gerne **lege** med sine venner.\n",
    "\n",
    "De forudtrænede indlejringer, vi har talt om, repræsenterer begge betydninger af ordet 'play' i den samme indlejring. For at overvinde denne begrænsning skal vi bygge indlejringer baseret på **sproglige modeller**, som er trænet på en stor tekstsamling og *forstår*, hvordan ord kan sættes sammen i forskellige kontekster. At diskutere kontekstuelle indlejringer ligger uden for rammerne af denne tutorial, men vi vender tilbage til dem, når vi taler om sproglige modeller i næste enhed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Ansvarsfraskrivelse**:  \nDette dokument er blevet oversat ved hjælp af AI-oversættelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selvom vi bestræber os på at sikre nøjagtighed, skal du være opmærksom på, at automatiserede oversættelser kan indeholde fejl eller unøjagtigheder. Det originale dokument på dets oprindelige sprog bør betragtes som den autoritative kilde. For kritisk information anbefales professionel menneskelig oversættelse. Vi påtager os ikke ansvar for eventuelle misforståelser eller fejltolkninger, der måtte opstå som følge af brugen af denne oversættelse.\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0cb620c6d4b9f7a635928804c26cf22403d89d98d79684e4529119355ee6d5a5"
  },
  "kernel_info": {
   "name": "conda-env-py37_tensorflow-py"
  },
  "kernelspec": {
   "display_name": "py37_tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "coopTranslator": {
   "original_hash": "b859482be7f61d1eadc2c6a2720a37e4",
   "translation_date": "2025-08-28T17:47:26+00:00",
   "source_file": "lessons/5-NLP/14-Embeddings/EmbeddingsTF.ipynb",
   "language_code": "da"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}