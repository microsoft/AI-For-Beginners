# Indlejring

## [Quiz f√∏r forel√¶sning](https://ff-quizzes.netlify.app/en/ai/quiz/27)

N√•r vi tr√¶nede klassifikatorer baseret p√• BoW eller TF/IDF, arbejdede vi med h√∏j-dimensionelle bag-of-words vektorer med l√¶ngden `vocab_size`, og vi konverterede eksplicit fra lav-dimensionelle positionsrepr√¶sentationsvektorer til sparsomme one-hot repr√¶sentationer. Denne one-hot repr√¶sentation er dog ikke hukommelseseffektiv. Derudover behandles hvert ord uafh√¶ngigt af hinanden, dvs. one-hot kodede vektorer udtrykker ikke nogen semantisk lighed mellem ord.

Ideen med **indlejring** er at repr√¶sentere ord med lav-dimensionelle t√¶tte vektorer, som p√• en eller anden m√•de afspejler den semantiske betydning af et ord. Vi vil senere diskutere, hvordan man bygger meningsfulde ordindlejringer, men for nu kan vi blot t√¶nke p√• indlejringer som en m√•de at reducere dimensionaliteten af en ordvektor.

S√• indlejringslaget vil tage et ord som input og producere en outputvektor med en specificeret `embedding_size`. P√• en m√•de minder det meget om et `Linear` lag, men i stedet for at tage en one-hot kodet vektor, vil det kunne tage et ordnummer som input, hvilket g√∏r det muligt at undg√• at skabe store one-hot kodede vektorer.

Ved at bruge et indlejringslag som det f√∏rste lag i vores klassifikationsnetv√¶rk kan vi skifte fra en bag-of-words til en **embedding bag** model, hvor vi f√∏rst konverterer hvert ord i vores tekst til den tilsvarende indlejring og derefter beregner en aggregeringsfunktion over alle disse indlejringer, s√•som `sum`, `average` eller `max`.

![Billede, der viser en indlejringsklassifikator for fem sekvensord.](../../../../../translated_images/da/embedding-classifier-example.b77f021a7ee67eee.webp)

> Billede af forfatteren

## ‚úçÔ∏è √òvelser: Indlejringer

Forts√¶t din l√¶ring i f√∏lgende notebooks:
* [Indlejringer med PyTorch](EmbeddingsPyTorch.ipynb)
* [Indlejringer med TensorFlow](EmbeddingsTF.ipynb)

## Semantiske indlejringer: Word2Vec

Mens indlejringslaget l√¶rte at kortl√¶gge ord til vektorrepr√¶sentationer, havde denne repr√¶sentation dog ikke n√∏dvendigvis meget semantisk betydning. Det ville v√¶re rart at l√¶re en vektorrepr√¶sentation, hvor lignende ord eller synonymer svarer til vektorer, der er t√¶t p√• hinanden i forhold til en eller anden vektordistance (f.eks. Euklidisk distance).

For at g√∏re dette skal vi fortr√¶ne vores indlejringsmodel p√• en stor samling af tekst p√• en specifik m√•de. En metode til at tr√¶ne semantiske indlejringer kaldes [Word2Vec](https://en.wikipedia.org/wiki/Word2vec). Det er baseret p√• to hovedarkitekturer, der bruges til at producere en distribueret repr√¶sentation af ord:

 - **Continuous bag-of-words** (CBoW) ‚Äî i denne arkitektur tr√¶ner vi modellen til at forudsige et ord ud fra den omkringliggende kontekst. Givet ngrammet $(W_{-2},W_{-1},W_0,W_1,W_2)$ er m√•let for modellen at forudsige $W_0$ ud fra $(W_{-2},W_{-1},W_1,W_2)$.
 - **Continuous skip-gram** er det modsatte af CBoW. Modellen bruger det omkringliggende vindue af kontekstord til at forudsige det aktuelle ord.

CBoW er hurtigere, mens skip-gram er langsommere, men g√∏r et bedre arbejde med at repr√¶sentere sj√¶ldne ord.

![Billede, der viser b√•de CBoW og Skip-Gram algoritmer til at konvertere ord til vektorer.](../../../../../translated_images/da/example-algorithms-for-converting-words-to-vectors.fbe9207a726922f6.webp)

> Billede fra [denne artikel](https://arxiv.org/pdf/1301.3781.pdf)

Fortr√¶nede Word2Vec-indlejringer (s√•vel som andre lignende modeller, s√•som GloVe) kan ogs√• bruges i stedet for indlejringslaget i neurale netv√¶rk. Vi skal dog h√•ndtere ordforr√•d, fordi ordforr√•det, der blev brugt til at fortr√¶ne Word2Vec/GloVe, sandsynligvis adskiller sig fra ordforr√•det i vores tekstkorpus. Kig i de ovenst√•ende notebooks for at se, hvordan dette problem kan l√∏ses.

## Kontekstuelle indlejringer

En vigtig begr√¶nsning ved traditionelle fortr√¶nede indlejringsrepr√¶sentationer som Word2Vec er problemet med ords betydningsafklaring. Mens fortr√¶nede indlejringer kan fange noget af betydningen af ord i kontekst, er hver mulig betydning af et ord kodet i den samme indlejring. Dette kan skabe problemer i efterf√∏lgende modeller, da mange ord, s√•som ordet 'play', har forskellige betydninger afh√¶ngigt af den kontekst, de bruges i.

For eksempel har ordet 'play' i disse to forskellige s√¶tninger ret forskellige betydninger:

- Jeg gik til et **skuespil** p√• teatret.
- John vil gerne **lege** med sine venner.

De fortr√¶nede indlejringer ovenfor repr√¶senterer begge disse betydninger af ordet 'play' i den samme indlejring. For at overvinde denne begr√¶nsning skal vi bygge indlejringer baseret p√• **sproglaget**, som er tr√¶net p√• en stor tekstkorpus og *ved*, hvordan ord kan s√¶ttes sammen i forskellige kontekster. Diskussionen om kontekstuelle indlejringer er uden for rammerne af denne tutorial, men vi vil vende tilbage til dem, n√•r vi taler om sproglag senere i kurset.

## Konklusion

I denne lektion opdagede du, hvordan man bygger og bruger indlejringslag i TensorFlow og Pytorch for bedre at afspejle den semantiske betydning af ord.

## üöÄ Udfordring

Word2Vec er blevet brugt til nogle interessante anvendelser, herunder generering af sangtekster og poesi. Tag et kig p√• [denne artikel](https://www.politetype.com/blog/word2vec-color-poems), som gennemg√•r, hvordan forfatteren brugte Word2Vec til at generere poesi. Se ogs√• [denne video af Dan Shiffmann](https://www.youtube.com/watch?v=LSS_bos_TPI&ab_channel=TheCodingTrain) for at opdage en anden forklaring p√• denne teknik. Pr√∏v derefter at anvende disse teknikker p√• din egen tekstkorpus, m√•ske hentet fra Kaggle.

## [Quiz efter forel√¶sning](https://ff-quizzes.netlify.app/en/ai/quiz/28)

## Gennemgang & Selvstudie

L√¶s denne artikel om Word2Vec: [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf)

## [Opgave: Notebooks](assignment.md)

---

