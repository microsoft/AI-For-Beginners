# Rekurrente Neurale Netv√¶rk

## [Quiz f√∏r forel√¶sning](https://ff-quizzes.netlify.app/en/ai/quiz/31)

I de tidligere afsnit har vi brugt rige semantiske repr√¶sentationer af tekst og en simpel line√¶r klassifikator oven p√• embeddings. Denne arkitektur fanger den samlede betydning af ord i en s√¶tning, men tager ikke h√∏jde for **r√¶kkef√∏lgen** af ord, da aggregeringsoperationen oven p√• embeddings fjerner denne information fra den oprindelige tekst. Fordi disse modeller ikke kan modellere ords r√¶kkef√∏lge, kan de ikke l√∏se mere komplekse eller tvetydige opgaver som tekstgenerering eller sp√∏rgsm√•l-svar.

For at fange betydningen af tekstsekvenser skal vi bruge en anden neural netv√¶rksarkitektur, som kaldes et **rekurrent neuralt netv√¶rk**, eller RNN. I RNN sender vi vores s√¶tning gennem netv√¶rket √©n symbol ad gangen, og netv√¶rket producerer en **tilstand**, som vi derefter sender tilbage til netv√¶rket sammen med det n√¶ste symbol.

![RNN](../../../../../translated_images/da/rnn.27f5c29c53d727b5.webp)

> Billede af forfatteren

Givet inputsekvensen af tokens X<sub>0</sub>,...,X<sub>n</sub>, skaber RNN en sekvens af neurale netv√¶rksblokke og tr√¶ner denne sekvens end-to-end ved hj√¶lp af backpropagation. Hver netv√¶rksblok tager et par (X<sub>i</sub>,S<sub>i</sub>) som input og producerer S<sub>i+1</sub> som resultat. Den endelige tilstand S<sub>n</sub> eller (output Y<sub>n</sub>) sendes til en line√¶r klassifikator for at producere resultatet. Alle netv√¶rksblokke deler de samme v√¶gte og tr√¶nes end-to-end med √©n backpropagation-pass.

Fordi tilstandsvektorerne S<sub>0</sub>,...,S<sub>n</sub> sendes gennem netv√¶rket, er det i stand til at l√¶re de sekventielle afh√¶ngigheder mellem ord. For eksempel, n√•r ordet *ikke* optr√¶der et sted i sekvensen, kan det l√¶re at negere visse elementer inden for tilstandsvektoren, hvilket resulterer i negation.

> ‚úÖ Da v√¶gtene for alle RNN-blokke p√• billedet ovenfor er delt, kan det samme billede repr√¶senteres som √©n blok (til h√∏jre) med en rekurrent feedback-loop, der sender netv√¶rkets outputtilstand tilbage til input.

## Anatomi af en RNN-celle

Lad os se, hvordan en simpel RNN-celle er organiseret. Den accepterer den tidligere tilstand S<sub>i-1</sub> og det aktuelle symbol X<sub>i</sub> som input og skal producere outputtilstanden S<sub>i</sub> (og nogle gange er vi ogs√• interesserede i et andet output Y<sub>i</sub>, som i tilf√¶ldet med generative netv√¶rk).

En simpel RNN-celle har to v√¶gtmatricer indeni: √©n transformerer et inputsymbol (lad os kalde den W), og en anden transformerer en inputtilstand (H). I dette tilf√¶lde beregnes netv√¶rkets output som &sigma;(W&times;X<sub>i</sub>+H&times;S<sub>i-1</sub>+b), hvor &sigma; er aktiveringsfunktionen, og b er en ekstra bias.

<img alt="RNN Celle Anatomi" src="../../../../../translated_images/da/rnn-anatomy.79ee3f3920b3294b.webp" width="50%"/>

> Billede af forfatteren

I mange tilf√¶lde sendes inputtokens gennem embedding-laget, f√∏r de g√•r ind i RNN for at reducere dimensionaliteten. I dette tilf√¶lde, hvis dimensionen af inputvektorerne er *emb_size*, og tilstandsvektoren er *hid_size* - er st√∏rrelsen af W *emb_size*&times;*hid_size*, og st√∏rrelsen af H er *hid_size*&times;*hid_size*.

## Long Short Term Memory (LSTM)

Et af de st√∏rste problemer med klassiske RNN'er er det s√•kaldte **vanishing gradients**-problem. Fordi RNN'er tr√¶nes end-to-end i √©n backpropagation-pass, har de sv√¶rt ved at propagere fejl til de f√∏rste lag i netv√¶rket, og dermed kan netv√¶rket ikke l√¶re relationer mellem fjerne tokens. En af m√•derne at undg√• dette problem p√• er at introducere **eksplicit tilstandsh√•ndtering** ved hj√¶lp af s√•kaldte **gates**. Der er to velkendte arkitekturer af denne type: **Long Short Term Memory** (LSTM) og **Gated Relay Unit** (GRU).

![Billede der viser et eksempel p√• en long short term memory-celle](../../../../../lessons/5-NLP/16-RNN/images/long-short-term-memory-cell.svg)

> Billedkilde TBD

LSTM-netv√¶rket er organiseret p√• en m√•de, der ligner RNN, men der er to tilstande, der sendes fra lag til lag: den faktiske tilstand C og den skjulte vektor H. Ved hver enhed concateneres den skjulte vektor H<sub>i</sub> med input X<sub>i</sub>, og de styrer, hvad der sker med tilstanden C via **gates**. Hver gate er et neuralt netv√¶rk med sigmoid-aktivering (output i intervallet [0,1]), som kan betragtes som en bitmaske, n√•r den multipliceres med tilstandsvektoren. Der er f√∏lgende gates (fra venstre til h√∏jre p√• billedet ovenfor):

* **Forget gate** tager en skjult vektor og bestemmer, hvilke komponenter af vektoren C vi skal glemme, og hvilke vi skal lade passere.
* **Input gate** tager noget information fra input- og skjulte vektorer og inds√¶tter det i tilstanden.
* **Output gate** transformerer tilstanden via et line√¶rt lag med *tanh*-aktivering og v√¶lger derefter nogle af dens komponenter ved hj√¶lp af en skjult vektor H<sub>i</sub> for at producere en ny tilstand C<sub>i+1</sub>.

Komponenterne i tilstanden C kan betragtes som nogle flag, der kan t√¶ndes og slukkes. For eksempel, n√•r vi st√∏der p√• navnet *Alice* i sekvensen, kan vi antage, at det refererer til en kvindelig karakter og s√¶tte flaget i tilstanden, der angiver, at vi har et kvindeligt substantiv i s√¶tningen. N√•r vi senere st√∏der p√• s√¶tningen *og Tom*, vil vi s√¶tte flaget, der angiver, at vi har et flertal substantiv. Ved at manipulere tilstanden kan vi s√•ledes holde styr p√• de grammatiske egenskaber ved s√¶tningens dele.

> ‚úÖ En fremragende ressource til at forst√• LSTM's interne funktioner er denne fantastiske artikel [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) af Christopher Olah.

## Bidirektionelle og flerlags RNN'er

Vi har diskuteret rekurrente netv√¶rk, der opererer i √©n retning, fra begyndelsen af en sekvens til slutningen. Det virker naturligt, fordi det minder om den m√•de, vi l√¶ser og lytter til tale. Men da vi i mange praktiske tilf√¶lde har tilf√¶ldig adgang til inputsekvensen, kan det give mening at k√∏re rekurrent beregning i begge retninger. S√•danne netv√¶rk kaldes **bidirektionelle** RNN'er. N√•r vi arbejder med et bidirektionelt netv√¶rk, har vi brug for to skjulte tilstandsvektorer, √©n for hver retning.

Et rekurrent netv√¶rk, enten √©n-retnings eller bidirektionelt, fanger visse m√∏nstre inden for en sekvens og kan gemme dem i en tilstandsvektor eller sende dem til output. Ligesom med konvolutionelle netv√¶rk kan vi bygge et andet rekurrent lag oven p√• det f√∏rste for at fange h√∏jere niveau m√∏nstre og bygge videre p√• lav-niveau m√∏nstre, der er udtrukket af det f√∏rste lag. Dette f√∏rer os til begrebet **flerlags RNN**, som best√•r af to eller flere rekurrente netv√¶rk, hvor output fra det foreg√•ende lag sendes til det n√¶ste lag som input.

![Billede der viser et flerlags long-short-term-memory RNN](../../../../../translated_images/da/multi-layer-lstm.dd975e29bb2a59fe.webp)

*Billede fra [denne vidunderlige artikel](https://towardsdatascience.com/from-a-lstm-cell-to-a-multilayer-lstm-network-with-pytorch-2899eb5696f3) af Fernando L√≥pez*

## ‚úçÔ∏è √òvelser: Embeddings

Forts√¶t din l√¶ring i f√∏lgende notebooks:

* [RNN'er med PyTorch](RNNPyTorch.ipynb)
* [RNN'er med TensorFlow](RNNTF.ipynb)

## Konklusion

I denne enhed har vi set, at RNN'er kan bruges til sekvensklassifikation, men de kan faktisk h√•ndtere mange flere opgaver, s√•som tekstgenerering, maskinovers√¶ttelse og mere. Vi vil se n√¶rmere p√• disse opgaver i den n√¶ste enhed.

## üöÄ Udfordring

L√¶s noget litteratur om LSTM'er og overvej deres anvendelser:

- [Grid Long Short-Term Memory](https://arxiv.org/pdf/1507.01526v1.pdf)
- [Show, Attend and Tell: Neural Image Caption
Generation with Visual Attention](https://arxiv.org/pdf/1502.03044v2.pdf)

## [Quiz efter forel√¶sning](https://ff-quizzes.netlify.app/en/ai/quiz/32)

## Gennemgang & Selvstudie

- [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) af Christopher Olah.

## [Opgave: Notebooks](assignment.md)

---

