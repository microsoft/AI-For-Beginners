{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative netværk\n",
    "\n",
    "Recurrent Neural Networks (RNNs) og deres gatede cellevarianter såsom Long Short Term Memory Cells (LSTMs) og Gated Recurrent Units (GRUs) gav en mekanisme til sproglig modellering, dvs. de kan lære ordstilling og give forudsigelser for det næste ord i en sekvens. Dette gør det muligt for os at bruge RNNs til **generative opgaver**, såsom almindelig tekstgenerering, maskinoversættelse og endda billedtekstning.\n",
    "\n",
    "I RNN-arkitekturen, som vi diskuterede i den forrige enhed, producerede hver RNN-enhed den næste skjulte tilstand som output. Men vi kan også tilføje et andet output til hver rekurrent enhed, hvilket giver os mulighed for at generere en **sekvens** (som har samme længde som den oprindelige sekvens). Desuden kan vi bruge RNN-enheder, der ikke modtager input ved hvert trin, men blot tager en initial tilstandsvektor og derefter producerer en sekvens af outputs.\n",
    "\n",
    "I denne notebook vil vi fokusere på simple generative modeller, der hjælper os med at generere tekst. For enkelhedens skyld lad os bygge et **karakter-niveau netværk**, som genererer tekst bogstav for bogstav. Under træning skal vi tage en tekstkorpus og opdele den i bogstavsekvenser.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "\n",
    "ds_train, ds_test = tfds.load('ag_news_subset').values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opbygning af tegnordforråd\n",
    "\n",
    "For at opbygge et generativt netværk på tegnniveau skal vi dele tekst op i individuelle tegn i stedet for ord. `TextVectorization`-laget, som vi tidligere har brugt, kan ikke gøre dette, så vi har to muligheder:\n",
    "\n",
    "* Indlæse tekst manuelt og udføre tokenisering 'i hånden', som vist i [dette officielle Keras-eksempel](https://keras.io/examples/generative/lstm_character_level_text_generation/)\n",
    "* Bruge `Tokenizer`-klassen til tokenisering på tegnniveau.\n",
    "\n",
    "Vi vælger den anden mulighed. `Tokenizer` kan også bruges til at tokenisere i ord, så det bør være nemt at skifte fra tokenisering på tegnniveau til ordniveau.\n",
    "\n",
    "For at udføre tokenisering på tegnniveau skal vi angive parameteren `char_level=True`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(x):\n",
    "    return x['title']+' '+x['description']\n",
    "\n",
    "def tupelize(x):\n",
    "    return (extract_text(x),x['label'])\n",
    "\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(char_level=True,lower=False)\n",
    "tokenizer.fit_on_texts([x['title'].numpy().decode('utf-8') for x in ds_train])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi ønsker også at bruge en speciel token til at angive **slut på sekvens**, som vi vil kalde `<eos>`. Lad os tilføje den manuelt til ordforrådet:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "eos_token = len(tokenizer.word_index)+1\n",
    "tokenizer.word_index['<eos>'] = eos_token\n",
    "\n",
    "vocab_size = eos_token + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nu, for at kode tekst til sekvenser af tal, kan vi bruge:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[48, 2, 10, 10, 5, 44, 1, 25, 5, 8, 10, 13, 78]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences(['Hello, world!'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Træning af en generativ RNN til at generere titler\n",
    "\n",
    "Måden, vi vil træne RNN til at generere nyhedstitler, er som følger. Ved hvert trin tager vi en titel, som bliver fodret ind i en RNN, og for hvert inputtegn beder vi netværket om at generere det næste outputtegn:\n",
    "\n",
    "![Billede, der viser et eksempel på RNN-generering af ordet 'HELLO'.](../../../../../translated_images/da/rnn-generate.56c54afb52f9781d.webp)\n",
    "\n",
    "For det sidste tegn i vores sekvens beder vi netværket om at generere `<eos>`-token.\n",
    "\n",
    "Den største forskel mellem den generative RNN, vi bruger her, er, at vi tager output fra hvert trin i RNN'en og ikke kun fra den sidste celle. Dette kan opnås ved at angive parameteren `return_sequences` til RNN-cellen.\n",
    "\n",
    "Således vil input til netværket under træningen være en sekvens af kodede tegn af en vis længde, og output vil være en sekvens af samme længde, men forskudt med ét element og afsluttet med `<eos>`. Minibatch vil bestå af flere sådanne sekvenser, og vi vil være nødt til at bruge **padding** for at justere alle sekvenser.\n",
    "\n",
    "Lad os oprette funktioner, der vil transformere datasættet for os. Fordi vi ønsker at padde sekvenser på minibatch-niveau, vil vi først batch'e datasættet ved at kalde `.batch()`, og derefter `map` det for at udføre transformationen. Så transformationsfunktionen vil tage en hel minibatch som parameter:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def title_batch(x):\n",
    "    x = [t.numpy().decode('utf-8') for t in x]\n",
    "    z = tokenizer.texts_to_sequences(x)\n",
    "    z = tf.keras.preprocessing.sequence.pad_sequences(z)\n",
    "    return tf.one_hot(z,vocab_size), tf.one_hot(tf.concat([z[:,1:],tf.constant(eos_token,shape=(len(z),1))],axis=1),vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et par vigtige ting, vi gør her:\n",
    "* Vi starter med at udtrække den faktiske tekst fra string-tensoren\n",
    "* `text_to_sequences` konverterer listen af strenge til en liste af heltals-tensorer\n",
    "* `pad_sequences` udfylder disse tensorer til deres maksimale længde\n",
    "* Til sidst one-hot-enkoder vi alle tegnene og udfører også forskydning og tilføjelse af `<eos>`. Vi vil snart se, hvorfor vi har brug for one-hot-enkodede tegn\n",
    "\n",
    "Dog er denne funktion **Pythonisk**, dvs. den kan ikke automatisk oversættes til Tensorflow's beregningsgraf. Vi vil få fejl, hvis vi forsøger at bruge denne funktion direkte i `Dataset.map`-funktionen. Vi er nødt til at omslutte dette Pythoniske kald ved at bruge `py_function`-wrapperen:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def title_batch_fn(x):\n",
    "    x = x['title']\n",
    "    a,b = tf.py_function(title_batch,inp=[x],Tout=(tf.float32,tf.float32))\n",
    "    return a,b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note**: Det kan virke lidt for komplekst at skelne mellem Pythoniske og Tensorflow-transformationsfunktioner, og du undrer dig måske over, hvorfor vi ikke transformerer datasættet ved hjælp af standard Python-funktioner, før vi sender det til `fit`. Selvom dette bestemt kan lade sig gøre, har brugen af `Dataset.map` en stor fordel, fordi datatransformationspipeline udføres ved hjælp af Tensorflows beregningsgraf, som drager fordel af GPU-beregninger og minimerer behovet for at overføre data mellem CPU/GPU.\n",
    "\n",
    "Nu kan vi bygge vores generatornetværk og begynde træningen. Det kan baseres på enhver rekurrent celle, som vi diskuterede i den foregående enhed (simpel, LSTM eller GRU). I vores eksempel vil vi bruge LSTM.\n",
    "\n",
    "Da netværket tager tegn som input, og ordforrådsstørrelsen er ret lille, har vi ikke brug for et embedding-lag; one-hot-kodet input kan direkte sendes ind i LSTM-cellen. Outputlaget vil være en `Dense` klassifikator, der konverterer LSTM-output til one-hot-kodede token-numre.\n",
    "\n",
    "Derudover, da vi arbejder med sekvenser af variabel længde, kan vi bruge `Masking`-laget til at skabe en maske, der ignorerer den polstrede del af strengen. Dette er ikke strengt nødvendigt, da vi ikke er særligt interesserede i alt, der går ud over `<eos>`-tokenet, men vi vil bruge det for at få noget erfaring med denne lagtype. `input_shape` vil være `(None, vocab_size)`, hvor `None` angiver sekvensen af variabel længde, og outputformen er også `(None, vocab_size)`, som du kan se fra `summary`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "masking (Masking)            (None, None, 84)          0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, None, 128)         109056    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, None, 84)          10836     \n",
      "=================================================================\n",
      "Total params: 119,892\n",
      "Trainable params: 119,892\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "15000/15000 [==============================] - 229s 15ms/step - loss: 1.5385\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fa40c1245e0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Masking(input_shape=(None,vocab_size)),\n",
    "    keras.layers.LSTM(128,return_sequences=True),\n",
    "    keras.layers.Dense(vocab_size,activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy')\n",
    "\n",
    "model.fit(ds_train.batch(8).map(title_batch_fn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generering af output\n",
    "\n",
    "Nu hvor vi har trænet modellen, vil vi bruge den til at generere noget output. Først og fremmest har vi brug for en måde at dekode tekst repræsenteret ved en sekvens af token-numre. For at gøre dette kunne vi bruge funktionen `tokenizer.sequences_to_texts`; dog fungerer den ikke godt med tokenisering på tegnniveau. Derfor vil vi tage en ordbog af tokens fra tokenizer (kaldet `word_index`), opbygge et omvendt kort og skrive vores egen dekoderingsfunktion:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_map = {val:key for key, val in tokenizer.word_index.items()}\n",
    "\n",
    "def decode(x):\n",
    "    return ''.join([reverse_map[t] for t in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nu begynder vi med en streng `start`, koder den til en sekvens `inp`, og derefter kalder vi vores netværk på hvert trin for at udlede det næste tegn.\n",
    "\n",
    "Outputtet fra netværket `out` er en vektor med `vocab_size` elementer, der repræsenterer sandsynlighederne for hver token, og vi kan finde det mest sandsynlige token-nummer ved at bruge `argmax`. Derefter tilføjer vi dette tegn til den genererede liste af tokens og fortsætter med genereringen. Denne proces med at generere ét tegn gentages `size` gange for at generere det ønskede antal tegn, og vi afslutter tidligt, når `eos_token` optræder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Today #39;s lead to strike for the strike for the strike for the strike (AFP)'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate(model,size=100,start='Today '):\n",
    "        inp = tokenizer.texts_to_sequences([start])[0]\n",
    "        chars = inp\n",
    "        for i in range(size):\n",
    "            out = model(tf.expand_dims(tf.one_hot(inp,vocab_size),0))[0][-1]\n",
    "            nc = tf.argmax(out)\n",
    "            if nc==eos_token:\n",
    "                break\n",
    "            chars.append(nc.numpy())\n",
    "            inp = inp+[nc]\n",
    "        return decode(chars)\n",
    "    \n",
    "generate(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Udtagning af output under træning\n",
    "\n",
    "Da vi ikke har nogen nyttige målepunkter som *nøjagtighed*, er den eneste måde, vi kan se, at vores model bliver bedre, ved at **udtage** genererede strenge under træningen. For at gøre dette vil vi bruge **callbacks**, dvs. funktioner, som vi kan give videre til `fit`-funktionen, og som vil blive kaldt periodisk under træningen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "15000/15000 [==============================] - 226s 15ms/step - loss: 1.2703\n",
      "Today #39;s a lead in the company for the strike\n",
      "Epoch 2/3\n",
      "15000/15000 [==============================] - 227s 15ms/step - loss: 1.2057\n",
      "Today #39;s the Market Service on Security Start (AP)\n",
      "Epoch 3/3\n",
      "15000/15000 [==============================] - 226s 15ms/step - loss: 1.1752\n",
      "Today #39;s a line on the strike to start for the start\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fa40c74e3d0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampling_callback = keras.callbacks.LambdaCallback(\n",
    "  on_epoch_end = lambda batch, logs: print(generate(model))\n",
    ")\n",
    "\n",
    "model.fit(ds_train.batch(8).map(title_batch_fn),callbacks=[sampling_callback],epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dette eksempel genererer allerede ret god tekst, men det kan forbedres yderligere på flere måder:\n",
    "\n",
    "* **Mere tekst**. Vi har kun brugt titler til vores opgave, men du kan eksperimentere med fuld tekst. Husk, at RNN'er ikke er så gode til at håndtere lange sekvenser, så det giver mening enten at opdele dem i kortere sætninger eller altid at træne på en fast sekvenslængde af en foruddefineret værdi `num_chars` (for eksempel 256). Du kan prøve at ændre eksemplet ovenfor til en sådan arkitektur ved at bruge [den officielle Keras-tutorial](https://keras.io/examples/generative/lstm_character_level_text_generation/) som inspiration.\n",
    "\n",
    "* **Multilags LSTM**. Det giver mening at prøve 2 eller 3 lag af LSTM-celler. Som vi nævnte i den tidligere enhed, udtrækker hvert lag af LSTM visse mønstre fra teksten, og i tilfælde af en generator på tegnniveau kan vi forvente, at det laveste LSTM-lag er ansvarligt for at udtrække stavelser, og de højere lag - for ord og ordkombinationer. Dette kan nemt implementeres ved at sende parameteren for antal lag til LSTM-konstruktøren.\n",
    "\n",
    "* Du kan også eksperimentere med **GRU-enheder** og se, hvilke der fungerer bedre, samt med **forskellige størrelser på skjulte lag**. Et for stort skjult lag kan resultere i overtilpasning (f.eks. netværket vil lære præcis tekst), og en mindre størrelse kan muligvis ikke producere gode resultater.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blød tekstgenerering og temperatur\n",
    "\n",
    "I den tidligere definition af `generate` valgte vi altid det tegn med den højeste sandsynlighed som det næste tegn i den genererede tekst. Dette resulterede ofte i, at teksten \"cyklede\" mellem de samme tegnsekvenser igen og igen, som i dette eksempel:\n",
    "```\n",
    "today of the second the company and a second the company ...\n",
    "```\n",
    "\n",
    "Men hvis vi ser på sandsynlighedsfordelingen for det næste tegn, kan det være, at forskellen mellem de højeste sandsynligheder ikke er særlig stor, f.eks. kan ét tegn have en sandsynlighed på 0,2, mens et andet har 0,19 osv. For eksempel, når vi leder efter det næste tegn i sekvensen '*play*', kan det næste tegn lige så godt være et mellemrum eller **e** (som i ordet *player*).\n",
    "\n",
    "Dette fører os til konklusionen, at det ikke altid er \"retfærdigt\" at vælge tegnet med den højeste sandsynlighed, fordi det at vælge det næsthøjeste stadig kan føre til meningsfuld tekst. Det er mere fornuftigt at **udvælge** tegn fra sandsynlighedsfordelingen givet af netværkets output.\n",
    "\n",
    "Denne udvælgelse kan udføres ved hjælp af funktionen `np.multinomial`, som implementerer den såkaldte **multinomialfordeling**. En funktion, der implementerer denne **bløde** tekstgenerering, er defineret nedenfor:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Temperature = 0.3\n",
      "Today #39;s strike #39; to start at the store return\n",
      "On Sunday PO to Be Data Profit Up (Reuters)\n",
      "Moscow, SP wins straight to the Microsoft #39;s control of the space start\n",
      "President olding of the blast start for the strike to pay &lt;b&gt;...&lt;/b&gt;\n",
      "Little red riding hood ficed to the spam countered in European &lt;b&gt;...&lt;/b&gt;\n",
      "\n",
      "--- Temperature = 0.8\n",
      "Today countie strikes ryder missile faces food market blut\n",
      "On Sunday collores lose-toppy of sale of Bullment in &lt;b&gt;...&lt;/b&gt;\n",
      "Moscow, IBM Diffeiting in Afghan Software Hotels (Reuters)\n",
      "President Ol Luster for Profit Peaced Raised (AP)\n",
      "Little red riding hood dace on depart talks #39; bank up\n",
      "\n",
      "--- Temperature = 1.0\n",
      "Today wits House buiting debate fixes #39; supervice stake again\n",
      "On Sunday arling digital poaching In for level\n",
      "Moscow, DS Up 7, Top Proble Protest Caprey Mamarian Strike\n",
      "President teps help of roubler stepted lessabul-Dhalitics (AFP)\n",
      "Little red riding hood signs on cash in Carter-youb\n",
      "\n",
      "--- Temperature = 1.3\n",
      "Today wits flawer ro, pSIA figat's co DroftwavesIs Talo up\n",
      "On Sunday hround elitwing wint EU Powerburlinetien\n",
      "Moscow, Bazz #39;s sentries olymen winnelds' next for Olympite Huc?\n",
      "President lost securitys from power Elections in Smiltrials\n",
      "Little red riding hood vides profit, exponituity, profitmainalist-at said listers\n",
      "\n",
      "--- Temperature = 1.8\n",
      "Today #39;It: He deat: N.KA Asside\n",
      "On Sunday i arry Par aldeup patient Wo stele1\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-db32367a0feb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\n--- Temperature = {i}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerate_soft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-33-db32367a0feb>\u001b[0m in \u001b[0;36mgenerate_soft\u001b[0;34m(model, size, start, temperature)\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mchars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'Today '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'On Sunday '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Moscow, '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'President '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Little red riding hood '\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-3f5fa6130b1d>\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mreverse_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-3f5fa6130b1d>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mreverse_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "def generate_soft(model,size=100,start='Today ',temperature=1.0):\n",
    "        inp = tokenizer.texts_to_sequences([start])[0]\n",
    "        chars = inp\n",
    "        for i in range(size):\n",
    "            out = model(tf.expand_dims(tf.one_hot(inp,vocab_size),0))[0][-1]\n",
    "            probs = tf.exp(tf.math.log(out)/temperature).numpy().astype(np.float64)\n",
    "            probs = probs/np.sum(probs)\n",
    "            nc = np.argmax(np.random.multinomial(1,probs,1))\n",
    "            if nc==eos_token:\n",
    "                break\n",
    "            chars.append(nc)\n",
    "            inp = inp+[nc]\n",
    "        return decode(chars)\n",
    "\n",
    "words = ['Today ','On Sunday ','Moscow, ','President ','Little red riding hood ']\n",
    "    \n",
    "for i in [0.3,0.8,1.0,1.3,1.8]:\n",
    "    print(f\"\\n--- Temperature = {i}\")\n",
    "    for j in range(5):\n",
    "        print(generate_soft(model,size=300,start=words[j],temperature=i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi har introduceret en ekstra parameter kaldet **temperatur**, som bruges til at angive, hvor strengt vi skal holde os til den højeste sandsynlighed. Hvis temperaturen er 1,0, udfører vi retfærdig multinomial sampling, og når temperaturen går mod uendelig - bliver alle sandsynligheder lige, og vi vælger tilfældigt den næste karakter. I eksemplet nedenfor kan vi observere, at teksten bliver meningsløs, når vi øger temperaturen for meget, og den ligner \"cyklet\" hårdt-genereret tekst, når den nærmer sig 0.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Ansvarsfraskrivelse**:  \nDette dokument er blevet oversat ved hjælp af AI-oversættelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selvom vi bestræber os på nøjagtighed, skal du være opmærksom på, at automatiserede oversættelser kan indeholde fejl eller unøjagtigheder. Det originale dokument på dets oprindelige sprog bør betragtes som den autoritative kilde. For kritisk information anbefales professionel menneskelig oversættelse. Vi påtager os ikke ansvar for eventuelle misforståelser eller fejltolkninger, der opstår som følge af brugen af denne oversættelse.\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "16af2a8bbb083ea23e5e41c7f5787656b2ce26968575d8763f2c4b17f9cd711f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('py38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "coopTranslator": {
   "original_hash": "9fbb7d5fda708537649f71f5f646fcde",
   "translation_date": "2025-08-28T17:25:35+00:00",
   "source_file": "lessons/5-NLP/17-GenerativeNetworks/GenerativeTF.ipynb",
   "language_code": "da"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}