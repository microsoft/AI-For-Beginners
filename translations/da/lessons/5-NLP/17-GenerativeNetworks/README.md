# Generative netv√¶rk

## [Quiz f√∏r forel√¶sning](https://ff-quizzes.netlify.app/en/ai/quiz/33)

Recurrent Neural Networks (RNNs) og deres gated cell-varianter s√•som Long Short Term Memory Cells (LSTMs) og Gated Recurrent Units (GRUs) giver en mekanisme til sproglig modellering, da de kan l√¶re ords r√¶kkef√∏lge og forudsige det n√¶ste ord i en sekvens. Dette g√∏r det muligt at bruge RNNs til **generative opgaver**, s√•som almindelig tekstgenerering, maskinovers√¶ttelse og endda billedbeskrivelse.

> ‚úÖ T√¶nk over alle de gange, du har haft gavn af generative opgaver som tekstfuldf√∏relse, mens du skriver. Unders√∏g dine yndlingsapplikationer for at se, om de har anvendt RNNs.

I RNN-arkitekturen, som vi diskuterede i den forrige enhed, producerede hver RNN-enhed den n√¶ste skjulte tilstand som output. Men vi kan ogs√• tilf√∏je et andet output til hver rekurrent enhed, hvilket giver os mulighed for at generere en **sekvens** (som er lige s√• lang som den oprindelige sekvens). Desuden kan vi bruge RNN-enheder, der ikke modtager input ved hvert trin, men blot tager en initial tilstandsvektor og derefter producerer en sekvens af outputs.

Dette muligg√∏r forskellige neurale arkitekturer, som vist p√• billedet nedenfor:

![Billede, der viser almindelige m√∏nstre for rekurrente neurale netv√¶rk.](../../../../../translated_images/da/unreasonable-effectiveness-of-rnn.541ead816778f42d.webp)

> Billede fra blogindl√¶gget [Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) af [Andrej Karpaty](http://karpathy.github.io/)

* **One-to-one** er et traditionelt neuralt netv√¶rk med √©n input og √©t output
* **One-to-many** er en generativ arkitektur, der modtager √©n inputv√¶rdi og genererer en sekvens af outputv√¶rdier. For eksempel, hvis vi vil tr√¶ne et **billedbeskrivende** netv√¶rk, der genererer en tekstbeskrivelse af et billede, kan vi tage et billede som input, lade det passere gennem en CNN for at opn√• dets skjulte tilstand og derefter lade en rekurrent k√¶de generere beskrivelsen ord for ord.
* **Many-to-one** svarer til de RNN-arkitekturer, vi beskrev i den forrige enhed, s√•som tekstklassifikation.
* **Many-to-many**, eller **sequence-to-sequence**, svarer til opgaver som **maskinovers√¶ttelse**, hvor vi f√∏rst lader en RNN samle al information fra inputsekvensen i den skjulte tilstand, og en anden RNN-k√¶de udfolder denne tilstand til outputsekvensen.

I denne enhed vil vi fokusere p√• simple generative modeller, der hj√¶lper os med at generere tekst. For enkelhedens skyld vil vi bruge tokenisering p√• tegnniveau.

Vi vil tr√¶ne denne RNN til at generere tekst trin for trin. Ved hvert trin tager vi en sekvens af tegn med l√¶ngden `nchars` og beder netv√¶rket om at generere det n√¶ste outputtegn for hvert inputtegn:

![Billede, der viser et eksempel p√• RNN-generering af ordet 'HELLO'.](../../../../../translated_images/da/rnn-generate.56c54afb52f9781d.webp)

N√•r vi genererer tekst (under inferens), starter vi med en **prompt**, som sendes gennem RNN-celler for at generere dens mellemliggende tilstand, og derefter starter genereringen fra denne tilstand. Vi genererer √©t tegn ad gangen og sender tilstanden og det genererede tegn til en anden RNN-celle for at generere det n√¶ste, indtil vi har genereret nok tegn.

<img src="../../../../../translated_images/da/rnn-generate-inf.5168dc65e0370eea.webp" width="60%"/>

> Billede af forfatteren

## ‚úçÔ∏è √òvelser: Generative netv√¶rk

Forts√¶t din l√¶ring i f√∏lgende notebooks:

* [Generative netv√¶rk med PyTorch](GenerativePyTorch.ipynb)
* [Generative netv√¶rk med TensorFlow](GenerativeTF.ipynb)

## Bl√∏d tekstgenerering og temperatur

Outputtet fra hver RNN-celle er en sandsynlighedsfordeling af tegn. Hvis vi altid v√¶lger det tegn med den h√∏jeste sandsynlighed som det n√¶ste tegn i den genererede tekst, kan teksten ofte ende med at "cirkulere" mellem de samme tegnsekvenser igen og igen, som i dette eksempel:

```
today of the second the company and a second the company ...
```

Men hvis vi ser p√• sandsynlighedsfordelingen for det n√¶ste tegn, kan det v√¶re, at forskellen mellem de h√∏jeste sandsynligheder ikke er stor, f.eks. kan √©t tegn have sandsynligheden 0.2, og et andet 0.19 osv. For eksempel, n√•r vi leder efter det n√¶ste tegn i sekvensen '*play*', kan det n√¶ste tegn lige s√• godt v√¶re enten et mellemrum eller **e** (som i ordet *player*).

Dette leder os til konklusionen, at det ikke altid er "retf√¶rdigt" at v√¶lge tegnet med den h√∏jeste sandsynlighed, da det at v√¶lge det n√¶sth√∏jeste stadig kan f√∏re til meningsfuld tekst. Det er mere fornuftigt at **udv√¶lge** tegn fra sandsynlighedsfordelingen givet af netv√¶rkets output. Vi kan ogs√• bruge en parameter, **temperatur**, der kan udj√¶vne sandsynlighedsfordelingen, hvis vi √∏nsker at tilf√∏je mere tilf√¶ldighed, eller g√∏re den stejlere, hvis vi vil holde os t√¶ttere til de tegn med h√∏jeste sandsynlighed.

Unders√∏g, hvordan denne bl√∏de tekstgenerering er implementeret i de notebooks, der er linket ovenfor.

## Konklusion

Selvom tekstgenerering kan v√¶re nyttig i sig selv, kommer de st√∏rste fordele fra evnen til at generere tekst ved hj√¶lp af RNNs fra en initial feature-vektor. For eksempel bruges tekstgenerering som en del af maskinovers√¶ttelse (sequence-to-sequence, i dette tilf√¶lde bruges tilstandsvektoren fra *encoder* til at generere eller *decode* den oversatte besked) eller til at generere tekstbeskrivelser af et billede (i hvilket tilf√¶lde feature-vektoren ville komme fra CNN-ekstraktoren).

## üöÄ Udfordring

Tag nogle lektioner p√• Microsoft Learn om dette emne

* Tekstgenerering med [PyTorch](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-pytorch/6-generative-networks/?WT.mc_id=academic-77998-cacaste)/[TensorFlow](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-tensorflow/5-generative-networks/?WT.mc_id=academic-77998-cacaste)

## [Quiz efter forel√¶sning](https://ff-quizzes.netlify.app/en/ai/quiz/34)

## Gennemgang & Selvstudie

Her er nogle artikler til at udvide din viden

* Forskellige tilgange til tekstgenerering med Markov Chain, LSTM og GPT-2: [blogindl√¶g](https://towardsdatascience.com/text-generation-gpt-2-lstm-markov-chain-9ea371820e1e)
* Eksempel p√• tekstgenerering i [Keras dokumentation](https://keras.io/examples/generative/lstm_character_level_text_generation/)

## [Opgave](lab/README.md)

Vi har set, hvordan man genererer tekst tegn for tegn. I laboratoriet vil du udforske tekstgenerering p√• ordniveau.

---

