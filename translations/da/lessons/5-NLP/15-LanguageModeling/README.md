# Sproglig Modellering

Semantiske indlejringer, s친som Word2Vec og GloVe, er faktisk et f칮rste skridt mod **sproglig modellering** - at skabe modeller, der p친 en eller anden m친de *forst친r* (eller *repr칝senterer*) sprogets natur.

## [Quiz f칮r lektionen](https://ff-quizzes.netlify.app/en/ai/quiz/29)

Hovedideen bag sproglig modellering er at tr칝ne dem p친 ulabelerede datas칝t p친 en usuperviseret m친de. Dette er vigtigt, fordi vi har enorme m칝ngder ulabeleret tekst til r친dighed, mens m칝ngden af labeleret tekst altid vil v칝re begr칝nset af den indsats, vi kan bruge p친 at labelere. Oftest kan vi bygge sproglige modeller, der kan **forudsige manglende ord** i teksten, fordi det er nemt at maskere et tilf칝ldigt ord i teksten og bruge det som en tr칝ningspr칮ve.

## Tr칝ning af Indlejringer

I vores tidligere eksempler brugte vi fortr칝nede semantiske indlejringer, men det er interessant at se, hvordan disse indlejringer kan tr칝nes. Der er flere mulige id칠er, der kan anvendes:

* **N-Gram** sproglig modellering, hvor vi forudsiger et token ved at kigge p친 N tidligere tokens (N-gram).
* **Continuous Bag-of-Words** (CBoW), hvor vi forudsiger det midterste token $W_0$ i en token-sekvens $W_{-N}$, ..., $W_N$.
* **Skip-gram**, hvor vi forudsiger et s칝t af nabotokens {$W_{-N},\dots, W_{-1}, W_1,\dots, W_N$} ud fra det midterste token $W_0$.

![billede fra artikel om konvertering af ord til vektorer](../../../../../translated_images/da/example-algorithms-for-converting-words-to-vectors.fbe9207a726922f6.webp)

> Billede fra [denne artikel](https://arxiv.org/pdf/1301.3781.pdf)

## 九꽲잺 Eksempel Notebooks: Tr칝ning af CBoW-model

Forts칝t din l칝ring i f칮lgende notebooks:

* [Tr칝ning af CBoW Word2Vec med TensorFlow](CBoW-TF.ipynb)
* [Tr칝ning af CBoW Word2Vec med PyTorch](CBoW-PyTorch.ipynb)

## Konklusion

I den forrige lektion har vi set, at ordindlejringer virker som magi! Nu ved vi, at tr칝ning af ordindlejringer ikke er en s칝rlig kompleks opgave, og vi b칮r v칝re i stand til at tr칝ne vores egne ordindlejringer til dom칝nespecifik tekst, hvis det er n칮dvendigt.

## [Quiz efter lektionen](https://ff-quizzes.netlify.app/en/ai/quiz/30)

## Gennemgang & Selvstudie

* [Officiel PyTorch-tutorial om sproglig modellering](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html).
* [Officiel TensorFlow-tutorial om tr칝ning af Word2Vec-model](https://www.TensorFlow.org/tutorials/text/word2vec).
* Brug af **gensim**-frameworket til at tr칝ne de mest almindeligt anvendte indlejringer med f친 linjer kode er beskrevet [i denne dokumentation](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html).

## 游 [Opgave: Tr칝n Skip-Gram Model](lab/README.md)

I laboratoriet udfordrer vi dig til at 칝ndre koden fra denne lektion for at tr칝ne en skip-gram model i stedet for CBoW. [L칝s detaljerne](lab/README.md)

---

