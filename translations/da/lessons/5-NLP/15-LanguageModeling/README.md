<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "31b46ba1f3aa78578134d4829f88be53",
  "translation_date": "2025-08-28T15:53:31+00:00",
  "source_file": "lessons/5-NLP/15-LanguageModeling/README.md",
  "language_code": "da"
}
-->
# Sprogsmodellering

Semantiske indlejringer, s친som Word2Vec og GloVe, er faktisk et f칮rste skridt mod **sprogsmodellering** - at skabe modeller, der p친 en eller anden m친de *forst친r* (eller *repr칝senterer*) sprogets natur.

## [Quiz f칮r lektionen](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/115)

Hovedideen bag sprogsmodellering er at tr칝ne dem p친 ulabelerede datas칝t p친 en usuperviseret m친de. Dette er vigtigt, fordi vi har enorme m칝ngder ulabeleret tekst tilg칝ngelig, mens m칝ngden af labeleret tekst altid vil v칝re begr칝nset af den indsats, vi kan bruge p친 at labelere. Oftest kan vi bygge sprogsmodeller, der kan **forudsige manglende ord** i teksten, fordi det er nemt at maskere et tilf칝ldigt ord i teksten og bruge det som en tr칝ningspr칮ve.

## Tr칝ning af indlejringer

I vores tidligere eksempler brugte vi fortr칝nede semantiske indlejringer, men det er interessant at se, hvordan disse indlejringer kan tr칝nes. Der er flere mulige id칠er, der kan bruges:

* **N-Gram** sprogsmodellering, hvor vi forudsiger et token ved at kigge p친 N tidligere tokens (N-gram).
* **Continuous Bag-of-Words** (CBoW), hvor vi forudsiger det midterste token $W_0$ i en token-sekvens $W_{-N}$, ..., $W_N$.
* **Skip-gram**, hvor vi forudsiger et s칝t af nabotokens {$W_{-N},\dots, W_{-1}, W_1,\dots, W_N$} ud fra det midterste token $W_0$.

![billede fra artikel om konvertering af ord til vektorer](../../../../../translated_images/example-algorithms-for-converting-words-to-vectors.fbe9207a726922f6f0f5de66427e8a6eda63809356114e28fb1fa5f4a83ebda7.da.png)

> Billede fra [denne artikel](https://arxiv.org/pdf/1301.3781.pdf)

## 九꽲잺 Eksempel Notebooks: Tr칝ning af CBoW-model

Forts칝t din l칝ring i f칮lgende notebooks:

* [Tr칝ning af CBoW Word2Vec med TensorFlow](CBoW-TF.ipynb)
* [Tr칝ning af CBoW Word2Vec med PyTorch](CBoW-PyTorch.ipynb)

## Konklusion

I den tidligere lektion har vi set, at ordindlejringer virker som magi! Nu ved vi, at tr칝ning af ordindlejringer ikke er en s칝rlig kompleks opgave, og vi b칮r v칝re i stand til at tr칝ne vores egne ordindlejringer til dom칝nespecifik tekst, hvis det er n칮dvendigt.

## [Quiz efter lektionen](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/215)

## Gennemgang & Selvstudie

* [Officiel PyTorch-tutorial om sprogsmodellering](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html).
* [Officiel TensorFlow-tutorial om tr칝ning af Word2Vec-model](https://www.TensorFlow.org/tutorials/text/word2vec).
* Brug af **gensim**-frameworket til at tr칝ne de mest almindeligt anvendte indlejringer med f친 linjer kode er beskrevet [i denne dokumentation](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html).

## 游 [Opgave: Tr칝n Skip-Gram Model](lab/README.md)

I laboratoriet udfordrer vi dig til at 칝ndre koden fra denne lektion for at tr칝ne en skip-gram model i stedet for CBoW. [L칝s detaljerne](lab/README.md)

---

**Ansvarsfraskrivelse**:  
Dette dokument er blevet oversat ved hj칝lp af AI-overs칝ttelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selvom vi bestr칝ber os p친 n칮jagtighed, skal du v칝re opm칝rksom p친, at automatiserede overs칝ttelser kan indeholde fejl eller un칮jagtigheder. Det originale dokument p친 dets oprindelige sprog b칮r betragtes som den autoritative kilde. For kritisk information anbefales professionel menneskelig overs칝ttelse. Vi er ikke ansvarlige for eventuelle misforst친elser eller fejltolkninger, der m친tte opst친 som f칮lge af brugen af denne overs칝ttelse.