# Repr칝sentation af tekst som tensorer

## [Quiz f칮r forel칝sning](https://ff-quizzes.netlify.app/en/ai/quiz/25)

## Tekstklassifikation

I den f칮rste del af dette afsnit vil vi fokusere p친 opgaven **tekstklassifikation**. Vi vil bruge [AG News](https://www.kaggle.com/amananandrai/ag-news-classification-dataset)-datas칝ttet, som indeholder nyhedsartikler som f칮lgende:

* Kategori: Sci/Tech
* Titel: Ky. Firma vinder tilskud til at studere peptider (AP)
* Br칮dtekst: AP - Et firma grundlagt af en kemiforsker ved University of Louisville vandt et tilskud til at udvikle...

Vores m친l vil v칝re at klassificere nyhedsartiklen i en af kategorierne baseret p친 teksten.

## Repr칝sentation af tekst

Hvis vi vil l칮se opgaver inden for Natural Language Processing (NLP) med neurale netv칝rk, skal vi finde en m친de at repr칝sentere tekst som tensorer. Computere repr칝senterer allerede teksttegn som tal, der kortl칝gges til skrifttyper p친 din sk칝rm ved hj칝lp af kodninger som ASCII eller UTF-8.

<img alt="Billede, der viser diagrammet, der kortl칝gger et tegn til en ASCII- og bin칝r repr칝sentation" src="../../../../../translated_images/da/ascii-character-map.18ed6aa7f3b0a7ff.webp" width="50%"/>

> [Billedkilde](https://www.seobility.net/en/wiki/ASCII)

Som mennesker forst친r vi, hvad hvert bogstav **repr칝senterer**, og hvordan alle tegnene samles for at danne ordene i en s칝tning. Computere har dog ikke en s친dan forst친else af sig selv, og det neurale netv칝rk skal l칝re betydningen under tr칝ning.

Derfor kan vi bruge forskellige tilgange til at repr칝sentere tekst:

* **Tegnniveau-repr칝sentation**, hvor vi repr칝senterer tekst ved at behandle hvert tegn som et tal. Givet at vi har *C* forskellige tegn i vores tekstkorpus, vil ordet *Hello* blive repr칝senteret af en 5x*C* tensor. Hvert bogstav svarer til en tensorkolonne i one-hot encoding.
* **Ordniveau-repr칝sentation**, hvor vi opretter et **ordforr친d** af alle ord i vores tekst og derefter repr칝senterer ord ved hj칝lp af one-hot encoding. Denne tilgang er p친 en m친de bedre, fordi hvert bogstav i sig selv ikke har meget betydning, og ved at bruge h칮jere niveau semantiske begreber - ord - forenkler vi opgaven for det neurale netv칝rk. Men p친 grund af den store ordbogsst칮rrelse skal vi h친ndtere h칮jdimensionelle sparse tensorer.

Uanset repr칝sentationen skal vi f칮rst konvertere teksten til en sekvens af **tokens**, hvor en token enten er et tegn, et ord eller nogle gange endda en del af et ord. Derefter konverterer vi tokenen til et tal, typisk ved hj칝lp af **ordforr친d**, og dette tal kan f칮res ind i et neuralt netv칝rk ved hj칝lp af one-hot encoding.

## N-Grams

I naturligt sprog kan den pr칝cise betydning af ord kun bestemmes i kontekst. For eksempel er betydningerne af *neural network* og *fishing network* helt forskellige. En af m친derne at tage dette i betragtning er at bygge vores model p친 par af ord og betragte ordpar som separate tokens i ordforr친det. P친 denne m친de vil s칝tningen *I like to go fishing* blive repr칝senteret af f칮lgende sekvens af tokens: *I like*, *like to*, *to go*, *go fishing*. Problemet med denne tilgang er, at ordbogsst칮rrelsen vokser betydeligt, og kombinationer som *go fishing* og *go shopping* pr칝senteres af forskellige tokens, som ikke deler nogen semantisk lighed p친 trods af det samme verbum.

I nogle tilf칝lde kan vi overveje at bruge tri-grams -- kombinationer af tre ord -- ogs친. Derfor kaldes denne tilgang ofte **n-grams**. Det giver ogs친 mening at bruge n-grams med tegnniveau-repr칝sentation, hvor n-grams groft sagt svarer til forskellige stavelser.

## Bag-of-Words og TF/IDF

N친r vi l칮ser opgaver som tekstklassifikation, skal vi kunne repr칝sentere tekst med 칠n vektor af fast st칮rrelse, som vi vil bruge som input til den endelige t칝tte klassifikator. En af de enkleste m친der at g칮re dette p친 er at kombinere alle individuelle ordrepr칝sentationer, f.eks. ved at l칝gge dem sammen. Hvis vi l칝gger one-hot encodings af hvert ord sammen, ender vi med en vektor af frekvenser, der viser, hvor mange gange hvert ord optr칝der i teksten. En s친dan repr칝sentation af tekst kaldes **bag of words** (BoW).

<img src="../../../../../translated_images/da/bow.3811869cff59368d.webp" width="90%"/>

> Billede af forfatteren

En BoW repr칝senterer i bund og grund, hvilke ord der optr칝der i teksten, og i hvilke m칝ngder, hvilket kan v칝re en god indikator for, hvad teksten handler om. For eksempel vil en nyhedsartikel om politik sandsynligvis indeholde ord som *pr칝sident* og *land*, mens en videnskabelig publikation ville have noget som *collider*, *opdaget* osv. S친 ordfrekvenser kan i mange tilf칝lde v칝re en god indikator for tekstens indhold.

Problemet med BoW er, at visse almindelige ord, s친som *og*, *er* osv., optr칝der i de fleste tekster og har de h칮jeste frekvenser, hvilket skygger for de ord, der virkelig er vigtige. Vi kan reducere betydningen af disse ord ved at tage h칮jde for den frekvens, hvormed ord optr칝der i hele dokumentkollektionen. Dette er hovedideen bag TF/IDF-tilgangen, som er d칝kket mere detaljeret i de notebooks, der er knyttet til denne lektion.

Ingen af disse tilgange kan dog fuldt ud tage h칮jde for tekstens **semantik**. Vi har brug for mere kraftfulde neurale netv칝rksmodeller for at g칮re dette, hvilket vi vil diskutere senere i dette afsnit.

## 九꽲잺 칒velser: Tekstrepr칝sentation

Forts칝t din l칝ring i f칮lgende notebooks:

* [Tekstrepr칝sentation med PyTorch](TextRepresentationPyTorch.ipynb)
* [Tekstrepr칝sentation med TensorFlow](TextRepresentationTF.ipynb)

## Konklusion

Indtil videre har vi studeret teknikker, der kan tilf칮je frekvensv칝gt til forskellige ord. De er dog ikke i stand til at repr칝sentere betydning eller r칝kkef칮lge. Som den ber칮mte lingvist J. R. Firth sagde i 1935: "Den fulde betydning af et ord er altid kontekstuel, og ingen unders칮gelse af betydning uden for kontekst kan tages seri칮st." Senere i kurset vil vi l칝re, hvordan man fanger kontekstuel information fra tekst ved hj칝lp af sproglig modellering.

## 游 Udfordring

Pr칮v nogle andre 칮velser med bag-of-words og forskellige datamodeller. Du kan finde inspiration i denne [konkurrence p친 Kaggle](https://www.kaggle.com/competitions/word2vec-nlp-tutorial/overview/part-1-for-beginners-bag-of-words)

## [Quiz efter forel칝sning](https://ff-quizzes.netlify.app/en/ai/quiz/26)

## Gennemgang & Selvstudie

칒v dine f칝rdigheder med tekstindlejring og bag-of-words-teknikker p친 [Microsoft Learn](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-pytorch/?WT.mc_id=academic-77998-cacaste)

## [Opgave: Notebooks](assignment.md)

---

