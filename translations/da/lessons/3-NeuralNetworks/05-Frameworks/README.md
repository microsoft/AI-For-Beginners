# Neural Netv√¶rk Frameworks

Som vi allerede har l√¶rt, skal vi g√∏re to ting for at kunne tr√¶ne neurale netv√¶rk effektivt:

* Arbejde med tensorer, f.eks. multiplicere, addere og beregne funktioner som sigmoid eller softmax
* Beregne gradienter af alle udtryk for at udf√∏re gradient descent optimering

## [Quiz f√∏r lektionen](https://ff-quizzes.netlify.app/en/ai/quiz/9)

Mens `numpy` biblioteket kan klare den f√∏rste del, har vi brug for en mekanisme til at beregne gradienter. I [vores framework](../04-OwnFramework/OwnFramework.ipynb), som vi udviklede i det foreg√•ende afsnit, var vi n√∏dt til manuelt at programmere alle afledte funktioner i `backward` metoden, der udf√∏rer backpropagation. Ideelt set b√∏r et framework give os mulighed for at beregne gradienter af *ethvert udtryk*, vi kan definere.

En anden vigtig ting er at kunne udf√∏re beregninger p√• GPU eller andre specialiserede beregningsenheder, s√•som [TPU](https://en.wikipedia.org/wiki/Tensor_Processing_Unit). Tr√¶ning af dybe neurale netv√¶rk kr√¶ver *mange* beregninger, og det er meget vigtigt at kunne parallelisere disse beregninger p√• GPU'er.

> ‚úÖ Udtrykket 'parallelisere' betyder at fordele beregningerne over flere enheder.

De to mest popul√¶re neurale frameworks i √∏jeblikket er: [TensorFlow](http://TensorFlow.org) og [PyTorch](https://pytorch.org/). Begge tilbyder en lav-niveau API til at arbejde med tensorer p√• b√•de CPU og GPU. Oven p√• lav-niveau API'en findes der ogs√• h√∏j-niveau API'er, kaldet [Keras](https://keras.io/) og [PyTorch Lightning](https://pytorchlightning.ai/) henholdsvis.

Low-Level API | [TensorFlow](http://TensorFlow.org) | [PyTorch](https://pytorch.org/)
--------------|-------------------------------------|--------------------------------
High-level API| [Keras](https://keras.io/) | [PyTorch Lightning](https://pytorchlightning.ai/)

**Lav-niveau API'er** i begge frameworks giver dig mulighed for at bygge s√•kaldte **beregningsgrafer**. Denne graf definerer, hvordan output (normalt tab-funktionen) beregnes med givne inputparametre og kan sendes til beregning p√• GPU, hvis den er tilg√¶ngelig. Der findes funktioner til at differentiere denne beregningsgraf og beregne gradienter, som derefter kan bruges til at optimere modelparametre.

**H√∏j-niveau API'er** betragter stort set neurale netv√¶rk som en **sekvens af lag** og g√∏r konstruktionen af de fleste neurale netv√¶rk meget lettere. Tr√¶ning af modellen kr√¶ver normalt, at man forbereder dataene og derefter kalder en `fit` funktion for at udf√∏re arbejdet.

H√∏j-niveau API'en giver dig mulighed for hurtigt at konstruere typiske neurale netv√¶rk uden at bekymre dig om mange detaljer. Samtidig tilbyder lav-niveau API'en meget mere kontrol over tr√¶ningsprocessen og bruges derfor ofte i forskning, n√•r man arbejder med nye neurale netv√¶rksarkitekturer.

Det er ogs√• vigtigt at forst√•, at du kan bruge begge API'er sammen, f.eks. kan du udvikle din egen netv√¶rkslagsarkitektur ved hj√¶lp af lav-niveau API'en og derefter bruge den i et st√∏rre netv√¶rk, der er konstrueret og tr√¶net med h√∏j-niveau API'en. Eller du kan definere et netv√¶rk ved hj√¶lp af h√∏j-niveau API'en som en sekvens af lag og derefter bruge din egen lav-niveau tr√¶ningssl√∏jfe til at udf√∏re optimering. Begge API'er bruger de samme grundl√¶ggende underliggende begreber og er designet til at fungere godt sammen.

## L√¶ring

I dette kursus tilbyder vi det meste af indholdet b√•de for PyTorch og TensorFlow. Du kan v√¶lge dit foretrukne framework og kun gennemg√• de tilsvarende notebooks. Hvis du ikke er sikker p√•, hvilket framework du skal v√¶lge, kan du l√¶se nogle diskussioner p√• internettet om **PyTorch vs. TensorFlow**. Du kan ogs√• kigge p√• begge frameworks for at f√• en bedre forst√•else.

Hvor det er muligt, vil vi bruge h√∏j-niveau API'er for enkelhedens skyld. Vi mener dog, at det er vigtigt at forst√•, hvordan neurale netv√¶rk fungerer fra bunden, s√• i starten begynder vi med at arbejde med lav-niveau API'er og tensorer. Hvis du imidlertid √∏nsker at komme hurtigt i gang og ikke vil bruge meget tid p√• at l√¶re disse detaljer, kan du springe dem over og g√• direkte til h√∏j-niveau API notebooks.

## ‚úçÔ∏è √òvelser: Frameworks

Forts√¶t din l√¶ring i f√∏lgende notebooks:

Low-Level API | [TensorFlow+Keras Notebook](IntroKerasTF.ipynb) | [PyTorch](IntroPyTorch.ipynb)
--------------|-------------------------------------|--------------------------------
High-level API| [Keras](IntroKeras.ipynb) | *PyTorch Lightning*

Efter at have mestret frameworks, lad os genopfriske begrebet overfitting.

# Overfitting

Overfitting er et ekstremt vigtigt begreb inden for maskinl√¶ring, og det er meget vigtigt at forst√• det korrekt!

Overvej f√∏lgende problem med at approximere 5 punkter (repr√¶senteret ved `x` p√• graferne nedenfor):

![linear](../../../../../translated_images/da/overfit1.f24b71c6f652e59e.webp) | ![overfit](../../../../../translated_images/da/overfit2.131f5800ae10ca5e.webp)
-------------------------|--------------------------
**Line√¶r model, 2 parametre** | **Ikke-line√¶r model, 7 parametre**
Tr√¶ningsfejl = 5.3 | Tr√¶ningsfejl = 0
Valideringsfejl = 5.1 | Valideringsfejl = 20

* Til venstre ser vi en god ret linje-approksimation. Fordi antallet af parametre er passende, forst√•r modellen punktfordelingen korrekt.
* Til h√∏jre er modellen for kraftfuld. Fordi vi kun har 5 punkter, og modellen har 7 parametre, kan den justere sig s√•dan, at den passer gennem alle punkter, hvilket g√∏r tr√¶ningsfejlen til 0. Dette forhindrer dog modellen i at forst√• det korrekte m√∏nster bag dataene, hvilket resulterer i en meget h√∏j valideringsfejl.

Det er meget vigtigt at finde den rette balance mellem modellens kompleksitet (antal parametre) og antallet af tr√¶ningspr√∏ver.

## Hvorfor opst√•r overfitting

  * Ikke nok tr√¶ningsdata
  * For kraftfuld model
  * For meget st√∏j i inputdata

## Hvordan man opdager overfitting

Som du kan se p√• grafen ovenfor, kan overfitting opdages ved en meget lav tr√¶ningsfejl og en h√∏j valideringsfejl. Normalt under tr√¶ning vil vi se b√•de tr√¶nings- og valideringsfejl begynde at falde, og p√• et tidspunkt kan valideringsfejlen stoppe med at falde og begynde at stige. Dette vil v√¶re et tegn p√• overfitting og en indikator for, at vi sandsynligvis b√∏r stoppe tr√¶ningen p√• dette tidspunkt (eller i det mindste tage et snapshot af modellen).

![overfitting](../../../../../translated_images/da/Overfitting.408ad91cd90b4371.webp)

## Hvordan man forhindrer overfitting

Hvis du kan se, at overfitting opst√•r, kan du g√∏re f√∏lgende:

 * √òge m√¶ngden af tr√¶ningsdata
 * Mindske modellens kompleksitet
 * Bruge en [regulariseringsteknik](../../4-ComputerVision/08-TransferLearning/TrainingTricks.md), s√•som [Dropout](../../4-ComputerVision/08-TransferLearning/TrainingTricks.md#Dropout), som vi vil overveje senere.

## Overfitting og Bias-Variance Tradeoff

Overfitting er faktisk et tilf√¶lde af et mere generelt problem inden for statistik kaldet [Bias-Variance Tradeoff](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff). Hvis vi overvejer de mulige kilder til fejl i vores model, kan vi se to typer fejl:

* **Bias fejl** skyldes, at vores algoritme ikke er i stand til korrekt at fange forholdet mellem tr√¶ningsdata. Det kan skyldes, at vores model ikke er kraftfuld nok (**underfitting**).
* **Variance fejl**, som skyldes, at modellen approximere st√∏j i inputdataene i stedet for meningsfulde forhold (**overfitting**).

Under tr√¶ning falder bias-fejlen (da vores model l√¶rer at approximere dataene), og variance-fejlen stiger. Det er vigtigt at stoppe tr√¶ningen - enten manuelt (n√•r vi opdager overfitting) eller automatisk (ved at introducere regularisering) - for at forhindre overfitting.

## Konklusion

I denne lektion l√¶rte du om forskellene mellem de forskellige API'er for de to mest popul√¶re AI frameworks, TensorFlow og PyTorch. Derudover l√¶rte du om et meget vigtigt emne, overfitting.

## üöÄ Udfordring

I de medf√∏lgende notebooks finder du 'opgaver' nederst; gennemg√• notebooks og fuldf√∏r opgaverne.

## [Quiz efter lektionen](https://ff-quizzes.netlify.app/en/ai/quiz/10)

## Gennemgang & Selvstudie

Unders√∏g f√∏lgende emner:

- TensorFlow
- PyTorch
- Overfitting

Sp√∏rg dig selv f√∏lgende sp√∏rgsm√•l:

- Hvad er forskellen mellem TensorFlow og PyTorch?
- Hvad er forskellen mellem overfitting og underfitting?

## [Opgave](lab/README.md)

I denne lab skal du l√∏se to klassifikationsproblemer ved hj√¶lp af enkelt- og flerlagede fuldt forbundne netv√¶rk med PyTorch eller TensorFlow.

* [Instruktioner](lab/README.md)
* [Notebook](lab/LabFrameworks.ipynb)

---

