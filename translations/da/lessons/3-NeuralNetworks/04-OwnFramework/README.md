# Introduktion til Neurale Netv√¶rk. Multi-Layered Perceptron

I den forrige sektion l√¶rte du om den simpleste model for neurale netv√¶rk - enlaget perceptron, en line√¶r to-klasse klassifikationsmodel.

I denne sektion vil vi udvide denne model til en mere fleksibel ramme, der giver os mulighed for at:

* udf√∏re **multi-klasse klassifikation** ud over to-klasse
* l√∏se **regressionsproblemer** ud over klassifikation
* adskille klasser, der ikke er line√¶rt adskillelige

Vi vil ogs√• udvikle vores egen modul√¶re ramme i Python, som giver os mulighed for at konstruere forskellige arkitekturer for neurale netv√¶rk.

## [Quiz f√∏r lektionen](https://ff-quizzes.netlify.app/en/ai/quiz/7)

## Formalisering af Maskinl√¶ring

Lad os starte med at formalisere maskinl√¶ringsproblemet. Antag, at vi har et tr√¶ningsdatas√¶t **X** med labels **Y**, og vi skal bygge en model *f*, der kan lave de mest pr√¶cise forudsigelser. Kvaliteten af forudsigelserne m√•les ved hj√¶lp af **tab-funktionen** &lagran;. F√∏lgende tab-funktioner bruges ofte:

* For regressionsproblemer, hvor vi skal forudsige et tal, kan vi bruge **absolut fejl** &sum;<sub>i</sub>|f(x<sup>(i)</sup>)-y<sup>(i)</sup>| eller **kvadreret fejl** &sum;<sub>i</sub>(f(x<sup>(i)</sup>)-y<sup>(i)</sup>)<sup>2</sup>
* For klassifikation bruger vi **0-1 tab** (som i bund og grund er det samme som modellens **n√∏jagtighed**) eller **logistisk tab**.

For enlaget perceptron blev funktionen *f* defineret som en line√¶r funktion *f(x)=wx+b* (her er *w* v√¶gtmatricen, *x* er vektoren af inputfunktioner, og *b* er bias-vektoren). For forskellige arkitekturer af neurale netv√¶rk kan denne funktion antage en mere kompleks form.

> I tilf√¶lde af klassifikation er det ofte √∏nskv√¶rdigt at f√• sandsynligheder for de tilsvarende klasser som netv√¶rkets output. For at konvertere vilk√•rlige tal til sandsynligheder (f.eks. for at normalisere outputtet) bruger vi ofte **softmax**-funktionen &sigma;, og funktionen *f* bliver *f(x)=&sigma;(wx+b)*

I definitionen af *f* ovenfor kaldes *w* og *b* for **parametre** &theta;=‚ü®*w,b*‚ü©. Givet datas√¶ttet ‚ü®**X**,**Y**‚ü© kan vi beregne en samlet fejl p√• hele datas√¶ttet som en funktion af parametrene &theta;.

> ‚úÖ **M√•let med tr√¶ning af neurale netv√¶rk er at minimere fejlen ved at variere parametrene &theta;**

## Gradient Descent Optimering

Der findes en velkendt metode til funktionsoptimering kaldet **gradient descent**. Ideen er, at vi kan beregne en afledt (i det flerdimensionale tilf√¶lde kaldet **gradient**) af tab-funktionen med hensyn til parametrene og variere parametrene p√• en m√•de, s√• fejlen mindskes. Dette kan formaliseres som f√∏lger:

* Initialiser parametrene med nogle tilf√¶ldige v√¶rdier w<sup>(0)</sup>, b<sup>(0)</sup>
* Gentag f√∏lgende trin mange gange:
    - w<sup>(i+1)</sup> = w<sup>(i)</sup>-&eta;&part;&lagran;/&part;w
    - b<sup>(i+1)</sup> = b<sup>(i)</sup>-&eta;&part;&lagran;/&part;b

Under tr√¶ningen skal optimeringstrinene beregnes med hensyn til hele datas√¶ttet (husk, at tabet beregnes som en sum gennem alle tr√¶ningspr√∏ver). I praksis tager vi dog sm√• dele af datas√¶ttet, kaldet **minibatches**, og beregner gradienter baseret p√• et delm√¶ngde af data. Fordi delm√¶ngden v√¶lges tilf√¶ldigt hver gang, kaldes denne metode **stochastic gradient descent** (SGD).

## Multi-Layered Perceptrons og Backpropagation

Et enlaget netv√¶rk, som vi har set ovenfor, er i stand til at klassificere line√¶rt adskillelige klasser. For at bygge en rigere model kan vi kombinere flere lag i netv√¶rket. Matematisk betyder det, at funktionen *f* f√•r en mere kompleks form og beregnes i flere trin:
* z<sub>1</sub>=w<sub>1</sub>x+b<sub>1</sub>
* z<sub>2</sub>=w<sub>2</sub>&alpha;(z<sub>1</sub>)+b<sub>2</sub>
* f = &sigma;(z<sub>2</sub>)

Her er &alpha; en **ikke-line√¶r aktiveringsfunktion**, &sigma; er en softmax-funktion, og parametrene er &theta;=<*w<sub>1</sub>,b<sub>1</sub>,w<sub>2</sub>,b<sub>2</sub>*>.

Gradient descent-algoritmen forbliver den samme, men det bliver mere udfordrende at beregne gradienter. Ved hj√¶lp af k√¶dereglen for differentiation kan vi beregne afledte som:

* &part;&lagran;/&part;w<sub>2</sub> = (&part;&lagran;/&part;&sigma;)(&part;&sigma;/&part;z<sub>2</sub>)(&part;z<sub>2</sub>/&part;w<sub>2</sub>)
* &part;&lagran;/&part;w<sub>1</sub> = (&part;&lagran;/&part;&sigma;)(&part;&sigma;/&part;z<sub>2</sub>)(&part;z<sub>2</sub>/&part;&alpha;)(&part;&alpha;/&part;z<sub>1</sub>)(&part;z<sub>1</sub>/&part;w<sub>1</sub>)

> ‚úÖ K√¶dereglen bruges til at beregne afledte af tab-funktionen med hensyn til parametrene.

Bem√¶rk, at den venstre del af alle disse udtryk er den samme, og derfor kan vi effektivt beregne afledte ved at starte fra tab-funktionen og g√• "bagl√¶ns" gennem beregningsgrafen. Derfor kaldes metoden til tr√¶ning af et flerlaget perceptron for **backpropagation**, eller 'backprop'.

<img alt="beregningsgraf" src="../../../../../translated_images/da/ComputeGraphGrad.4626252c0de03507.webp"/>

> TODO: billedhenvisning

> ‚úÖ Vi vil d√¶kke backpropagation meget mere detaljeret i vores notebook-eksempel.  

## Konklusion

I denne lektion har vi bygget vores eget bibliotek til neurale netv√¶rk, og vi har brugt det til en simpel todimensionel klassifikationsopgave.

## üöÄ Udfordring

I den medf√∏lgende notebook vil du implementere din egen ramme til at bygge og tr√¶ne flerlagede perceptrons. Du vil kunne se i detaljer, hvordan moderne neurale netv√¶rk fungerer.

G√• videre til [OwnFramework](OwnFramework.ipynb)-notebooken og arbejd dig igennem den.

## [Quiz efter lektionen](https://ff-quizzes.netlify.app/en/ai/quiz/8)

## Gennemgang & Selvstudie

Backpropagation er en almindelig algoritme, der bruges i AI og ML, og det er v√¶rd at studere [mere detaljeret](https://wikipedia.org/wiki/Backpropagation)

## [Opgave](lab/README.md)

I dette laboratorium bliver du bedt om at bruge den ramme, du har konstrueret i denne lektion, til at l√∏se MNIST-h√•ndskrevne cifre klassifikation.

* [Instruktioner](lab/README.md)
* [Notebook](lab/MyFW_MNIST.ipynb)

---

