<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "dbacf9b1915612981d76059678e563e5",
  "translation_date": "2025-08-28T15:06:37+00:00",
  "source_file": "lessons/6-Other/22-DeepRL/README.md",
  "language_code": "da"
}
-->
# Deep Reinforcement Learning

Forst칝rkningsl칝ring (RL) betragtes som en af de grundl칝ggende maskinl칝ringsparadigmer, ved siden af superviseret l칝ring og usuperviseret l칝ring. Mens vi i superviseret l칝ring er afh칝ngige af datas칝t med kendte resultater, er RL baseret p친 **at l칝re ved at g칮re**. For eksempel, n친r vi f칮rste gang ser et computerspil, begynder vi at spille, selv uden at kende reglerne, og snart er vi i stand til at forbedre vores f칝rdigheder blot gennem processen med at spille og justere vores adf칝rd.

## [Pre-lecture quiz](https://ff-quizzes.netlify.app/en/ai/quiz/43)

For at udf칮re RL har vi brug for:

* Et **milj칮** eller en **simulator**, der fasts칝tter spillets regler. Vi skal kunne k칮re eksperimenter i simulatoren og observere resultaterne.
* En **bel칮nningsfunktion**, som angiver, hvor succesfuldt vores eksperiment var. I tilf칝lde af at l칝re at spille et computerspil, ville bel칮nningen v칝re vores endelige score.

Baseret p친 bel칮nningsfunktionen skal vi kunne justere vores adf칝rd og forbedre vores f칝rdigheder, s친 vi n칝ste gang spiller bedre. Den st칮rste forskel mellem andre typer maskinl칝ring og RL er, at vi i RL typisk ikke ved, om vi vinder eller taber, f칮r vi har afsluttet spillet. Derfor kan vi ikke sige, om et bestemt tr칝k alene er godt eller ej - vi modtager kun en bel칮nning i slutningen af spillet.

Under RL udf칮rer vi typisk mange eksperimenter. Under hvert eksperiment skal vi balancere mellem at f칮lge den optimale strategi, vi har l칝rt indtil videre (**udnyttelse**), og at udforske nye mulige tilstande (**udforskning**).

## OpenAI Gym

Et fantastisk v칝rkt칮j til RL er [OpenAI Gym](https://gym.openai.com/) - et **simuleringsmilj칮**, der kan simulere mange forskellige milj칮er, lige fra Atari-spil til fysikken bag stangbalancering. Det er et af de mest popul칝re simuleringsmilj칮er til tr칝ning af forst칝rkningsl칝ringsalgoritmer og vedligeholdes af [OpenAI](https://openai.com/).

> **Note**: Du kan se alle de milj칮er, der er tilg칝ngelige fra OpenAI Gym [her](https://gym.openai.com/envs/#classic_control).

## CartPole Balancering

I har sikkert alle set moderne balanceringsenheder som *Segway* eller *Gyroscooters*. De er i stand til automatisk at balancere ved at justere deres hjul som reaktion p친 et signal fra en accelerometer eller gyroskop. I dette afsnit vil vi l칝re, hvordan man l칮ser et lignende problem - at balancere en stang. Det ligner en situation, hvor en cirkusartist skal balancere en stang p친 sin h친nd - men denne stangbalancering foreg친r kun i 1D.

En forenklet version af balancering er kendt som et **CartPole**-problem. I CartPole-verdenen har vi en horisontal slider, der kan bev칝ge sig til venstre eller h칮jre, og m친let er at balancere en lodret stang oven p친 slideren, mens den bev칝ger sig.

<img alt="a cartpole" src="images/cartpole.png" width="200"/>

For at oprette og bruge dette milj칮 har vi brug for et par linjer Python-kode:

```python
import gym
env = gym.make("CartPole-v1")

env.reset()
done = False
total_reward = 0
while not done:
   env.render()
   action = env.action_space.sample()
   observaton, reward, done, info = env.step(action)
   total_reward += reward

print(f"Total reward: {total_reward}")
```

Hvert milj칮 kan tilg친s p친 pr칝cis samme m친de:
* `env.reset` starter et nyt eksperiment
* `env.step` udf칮rer et simuleringsskridt. Det modtager en **handling** fra **handlingsrummet** og returnerer en **observation** (fra observationsrummet) samt en bel칮nning og en afslutningsindikator.

I eksemplet ovenfor udf칮rer vi en tilf칝ldig handling ved hvert skridt, hvilket er grunden til, at eksperimentets levetid er meget kort:

![non-balancing cartpole](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-nobalance.gif)

M친let med en RL-algoritme er at tr칝ne en model - den s친kaldte **policy**  - som vil returnere handlingen som svar p친 en given tilstand. Vi kan ogs친 betragte policy som probabilistisk, dvs. for enhver tilstand *s* og handling *a* vil den returnere sandsynligheden (*a*|*s*) for, at vi b칮r tage *a* i tilstand *s*.

## Policy Gradients Algoritme

Den mest oplagte m친de at modellere en policy p친 er ved at oprette et neuralt netv칝rk, der tager tilstande som input og returnerer tilsvarende handlinger (eller rettere sandsynlighederne for alle handlinger). P친 en m친de ville det ligne en normal klassifikationsopgave, med en v칝sentlig forskel - vi ved ikke p친 forh친nd, hvilke handlinger vi skal tage ved hvert skridt.

Ideen her er at estimere disse sandsynligheder. Vi bygger en vektor af **kumulative bel칮nninger**, som viser vores samlede bel칮nning ved hvert skridt i eksperimentet. Vi anvender ogs친 **bel칮nningsdiskontering** ved at multiplicere tidligere bel칮nninger med en koefficient 풥=0.99 for at mindske betydningen af tidligere bel칮nninger. Derefter forst칝rker vi de skridt langs eksperimentets sti, der giver st칮rre bel칮nninger.

> L칝r mere om Policy Gradient-algoritmen og se den i aktion i [eksempelsnotebooken](CartPole-RL-TF.ipynb).

## Actor-Critic Algoritme

En forbedret version af Policy Gradients-tilgangen kaldes **Actor-Critic**. Hovedideen bag den er, at det neurale netv칝rk skal tr칝nes til at returnere to ting:

* Policy, som bestemmer, hvilken handling der skal tages. Denne del kaldes **actor**.
* Estimationen af den samlede bel칮nning, vi kan forvente at f친 i denne tilstand - denne del kaldes **critic**.

P친 en m친de minder denne arkitektur om en [GAN](../../4-ComputerVision/10-GANs/README.md), hvor vi har to netv칝rk, der tr칝nes mod hinanden. I actor-critic-modellen foresl친r actor den handling, vi skal tage, og critic fors칮ger at v칝re kritisk og estimere resultatet. Vores m친l er dog at tr칝ne disse netv칝rk i harmoni.

Fordi vi kender b친de de reelle kumulative bel칮nninger og resultaterne returneret af critic under eksperimentet, er det relativt nemt at bygge en tab-funktion, der minimerer forskellen mellem dem. Det giver os **critic loss**. Vi kan beregne **actor loss** ved at bruge samme tilgang som i policy gradient-algoritmen.

Efter at have k칮rt en af disse algoritmer kan vi forvente, at vores CartPole opf칮rer sig s친dan her:

![a balancing cartpole](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-balance.gif)

## 九꽲잺 칒velser: Policy Gradients og Actor-Critic RL

Forts칝t din l칝ring i f칮lgende notebooks:

* [RL i TensorFlow](CartPole-RL-TF.ipynb)
* [RL i PyTorch](CartPole-RL-PyTorch.ipynb)

## Andre RL Opgaver

Forst칝rkningsl칝ring er i dag et hurtigt voksende forskningsfelt. Nogle interessante eksempler p친 forst칝rkningsl칝ring er:

* At l칝re en computer at spille **Atari-spil**. Den udfordrende del i dette problem er, at vi ikke har en simpel tilstand repr칝senteret som en vektor, men snarere et sk칝rmbillede - og vi skal bruge CNN til at konvertere dette sk칝rmbillede til en funktionsvektor eller til at udtr칝kke bel칮nningsinformation. Atari-spil er tilg칝ngelige i Gym.
* At l칝re en computer at spille br칝tspil som skak og Go. For nylig blev avancerede programmer som **Alpha Zero** tr칝net fra bunden af to agenter, der spillede mod hinanden og forbedrede sig ved hvert skridt.
* I industrien bruges RL til at skabe kontrolsystemer fra simulering. En tjeneste kaldet [Bonsai](https://azure.microsoft.com/services/project-bonsai/?WT.mc_id=academic-77998-cacaste) er specifikt designet til dette.

## Konklusion

Vi har nu l칝rt, hvordan man tr칝ner agenter til at opn친 gode resultater blot ved at give dem en bel칮nningsfunktion, der definerer den 칮nskede tilstand af spillet, og ved at give dem mulighed for intelligent at udforske s칮geomr친det. Vi har med succes pr칮vet to algoritmer og opn친et et godt resultat p친 relativt kort tid. Dette er dog kun begyndelsen p친 din rejse ind i RL, og du b칮r bestemt overveje at tage et separat kursus, hvis du vil dykke dybere.

## 游 Udfordring

Udforsk de applikationer, der er n칝vnt i afsnittet 'Andre RL Opgaver', og pr칮v at implementere en!

## [Post-lecture quiz](https://ff-quizzes.netlify.app/en/ai/quiz/44)

## Gennemgang & Selvstudie

L칝r mere om klassisk forst칝rkningsl칝ring i vores [Maskinl칝ring for Begyndere Curriculum](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/README.md).

Se [denne fantastiske video](https://www.youtube.com/watch?v=qv6UVOQ0F44), der taler om, hvordan en computer kan l칝re at spille Super Mario.

## Opgave: [Tr칝n en Mountain Car](lab/README.md)

Dit m친l under denne opgave vil v칝re at tr칝ne et andet Gym-milj칮 - [Mountain Car](https://www.gymlibrary.ml/environments/classic_control/mountain_car/).

---

**Ansvarsfraskrivelse**:  
Dette dokument er blevet oversat ved hj칝lp af AI-overs칝ttelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selvom vi bestr칝ber os p친 at sikre n칮jagtighed, skal du v칝re opm칝rksom p친, at automatiserede overs칝ttelser kan indeholde fejl eller un칮jagtigheder. Det originale dokument p친 dets oprindelige sprog b칮r betragtes som den autoritative kilde. For kritisk information anbefales professionel menneskelig overs칝ttelse. Vi p친tager os ikke ansvar for eventuelle misforst친elser eller fejltolkninger, der m친tte opst친 som f칮lge af brugen af denne overs칝ttelse.