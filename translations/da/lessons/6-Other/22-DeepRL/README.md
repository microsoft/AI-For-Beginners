# Deep Reinforcement Learning

Forst칝rkningsl칝ring (RL) betragtes som en af de grundl칝ggende paradigmer inden for maskinl칝ring, ved siden af superviseret l칝ring og usuperviseret l칝ring. Mens vi i superviseret l칝ring baserer os p친 datas칝t med kendte resultater, er RL baseret p친 **at l칝re ved at g칮re**. For eksempel, n친r vi ser et computerspil for f칮rste gang, begynder vi at spille, selv uden at kende reglerne, og snart bliver vi bedre, blot ved at spille og justere vores adf칝rd.

## [Quiz f칮r lektionen](https://ff-quizzes.netlify.app/en/ai/quiz/43)

For at udf칮re RL har vi brug for:

* Et **milj칮** eller en **simulator**, der fasts칝tter reglerne for spillet. Vi skal kunne udf칮re eksperimenter i simulatoren og observere resultaterne.
* En **bel칮nningsfunktion**, der angiver, hvor succesfuldt vores eksperiment var. N친r vi l칝rer at spille et computerspil, ville bel칮nningen v칝re vores endelige score.

Baseret p친 bel칮nningsfunktionen skal vi kunne justere vores adf칝rd og forbedre vores f칝rdigheder, s친 vi spiller bedre n칝ste gang. Den st칮rste forskel mellem andre typer maskinl칝ring og RL er, at vi i RL typisk ikke ved, om vi vinder eller taber, f칮r spillet er slut. Derfor kan vi ikke sige, om et bestemt tr칝k alene er godt eller ej - vi modtager kun en bel칮nning ved slutningen af spillet.

Under RL udf칮rer vi typisk mange eksperimenter. Under hvert eksperiment skal vi balancere mellem at f칮lge den optimale strategi, vi har l칝rt indtil videre (**udnyttelse**), og at udforske nye mulige tilstande (**udforskning**).

## OpenAI Gym

Et fantastisk v칝rkt칮j til RL er [OpenAI Gym](https://gym.openai.com/) - et **simuleringsmilj칮**, der kan simulere mange forskellige milj칮er, lige fra Atari-spil til fysikken bag stangbalancering. Det er et af de mest popul칝re simuleringsmilj칮er til tr칝ning af forst칝rkningsl칝ringsalgoritmer og vedligeholdes af [OpenAI](https://openai.com/).

> **Note**: Du kan se alle de milj칮er, der er tilg칝ngelige fra OpenAI Gym [her](https://gym.openai.com/envs/#classic_control).

## CartPole Balancering

I har sikkert alle set moderne balanceringsenheder som *Segway* eller *Gyroscooters*. De kan automatisk balancere ved at justere deres hjul som reaktion p친 signaler fra en accelerometer eller gyroskop. I denne sektion vil vi l칝re at l칮se et lignende problem - at balancere en stang. Det minder om en situation, hvor en cirkusartist skal balancere en stang p친 sin h친nd - men denne stangbalancering foreg친r kun i 1D.

En forenklet version af balancering er kendt som **CartPole**-problemet. I CartPole-verdenen har vi en horisontal slider, der kan bev칝ge sig til venstre eller h칮jre, og m친let er at balancere en vertikal stang oven p친 slideren, mens den bev칝ger sig.

<img alt="en cartpole" src="../../../../../translated_images/da/cartpole.f52a67f27e058170.webp" width="200"/>

For at oprette og bruge dette milj칮 har vi brug for et par linjer Python-kode:

```python
import gym
env = gym.make("CartPole-v1")

env.reset()
done = False
total_reward = 0
while not done:
   env.render()
   action = env.action_space.sample()
   observaton, reward, done, info = env.step(action)
   total_reward += reward

print(f"Total reward: {total_reward}")
```

Hvert milj칮 kan tilg친s p친 pr칝cis samme m친de:
* `env.reset` starter et nyt eksperiment
* `env.step` udf칮rer et simuleringsskridt. Det modtager en **handling** fra **handlingsrummet** og returnerer en **observation** (fra observationsrummet), samt en bel칮nning og en afslutningsflag.

I eksemplet ovenfor udf칮rer vi en tilf칝ldig handling ved hvert skridt, hvilket er grunden til, at eksperimentets levetid er meget kort:

![ikke-balancerende cartpole](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-nobalance.gif)

M친let med en RL-algoritme er at tr칝ne en model - den s친kaldte **policy** &pi; - som vil returnere handlingen som svar p친 en given tilstand. Vi kan ogs친 betragte policy som probabilistisk, dvs. for enhver tilstand *s* og handling *a* vil den returnere sandsynligheden &pi;(*a*|*s*) for, at vi b칮r tage *a* i tilstand *s*.

## Policy Gradients Algoritme

Den mest oplagte m친de at modellere en policy p친 er ved at oprette et neuralt netv칝rk, der tager tilstande som input og returnerer tilsvarende handlinger (eller rettere sandsynlighederne for alle handlinger). P친 en m친de ville det ligne en normal klassifikationsopgave, med en v칝sentlig forskel - vi ved ikke p친 forh친nd, hvilke handlinger vi skal tage ved hvert skridt.

Ideen her er at estimere disse sandsynligheder. Vi bygger en vektor af **kumulative bel칮nninger**, der viser vores samlede bel칮nning ved hvert skridt i eksperimentet. Vi anvender ogs친 **bel칮nningsdiskontering** ved at multiplicere tidligere bel칮nninger med en koefficient &gamma;=0.99 for at mindske betydningen af tidligere bel칮nninger. Derefter forst칝rker vi de skridt langs eksperimentets sti, der giver st칮rre bel칮nninger.

> L칝r mere om Policy Gradient-algoritmen og se den i aktion i [eksempelsnotebooken](CartPole-RL-TF.ipynb).

## Actor-Critic Algoritme

En forbedret version af Policy Gradients-tilgangen kaldes **Actor-Critic**. Hovedideen bag den er, at det neurale netv칝rk skal tr칝nes til at returnere to ting:

* Policy, der bestemmer, hvilken handling der skal tages. Denne del kaldes **actor**.
* Estimeringen af den samlede bel칮nning, vi kan forvente at f친 i denne tilstand - denne del kaldes **critic**.

P친 en m친de minder denne arkitektur om en [GAN](../../4-ComputerVision/10-GANs/README.md), hvor vi har to netv칝rk, der tr칝nes mod hinanden. I actor-critic-modellen foresl친r actor den handling, vi skal tage, og critic fors칮ger at v칝re kritisk og estimere resultatet. Vores m친l er dog at tr칝ne disse netv칝rk i f칝llesskab.

Fordi vi kender b친de de reelle kumulative bel칮nninger og de resultater, der returneres af critic under eksperimentet, er det relativt nemt at opbygge en tab-funktion, der minimerer forskellen mellem dem. Det giver os **critic loss**. Vi kan beregne **actor loss** ved at bruge samme tilgang som i policy gradient-algoritmen.

Efter at have k칮rt en af disse algoritmer kan vi forvente, at vores CartPole opf칮rer sig s친dan her:

![en balancerende cartpole](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-balance.gif)

## 九꽲잺 칒velser: Policy Gradients og Actor-Critic RL

Forts칝t din l칝ring i f칮lgende notebooks:

* [RL i TensorFlow](CartPole-RL-TF.ipynb)
* [RL i PyTorch](CartPole-RL-PyTorch.ipynb)

## Andre RL-opgaver

Forst칝rkningsl칝ring er i dag et hurtigt voksende forskningsomr친de. Nogle interessante eksempler p친 forst칝rkningsl칝ring er:

* At l칝re en computer at spille **Atari-spil**. Den udfordrende del af dette problem er, at vi ikke har en simpel tilstand repr칝senteret som en vektor, men snarere et sk칝rmbillede - og vi skal bruge CNN til at konvertere dette sk칝rmbillede til en feature-vektor eller til at udtr칝kke bel칮nningsinformation. Atari-spil er tilg칝ngelige i Gym.
* At l칝re en computer at spille br칝tspil som skak og Go. For nylig blev avancerede programmer som **Alpha Zero** tr칝net fra bunden af to agenter, der spillede mod hinanden og forbedrede sig ved hvert skridt.
* I industrien bruges RL til at skabe kontrolsystemer fra simulering. En tjeneste kaldet [Bonsai](https://azure.microsoft.com/services/project-bonsai/?WT.mc_id=academic-77998-cacaste) er specifikt designet til dette.

## Konklusion

Vi har nu l칝rt, hvordan man tr칝ner agenter til at opn친 gode resultater blot ved at give dem en bel칮nningsfunktion, der definerer den 칮nskede tilstand af spillet, og ved at give dem mulighed for intelligent at udforske s칮geomr친det. Vi har med succes pr칮vet to algoritmer og opn친et et godt resultat p친 relativt kort tid. Dette er dog kun begyndelsen p친 din rejse ind i RL, og du b칮r bestemt overveje at tage et separat kursus, hvis du vil dykke dybere.

## 游 Udfordring

Udforsk de applikationer, der er n칝vnt i afsnittet 'Andre RL-opgaver', og pr칮v at implementere en af dem!

## [Quiz efter lektionen](https://ff-quizzes.netlify.app/en/ai/quiz/44)

## Gennemgang & Selvstudie

L칝r mere om klassisk forst칝rkningsl칝ring i vores [Machine Learning for Beginners Curriculum](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/README.md).

Se [denne fantastiske video](https://www.youtube.com/watch?v=qv6UVOQ0F44), der taler om, hvordan en computer kan l칝re at spille Super Mario.

## Opgave: [Tr칝n en Mountain Car](lab/README.md)

Dit m친l under denne opgave vil v칝re at tr칝ne et andet Gym-milj칮 - [Mountain Car](https://www.gymlibrary.ml/environments/classic_control/mountain_car/).

---

