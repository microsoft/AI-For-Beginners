# ‡∏Å‡∏•‡πÑ‡∏Å Attention ‡πÅ‡∏•‡∏∞ Transformers

## [‡πÅ‡∏ö‡∏ö‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏£‡∏µ‡∏¢‡∏ô](https://ff-quizzes.netlify.app/en/ai/quiz/35)

‡∏´‡∏ô‡∏∂‡πà‡∏á‡πÉ‡∏ô‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏ó‡∏µ‡πà‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡πÉ‡∏ô‡∏î‡πâ‡∏≤‡∏ô NLP ‡∏Ñ‡∏∑‡∏≠ **‡∏Å‡∏≤‡∏£‡πÅ‡∏õ‡∏•‡∏†‡∏≤‡∏©‡∏≤‡πÇ‡∏î‡∏¢‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á** ‡∏ã‡∏∂‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏á‡∏≤‡∏ô‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏´‡∏•‡∏±‡∏á‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏°‡∏∑‡∏≠‡∏≠‡∏¢‡πà‡∏≤‡∏á Google Translate ‡πÉ‡∏ô‡∏™‡πà‡∏ß‡∏ô‡∏ô‡∏µ‡πâ ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏°‡∏∏‡πà‡∏á‡πÄ‡∏ô‡πâ‡∏ô‡πÑ‡∏õ‡∏ó‡∏µ‡πà‡∏Å‡∏≤‡∏£‡πÅ‡∏õ‡∏•‡∏†‡∏≤‡∏©‡∏≤‡πÇ‡∏î‡∏¢‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á ‡∏´‡∏£‡∏∑‡∏≠‡πÉ‡∏ô‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°‡∏Å‡∏ß‡πâ‡∏≤‡∏á‡πÜ ‡∏Ñ‡∏∑‡∏≠ ‡∏á‡∏≤‡∏ô *sequence-to-sequence* (‡∏ã‡∏∂‡πà‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡∏≠‡∏µ‡∏Å‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ß‡πà‡∏≤ **sentence transduction**)

‡∏î‡πâ‡∏ß‡∏¢ RNNs ‡∏á‡∏≤‡∏ô sequence-to-sequence ‡∏ñ‡∏π‡∏Å‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡πÇ‡∏î‡∏¢‡πÄ‡∏Ñ‡∏£‡∏∑‡∏≠‡∏Ç‡πà‡∏≤‡∏¢ recurrent ‡∏™‡∏≠‡∏á‡∏ï‡∏±‡∏ß ‡πÇ‡∏î‡∏¢‡πÄ‡∏Ñ‡∏£‡∏∑‡∏≠‡∏Ç‡πà‡∏≤‡∏¢‡∏´‡∏ô‡∏∂‡πà‡∏á‡∏Ñ‡∏∑‡∏≠ **encoder** ‡∏ó‡∏µ‡πà‡∏ö‡∏µ‡∏ö‡∏≠‡∏±‡∏î‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÄ‡∏õ‡πá‡∏ô hidden state ‡πÉ‡∏ô‡∏Ç‡∏ì‡∏∞‡∏ó‡∏µ‡πà‡∏≠‡∏µ‡∏Å‡πÄ‡∏Ñ‡∏£‡∏∑‡∏≠‡∏Ç‡πà‡∏≤‡∏¢‡∏´‡∏ô‡∏∂‡πà‡∏á‡∏Ñ‡∏∑‡∏≠ **decoder** ‡∏ó‡∏µ‡πà‡∏Ñ‡∏•‡∏≤‡∏¢ hidden state ‡∏≠‡∏≠‡∏Å‡∏°‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà‡πÅ‡∏õ‡∏•‡πÅ‡∏•‡πâ‡∏ß ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£‡∏Å‡πá‡∏ï‡∏≤‡∏° ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ô‡∏µ‡πâ‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏ö‡∏≤‡∏á‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏£:

* ‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á‡πÄ‡∏Ñ‡∏£‡∏∑‡∏≠‡∏Ç‡πà‡∏≤‡∏¢ encoder ‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏Å‡∏•‡∏≥‡∏ö‡∏≤‡∏Å‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏à‡∏î‡∏à‡∏≥‡∏à‡∏∏‡∏î‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏Ç‡∏≠‡∏á‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ ‡∏ã‡∏∂‡πà‡∏á‡∏™‡πà‡∏á‡∏ú‡∏•‡πÉ‡∏´‡πâ‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏•‡∏î‡∏•‡∏á‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡∏ó‡∏µ‡πà‡∏¢‡∏≤‡∏ß
* ‡∏Ñ‡∏≥‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏≥‡πÉ‡∏ô‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏°‡∏µ‡∏ú‡∏•‡∏Å‡∏£‡∏∞‡∏ó‡∏ö‡∏ï‡πà‡∏≠‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÄ‡∏ó‡πà‡∏≤‡∏Å‡∏±‡∏ô ‡πÉ‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡∏à‡∏£‡∏¥‡∏á ‡∏Ñ‡∏≥‡∏ö‡∏≤‡∏á‡∏Ñ‡∏≥‡πÉ‡∏ô‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Ç‡πâ‡∏≤‡∏≠‡∏≤‡∏à‡∏°‡∏µ‡∏ú‡∏•‡∏Å‡∏£‡∏∞‡∏ó‡∏ö‡∏ï‡πà‡∏≠‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤‡∏Ñ‡∏≥‡∏≠‡∏∑‡πà‡∏ô‡πÜ

**‡∏Å‡∏•‡πÑ‡∏Å Attention** ‡πÉ‡∏´‡πâ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ñ‡πà‡∏ß‡∏á‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏ú‡∏•‡∏Å‡∏£‡∏∞‡∏ó‡∏ö‡πÄ‡∏ä‡∏¥‡∏á‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÄ‡∏ß‡∏Å‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Ç‡∏≠‡∏á RNN ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡∏ô‡∏µ‡πâ‡∏ñ‡∏π‡∏Å‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡πÇ‡∏î‡∏¢‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ó‡∏≤‡∏á‡∏•‡∏±‡∏î‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏Å‡∏•‡∏≤‡∏á‡∏Ç‡∏≠‡∏á RNN ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Ç‡πâ‡∏≤‡πÅ‡∏•‡∏∞ RNN ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏≠‡∏Å ‡∏î‡πâ‡∏ß‡∏¢‡∏ß‡∏¥‡∏ò‡∏µ‡∏ô‡∏µ‡πâ ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏™‡∏±‡∏ç‡∏•‡∏±‡∏Å‡∏©‡∏ì‡πå‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå y<sub>t</sub> ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞ hidden ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î h<sub>i</sub> ‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Ç‡πâ‡∏≤ ‡πÇ‡∏î‡∏¢‡∏°‡∏µ‡∏Ñ‡πà‡∏≤‡∏™‡∏±‡∏°‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡πå‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏ó‡∏µ‡πà‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô &alpha;<sub>t,i</sub>

![‡∏†‡∏≤‡∏û‡πÅ‡∏™‡∏î‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏• encoder/decoder ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏ä‡∏±‡πâ‡∏ô attention ‡πÅ‡∏ö‡∏ö additive](../../../../../translated_images/th/encoder-decoder-attention.7a726296894fb567.webp)

> ‡πÇ‡∏°‡πÄ‡∏î‡∏• encoder-decoder ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏•‡πÑ‡∏Å attention ‡πÅ‡∏ö‡∏ö additive ‡πÉ‡∏ô [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) ‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏à‡∏≤‡∏Å [‡∏ö‡∏•‡πá‡∏≠‡∏Å‡πÇ‡∏û‡∏™‡∏ï‡πå‡∏ô‡∏µ‡πâ](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)

‡πÄ‡∏°‡∏ó‡∏£‡∏¥‡∏Å‡∏ã‡πå attention {&alpha;<sub>i,j</sub>} ‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÅ‡∏ó‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ó‡∏µ‡πà‡∏Ñ‡∏≥‡∏ö‡∏≤‡∏á‡∏Ñ‡∏≥‡πÉ‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏µ‡∏ö‡∏ó‡∏ö‡∏≤‡∏ó‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÉ‡∏ô‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå ‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡∏≠‡∏á‡πÄ‡∏°‡∏ó‡∏£‡∏¥‡∏Å‡∏ã‡πå‡∏î‡∏±‡∏á‡∏Å‡∏•‡πà‡∏≤‡∏ß:

![‡∏†‡∏≤‡∏û‡πÅ‡∏™‡∏î‡∏á‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡πÅ‡∏ô‡∏ß‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏û‡∏ö‡πÇ‡∏î‡∏¢ RNNsearch-50 ‡∏à‡∏≤‡∏Å Bahdanau - arviz.org](../../../../../translated_images/th/bahdanau-fig3.09ba2d37f202a6af.webp)

> ‡∏†‡∏≤‡∏û‡∏à‡∏≤‡∏Å [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) (Fig.3)

‡∏Å‡∏•‡πÑ‡∏Å Attention ‡∏°‡∏µ‡∏ö‡∏ó‡∏ö‡∏≤‡∏ó‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡πÉ‡∏ô‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡πÉ‡∏Å‡∏•‡πâ‡πÄ‡∏Ñ‡∏µ‡∏¢‡∏á‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô‡∏Ç‡∏≠‡∏á NLP ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£‡∏Å‡πá‡∏ï‡∏≤‡∏° ‡∏Å‡∏≤‡∏£‡πÄ‡∏û‡∏¥‡πà‡∏° Attention ‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡∏∂‡πâ‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏°‡∏≤‡∏Å ‡∏ã‡∏∂‡πà‡∏á‡∏ô‡∏≥‡πÑ‡∏õ‡∏™‡∏π‡πà‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏Ç‡∏ô‡∏≤‡∏î‡∏Å‡∏±‡∏ö RNNs ‡∏Ç‡πâ‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏Ç‡∏ô‡∏≤‡∏î RNNs ‡∏Ñ‡∏∑‡∏≠‡∏ò‡∏£‡∏£‡∏°‡∏ä‡∏≤‡∏ï‡∏¥‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÅ‡∏ö‡∏ö recurrent ‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏¢‡∏≤‡∏Å‡∏ï‡πà‡∏≠‡∏Å‡∏≤‡∏£ batch ‡πÅ‡∏•‡∏∞ parallelize ‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å ‡πÉ‡∏ô RNN ‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏≠‡∏á‡∏Ñ‡πå‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏Ç‡∏≠‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏ï‡πâ‡∏≠‡∏á‡∏ñ‡∏π‡∏Å‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏ï‡∏≤‡∏°‡∏•‡∏≥‡∏î‡∏±‡∏ö ‡∏ã‡∏∂‡πà‡∏á‡∏´‡∏°‡∏≤‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ß‡πà‡∏≤‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ parallelize ‡πÑ‡∏î‡πâ‡∏á‡πà‡∏≤‡∏¢

![Encoder Decoder ‡∏û‡∏£‡πâ‡∏≠‡∏° Attention](../../../../../lessons/5-NLP/18-Transformers/images/EncDecAttention.gif)

> ‡∏†‡∏≤‡∏û‡∏à‡∏≤‡∏Å [‡∏ö‡∏•‡πá‡∏≠‡∏Å‡∏Ç‡∏≠‡∏á Google](https://research.googleblog.com/2016/09/a-neural-network-for-machine.html)

‡∏Å‡∏≤‡∏£‡∏ô‡∏≥‡∏Å‡∏•‡πÑ‡∏Å Attention ‡∏°‡∏≤‡πÉ‡∏ä‡πâ‡∏£‡πà‡∏ß‡∏°‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏ô‡∏µ‡πâ‡∏ô‡∏≥‡πÑ‡∏õ‡∏™‡∏π‡πà‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏• Transformer ‡∏ã‡∏∂‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô‡∏Ç‡∏≠‡∏á‡πÄ‡∏ó‡∏Ñ‡πÇ‡∏ô‡πÇ‡∏•‡∏¢‡∏µ ‡πÄ‡∏ä‡πà‡∏ô BERT ‡πÅ‡∏•‡∏∞ Open-GPT3 ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô

## ‡πÇ‡∏°‡πÄ‡∏î‡∏• Transformer

‡∏´‡∏ô‡∏∂‡πà‡∏á‡πÉ‡∏ô‡πÅ‡∏ô‡∏ß‡∏Ñ‡∏¥‡∏î‡∏´‡∏•‡∏±‡∏Å‡∏Ç‡∏≠‡∏á Transformers ‡∏Ñ‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡∏´‡∏•‡∏µ‡∏Å‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á‡∏ò‡∏£‡∏£‡∏°‡∏ä‡∏≤‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÅ‡∏ö‡∏ö‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ç‡∏≠‡∏á RNNs ‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ parallelize ‡πÑ‡∏î‡πâ‡πÉ‡∏ô‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å ‡∏™‡∏¥‡πà‡∏á‡∏ô‡∏µ‡πâ‡∏ó‡∏≥‡πÑ‡∏î‡πâ‡πÇ‡∏î‡∏¢‡∏Å‡∏≤‡∏£‡∏ô‡∏≥‡πÅ‡∏ô‡∏ß‡∏Ñ‡∏¥‡∏î‡∏™‡∏≠‡∏á‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏°‡∏≤‡πÉ‡∏ä‡πâ:

* positional encoding
* ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏Å‡∏•‡πÑ‡∏Å self-attention ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏à‡∏±‡∏ö‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÅ‡∏ó‡∏ô RNNs (‡∏´‡∏£‡∏∑‡∏≠ CNNs) (‡∏ô‡∏±‡πà‡∏ô‡∏Ñ‡∏∑‡∏≠‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡∏ó‡∏µ‡πà‡∏á‡∏≤‡∏ô‡∏ß‡∏¥‡∏à‡∏±‡∏¢‡∏ó‡∏µ‡πà‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥ Transformers ‡∏°‡∏µ‡∏ä‡∏∑‡πà‡∏≠‡∏ß‡πà‡∏≤ *[Attention is all you need](https://arxiv.org/abs/1706.03762)*)

### Positional Encoding/Embedding

‡πÅ‡∏ô‡∏ß‡∏Ñ‡∏¥‡∏î‡∏Ç‡∏≠‡∏á positional encoding ‡∏°‡∏µ‡∏î‡∏±‡∏á‡∏ô‡∏µ‡πâ:
1. ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ RNNs ‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ó‡∏ò‡πå‡∏Ç‡∏≠‡∏á tokens ‡∏ñ‡∏π‡∏Å‡πÅ‡∏ó‡∏ô‡∏î‡πâ‡∏ß‡∏¢‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô ‡∏î‡∏±‡∏á‡∏ô‡∏±‡πâ‡∏ô‡∏à‡∏∂‡∏á‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏™‡∏î‡∏á‡∏≠‡∏≠‡∏Å‡∏°‡∏≤‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô
2. ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£‡∏Å‡πá‡∏ï‡∏≤‡∏° ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏£‡∏≤‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÑ‡∏õ‡πÉ‡∏ä‡πâ Attention ‡πÄ‡∏£‡∏≤‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡∏ó‡∏£‡∏≤‡∏ö‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ó‡∏ò‡πå‡∏Ç‡∏≠‡∏á tokens ‡πÉ‡∏ô‡∏•‡∏≥‡∏î‡∏±‡∏ö
3. ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÑ‡∏î‡πâ positional encoding ‡πÄ‡∏£‡∏≤‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏•‡∏≥‡∏î‡∏±‡∏ö tokens ‡∏î‡πâ‡∏ß‡∏¢‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏Ç‡∏≠‡∏á tokens ‡πÉ‡∏ô‡∏•‡∏≥‡∏î‡∏±‡∏ö (‡πÄ‡∏ä‡πà‡∏ô ‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç 0,1,...)
4. ‡∏à‡∏≤‡∏Å‡∏ô‡∏±‡πâ‡∏ô‡πÄ‡∏£‡∏≤‡∏ú‡∏™‡∏°‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á token ‡∏Å‡∏±‡∏ö‡πÄ‡∏ß‡∏Å‡πÄ‡∏ï‡∏≠‡∏£‡πå embedding ‡∏Ç‡∏≠‡∏á token ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏õ‡∏•‡∏á‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á (‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏ï‡πá‡∏°) ‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏ß‡∏Å‡πÄ‡∏ï‡∏≠‡∏£‡πå ‡πÄ‡∏£‡∏≤‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏ä‡πâ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡∏ï‡πà‡∏≤‡∏á‡πÜ:

* embedding ‡∏ó‡∏µ‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ù‡∏∂‡∏Å‡πÑ‡∏î‡πâ ‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏Å‡∏±‡∏ö embedding ‡∏Ç‡∏≠‡∏á token ‡∏ô‡∏µ‡πà‡∏Ñ‡∏∑‡∏≠‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡πÉ‡∏ô‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πâ ‡πÄ‡∏£‡∏≤‡πÉ‡∏ä‡πâ‡∏ä‡∏±‡πâ‡∏ô embedding ‡∏Å‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á tokens ‡πÅ‡∏•‡∏∞‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏Ç‡∏≠‡∏á‡∏û‡∏ß‡∏Å‡∏°‡∏±‡∏ô ‡∏ã‡∏∂‡πà‡∏á‡∏™‡πà‡∏á‡∏ú‡∏•‡πÉ‡∏´‡πâ‡πÑ‡∏î‡πâ‡πÄ‡∏ß‡∏Å‡πÄ‡∏ï‡∏≠‡∏£‡πå embedding ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏°‡∏¥‡∏ï‡∏¥‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô ‡∏à‡∏≤‡∏Å‡∏ô‡∏±‡πâ‡∏ô‡πÄ‡∏£‡∏≤‡∏à‡∏∂‡∏á‡∏ô‡∏≥‡∏°‡∏≤‡∏ö‡∏ß‡∏Å‡∏Å‡∏±‡∏ô
* ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô encoding ‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡πÅ‡∏ö‡∏ö‡∏Ñ‡∏á‡∏ó‡∏µ‡πà ‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡πÄ‡∏™‡∏ô‡∏≠‡πÉ‡∏ô‡∏á‡∏≤‡∏ô‡∏ß‡∏¥‡∏à‡∏±‡∏¢‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö

<img src="../../../../../translated_images/th/pos-embedding.e41ce9b6cf6078af.webp" width="50%"/>

> ‡∏†‡∏≤‡∏û‡πÇ‡∏î‡∏¢‡∏ú‡∏π‡πâ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô

‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å positional embedding ‡∏à‡∏∞‡∏ù‡∏±‡∏á‡∏ó‡∏±‡πâ‡∏á token ‡∏î‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏¥‡∏°‡πÅ‡∏•‡∏∞‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏Ç‡∏≠‡∏á‡∏°‡∏±‡∏ô‡πÉ‡∏ô‡∏•‡∏≥‡∏î‡∏±‡∏ö

### Multi-Head Self-Attention

‡∏ï‡πà‡∏≠‡πÑ‡∏õ ‡πÄ‡∏£‡∏≤‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡∏à‡∏±‡∏ö‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ö‡∏≤‡∏á‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÉ‡∏ô‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ó‡∏≥‡∏™‡∏¥‡πà‡∏á‡∏ô‡∏µ‡πâ Transformers ‡πÉ‡∏ä‡πâ‡∏Å‡∏•‡πÑ‡∏Å **self-attention** ‡∏ã‡∏∂‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô Attention ‡∏ó‡∏µ‡πà‡∏ô‡∏≥‡πÑ‡∏õ‡πÉ‡∏ä‡πâ‡∏Å‡∏±‡∏ö‡∏•‡∏≥‡∏î‡∏±‡∏ö‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Ç‡πâ‡∏≤‡πÅ‡∏•‡∏∞‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏≠‡∏Å ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ self-attention ‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ‡πÄ‡∏£‡∏≤‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤ **‡∏ö‡∏£‡∏¥‡∏ö‡∏ó** ‡∏†‡∏≤‡∏¢‡πÉ‡∏ô‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ ‡πÅ‡∏•‡∏∞‡∏î‡∏π‡∏ß‡πà‡∏≤‡∏Ñ‡∏≥‡πÉ‡∏î‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏Å‡∏±‡∏ô ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏ä‡πà‡∏ô ‡∏°‡∏±‡∏ô‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ‡πÄ‡∏£‡∏≤‡πÄ‡∏´‡πá‡∏ô‡∏ß‡πà‡∏≤‡∏Ñ‡∏≥‡πÉ‡∏î‡∏ñ‡∏π‡∏Å‡∏≠‡πâ‡∏≤‡∏á‡∏ñ‡∏∂‡∏á‡πÇ‡∏î‡∏¢‡∏Ñ‡∏≥‡∏™‡∏£‡∏£‡∏û‡∏ô‡∏≤‡∏° ‡πÄ‡∏ä‡πà‡∏ô *it* ‡πÅ‡∏•‡∏∞‡∏¢‡∏±‡∏á‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏î‡πâ‡∏ß‡∏¢:

![](../../../../../translated_images/th/CoreferenceResolution.861924d6d384a7d6.webp)

> ‡∏†‡∏≤‡∏û‡∏à‡∏≤‡∏Å [‡∏ö‡∏•‡πá‡∏≠‡∏Å‡∏Ç‡∏≠‡∏á Google](https://research.googleblog.com/2017/08/transformer-novel-neural-network.html)

‡πÉ‡∏ô Transformers ‡πÄ‡∏£‡∏≤‡πÉ‡∏ä‡πâ **Multi-Head Attention** ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÄ‡∏Ñ‡∏£‡∏∑‡∏≠‡∏Ç‡πà‡∏≤‡∏¢‡∏°‡∏µ‡∏û‡∏•‡∏±‡∏á‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏´‡∏•‡∏≤‡∏¢‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó ‡πÄ‡∏ä‡πà‡∏ô ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏£‡∏∞‡∏¢‡∏∞‡∏¢‡∏≤‡∏ß‡∏Å‡∏±‡∏ö‡∏£‡∏∞‡∏¢‡∏∞‡∏™‡∏±‡πâ‡∏ô ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡πÅ‡∏ö‡∏ö co-reference ‡∏Å‡∏±‡∏ö‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏≠‡∏∑‡πà‡∏ô ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏ô

[TensorFlow Notebook](TransformersTF.ipynb) ‡∏°‡∏µ‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡∏Ç‡∏≠‡∏á‡∏ä‡∏±‡πâ‡∏ô Transformer

### Encoder-Decoder Attention

‡πÉ‡∏ô Transformers ‡∏Å‡∏•‡πÑ‡∏Å Attention ‡∏ñ‡∏π‡∏Å‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏™‡∏≠‡∏á‡∏ó‡∏µ‡πà:

* ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏à‡∏±‡∏ö‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏†‡∏≤‡∏¢‡πÉ‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Ç‡πâ‡∏≤‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ self-attention
* ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡πÅ‡∏õ‡∏•‡∏•‡∏≥‡∏î‡∏±‡∏ö - ‡∏ã‡∏∂‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏±‡πâ‡∏ô Attention ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á encoder ‡πÅ‡∏•‡∏∞ decoder

Encoder-decoder attention ‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏Ñ‡∏•‡∏∂‡∏á‡∏Å‡∏±‡∏ö‡∏Å‡∏•‡πÑ‡∏Å Attention ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÉ‡∏ô RNNs ‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡πÉ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ï‡πâ‡∏ô‡∏Ç‡∏≠‡∏á‡∏™‡πà‡∏ß‡∏ô‡∏ô‡∏µ‡πâ ‡πÅ‡∏ú‡∏ô‡∏†‡∏≤‡∏û‡πÅ‡∏ö‡∏ö‡πÄ‡∏Ñ‡∏•‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏´‡∏ß‡∏ô‡∏µ‡πâ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏ö‡∏ó‡∏ö‡∏≤‡∏ó‡∏Ç‡∏≠‡∏á encoder-decoder attention

![GIF ‡πÅ‡∏ö‡∏ö‡πÄ‡∏Ñ‡∏•‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏´‡∏ß‡πÅ‡∏™‡∏î‡∏á‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏•‡πÉ‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏• Transformer](../../../../../lessons/5-NLP/18-Transformers/images/transformer-animated-explanation.gif)

‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Ç‡πâ‡∏≤‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏ñ‡∏π‡∏Å‡πÅ‡∏°‡∏õ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏≠‡∏¥‡∏™‡∏£‡∏∞‡πÑ‡∏õ‡∏¢‡∏±‡∏á‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏≠‡∏Å‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á Transformers ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ parallelize ‡πÑ‡∏î‡πâ‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤ RNNs ‡∏ã‡∏∂‡πà‡∏á‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏†‡∏≤‡∏©‡∏≤‡∏ó‡∏µ‡πà‡πÉ‡∏´‡∏ç‡πà‡∏Ç‡∏∂‡πâ‡∏ô‡πÅ‡∏•‡∏∞‡πÅ‡∏™‡∏î‡∏á‡∏≠‡∏≠‡∏Å‡πÑ‡∏î‡πâ‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô ‡∏´‡∏±‡∏ß Attention ‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏´‡∏±‡∏ß‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏ä‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô ‡∏ã‡∏∂‡πà‡∏á‡∏ä‡πà‡∏ß‡∏¢‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏á‡∏≤‡∏ô‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏†‡∏≤‡∏©‡∏≤‡∏ò‡∏£‡∏£‡∏°‡∏ä‡∏≤‡∏ï‡∏¥‡πÉ‡∏ô‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ï‡πà‡∏≠‡πÑ‡∏õ

## BERT

**BERT** (Bidirectional Encoder Representations from Transformers) ‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏Ñ‡∏£‡∏∑‡∏≠‡∏Ç‡πà‡∏≤‡∏¢ Transformer ‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏´‡∏•‡∏≤‡∏¢‡∏ä‡∏±‡πâ‡∏ô ‡πÇ‡∏î‡∏¢‡∏°‡∏µ 12 ‡∏ä‡∏±‡πâ‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö *BERT-base* ‡πÅ‡∏•‡∏∞ 24 ‡∏ä‡∏±‡πâ‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö *BERT-large* ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ô‡∏µ‡πâ‡∏ñ‡∏π‡∏Å‡∏ù‡∏∂‡∏Å‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô‡∏î‡πâ‡∏ß‡∏¢‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà (WikiPedia + ‡∏´‡∏ô‡∏±‡∏á‡∏™‡∏∑‡∏≠) ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡πÅ‡∏ö‡∏ö‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏Å‡∏≥‡∏Å‡∏±‡∏ö‡∏î‡∏π‡πÅ‡∏• (‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏õ‡∏¥‡∏î‡∏ö‡∏±‡∏á‡πÉ‡∏ô‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ) ‡πÉ‡∏ô‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏à‡∏∞‡∏î‡∏π‡∏î‡∏ã‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏†‡∏≤‡∏©‡∏≤‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏™‡∏π‡∏á ‡∏ã‡∏∂‡πà‡∏á‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ô‡∏≥‡πÑ‡∏õ‡πÉ‡∏ä‡πâ‡∏Å‡∏±‡∏ö‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏∑‡πà‡∏ô‡πÜ ‡∏ú‡πà‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏á‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏° ‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏ô‡∏µ‡πâ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡∏ß‡πà‡∏≤ **transfer learning**

![‡∏†‡∏≤‡∏û‡∏à‡∏≤‡∏Å http://jalammar.github.io/illustrated-bert/](../../../../../translated_images/th/jalammarBERT-language-modeling-masked-lm.34f113ea5fec4362.webp)

> ‡∏†‡∏≤‡∏û [‡πÅ‡∏´‡∏•‡πà‡∏á‡∏ó‡∏µ‡πà‡∏°‡∏≤](http://jalammar.github.io/illustrated-bert/)

## ‚úçÔ∏è ‡πÅ‡∏ö‡∏ö‡∏ù‡∏∂‡∏Å‡∏´‡∏±‡∏î: Transformers

‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡πÉ‡∏ô‡πÇ‡∏ô‡πâ‡∏ï‡∏ö‡∏∏‡πä‡∏Å‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ:

* [Transformers ‡πÉ‡∏ô PyTorch](TransformersPyTorch.ipynb)
* [Transformers ‡πÉ‡∏ô TensorFlow](TransformersTF.ipynb)

## ‡∏™‡∏£‡∏∏‡∏õ

‡πÉ‡∏ô‡∏ö‡∏ó‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏ô‡∏µ‡πâ‡∏Ñ‡∏∏‡∏ì‡πÑ‡∏î‡πâ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö Transformers ‡πÅ‡∏•‡∏∞‡∏Å‡∏•‡πÑ‡∏Å Attention ‡∏ã‡∏∂‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏°‡∏∑‡∏≠‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡πÉ‡∏ô‡∏ä‡∏∏‡∏î‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏°‡∏∑‡∏≠ NLP ‡∏°‡∏µ‡∏™‡∏ñ‡∏≤‡∏õ‡∏±‡∏ï‡∏¢‡∏Å‡∏£‡∏£‡∏° Transformer ‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö ‡πÄ‡∏ä‡πà‡∏ô BERT, DistilBERT, BigBird, OpenGPT3 ‡πÅ‡∏•‡∏∞‡∏≠‡∏∑‡πà‡∏ô‡πÜ ‡∏ó‡∏µ‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏á‡πÑ‡∏î‡πâ [‡πÅ‡∏û‡πá‡∏Å‡πÄ‡∏Å‡∏à HuggingFace](https://github.com/huggingface/) ‡∏°‡∏µ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡πá‡∏ö‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡∏™‡∏ñ‡∏≤‡∏õ‡∏±‡∏ï‡∏¢‡∏Å‡∏£‡∏£‡∏°‡πÄ‡∏´‡∏•‡πà‡∏≤‡∏ô‡∏µ‡πâ‡∏´‡∏•‡∏≤‡∏¢‡πÅ‡∏ö‡∏ö‡∏î‡πâ‡∏ß‡∏¢ PyTorch ‡πÅ‡∏•‡∏∞ TensorFlow

## üöÄ ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡πâ‡∏≤‡∏ó‡∏≤‡∏¢

## [‡πÅ‡∏ö‡∏ö‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏´‡∏•‡∏±‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏ô](https://ff-quizzes.netlify.app/en/ai/quiz/36)

## ‡∏Å‡∏≤‡∏£‡∏ó‡∏ö‡∏ó‡∏ß‡∏ô‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏î‡πâ‡∏ß‡∏¢‡∏ï‡∏ô‡πÄ‡∏≠‡∏á

* [‡∏ö‡∏•‡πá‡∏≠‡∏Å‡πÇ‡∏û‡∏™‡∏ï‡πå](https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/) ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏ß‡∏¥‡∏à‡∏±‡∏¢‡∏Ñ‡∏•‡∏≤‡∏™‡∏™‡∏¥‡∏Å [Attention is all you need](https://arxiv.org/abs/1706.03762) ‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö Transformers
* [‡∏ä‡∏∏‡∏î‡∏ö‡∏•‡πá‡∏≠‡∏Å‡πÇ‡∏û‡∏™‡∏ï‡πå](https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452) ‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö Transformers ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏™‡∏ñ‡∏≤‡∏õ‡∏±‡∏ï‡∏¢‡∏Å‡∏£‡∏£‡∏°‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î

## [‡∏á‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏°‡∏≠‡∏ö‡∏´‡∏°‡∏≤‡∏¢](assignment.md)

---

