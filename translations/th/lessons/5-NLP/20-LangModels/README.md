# โมเดลภาษาขนาดใหญ่ที่ผ่านการฝึกฝนล่วงหน้า

ในทุกงานที่เราเคยทำมาก่อนหน้านี้ เราได้ฝึกฝนเครือข่ายประสาทเพื่อทำงานบางอย่างโดยใช้ชุดข้อมูลที่มีการติดป้ายกำกับ แต่สำหรับโมเดลทรานส์ฟอร์เมอร์ขนาดใหญ่ เช่น BERT เราใช้การสร้างแบบจำลองภาษาในรูปแบบที่เรียนรู้ด้วยตัวเอง (self-supervised) เพื่อสร้างโมเดลภาษา ซึ่งต่อมาจะถูกปรับให้เหมาะสมกับงานเฉพาะทางด้วยการฝึกฝนเพิ่มเติมในโดเมนเฉพาะ อย่างไรก็ตาม มีการพิสูจน์แล้วว่าโมเดลภาษาขนาดใหญ่สามารถแก้ไขงานหลายอย่างได้โดยไม่ต้องมีการฝึกฝนเฉพาะทางเลย โมเดลที่สามารถทำเช่นนี้ได้เรียกว่า **GPT**: Generative Pre-Trained Transformer

## [แบบทดสอบก่อนเรียน](https://ff-quizzes.netlify.app/en/ai/quiz/39)

## การสร้างข้อความและ Perplexity

แนวคิดของเครือข่ายประสาทที่สามารถทำงานทั่วไปได้โดยไม่ต้องมีการฝึกฝนเพิ่มเติมถูกนำเสนอในงานวิจัย [Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) แนวคิดหลักคือ งานอื่นๆ หลายงานสามารถถูกจำลองโดยใช้ **การสร้างข้อความ** เพราะการเข้าใจข้อความหมายถึงการสามารถสร้างข้อความได้ เนื่องจากโมเดลถูกฝึกฝนด้วยข้อความจำนวนมหาศาลที่ครอบคลุมความรู้ของมนุษย์ มันจึงมีความรู้เกี่ยวกับหัวข้อที่หลากหลาย

> การเข้าใจและสามารถสร้างข้อความได้ยังหมายถึงการรู้บางสิ่งเกี่ยวกับโลกที่อยู่รอบตัวเราอีกด้วย คนเราก็เรียนรู้จากการอ่านในระดับมาก และเครือข่าย GPT ก็คล้ายคลึงในแง่นี้

เครือข่ายการสร้างข้อความทำงานโดยการทำนายความน่าจะเป็นของคำถัดไป $$P(w_N)$$ อย่างไรก็ตาม ความน่าจะเป็นแบบไม่มีเงื่อนไขของคำถัดไปเท่ากับความถี่ของคำนี้ในชุดข้อความ GPT สามารถให้ **ความน่าจะเป็นแบบมีเงื่อนไข** ของคำถัดไป โดยพิจารณาจากคำก่อนหน้า: $$P(w_N | w_{n-1}, ..., w_0)$$

> คุณสามารถอ่านเพิ่มเติมเกี่ยวกับความน่าจะเป็นใน [หลักสูตร Data Science สำหรับผู้เริ่มต้น](https://github.com/microsoft/Data-Science-For-Beginners/tree/main/1-Introduction/04-stats-and-probability)

คุณภาพของโมเดลการสร้างข้อความสามารถวัดได้โดยใช้ **perplexity** ซึ่งเป็นตัวชี้วัดภายในที่ช่วยให้เราวัดคุณภาพของโมเดลโดยไม่ต้องใช้ชุดข้อมูลเฉพาะงาน มันอิงตามแนวคิดของ *ความน่าจะเป็นของประโยค* - โมเดลจะกำหนดความน่าจะเป็นสูงให้กับประโยคที่มีแนวโน้มว่าจะเป็นจริง (เช่น โมเดลไม่ **perplexed** กับมัน) และความน่าจะเป็นต่ำให้กับประโยคที่ดูไม่มีเหตุผล (เช่น *Can it does what?*) เมื่อเราให้โมเดลของเราประโยคจากชุดข้อความจริง เราคาดหวังว่ามันจะมีความน่าจะเป็นสูง และ **perplexity** ต่ำ คำนิยามทางคณิตศาสตร์คือความน่าจะเป็นผกผันที่ถูกปรับให้เป็นปกติของชุดทดสอบ:
$$
\mathrm{Perplexity}(W) = \sqrt[N]{1\over P(W_1,...,W_N)}
$$ 

**คุณสามารถทดลองการสร้างข้อความโดยใช้ [ตัวแก้ไขข้อความที่ขับเคลื่อนด้วย GPT จาก Hugging Face](https://transformer.huggingface.co/doc/gpt2-large)** ในตัวแก้ไขนี้ คุณเริ่มเขียนข้อความของคุณ และเมื่อกด **[TAB]** จะมีตัวเลือกการเติมข้อความให้คุณ หากข้อความสั้นเกินไป หรือคุณไม่พอใจกับมัน - กด [TAB] อีกครั้ง และคุณจะมีตัวเลือกเพิ่มเติม รวมถึงข้อความที่ยาวขึ้น

## GPT คือครอบครัวของโมเดล

GPT ไม่ใช่โมเดลเดียว แต่เป็นชุดของโมเดลที่ถูกพัฒนาและฝึกฝนโดย [OpenAI](https://openai.com)

ภายใต้โมเดล GPT เรามี:

| [GPT-2](https://huggingface.co/docs/transformers/model_doc/gpt2#openai-gpt2) | [GPT 3](https://openai.com/research/language-models-are-few-shot-learners) | [GPT-4](https://openai.com/gpt-4) |
| -- | -- | -- |
|โมเดลภาษาที่มีพารามิเตอร์สูงสุดถึง 1.5 พันล้านตัว | โมเดลภาษาที่มีพารามิเตอร์สูงสุดถึง 175 พันล้านตัว | 100T พารามิเตอร์และรองรับทั้งข้อมูลภาพและข้อความ และส่งออกเป็นข้อความ |

โมเดล GPT-3 และ GPT-4 มีให้บริการ [ในรูปแบบบริการ Cognitive Service จาก Microsoft Azure](https://azure.microsoft.com/en-us/services/cognitive-services/openai-service/#overview?WT.mc_id=academic-77998-cacaste) และในรูปแบบ [OpenAI API](https://openai.com/api/)

## การออกแบบคำสั่ง (Prompt Engineering)

เนื่องจาก GPT ได้รับการฝึกฝนด้วยข้อมูลจำนวนมหาศาลเพื่อเข้าใจภาษาและโค้ด มันจึงให้ผลลัพธ์ในตอบสนองต่ออินพุต (คำสั่งหรือ prompt) คำสั่งคืออินพุตหรือคำถามที่เรามอบให้โมเดลเพื่อให้มันทำงานที่เราต้องการให้เสร็จสมบูรณ์ เพื่อให้ได้ผลลัพธ์ที่ต้องการ คุณต้องมีคำสั่งที่มีประสิทธิภาพที่สุด ซึ่งเกี่ยวข้องกับการเลือกคำ รูปแบบ วลี หรือแม้แต่สัญลักษณ์ที่เหมาะสม วิธีการนี้เรียกว่า [Prompt Engineering](https://learn.microsoft.com/en-us/shows/ai-show/the-basics-of-prompt-engineering-with-azure-openai-service?WT.mc_id=academic-77998-bethanycheum)

[เอกสารนี้](https://learn.microsoft.com/en-us/semantic-kernel/prompt-engineering/?WT.mc_id=academic-77998-bethanycheum) ให้ข้อมูลเพิ่มเติมเกี่ยวกับการออกแบบคำสั่ง

## ✍️ ตัวอย่างโน้ตบุ๊ก: [เล่นกับ OpenAI-GPT](GPT-PyTorch.ipynb)

เรียนรู้เพิ่มเติมในโน้ตบุ๊กต่อไปนี้:

* [การสร้างข้อความด้วย OpenAI-GPT และ Hugging Face Transformers](GPT-PyTorch.ipynb)

## สรุป

โมเดลภาษาที่ผ่านการฝึกฝนล่วงหน้าแบบทั่วไปไม่ได้เพียงแค่จำลองโครงสร้างภาษา แต่ยังมีข้อมูลจำนวนมหาศาลในภาษาธรรมชาติ ดังนั้นมันจึงสามารถถูกใช้แก้ไขงาน NLP บางอย่างได้อย่างมีประสิทธิภาพในรูปแบบ zero-shot หรือ few-shot

## [แบบทดสอบหลังเรียน](https://ff-quizzes.netlify.app/en/ai/quiz/40)

---

