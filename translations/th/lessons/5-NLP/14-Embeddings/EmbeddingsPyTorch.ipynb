{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## การฝังข้อมูล\n",
    "\n",
    "ในตัวอย่างก่อนหน้านี้ เราได้ทำงานกับเวกเตอร์แบบถุงคำ (bag-of-words) ที่มีมิติสูงและมีความยาวเท่ากับ `vocab_size` โดยเราได้แปลงจากเวกเตอร์ตัวแทนตำแหน่งที่มีมิติต่ำไปเป็นตัวแทนแบบ one-hot ที่มีความเบาบาง (sparse) อย่างชัดเจน อย่างไรก็ตาม ตัวแทนแบบ one-hot นี้ไม่ประหยัดหน่วยความจำ และนอกจากนี้ แต่ละคำยังถูกพิจารณาแยกจากกันโดยสิ้นเชิง กล่าวคือ เวกเตอร์ที่เข้ารหัสแบบ one-hot ไม่ได้แสดงถึงความคล้ายคลึงทางความหมายระหว่างคำต่าง ๆ\n",
    "\n",
    "ในหน่วยนี้ เราจะสำรวจชุดข้อมูล **News AG** ต่อไป เพื่อเริ่มต้น เรามาโหลดข้อมูลและดึงคำจำกัดความบางส่วนจากสมุดบันทึกก่อนหน้านี้กัน\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\WORK\\ai-for-beginners\\5-NLP\\14-Embeddings\\data\\train.csv: 29.5MB [00:01, 18.8MB/s]                            \n",
      "d:\\WORK\\ai-for-beginners\\5-NLP\\14-Embeddings\\data\\test.csv: 1.86MB [00:00, 11.2MB/s]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building vocab...\n",
      "Vocab size =  95812\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "import numpy as np\n",
    "from torchnlp import *\n",
    "train_dataset, test_dataset, classes, vocab = load_dataset()\n",
    "vocab_size = len(vocab)\n",
    "print(\"Vocab size = \",vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## การฝังข้อมูลคืออะไร?\n",
    "\n",
    "แนวคิดของ **การฝังข้อมูล** คือการแทนคำด้วยเวกเตอร์ที่มีมิติที่ต่ำกว่าและหนาแน่น ซึ่งสะท้อนถึงความหมายเชิงความหมายของคำในบางรูปแบบ เราจะพูดถึงวิธีการสร้างการฝังคำที่มีความหมายในภายหลัง แต่ตอนนี้ให้คิดว่าการฝังข้อมูลเป็นวิธีลดมิติของเวกเตอร์คำ\n",
    "\n",
    "ดังนั้น ชั้นการฝังข้อมูลจะรับคำเป็นข้อมูลเข้า และสร้างเวกเตอร์ผลลัพธ์ที่มีขนาด `embedding_size` ที่กำหนดไว้ ในแง่หนึ่ง มันคล้ายกับชั้น `Linear` แต่แทนที่จะรับเวกเตอร์ที่เข้ารหัสแบบ one-hot มันจะสามารถรับหมายเลขคำเป็นข้อมูลเข้าได้\n",
    "\n",
    "โดยการใช้ชั้นการฝังข้อมูลเป็นชั้นแรกในเครือข่ายของเรา เราสามารถเปลี่ยนจากโมเดล bag-of-words ไปเป็นโมเดล **embedding bag** ซึ่งเราจะเปลี่ยนคำแต่ละคำในข้อความของเราให้เป็นการฝังข้อมูลที่สอดคล้องกัน และจากนั้นคำนวณฟังก์ชันรวมบางอย่างจากการฝังข้อมูลเหล่านั้น เช่น `sum`, `average` หรือ `max`\n",
    "\n",
    "![ภาพแสดงตัวอย่างตัวจำแนกที่ใช้การฝังข้อมูลสำหรับคำในลำดับห้าคำ](../../../../../translated_images/th/embedding-classifier-example.b77f021a7ee67eee.webp)\n",
    "\n",
    "เครือข่ายประสาทเทียมของเราจะเริ่มต้นด้วยชั้นการฝังข้อมูล ตามด้วยชั้นการรวม และตัวจำแนกเชิงเส้นที่อยู่ด้านบน\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbedClassifier(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super().__init__()\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, embed_dim)\n",
    "        self.fc = torch.nn.Linear(embed_dim, num_class)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = torch.mean(x,dim=1)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### การจัดการกับขนาดลำดับตัวแปร\n",
    "\n",
    "จากสถาปัตยกรรมนี้ ทำให้เราจำเป็นต้องสร้าง minibatches สำหรับเครือข่ายของเราในรูปแบบที่เฉพาะเจาะจง ในหน่วยก่อนหน้า เมื่อเราใช้ bag-of-words ขนาดของ BoW tensors ในแต่ละ minibatch จะมีขนาดเท่ากันคือ `vocab_size` โดยไม่คำนึงถึงความยาวจริงของลำดับข้อความของเรา แต่เมื่อเราเปลี่ยนมาใช้ word embeddings เราจะพบว่าจำนวนคำในแต่ละตัวอย่างข้อความอาจแตกต่างกัน และเมื่อรวมตัวอย่างเหล่านั้นเข้าด้วยกันใน minibatches เราจำเป็นต้องใช้การเติมค่า (padding)\n",
    "\n",
    "สิ่งนี้สามารถทำได้โดยใช้เทคนิคเดียวกันกับการให้ฟังก์ชัน `collate_fn` กับ datasource:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padify(b):\n",
    "    # b is the list of tuples of length batch_size\n",
    "    #   - first element of a tuple = label, \n",
    "    #   - second = feature (text sequence)\n",
    "    # build vectorized sequence\n",
    "    v = [encode(x[1]) for x in b]\n",
    "    # first, compute max length of a sequence in this minibatch\n",
    "    l = max(map(len,v))\n",
    "    return ( # tuple of two tensors - labels and features\n",
    "        torch.LongTensor([t[0]-1 for t in b]),\n",
    "        torch.stack([torch.nn.functional.pad(torch.tensor(t),(0,l-len(t)),mode='constant',value=0) for t in v])\n",
    "    )\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, collate_fn=padify, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### การฝึกตัวแยกประเภทแบบฝังตัว\n",
    "\n",
    "ตอนนี้เมื่อเราได้กำหนด dataloader ที่เหมาะสมแล้ว เราสามารถฝึกโมเดลได้โดยใช้ฟังก์ชันการฝึกที่เราได้กำหนดไว้ในหน่วยก่อนหน้า:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200: acc=0.6415625\n",
      "6400: acc=0.6865625\n",
      "9600: acc=0.7103125\n",
      "12800: acc=0.726953125\n",
      "16000: acc=0.739375\n",
      "19200: acc=0.75046875\n",
      "22400: acc=0.7572321428571429\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.889799795315499, 0.7623160588611644)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = EmbedClassifier(vocab_size,32,len(classes)).to(device)\n",
    "train_epoch(net,train_loader, lr=1, epoch_size=25000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **หมายเหตุ**: ที่นี่เรากำลังฝึกเพียง 25,000 รายการ (น้อยกว่าหนึ่ง epoch เต็ม) เพื่อประหยัดเวลา แต่คุณสามารถฝึกต่อไปได้ เขียนฟังก์ชันเพื่อฝึกหลายๆ epoch และทดลองปรับพารามิเตอร์อัตราการเรียนรู้เพื่อให้ได้ความแม่นยำที่สูงขึ้น คุณควรจะสามารถเพิ่มความแม่นยำได้ถึงประมาณ 90%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ชั้น EmbeddingBag และการแสดงผลลำดับที่มีความยาวแปรผัน\n",
    "\n",
    "ในสถาปัตยกรรมก่อนหน้านี้ เราจำเป็นต้องเติมข้อมูลในทุกลำดับให้มีความยาวเท่ากันเพื่อให้สามารถใส่ลงในชุดข้อมูลย่อยได้ วิธีนี้ไม่ใช่วิธีที่มีประสิทธิภาพที่สุดในการแสดงผลลำดับที่มีความยาวแปรผัน - อีกวิธีหนึ่งคือการใช้ **เวกเตอร์ออฟเซ็ต** ซึ่งจะเก็บค่าตำแหน่งเริ่มต้นของลำดับทั้งหมดที่ถูกจัดเก็บในเวกเตอร์ขนาดใหญ่หนึ่งตัว\n",
    "\n",
    "![ภาพแสดงการแสดงผลลำดับแบบออฟเซ็ต](../../../../../translated_images/th/offset-sequence-representation.eb73fcefb29b46ee.webp)\n",
    "\n",
    "> **Note**: ในภาพด้านบน เราแสดงลำดับของตัวอักษร แต่ในตัวอย่างของเรา เรากำลังทำงานกับลำดับของคำ อย่างไรก็ตาม หลักการทั่วไปในการแสดงผลลำดับด้วยเวกเตอร์ออฟเซ็ตยังคงเหมือนเดิม\n",
    "\n",
    "ในการทำงานกับการแสดงผลแบบออฟเซ็ต เราใช้ชั้น [`EmbeddingBag`](https://pytorch.org/docs/stable/generated/torch.nn.EmbeddingBag.html) ซึ่งคล้ายกับ `Embedding` แต่จะรับเวกเตอร์เนื้อหาและเวกเตอร์ออฟเซ็ตเป็นข้อมูลนำเข้า และยังมีชั้นการเฉลี่ยที่สามารถเป็น `mean`, `sum` หรือ `max` ได้\n",
    "\n",
    "นี่คือตัวอย่างเครือข่ายที่ปรับปรุงแล้วซึ่งใช้ `EmbeddingBag`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbedClassifier(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super().__init__()\n",
    "        self.embedding = torch.nn.EmbeddingBag(vocab_size, embed_dim)\n",
    "        self.fc = torch.nn.Linear(embed_dim, num_class)\n",
    "\n",
    "    def forward(self, text, off):\n",
    "        x = self.embedding(text, off)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ในการเตรียมชุดข้อมูลสำหรับการฝึก เราจำเป็นต้องจัดเตรียมฟังก์ชันการแปลงที่จะเตรียมเวกเตอร์ออฟเซ็ต:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def offsetify(b):\n",
    "    # first, compute data tensor from all sequences\n",
    "    x = [torch.tensor(encode(t[1])) for t in b]\n",
    "    # now, compute the offsets by accumulating the tensor of sequence lengths\n",
    "    o = [0] + [len(t) for t in x]\n",
    "    o = torch.tensor(o[:-1]).cumsum(dim=0)\n",
    "    return ( \n",
    "        torch.LongTensor([t[0]-1 for t in b]), # labels\n",
    "        torch.cat(x), # text \n",
    "        o\n",
    "    )\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, collate_fn=offsetify, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "โปรดทราบว่าแตกต่างจากตัวอย่างก่อนหน้านี้ทั้งหมด เครือข่ายของเราตอนนี้รับพารามิเตอร์สองตัว: เวกเตอร์ข้อมูลและเวกเตอร์ออฟเซ็ต ซึ่งมีขนาดต่างกัน เช่นเดียวกัน ตัวโหลดข้อมูลของเรายังให้ค่ามา 3 ค่าแทนที่จะเป็น 2: ทั้งเวกเตอร์ข้อความและเวกเตอร์ออฟเซ็ตถูกจัดเตรียมเป็นฟีเจอร์ ดังนั้นเราจำเป็นต้องปรับฟังก์ชันการฝึกของเราเล็กน้อยเพื่อจัดการกับสิ่งนี้:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200: acc=0.6153125\n",
      "6400: acc=0.6615625\n",
      "9600: acc=0.6932291666666667\n",
      "12800: acc=0.715078125\n",
      "16000: acc=0.7270625\n",
      "19200: acc=0.7382291666666667\n",
      "22400: acc=0.7486160714285715\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(22.771553103007037, 0.7551983365323096)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = EmbedClassifier(vocab_size,32,len(classes)).to(device)\n",
    "\n",
    "def train_epoch_emb(net,dataloader,lr=0.01,optimizer=None,loss_fn = torch.nn.CrossEntropyLoss(),epoch_size=None, report_freq=200):\n",
    "    optimizer = optimizer or torch.optim.Adam(net.parameters(),lr=lr)\n",
    "    loss_fn = loss_fn.to(device)\n",
    "    net.train()\n",
    "    total_loss,acc,count,i = 0,0,0,0\n",
    "    for labels,text,off in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        labels,text,off = labels.to(device), text.to(device), off.to(device)\n",
    "        out = net(text, off)\n",
    "        loss = loss_fn(out,labels) #cross_entropy(out,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss+=loss\n",
    "        _,predicted = torch.max(out,1)\n",
    "        acc+=(predicted==labels).sum()\n",
    "        count+=len(labels)\n",
    "        i+=1\n",
    "        if i%report_freq==0:\n",
    "            print(f\"{count}: acc={acc.item()/count}\")\n",
    "        if epoch_size and count>epoch_size:\n",
    "            break\n",
    "    return total_loss.item()/count, acc.item()/count\n",
    "\n",
    "\n",
    "train_epoch_emb(net,train_loader, lr=4, epoch_size=25000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## การฝังความหมาย: Word2Vec\n",
    "\n",
    "ในตัวอย่างก่อนหน้านี้ เลเยอร์ฝังตัวแบบจำลองได้เรียนรู้ที่จะจับคู่คำกับการแสดงผลในรูปแบบเวกเตอร์ อย่างไรก็ตาม การแสดงผลนี้ไม่ได้มีความหมายในเชิงความหมายมากนัก จะดีกว่าหากเราสามารถเรียนรู้การแสดงผลในรูปแบบเวกเตอร์ที่คำที่มีความหมายคล้ายกันหรือคำพ้องความหมายจะถูกจับคู่กับเวกเตอร์ที่อยู่ใกล้กันในแง่ของระยะเวกเตอร์บางประเภท (เช่น ระยะทางแบบยุคลิด)\n",
    "\n",
    "เพื่อทำเช่นนั้น เราจำเป็นต้องฝึกฝนแบบจำลองการฝังตัวของเราล่วงหน้าด้วยชุดข้อความขนาดใหญ่ในวิธีการเฉพาะ หนึ่งในวิธีแรก ๆ ในการฝึกฝนการฝังความหมายเรียกว่า [Word2Vec](https://en.wikipedia.org/wiki/Word2vec) ซึ่งอิงตามสถาปัตยกรรมหลักสองแบบที่ใช้ในการสร้างการแสดงผลแบบกระจายของคำ:\n",
    "\n",
    " - **Continuous bag-of-words** (CBoW) — ในสถาปัตยกรรมนี้ เราฝึกแบบจำลองให้ทำนายคำจากบริบทโดยรอบ โดยให้ ngram $(W_{-2},W_{-1},W_0,W_1,W_2)$ เป้าหมายของแบบจำลองคือการทำนาย $W_0$ จาก $(W_{-2},W_{-1},W_1,W_2)$\n",
    " - **Continuous skip-gram** ตรงข้ามกับ CBoW แบบจำลองจะใช้หน้าต่างบริบทของคำรอบ ๆ เพื่อทำนายคำปัจจุบัน\n",
    "\n",
    "CBoW ทำงานได้เร็วกว่า ในขณะที่ skip-gram ช้ากว่า แต่สามารถแสดงคำที่พบได้น้อยได้ดีกว่า\n",
    "\n",
    "![ภาพแสดงอัลกอริทึม CBoW และ Skip-Gram สำหรับการแปลงคำเป็นเวกเตอร์](../../../../../translated_images/th/example-algorithms-for-converting-words-to-vectors.fbe9207a726922f6.webp)\n",
    "\n",
    "เพื่อทดลองใช้การฝัง Word2Vec ที่ได้รับการฝึกฝนล่วงหน้าบนชุดข้อมูล Google News เราสามารถใช้ไลบรารี **gensim** ด้านล่างนี้คือตัวอย่างการค้นหาคำที่คล้ายกับ 'neural' มากที่สุด\n",
    "\n",
    "> **Note:** เมื่อคุณสร้างเวกเตอร์คำครั้งแรก การดาวน์โหลดอาจใช้เวลาสักครู่!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "w2v = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neuronal -> 0.7804799675941467\n",
      "neurons -> 0.7326500415802002\n",
      "neural_circuits -> 0.7252851724624634\n",
      "neuron -> 0.7174385190010071\n",
      "cortical -> 0.6941086649894714\n",
      "brain_circuitry -> 0.6923246383666992\n",
      "synaptic -> 0.6699118614196777\n",
      "neural_circuitry -> 0.6638563275337219\n",
      "neurochemical -> 0.6555314064025879\n",
      "neuronal_activity -> 0.6531826257705688\n"
     ]
    }
   ],
   "source": [
    "for w,p in w2v.most_similar('neural'):\n",
    "    print(f\"{w} -> {p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "เรายังสามารถคำนวณเวกเตอร์เอมเบดดิ้งจากคำ เพื่อใช้ในการฝึกโมเดลการจำแนกประเภท (เราแสดงเฉพาะ 20 องค์ประกอบแรกของเวกเตอร์เพื่อความชัดเจน):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01226807,  0.06225586,  0.10693359,  0.05810547,  0.23828125,\n",
       "        0.03686523,  0.05151367, -0.20703125,  0.01989746,  0.10058594,\n",
       "       -0.03759766, -0.1015625 , -0.15820312, -0.08105469, -0.0390625 ,\n",
       "       -0.05053711,  0.16015625,  0.2578125 ,  0.10058594, -0.25976562],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.word_vec('play')[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('queen', 0.7118192911148071)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar(positive=['king','woman'],negative=['man'])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ทั้ง CBoW และ Skip-Grams เป็นการฝังคำแบบ “predictive” ซึ่งหมายความว่ามันพิจารณาเฉพาะบริบทในพื้นที่ใกล้เคียงเท่านั้น Word2Vec ไม่ได้ใช้ประโยชน์จากบริบทในภาพรวม\n",
    "\n",
    "**FastText** พัฒนาต่อยอดจาก Word2Vec โดยการเรียนรู้การแสดงผลแบบเวกเตอร์สำหรับแต่ละคำและ n-grams ของตัวอักษรที่พบในแต่ละคำ ค่าของการแสดงผลเหล่านี้จะถูกเฉลี่ยเป็นเวกเตอร์เดียวในแต่ละขั้นตอนการฝึก แม้ว่าวิธีนี้จะเพิ่มการคำนวณเพิ่มเติมในขั้นตอนการฝึกเบื้องต้น แต่มันช่วยให้การฝังคำสามารถเข้ารหัสข้อมูลในระดับย่อยของคำได้\n",
    "\n",
    "อีกวิธีหนึ่งคือ **GloVe** ซึ่งใช้แนวคิดของเมทริกซ์การเกิดร่วมกัน (co-occurrence matrix) และใช้วิธีการแบบประสาทเทียม (neural methods) ในการแยกเมทริกซ์การเกิดร่วมกันออกเป็นเวกเตอร์คำที่มีความหมายลึกซึ้งและไม่เป็นเชิงเส้นมากขึ้น\n",
    "\n",
    "คุณสามารถทดลองเปลี่ยนการฝังคำเป็น FastText และ GloVe ได้ เนื่องจาก gensim รองรับโมเดลการฝังคำที่หลากหลาย\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## การใช้ Pre-Trained Embeddings ใน PyTorch\n",
    "\n",
    "เราสามารถปรับตัวอย่างด้านบนเพื่อเติมข้อมูลในเมทริกซ์ของ embedding layer ด้วย embedding ที่มีความหมาย เช่น Word2Vec เราต้องคำนึงถึงว่าคำศัพท์ของ pre-trained embedding และ text corpus ของเราอาจไม่ตรงกัน ดังนั้นเราจะกำหนดค่า weights สำหรับคำที่ขาดหายไปด้วยค่าที่สุ่มขึ้นมา:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding size: 300\n",
      "Populating matrix, this will take some time...Done, found 41080 words, 54732 words missing\n"
     ]
    }
   ],
   "source": [
    "embed_size = len(w2v.get_vector('hello'))\n",
    "print(f'Embedding size: {embed_size}')\n",
    "\n",
    "net = EmbedClassifier(vocab_size,embed_size,len(classes))\n",
    "\n",
    "print('Populating matrix, this will take some time...',end='')\n",
    "found, not_found = 0,0\n",
    "for i,w in enumerate(vocab.get_itos()):\n",
    "    try:\n",
    "        net.embedding.weight[i].data = torch.tensor(w2v.get_vector(w))\n",
    "        found+=1\n",
    "    except:\n",
    "        net.embedding.weight[i].data = torch.normal(0.0,1.0,(embed_size,))\n",
    "        not_found+=1\n",
    "\n",
    "print(f\"Done, found {found} words, {not_found} words missing\")\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200: acc=0.6359375\n",
      "6400: acc=0.68109375\n",
      "9600: acc=0.7067708333333333\n",
      "12800: acc=0.723671875\n",
      "16000: acc=0.73625\n",
      "19200: acc=0.7463541666666667\n",
      "22400: acc=0.7560714285714286\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(214.1013875559821, 0.7626759436980166)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_epoch_emb(net,train_loader, lr=4, epoch_size=25000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ในกรณีของเรา เราไม่เห็นการเพิ่มขึ้นของความแม่นยำอย่างมาก ซึ่งน่าจะเกิดจากคำศัพท์ที่แตกต่างกันมาก  \n",
    "เพื่อแก้ปัญหาคำศัพท์ที่แตกต่างกัน เราสามารถใช้วิธีแก้ไขปัญหาดังต่อไปนี้:  \n",
    "* ฝึกโมเดล word2vec ใหม่โดยใช้คำศัพท์ของเรา  \n",
    "* โหลดชุดข้อมูลของเราด้วยคำศัพท์จากโมเดล word2vec ที่ถูกฝึกไว้ล่วงหน้า โดยสามารถกำหนดคำศัพท์ที่ใช้โหลดชุดข้อมูลได้ในระหว่างการโหลด  \n",
    "\n",
    "วิธีหลังดูเหมือนจะง่ายกว่า โดยเฉพาะอย่างยิ่งเพราะ PyTorch `torchtext` framework มีการรองรับ embeddings ในตัวอยู่แล้ว เราสามารถสร้างคำศัพท์ที่ใช้ GloVe ได้ในลักษณะดังนี้:  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 399999/400000 [00:15<00:00, 25411.14it/s]\n"
     ]
    }
   ],
   "source": [
    "vocab = torchtext.vocab.GloVe(name='6B', dim=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "คำศัพท์ที่โหลดมามีการดำเนินการพื้นฐานดังนี้:\n",
    "* พจนานุกรม `vocab.stoi` ช่วยให้เราสามารถแปลงคำให้เป็นดัชนีในพจนานุกรม\n",
    "* `vocab.itos` ทำงานตรงกันข้าม - แปลงตัวเลขกลับเป็นคำ\n",
    "* `vocab.vectors` คืออาร์เรย์ของเวกเตอร์ฝังตัว ดังนั้นเพื่อดึงเวกเตอร์ฝังตัวของคำ `s` เราจำเป็นต้องใช้ `vocab.vectors[vocab.stoi[s]]`\n",
    "\n",
    "นี่คือตัวอย่างการจัดการเวกเตอร์ฝังตัวเพื่อแสดงสมการ **kind-man+woman = queen** (ฉันต้องปรับค่าสัมประสิทธิ์เล็กน้อยเพื่อให้มันทำงานได้):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'queen'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the vector corresponding to kind-man+woman\n",
    "qvec = vocab.vectors[vocab.stoi['king']]-vocab.vectors[vocab.stoi['man']]+1.3*vocab.vectors[vocab.stoi['woman']]\n",
    "# find the index of the closest embedding vector \n",
    "d = torch.sum((vocab.vectors-qvec)**2,dim=1)\n",
    "min_idx = torch.argmin(d)\n",
    "# find the corresponding word\n",
    "vocab.itos[min_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ในการฝึกตัวจำแนกโดยใช้การฝังเหล่านั้น เราต้องเข้ารหัสชุดข้อมูลของเราก่อนโดยใช้คำศัพท์ของ GloVe:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def offsetify(b):\n",
    "    # first, compute data tensor from all sequences\n",
    "    x = [torch.tensor(encode(t[1],voc=vocab)) for t in b] # pass the instance of vocab to encode function!\n",
    "    # now, compute the offsets by accumulating the tensor of sequence lengths\n",
    "    o = [0] + [len(t) for t in x]\n",
    "    o = torch.tensor(o[:-1]).cumsum(dim=0)\n",
    "    return ( \n",
    "        torch.LongTensor([t[0]-1 for t in b]), # labels\n",
    "        torch.cat(x), # text \n",
    "        o\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ดังที่เราได้เห็นข้างต้น เวกเตอร์เอมเบดดิ้งทั้งหมดถูกเก็บไว้ในเมทริกซ์ `vocab.vectors` ซึ่งทำให้ง่ายมากที่จะโหลดเวกเตอร์เหล่านั้นไปยังน้ำหนักของเลเยอร์เอมเบดดิ้งโดยการคัดลอกอย่างง่าย:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = EmbedClassifier(len(vocab),len(vocab.vectors[0]),len(classes))\n",
    "net.embedding.weight.data = vocab.vectors\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200: acc=0.6271875\n",
      "6400: acc=0.68078125\n",
      "9600: acc=0.7030208333333333\n",
      "12800: acc=0.71984375\n",
      "16000: acc=0.7346875\n",
      "19200: acc=0.7455729166666667\n",
      "22400: acc=0.7529464285714286\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(35.53972978646833, 0.7575175943698017)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, collate_fn=offsetify, shuffle=True)\n",
    "train_epoch_emb(net,train_loader, lr=4, epoch_size=25000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "หนึ่งในเหตุผลที่เราไม่เห็นการเพิ่มขึ้นของความแม่นยำอย่างมีนัยสำคัญเป็นเพราะคำบางคำจากชุดข้อมูลของเราหายไปจากคำศัพท์ที่ผ่านการฝึกอบรมล่วงหน้าของ GloVe และดังนั้นคำเหล่านั้นจึงถูกละเลยไป เพื่อเอาชนะปัญหานี้ เราสามารถฝึกฝนการฝังตัวคำของเราเองบนชุดข้อมูลของเรา\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## การฝังบริบท (Contextual Embeddings)\n",
    "\n",
    "ข้อจำกัดสำคัญอย่างหนึ่งของการฝังคำแบบดั้งเดิมที่ผ่านการฝึกอบรมล่วงหน้า เช่น Word2Vec คือปัญหาเรื่องการแยกความหมายของคำ (word sense disambiguation) แม้ว่าการฝังคำที่ผ่านการฝึกอบรมล่วงหน้าจะสามารถจับความหมายบางส่วนของคำในบริบทได้ แต่ความหมายทุกแบบของคำจะถูกเข้ารหัสไว้ในรูปแบบการฝังเดียวกัน สิ่งนี้อาจทำให้เกิดปัญหาในโมเดลที่ใช้งานต่อเนื่อง เนื่องจากคำหลายคำ เช่นคำว่า 'play' มีความหมายที่แตกต่างกันขึ้นอยู่กับบริบทที่ใช้งาน\n",
    "\n",
    "ตัวอย่างเช่น คำว่า 'play' ในสองประโยคต่อไปนี้มีความหมายที่แตกต่างกันอย่างชัดเจน:\n",
    "- ฉันไปดู **ละคร** ที่โรงละคร\n",
    "- จอห์นอยากจะ **เล่น** กับเพื่อนของเขา\n",
    "\n",
    "การฝังคำที่ผ่านการฝึกอบรมล่วงหน้าข้างต้นแสดงถึงความหมายทั้งสองของคำว่า 'play' ในรูปแบบการฝังเดียวกัน เพื่อแก้ไขข้อจำกัดนี้ เราจำเป็นต้องสร้างการฝังคำที่อิงตาม **โมเดลภาษา** ซึ่งได้รับการฝึกอบรมจากคลังข้อความขนาดใหญ่ และ *เข้าใจ* ว่าคำสามารถนำมาประกอบกันในบริบทที่แตกต่างกันได้อย่างไร การพูดถึงการฝังบริบทอยู่นอกขอบเขตของบทเรียนนี้ แต่เราจะกลับมาพูดถึงเรื่องนี้อีกครั้งเมื่อพูดถึงโมเดลภาษาในหน่วยถัดไป\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**ข้อจำกัดความรับผิดชอบ**:  \nเอกสารนี้ได้รับการแปลโดยใช้บริการแปลภาษา AI [Co-op Translator](https://github.com/Azure/co-op-translator) แม้ว่าเราจะพยายามให้การแปลมีความถูกต้อง แต่โปรดทราบว่าการแปลอัตโนมัติอาจมีข้อผิดพลาดหรือความไม่แม่นยำ เอกสารต้นฉบับในภาษาต้นทางควรถือเป็นแหล่งข้อมูลที่เชื่อถือได้ สำหรับข้อมูลที่สำคัญ แนะนำให้ใช้บริการแปลภาษามนุษย์ที่เป็นมืออาชีพ เราไม่รับผิดชอบต่อความเข้าใจผิดหรือการตีความที่ผิดพลาดซึ่งเกิดจากการใช้การแปลนี้\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0cb620c6d4b9f7a635928804c26cf22403d89d98d79684e4529119355ee6d5a5"
  },
  "kernelspec": {
   "display_name": "py37_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "coopTranslator": {
   "original_hash": "f50b026abce5cf36783a560ea72cb9b1",
   "translation_date": "2025-08-29T10:59:25+00:00",
   "source_file": "lessons/5-NLP/14-Embeddings/EmbeddingsPyTorch.ipynb",
   "language_code": "th"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}