# Johdanto neuroverkkoihin. Monikerroksinen perceptron

Edellisess√§ osiossa opit yksinkertaisimmasta neuroverkkonmallista ‚Äì yksikerroksisesta perceptronista, joka on lineaarinen kahden luokan luokittelumalli.

T√§ss√§ osiossa laajennamme t√§t√§ mallia joustavammaksi kehykseksi, joka mahdollistaa:

* suorittaa **moniluokkaluokittelua** kahden luokan luokittelun lis√§ksi
* ratkaista **regressio-ongelmia** luokittelun lis√§ksi
* erottaa luokkia, jotka eiv√§t ole lineaarisesti erotettavissa

Kehit√§mme my√∂s oman modulaarisen Python-kehyksen, jonka avulla voimme rakentaa erilaisia neuroverkkorakenteita.

## [Ennakkokysely](https://ff-quizzes.netlify.app/en/ai/quiz/7)

## Koneoppimisen formalisointi

Aloitetaan koneoppimisongelman formalisoinnista. Oletetaan, ett√§ meill√§ on harjoitusaineisto **X** ja sen luokat **Y**, ja meid√§n t√§ytyy rakentaa malli *f*, joka tekee mahdollisimman tarkkoja ennusteita. Ennusteiden laatua mitataan **tappiofunktiolla** &lagran;. Seuraavia tappiofunktioita k√§ytet√§√§n usein:

* Regressio-ongelmassa, kun meid√§n t√§ytyy ennustaa luku, voimme k√§ytt√§√§ **absoluuttista virhett√§** &sum;<sub>i</sub>|f(x<sup>(i)</sup>)-y<sup>(i)</sup>| tai **neli√∂llist√§ virhett√§** &sum;<sub>i</sub>(f(x<sup>(i)</sup>)-y<sup>(i)</sup>)<sup>2</sup>
* Luokittelussa k√§yt√§mme **0-1 tappiofunktiota** (joka on k√§yt√§nn√∂ss√§ sama kuin mallin **tarkkuus**) tai **logistista tappiofunktiota**.

Yksikerroksisessa perceptronissa funktio *f* m√§√§riteltiin lineaarisena funktiona *f(x)=wx+b* (t√§ss√§ *w* on painomatriisi, *x* on sy√∂teominaisuuksien vektori ja *b* on bias-vektori). Eri neuroverkkorakenteissa t√§m√§ funktio voi olla monimutkaisempi.

> Luokittelussa on usein toivottavaa saada todenn√§k√∂isyydet vastaaville luokille verkon ulostulona. Muuttaaksemme satunnaiset luvut todenn√§k√∂isyyksiksi (esim. normalisoidaksemme ulostulon), k√§yt√§mme usein **softmax-funktiota** &sigma;, jolloin funktio *f* muuttuu muotoon *f(x)=&sigma;(wx+b)*.

Yll√§ olevassa *f*-m√§√§ritelm√§ss√§ *w* ja *b* kutsutaan **parametreiksi** &theta;=‚ü®*w,b*‚ü©. Kun aineisto ‚ü®**X**,**Y**‚ü© on annettu, voimme laskea kokonaisvirheen koko aineistolle parametrien &theta; funktiona.

> ‚úÖ **Neuroverkon koulutuksen tavoite on minimoida virhe muuttamalla parametreja &theta;**

## Gradienttimenetelm√§ optimointiin

On olemassa tunnettu optimointimenetelm√§ nimelt√§ **gradienttimenetelm√§**. Ideana on, ett√§ voimme laskea derivaatan (moniulotteisessa tapauksessa **gradientin**) tappiofunktion suhteen parametreihin ja muuttaa parametreja siten, ett√§ virhe pienenee. T√§m√§ voidaan formalisoida seuraavasti:

* Alusta parametrit satunnaisilla arvoilla w<sup>(0)</sup>, b<sup>(0)</sup>
* Toista seuraava vaihe monta kertaa:
    - w<sup>(i+1)</sup> = w<sup>(i)</sup>-&eta;&part;&lagran;/&part;w
    - b<sup>(i+1)</sup> = b<sup>(i)</sup>-&eta;&part;&lagran;/&part;b

Koulutuksen aikana optimointivaiheet lasketaan yleens√§ koko aineistoa k√§ytt√§en (muista, ett√§ tappio lasketaan summana kaikkien harjoitusn√§ytteiden l√§pi). K√§yt√§nn√∂ss√§ otamme kuitenkin pieni√§ osia aineistosta, joita kutsutaan **minibatcheiksi**, ja laskemme gradientit aineiston osajoukon perusteella. Koska osajoukko valitaan satunnaisesti joka kerta, menetelm√§√§ kutsutaan **stokastiseksi gradienttimenetelm√§ksi** (SGD).

## Monikerroksiset perceptronit ja takaisinkuljetus

Yksikerroksinen verkko, kuten olemme n√§hneet, pystyy luokittelemaan lineaarisesti erotettavia luokkia. Rikkaamman mallin rakentamiseksi voimme yhdist√§√§ useita verkon kerroksia. Matemaattisesti t√§m√§ tarkoittaisi, ett√§ funktio *f* olisi monimutkaisempi ja laskettaisiin useassa vaiheessa:
* z<sub>1</sub>=w<sub>1</sub>x+b<sub>1</sub>
* z<sub>2</sub>=w<sub>2</sub>&alpha;(z<sub>1</sub>)+b<sub>2</sub>
* f = &sigma;(z<sub>2</sub>)

T√§ss√§ &alpha; on **ep√§lineaarinen aktivointifunktio**, &sigma; on softmax-funktio, ja parametrit ovat &theta;=<*w<sub>1</sub>,b<sub>1</sub>,w<sub>2</sub>,b<sub>2</sub>*>.

Gradienttimenetelm√§ pysyy samana, mutta gradienttien laskeminen on monimutkaisempaa. Ketjulaskus√§√§nn√∂n avulla voimme laskea derivaatat seuraavasti:

* &part;&lagran;/&part;w<sub>2</sub> = (&part;&lagran;/&part;&sigma;)(&part;&sigma;/&part;z<sub>2</sub>)(&part;z<sub>2</sub>/&part;w<sub>2</sub>)
* &part;&lagran;/&part;w<sub>1</sub> = (&part;&lagran;/&part;&sigma;)(&part;&sigma;/&part;z<sub>2</sub>)(&part;z<sub>2</sub>/&part;&alpha;)(&part;&alpha;/&part;z<sub>1</sub>)(&part;z<sub>1</sub>/&part;w<sub>1</sub>)

> ‚úÖ Ketjulaskus√§√§nt√∂√§ k√§ytet√§√§n tappiofunktion derivaattojen laskemiseen parametrien suhteen.

Huomaa, ett√§ kaikkien n√§iden lausekkeiden vasemmanpuoleinen osa on sama, ja n√§in voimme tehokkaasti laskea derivaatat aloittaen tappiofunktiosta ja kulkemalla "taaksep√§in" laskentakaavion l√§pi. Siksi monikerroksisen perceptronin koulutusmenetelm√§√§ kutsutaan **takaisinkuljetukseksi** tai 'backpropiksi'.

<img alt="laskentakaavio" src="../../../../../translated_images/fi/ComputeGraphGrad.4626252c0de03507.webp"/>

> TODO: kuvan l√§hde

> ‚úÖ K√§ymme takaisinkuljetuksen paljon yksityiskohtaisemmin l√§pi esimerkkimuistikirjassamme.  

## Yhteenveto

T√§ss√§ oppitunnissa rakensimme oman neuroverkkokirjaston ja k√§ytimme sit√§ yksinkertaiseen kaksiulotteiseen luokitteluteht√§v√§√§n.

## üöÄ Haaste

Mukana olevassa muistikirjassa toteutat oman kehyksen monikerroksisten perceptronien rakentamiseen ja kouluttamiseen. N√§et yksityiskohtaisesti, miten modernit neuroverkot toimivat.

Siirry [OwnFramework](OwnFramework.ipynb) -muistikirjaan ja k√§y se l√§pi.

## [J√§lkikysely](https://ff-quizzes.netlify.app/en/ai/quiz/8)

## Kertaus ja itseopiskelu

Takaisinkuljetus on yleinen algoritmi teko√§lyss√§ ja koneoppimisessa, ja sit√§ kannattaa [tutkia tarkemmin](https://wikipedia.org/wiki/Backpropagation).

## [Teht√§v√§](lab/README.md)

T√§ss√§ laboratoriossa sinun tulee k√§ytt√§√§ t√§m√§n oppitunnin aikana rakentamaasi kehyst√§ MNIST-k√§sinkirjoitettujen numeroiden luokitteluun.

* [Ohjeet](lab/README.md)
* [Muistikirja](lab/MyFW_MNIST.ipynb)

---

