# Toistuvat Neuronaaliverkot

## [Ennakkokysely](https://ff-quizzes.netlify.app/en/ai/quiz/31)

Aiemmissa osioissa olemme k√§ytt√§neet tekstin semanttisia esityksi√§ ja yksinkertaista lineaarista luokittelijaa upotusten p√§√§ll√§. T√§m√§ arkkitehtuuri pyrkii vangitsemaan sanojen yhdistetyn merkityksen lauseessa, mutta se ei ota huomioon sanojen **j√§rjestyst√§**, koska upotusten p√§√§lle tehty aggregointitoiminto poistaa t√§m√§n tiedon alkuper√§isest√§ tekstist√§. Koska n√§m√§ mallit eiv√§t pysty mallintamaan sanojen j√§rjestyst√§, ne eiv√§t voi ratkaista monimutkaisempia tai ep√§selvempi√§ teht√§vi√§, kuten tekstin generointia tai kysymysten vastaamista.

Tekstijonon merkityksen vangitsemiseksi meid√§n on k√§ytett√§v√§ toisenlaista neuronaaliverkkoarkkitehtuuria, jota kutsutaan **toistuvaksi neuronaaliverkoksi** eli RNN:ksi. RNN:ss√§ sy√∂t√§mme lauseen verkon l√§pi yksi symboli kerrallaan, ja verkko tuottaa jonkin **tilan**, jonka sy√∂t√§mme verkkoon uudelleen seuraavan symbolin kanssa.

![RNN](../../../../../translated_images/fi/rnn.27f5c29c53d727b5.webp)

> Kuva: kirjoittaja

Kun sy√∂tteen√§ on tokenien jono X<sub>0</sub>,...,X<sub>n</sub>, RNN luo neuronaaliverkkolohkojen jonon ja kouluttaa t√§m√§n jonon p√§√§st√§ p√§√§h√§n takaisinkytkenn√§n avulla. Jokainen verkkolohko ottaa sy√∂tteen√§ parin (X<sub>i</sub>,S<sub>i</sub>) ja tuottaa tuloksena S<sub>i+1</sub>. Lopullinen tila S<sub>n</sub> (tai tulos Y<sub>n</sub>) sy√∂tet√§√§n lineaariseen luokittelijaan tuloksen tuottamiseksi. Kaikilla verkkolohkoilla on samat painot, ja ne koulutetaan p√§√§st√§ p√§√§h√§n yhdell√§ takaisinkytkent√§kierroksella.

Koska tilavektorit S<sub>0</sub>,...,S<sub>n</sub> kulkevat verkon l√§pi, se pystyy oppimaan sanojen v√§lisi√§ j√§rjestyksellisi√§ riippuvuuksia. Esimerkiksi, kun sana *ei* esiintyy jossain kohtaa jonoa, verkko voi oppia kumoamaan tiettyj√§ elementtej√§ tilavektorissa, mik√§ johtaa negaatioon.

> ‚úÖ Koska kaikkien RNN-lohkojen painot yll√§ olevassa kuvassa ovat jaettuja, sama kuva voidaan esitt√§√§ yhten√§ lohkona (oikealla) toistuvalla takaisinkytkent√§silmukalla, joka sy√∂tt√§√§ verkon ulostulotilan takaisin sy√∂tteeksi.

## RNN-solun anatomia

Katsotaan, miten yksinkertainen RNN-solu on j√§rjestetty. Se ottaa sy√∂tteen√§ edellisen tilan S<sub>i-1</sub> ja nykyisen symbolin X<sub>i</sub>, ja sen on tuotettava ulostulotila S<sub>i</sub> (ja joskus olemme kiinnostuneita my√∂s jostain muusta ulostulosta Y<sub>i</sub>, kuten generatiivisissa verkoissa).

Yksinkertaisessa RNN-solussa on kaksi painomatriisia: yksi muuntaa sy√∂tesymbolin (kutsutaan sit√§ W:ksi) ja toinen muuntaa sy√∂tetilan (H). T√§ss√§ tapauksessa verkon ulostulo lasketaan kaavalla &sigma;(W&times;X<sub>i</sub>+H&times;S<sub>i-1</sub>+b), miss√§ &sigma; on aktivointifunktio ja b on lis√§bias.

<img alt="RNN-solun anatomia" src="../../../../../translated_images/fi/rnn-anatomy.79ee3f3920b3294b.webp" width="50%"/>

> Kuva: kirjoittaja

Monissa tapauksissa sy√∂tetokenit kulkevat upotuskerroksen l√§pi ennen RNN:√§√§n sy√∂tt√§mist√§, jotta ulottuvuus pienenee. T√§ss√§ tapauksessa, jos sy√∂tevektorien ulottuvuus on *emb_size* ja tilavektorin ulottuvuus on *hid_size*, W:n koko on *emb_size*&times;*hid_size* ja H:n koko on *hid_size*&times;*hid_size*.

## Pitk√§kestoiset muistiyksik√∂t (LSTM)

Yksi klassisten RNN:ien suurimmista ongelmista on niin sanottu **h√§vi√§vien gradienttien** ongelma. Koska RNN:t koulutetaan p√§√§st√§ p√§√§h√§n yhdell√§ takaisinkytkent√§kierroksella, niill√§ on vaikeuksia v√§litt√§√§ virhett√§ verkon ensimm√§isiin kerroksiin, eik√§ verkko n√§in ollen pysty oppimaan kaukaisten tokenien v√§lisi√§ suhteita. Yksi tapa v√§ltt√§√§ t√§m√§ ongelma on ottaa k√§ytt√∂√∂n **eksplisiittinen tilanhallinta** k√§ytt√§m√§ll√§ niin sanottuja **portteja**. T√§llaisia arkkitehtuureja on kaksi tunnettua: **Pitk√§kestoinen muistiyksikk√∂** (LSTM) ja **Porttiv√§litteinen yksikk√∂** (GRU).

![Kuva, joka esitt√§√§ esimerkin pitk√§kestoisesta muistiyksik√∂st√§](../../../../../lessons/5-NLP/16-RNN/images/long-short-term-memory-cell.svg)

> Kuvan l√§hde TBD

LSTM-verkko on j√§rjestetty samalla tavalla kuin RNN, mutta kerroksesta toiseen kulkee kaksi tilaa: varsinainen tila C ja piilotettu vektori H. Jokaisessa yksik√∂ss√§ piilotettu vektori H<sub>i</sub> yhdistet√§√§n sy√∂tteeseen X<sub>i</sub>, ja ne ohjaavat, mit√§ tilalle C tapahtuu **porttien** avulla. Jokainen portti on neuronaaliverkko, jossa on sigmoid-aktivointi (ulostulo v√§lill√§ [0,1]), ja sit√§ voidaan ajatella bittimaskina, kun se kerrotaan tilavektorilla. Kuvassa yll√§ olevat portit (vasemmalta oikealle) ovat:

* **Unohtamisportti** ottaa piilotetun vektorin ja m√§√§ritt√§√§, mitk√§ tilavektorin C komponentit tulee unohtaa ja mitk√§ v√§litt√§√§ eteenp√§in.
* **Sy√∂tt√∂portti** ottaa tietoa sy√∂te- ja piilotetuista vektoreista ja lis√§√§ sen tilaan.
* **Ulostuloportti** muuntaa tilan lineaarisen kerroksen kautta, jossa on *tanh*-aktivointi, ja valitsee sen komponentteja piilotetun vektorin H<sub>i</sub> avulla tuottaakseen uuden tilan C<sub>i+1</sub>.

Tilan C komponentteja voidaan ajatella lippuina, jotka voidaan kytke√§ p√§√§lle ja pois. Esimerkiksi, kun kohtaamme nimen *Alice* jaksossa, voimme olettaa, ett√§ se viittaa naispuoliseen hahmoon, ja nostaa tilassa lipun, joka kertoo, ett√§ lauseessa on naispuolinen substantiivi. Kun my√∂hemmin kohtaamme fraasin *ja Tom*, nostamme lipun, joka kertoo, ett√§ lauseessa on monikollinen substantiivi. N√§in manipuloimalla tilaa voimme mahdollisesti seurata lauseen osien kieliopillisia ominaisuuksia.

> ‚úÖ Erinomainen resurssi LSTM:n sis√§isen toiminnan ymm√§rt√§miseen on Christopher Olahin artikkeli [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/).

## Kaksisuuntaiset ja monikerroksiset RNN:t

Olemme k√§sitelleet toistuvia verkkoja, jotka toimivat yhteen suuntaan, jaksosta alusta loppuun. T√§m√§ vaikuttaa luonnolliselta, koska se muistuttaa tapaa, jolla luemme ja kuuntelemme puhetta. Kuitenkin, koska monissa k√§yt√§nn√∂n tapauksissa meill√§ on satunnainen p√§√§sy sy√∂tej√§rjestykseen, voi olla j√§rkev√§√§ suorittaa toistuva laskenta molempiin suuntiin. T√§llaisia verkkoja kutsutaan **kaksisuuntaisiksi** RNN:iksi. Kaksisuuntaisessa verkossa tarvitsemme kaksi piilotettua tilavektoria, yhden kumpaankin suuntaan.

Toistuva verkko, joko yksisuuntainen tai kaksisuuntainen, tunnistaa tiettyj√§ kuvioita jaksossa ja voi tallentaa ne tilavektoriin tai v√§litt√§√§ ulostuloon. Kuten konvoluutiokerroksissa, voimme rakentaa toisen toistuvan kerroksen ensimm√§isen p√§√§lle tunnistaaksemme korkeamman tason kuvioita ja rakentaaksemme matalan tason kuvioista, jotka ensimm√§inen kerros on tunnistanut. T√§m√§ johtaa k√§sitteeseen **monikerroksinen RNN**, joka koostuu kahdesta tai useammasta toistuvasta verkosta, joissa edellisen kerroksen ulostulo sy√∂tet√§√§n seuraavaan kerrokseen.

![Kuva, joka esitt√§√§ monikerroksisen pitk√§kestoisen muistiyksik√∂n RNN:n](../../../../../translated_images/fi/multi-layer-lstm.dd975e29bb2a59fe.webp)

*Kuva [t√§st√§ upeasta artikkelista](https://towardsdatascience.com/from-a-lstm-cell-to-a-multilayer-lstm-network-with-pytorch-2899eb5696f3) kirjoittanut Fernando L√≥pez*

## ‚úçÔ∏è Harjoitukset: Upotukset

Jatka oppimista seuraavissa muistikirjoissa:

* [RNN:t PyTorchilla](RNNPyTorch.ipynb)
* [RNN:t TensorFlow'lla](RNNTF.ipynb)

## Yhteenveto

T√§ss√§ osiossa olemme n√§hneet, ett√§ RNN:it√§ voidaan k√§ytt√§√§ jaksoluokitteluun, mutta itse asiassa ne voivat k√§sitell√§ monia muita teht√§vi√§, kuten tekstin generointia, konek√§√§nn√∂st√§ ja paljon muuta. Tarkastelemme n√§it√§ teht√§vi√§ seuraavassa osiossa.

## üöÄ Haaste

Lue kirjallisuutta LSTM:ist√§ ja pohdi niiden sovelluksia:

- [Grid Long Short-Term Memory](https://arxiv.org/pdf/1507.01526v1.pdf)
- [Show, Attend and Tell: Neural Image Caption
Generation with Visual Attention](https://arxiv.org/pdf/1502.03044v2.pdf)

## [J√§lkikysely](https://ff-quizzes.netlify.app/en/ai/quiz/32)

## Kertaus ja itseopiskelu

- [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) kirjoittanut Christopher Olah.

## [Teht√§v√§: Muistikirjat](assignment.md)

---

