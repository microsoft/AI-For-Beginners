# Upotukset

## [Ennakkokysely](https://ff-quizzes.netlify.app/en/ai/quiz/27)

Kun koulutimme luokittelijoita BoW- tai TF/IDF-menetelmill√§, k√§ytimme korkean ulottuvuuden bag-of-words-vektoreita, joiden pituus oli `vocab_size`, ja muunsimme eksplisiittisesti matalan ulottuvuuden paikkavektorit harvoiksi yksiulotteisiksi vektoreiksi. T√§m√§ yksiulotteinen esitys ei kuitenkaan ole muistin kannalta tehokas. Lis√§ksi jokainen sana k√§sitell√§√§n toisistaan riippumattomana, eli yksiulotteiset vektorit eiv√§t ilmaise sanojen semanttista samankaltaisuutta.

**Upotuksen** idea on edustaa sanoja matalamman ulottuvuuden tiheill√§ vektoreilla, jotka jollain tavalla heijastavat sanan semanttista merkityst√§. My√∂hemmin k√§sittelemme, miten rakentaa merkityksellisi√§ sanaupotuksia, mutta toistaiseksi voimme ajatella upotuksia tapana pienent√§√§ sanavektorin ulottuvuutta.

Upotuskerros ottaa sanan sy√∂tteen√§ ja tuottaa ulostulovektorin, jonka pituus on m√§√§ritelty `embedding_size`. Tietyss√§ mieless√§ se on hyvin samanlainen kuin `Linear`-kerros, mutta sen sijaan, ett√§ se ottaisi yksiulotteisen vektorin, se voi ottaa sanan numeron sy√∂tteen√§, jolloin v√§lt√§mme suurten yksiulotteisten vektorien luomisen.

K√§ytt√§m√§ll√§ upotuskerrosta luokittelijaverkkomme ensimm√§isen√§ kerroksena voimme siirty√§ bag-of-words-mallista **embedding bag** -malliin, jossa ensin muutamme tekstimme jokaisen sanan vastaavaksi upotukseksi ja laskemme sitten jonkin aggregaattifunktion n√§iden upotusten yli, kuten `sum`, `average` tai `max`.

![Kuva, joka n√§ytt√§√§ upotusluokittelijan viidelle sanajonon sanalle.](../../../../../translated_images/fi/embedding-classifier-example.b77f021a7ee67eee.webp)

> Kuva tekij√§lt√§

## ‚úçÔ∏è Harjoitukset: Upotukset

Jatka oppimista seuraavissa muistikirjoissa:
* [Upotukset PyTorchilla](EmbeddingsPyTorch.ipynb)
* [Upotukset TensorFlowlla](EmbeddingsTF.ipynb)

## Semanttiset upotukset: Word2Vec

Vaikka upotuskerros oppi kartoittamaan sanat vektoriedustukseen, t√§m√§ edustus ei v√§ltt√§m√§tt√§ sis√§lt√§nyt paljon semanttista merkityst√§. Olisi hy√∂dyllist√§ oppia vektoriedustus, jossa samankaltaiset sanat tai synonyymit vastaavat vektoreita, jotka ovat l√§hell√§ toisiaan jonkin vektoriet√§isyyden (esim. euklidisen et√§isyyden) suhteen.

T√§m√§n saavuttamiseksi meid√§n t√§ytyy esikouluttaa upotusmallimme suurella tekstikokoelmalla erityisell√§ tavalla. Yksi tapa kouluttaa semanttisia upotuksia on nimelt√§√§n [Word2Vec](https://en.wikipedia.org/wiki/Word2vec). Se perustuu kahteen p√§√§arkkitehtuuriin, joita k√§ytet√§√§n sanojen hajautetun edustuksen tuottamiseen:

- **Jatkuva bag-of-words** (CBoW) ‚Äî t√§ss√§ arkkitehtuurissa koulutamme mallin ennustamaan sanan ymp√§r√∂iv√§st√§ kontekstista. Annetulla ngramilla $(W_{-2},W_{-1},W_0,W_1,W_2)$ mallin tavoitteena on ennustaa $W_0$ k√§ytt√§en $(W_{-2},W_{-1},W_1,W_2)$.
- **Jatkuva skip-gram** on CBoW:n vastakohta. Malli k√§ytt√§√§ ymp√§r√∂iv√§√§ kontekstisanan ikkunaa ennustaakseen nykyisen sanan.

CBoW on nopeampi, kun taas skip-gram on hitaampi, mutta se edustaa harvinaisia sanoja paremmin.

![Kuva, joka n√§ytt√§√§ sek√§ CBoW- ett√§ Skip-Gram-algoritmit sanojen muuntamiseksi vektoreiksi.](../../../../../translated_images/fi/example-algorithms-for-converting-words-to-vectors.fbe9207a726922f6.webp)

> Kuva [t√§st√§ artikkelista](https://arxiv.org/pdf/1301.3781.pdf)

Word2Vec-esikoulutettuja upotuksia (sek√§ muita vastaavia malleja, kuten GloVe) voidaan k√§ytt√§√§ my√∂s upotuskerroksen sijasta neuroverkoissa. Meid√§n t√§ytyy kuitenkin k√§sitell√§ sanastoja, koska Word2Vec/GloVe:n esikoulutuksessa k√§ytetty sanasto eroaa todenn√§k√∂isesti tekstikorpuksemme sanastosta. Katso yll√§ olevia muistikirjoja n√§hd√§ksesi, miten t√§m√§ ongelma voidaan ratkaista.

## Kontekstuaaliset upotukset

Yksi perinteisten esikoulutettujen upotusten, kuten Word2Vecin, keskeinen rajoitus on sanan merkityksen erottelun ongelma. Vaikka esikoulutetut upotukset voivat vangita osan sanojen merkityksest√§ kontekstissa, kaikki mahdolliset sanan merkitykset koodataan samaan upotukseen. T√§m√§ voi aiheuttaa ongelmia jatkomalleissa, koska monet sanat, kuten sana 'play', voivat tarkoittaa eri asioita riippuen kontekstista, jossa niit√§ k√§ytet√§√§n.

Esimerkiksi sana 'play' n√§iss√§ kahdessa eri lauseessa tarkoittaa hyvin eri asioita:

- K√§vin katsomassa **n√§ytelm√§√§** teatterissa.
- John haluaa **leikki√§** yst√§viens√§ kanssa.

Yll√§ olevat esikoulutetut upotukset edustavat molempia n√§ytelm√§n merkityksi√§ samassa upotuksessa. T√§m√§n rajoituksen voittamiseksi meid√§n t√§ytyy rakentaa upotuksia **kielimallin** perusteella, joka on koulutettu suurella tekstikorpuksella ja *tiet√§√§*, miten sanoja voidaan yhdist√§√§ eri konteksteissa. Kontekstuaalisten upotusten k√§sittely on t√§m√§n oppaan ulkopuolella, mutta palaamme niihin my√∂hemmin kurssilla, kun k√§sittelemme kielimalleja.

## Yhteenveto

T√§ss√§ oppitunnissa opit rakentamaan ja k√§ytt√§m√§√§n upotuskerroksia TensorFlowssa ja PyTorchissa, jotta sanojen semanttiset merkitykset heijastuisivat paremmin.

## üöÄ Haaste

Word2Veci√§ on k√§ytetty mielenkiintoisiin sovelluksiin, kuten laululyriikan ja runouden luomiseen. Tutustu [t√§h√§n artikkeliin](https://www.politetype.com/blog/word2vec-color-poems), jossa k√§yd√§√§n l√§pi, miten kirjoittaja k√§ytti Word2Veci√§ runouden luomiseen. Katso my√∂s [t√§m√§ Dan Shiffmannin video](https://www.youtube.com/watch?v=LSS_bos_TPI&ab_channel=TheCodingTrain), jossa selitet√§√§n t√§t√§ tekniikkaa eri tavalla. Kokeile sitten soveltaa n√§it√§ tekniikoita omaan tekstikorpukseesi, ehk√§ Kagglesta hankittuun.

## [J√§lkikysely](https://ff-quizzes.netlify.app/en/ai/quiz/28)

## Kertaus ja itseopiskelu

Lue t√§m√§ Word2Vec-artikkeli: [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf)

## [Teht√§v√§: Muistikirjat](assignment.md)

---

