# Generatiiviset verkot

## [Ennakkokysely](https://ff-quizzes.netlify.app/en/ai/quiz/33)

Toistuvat neuroverkot (RNN:t) ja niiden portitetut solumuunnelmat, kuten Long Short Term Memory Cells (LSTM:t) ja Gated Recurrent Units (GRU:t), tarjoavat mekanismin kielen mallintamiseen, koska ne voivat oppia sanojen j√§rjestyksen ja ennustaa seuraavan sanan sekvenssiss√§. T√§m√§ mahdollistaa RNN:ien k√§yt√∂n **generatiivisissa teht√§viss√§**, kuten tavallisessa tekstin generoinnissa, konek√§√§nn√∂ksess√§ ja jopa kuvatekstien luomisessa.

> ‚úÖ Mieti kaikkia niit√§ kertoja, kun olet hy√∂tynyt generatiivisista teht√§vist√§, kuten tekstin t√§ydennyksest√§ kirjoittaessasi. Tutki suosikkisovelluksiasi ja selvit√§, ovatko ne hy√∂dynt√§neet RNN:ej√§.

RNN-arkkitehtuurissa, jota k√§sittelimme edellisess√§ osiossa, jokainen RNN-yksikk√∂ tuotti seuraavan piilotetun tilan ulostulona. Voimme kuitenkin lis√§t√§ jokaiselle toistuvalle yksik√∂lle toisen ulostulon, joka mahdollistaa **sekvenssin** tuottamisen (joka on yht√§ pitk√§ kuin alkuper√§inen sekvenssi). Lis√§ksi voimme k√§ytt√§√§ RNN-yksik√∂it√§, jotka eiv√§t ota sy√∂tett√§ jokaisessa vaiheessa, vaan ottavat vain alkuper√§isen tilavektorin ja tuottavat sekvenssin ulostuloja.

T√§m√§ mahdollistaa erilaiset neuroarkkitehtuurit, jotka n√§kyv√§t alla olevassa kuvassa:

![Kuva, joka n√§ytt√§√§ yleisi√§ toistuvien neuroverkkojen malleja.](../../../../../translated_images/fi/unreasonable-effectiveness-of-rnn.541ead816778f42d.webp)

> Kuva blogikirjoituksesta [Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) kirjoittanut [Andrej Karpaty](http://karpathy.github.io/)

* **Yksi-yhteen** on perinteinen neuroverkko, jossa on yksi sy√∂te ja yksi ulostulo
* **Yksi-moneen** on generatiivinen arkkitehtuuri, joka ottaa yhden sy√∂tearvon ja tuottaa sekvenssin ulostuloarvoja. Esimerkiksi, jos haluamme kouluttaa **kuvatekstien luontiverkon**, joka tuottaa tekstuaalisen kuvauksen kuvasta, voimme sy√∂tt√§√§ kuvan, k√§sitell√§ sen CNN:n kautta saadaksemme piilotetun tilan ja sitten k√§ytt√§√§ toistuvaa ketjua tuottamaan kuvatekstin sana sanalta.
* **Monta-yhteen** vastaa RNN-arkkitehtuureja, joita kuvailimme edellisess√§ osiossa, kuten tekstiluokittelua.
* **Monta-moneen**, tai **sekvenssi-sekvenssi**, vastaa teht√§vi√§, kuten **konek√§√§nn√∂s**, jossa ensimm√§inen RNN ker√§√§ kaiken tiedon sy√∂tesekvenssist√§ piilotettuun tilaan, ja toinen RNN-ketju purkaa t√§m√§n tilan ulostulosekvenssiksi.

T√§ss√§ osiossa keskitymme yksinkertaisiin generatiivisiin malleihin, jotka auttavat meit√§ tuottamaan teksti√§. Yksinkertaisuuden vuoksi k√§yt√§mme merkkitasoista tokenointia.

Koulutamme t√§m√§n RNN:n tuottamaan teksti√§ askel askeleelta. Jokaisessa vaiheessa otamme `nchars`-pituisen merkkisekvenssin ja pyyd√§mme verkkoa tuottamaan seuraavan ulostulomerkin jokaiselle sy√∂temerkille:

![Kuva, joka n√§ytt√§√§ esimerkin RNN:n tuottamasta sanasta 'HELLO'.](../../../../../translated_images/fi/rnn-generate.56c54afb52f9781d.webp)

Kun tuotamme teksti√§ (inferenssin aikana), aloitamme jollain **aloitustekstill√§**, joka sy√∂tet√§√§n RNN-soluihin tuottamaan sen v√§limuistin, ja sitten t√§st√§ tilasta alkaa generointi. Tuotamme yhden merkin kerrallaan ja sy√∂t√§mme tilan ja tuotetun merkin seuraavaan RNN-soluun tuottamaan seuraavan, kunnes olemme tuottaneet tarpeeksi merkkej√§.

<img src="../../../../../translated_images/fi/rnn-generate-inf.5168dc65e0370eea.webp" width="60%"/>

> Kuva kirjoittajalta

## ‚úçÔ∏è Harjoitukset: Generatiiviset verkot

Jatka oppimista seuraavissa muistikirjoissa:

* [Generatiiviset verkot PyTorchilla](GenerativePyTorch.ipynb)
* [Generatiiviset verkot TensorFlow'lla](GenerativeTF.ipynb)

## Pehme√§ tekstin generointi ja l√§mp√∂tila

Jokaisen RNN-solun ulostulo on merkkien todenn√§k√∂isyysjakauma. Jos valitsemme aina merkin, jolla on korkein todenn√§k√∂isyys seuraavaksi merkiksi tuotetussa tekstiss√§, teksti voi usein "kiert√§√§" samoja merkkisekvenssej√§ uudelleen ja uudelleen, kuten t√§ss√§ esimerkiss√§:

```
today of the second the company and a second the company ...
```

Jos kuitenkin tarkastelemme seuraavan merkin todenn√§k√∂isyysjakaumaa, voi olla, ett√§ muutaman korkeimman todenn√§k√∂isyyden ero ei ole suuri, esimerkiksi yksi merkki voi olla todenn√§k√∂isyydell√§ 0.2, toinen 0.19 jne. Esimerkiksi, kun etsimme seuraavaa merkki√§ sekvenssiss√§ '*play*', seuraava merkki voi yht√§ hyvin olla joko v√§lily√∂nti tai **e** (kuten sanassa *player*).

T√§m√§ johtaa siihen johtop√§√§t√∂kseen, ett√§ ei ole aina "reilua" valita merkki√§, jolla on korkeampi todenn√§k√∂isyys, koska toisen korkein voi silti johtaa merkitykselliseen tekstiin. On viisaampaa **n√§ytteist√§√§** merkkej√§ verkon ulostulon antamasta todenn√§k√∂isyysjakaumasta. Voimme my√∂s k√§ytt√§√§ parametria, **l√§mp√∂tila**, joka tasoittaa todenn√§k√∂isyysjakaumaa, jos haluamme lis√§t√§ satunnaisuutta, tai tehd√§ siit√§ jyrkemm√§n, jos haluamme pysy√§ enemm√§n korkeimman todenn√§k√∂isyyden merkkien parissa.

Tutki, miten t√§m√§ pehme√§ tekstin generointi on toteutettu yll√§ linkitetyiss√§ muistikirjoissa.

## Yhteenveto

Vaikka tekstin generointi voi olla hy√∂dyllist√§ itsess√§√§n, suurimmat hy√∂dyt tulevat kyvyst√§ tuottaa teksti√§ RNN:ien avulla jostain alkuper√§isest√§ ominaisvektorista. Esimerkiksi tekstin generointia k√§ytet√§√§n osana konek√§√§nn√∂st√§ (sekvenssi-sekvenssi, t√§ss√§ tapauksessa *enkooderin* tilavektoria k√§ytet√§√§n tuottamaan tai *dekoodaamaan* k√§√§nnetty viesti) tai kuvan tekstuaalisen kuvauksen tuottamisessa (t√§ss√§ tapauksessa ominaisvektori tulee CNN-uutosta).

## üöÄ Haaste

Ota oppitunteja Microsoft Learnista t√§st√§ aiheesta

* Tekstin generointi [PyTorchilla](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-pytorch/6-generative-networks/?WT.mc_id=academic-77998-cacaste)/[TensorFlow'lla](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-tensorflow/5-generative-networks/?WT.mc_id=academic-77998-cacaste)

## [J√§lkikysely](https://ff-quizzes.netlify.app/en/ai/quiz/34)

## Kertaus ja itseopiskelu

T√§ss√§ joitakin artikkeleita tiet√§myksen laajentamiseksi

* Eri l√§hestymistavat tekstin generointiin Markov-ketjulla, LSTM:ll√§ ja GPT-2:lla: [blogikirjoitus](https://towardsdatascience.com/text-generation-gpt-2-lstm-markov-chain-9ea371820e1e)
* Tekstin generoinnin esimerkki [Keras-dokumentaatiossa](https://keras.io/examples/generative/lstm_character_level_text_generation/)

## [Teht√§v√§](lab/README.md)

Olemme n√§hneet, miten teksti√§ voidaan tuottaa merkki kerrallaan. Laboratoriossa tutkit sanatasoista tekstin generointia.

---

