{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Huomiomekanismit ja transformerit\n",
    "\n",
    "Yksi toistoverkkojen suurimmista heikkouksista on, että kaikki sanat jaksossa vaikuttavat tulokseen samalla tavalla. Tämä johtaa suboptimaaliseen suorituskykyyn tavanomaisilla LSTM-enkooderi-dekooderi-malleilla sekvenssistä sekvenssiin -tehtävissä, kuten nimettyjen entiteettien tunnistuksessa ja konekäännöksessä. Todellisuudessa tietyillä sanoilla syötejaksossa on usein suurempi vaikutus peräkkäisiin tulosteisiin kuin toisilla.\n",
    "\n",
    "Ajatellaan sekvenssistä sekvenssiin -mallia, kuten konekäännöstä. Se toteutetaan kahdella toistoverkolla, joissa yksi verkko (**enkooderi**) tiivistää syötejakson piilotilaan ja toinen verkko, **dekooderi**, purkaa tämän piilotilan käännetyksi tulokseksi. Tämän lähestymistavan ongelmana on, että verkon lopullisen tilan on vaikea muistaa lauseen alkua, mikä heikentää mallin laatua pitkissä lauseissa.\n",
    "\n",
    "**Huomiomekanismit** tarjoavat keinon painottaa kunkin syötevektorin kontekstuaalista vaikutusta RNN:n jokaisessa tulosennusteessa. Tämä toteutetaan luomalla oikopolkuja syötteen RNN:n välitilojen ja tulos-RNN:n välille. Näin ollen, kun tuotetaan tulossymbolia $y_t$, otamme huomioon kaikki syötteen piilotilat $h_i$, eri painokertoimilla $\\alpha_{t,i}$. \n",
    "\n",
    "![Kuva, joka esittää enkooderi/dekooderi-mallin additiivisella huomiokerroksella](../../../../../translated_images/fi/encoder-decoder-attention.7a726296894fb567.webp)\n",
    "*Enkooderi-dekooderi-malli additiivisella huomiomekanismilla [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf), lainattu [tästä blogikirjoituksesta](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)*\n",
    "\n",
    "Huomiomatriisi $\\{\\alpha_{i,j}\\}$ edustaa sitä, kuinka paljon tietyt syötteen sanat vaikuttavat tietyn sanan muodostumiseen tulosjaksossa. Alla on esimerkki tällaisesta matriisista:\n",
    "\n",
    "![Kuva, joka näyttää esimerkkikohdistuksen RNNsearch-50:llä, otettu Bahdanau - arviz.org](../../../../../translated_images/fi/bahdanau-fig3.09ba2d37f202a6af.webp)\n",
    "\n",
    "*Kuva otettu [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) (Kuva 3)*\n",
    "\n",
    "Huomiomekanismit ovat vastuussa suuresta osasta nykyistä tai lähes nykyistä huipputasoa luonnollisen kielen käsittelyssä. Huomion lisääminen kuitenkin kasvattaa huomattavasti mallin parametrien määrää, mikä johti skaalautumisongelmiin RNN:ien kanssa. Yksi RNN:ien skaalautumisen keskeisistä rajoitteista on, että mallien toistuva luonne tekee koulutuksen eräajosta ja rinnakkaistamisesta haastavaa. RNN:ssä jokainen sekvenssin elementti täytyy käsitellä järjestyksessä, mikä tarkoittaa, että sitä ei voida helposti rinnakkaistaa.\n",
    "\n",
    "Huomiomekanismien käyttöönotto yhdessä tämän rajoitteen kanssa johti nykyisten huipputason Transformer-mallien luomiseen, joita käytämme tänään, kuten BERT ja OpenGPT3.\n",
    "\n",
    "## Transformer-mallit\n",
    "\n",
    "Sen sijaan, että jokaisen edellisen ennusteen konteksti välitettäisiin seuraavaan arviointivaiheeseen, **transformer-mallit** käyttävät **paikkakoodauksia** ja **huomiota** tallentaakseen annetun syötteen kontekstin annetun tekstin ikkunan sisällä. Alla oleva kuva näyttää, kuinka paikkakoodaukset ja huomio voivat tallentaa kontekstin annetun ikkunan sisällä.\n",
    "\n",
    "![Animoitu GIF, joka näyttää, kuinka arvioinnit suoritetaan transformer-malleissa.](../../../../../lessons/5-NLP/18-Transformers/images/transformer-animated-explanation.gif) \n",
    "\n",
    "Koska jokainen syötepaikka kartoitetaan itsenäisesti jokaiseen tulospaikkaan, transformerit voivat rinnakkaistaa paremmin kuin RNN:t, mikä mahdollistaa paljon suuremmat ja ilmaisukykyisemmät kielimallit. Jokainen huomiopää voidaan käyttää oppimaan erilaisia suhteita sanojen välillä, mikä parantaa luonnollisen kielen käsittelyn tehtäviä.\n",
    "\n",
    "## Yksinkertaisen Transformer-mallin rakentaminen\n",
    "\n",
    "Keras ei sisällä sisäänrakennettua Transformer-kerrosta, mutta voimme rakentaa oman. Kuten aiemmin, keskitymme AG News -datan tekstiluokitteluun, mutta on syytä mainita, että Transformer-mallit saavuttavat parhaat tulokset vaikeammissa NLP-tehtävissä.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "\n",
    "ds_train, ds_test = tfds.load('ag_news_subset').values()\n",
    "\n",
    "def extract_text(x):\n",
    "    return x['title']+' '+x['description']\n",
    "\n",
    "def tupelize(x):\n",
    "    return (extract_text(x),x['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uudet kerrokset Kerasissa tulisi aliluokittaa `Layer`-luokka ja toteuttaa `call`-metodi. Aloitetaan **Positional Embedding** -kerroksella. Käytämme [joitakin koodinpätkiä virallisesta Keras-dokumentaatiosta](https://keras.io/examples/nlp/text_classification_with_transformer/). Oletamme, että täytämme kaikki syötesequencet pituuteen `maxlen`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(keras.layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = keras.layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "        self.maxlen = maxlen\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = self.maxlen\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x+positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tämä kerros koostuu kahdesta `Embedding`-kerroksesta: yksi tokenien upottamiseen (kuten aiemmin on käsitelty) ja toinen tokenien sijaintien upottamiseen. Tokenien sijainnit luodaan luonnollisten lukujen sekvenssinä välillä 0 ja `maxlen` käyttäen `tf.range`, ja ne syötetään upotuskerroksen läpi. Kaksi syntynyttä upotusvektoria yhdistetään yhteen, jolloin saadaan sijaintiin perustuva upotettu esitys syötteestä, jonka muoto on `maxlen`$\\times$`embed_dim`.\n",
    "\n",
    "Nyt toteutetaan transformer-lohko. Se ottaa syötteenä aiemmin määritellyn upotuskerroksen tuottaman ulostulon:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim, name='attn')\n",
    "        self.ffn = keras.Sequential(\n",
    "            [keras.layers.Dense(ff_dim, activation=\"relu\"), keras.layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = keras.layers.Dropout(rate)\n",
    "        self.dropout2 = keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nyt olemme valmiita määrittämään täydellisen transformer-mallin:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "text_vectorization (TextVect (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "token_and_position_embedding (None, 256, 32)           648192    \n",
      "_________________________________________________________________\n",
      "transformer_block (Transform (None, 256, 32)           10656     \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 20)                660       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 84        \n",
      "=================================================================\n",
      "Total params: 659,592\n",
      "Trainable params: 659,592\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 32  # Embedding size for each token\n",
    "num_heads = 2  # Number of attention heads\n",
    "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
    "maxlen = 256\n",
    "vocab_size = 20000\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.experimental.preprocessing.TextVectorization(max_tokens=vocab_size,output_sequence_length=maxlen, input_shape=(1,)),\n",
    "    TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim),\n",
    "    TransformerBlock(embed_dim, num_heads, ff_dim),\n",
    "    keras.layers.GlobalAveragePooling1D(),\n",
    "    keras.layers.Dropout(0.1),\n",
    "    keras.layers.Dense(20, activation=\"relu\"),\n",
    "    keras.layers.Dropout(0.1),\n",
    "    keras.layers.Dense(4, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokenizer\n",
      "938/938 [==============================] - 45s 39ms/step - loss: 0.4978 - acc: 0.8068 - val_loss: 0.2808 - val_acc: 0.9124\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9c2427a0d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Training tokenizer')\n",
    "model.layers[0].adapt(ds_train.map(extract_text))\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'], optimizer='adam')\n",
    "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT Transformer -mallit\n",
    "\n",
    "**BERT** (Bidirectional Encoder Representations from Transformers) on erittäin suuri monikerroksinen transformer-verkko, jossa on 12 kerrosta *BERT-base*-mallissa ja 24 kerrosta *BERT-large*-mallissa. Malli esikoulutetaan ensin suurella tekstiaineistolla (WikiPedia + kirjat) käyttämällä valvomatonta oppimista (ennustamalla peitettyjä sanoja lauseessa). Esikoulutuksen aikana malli omaksuu merkittävän määrän kielellistä ymmärrystä, jota voidaan hyödyntää muiden aineistojen kanssa hienosäädön avulla. Tätä prosessia kutsutaan **siirto-oppimiseksi**.\n",
    "\n",
    "![kuva osoitteesta http://jalammar.github.io/illustrated-bert/](../../../../../translated_images/fi/jalammarBERT-language-modeling-masked-lm.34f113ea5fec4362.webp)\n",
    "\n",
    "Transformer-arkkitehtuureista on monia variaatioita, kuten BERT, DistilBERT, BigBird, OpenGPT3 ja muita, joita voidaan hienosäätää.\n",
    "\n",
    "Katsotaanpa, kuinka voimme käyttää esikoulutettua BERT-mallia perinteisen sekvenssiluokitusongelman ratkaisemiseen. Hyödynnämme ideaa ja hieman koodia [virallisesta dokumentaatiosta](https://www.tensorflow.org/text/tutorials/classify_text_with_bert).\n",
    "\n",
    "Esikoulutettujen mallien lataamiseen käytämme **Tensorflow hub** -kirjastoa. Aloitetaan lataamalla BERT-spesifinen vektoroija:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_41180/4216669875.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow_text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow_hub\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mhub\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhub\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mKerasLayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_text'"
     ]
    }
   ],
   "source": [
    "import tensorflow_text \n",
    "import tensorflow_hub as hub\n",
    "vectorizer = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_type_ids': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       " array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "       dtype=int32)>,\n",
       " 'input_word_ids': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       " array([[  101,  1045,  2293, 19081,   102,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]], dtype=int32)>,\n",
       " 'input_mask': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       " array([[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "       dtype=int32)>}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer(['I love transformers'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On tärkeää käyttää samaa vektoroijaa, jolla alkuperäinen verkko on koulutettu. Lisäksi BERT-vektoroija palauttaa kolme komponenttia:\n",
    "* `input_word_ids`, joka on syötelauseen token-numeroiden sekvenssi\n",
    "* `input_mask`, joka näyttää, mikä osa sekvenssistä sisältää varsinaisen syötteen ja mikä osa on täytettä. Se muistuttaa `Masking`-kerroksen tuottamaa maskia\n",
    "* `input_type_ids` käytetään kielimallinnustehtävissä ja mahdollistaa kahden syötelauseen määrittämisen yhdeksi sekvenssiksi.\n",
    "\n",
    "Sen jälkeen voimme luoda BERT-ominaisuuksien erottimen:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = hub.KerasLayer('https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pooled_output -> (1, 128)\n",
      "encoder_outputs -> 4\n",
      "sequence_output -> (1, 128, 128)\n",
      "default -> (1, 128)\n"
     ]
    }
   ],
   "source": [
    "z = bert(vectorizer(['I love transformers']))\n",
    "for i,x in z.items():\n",
    "    print(f\"{i} -> { len(x) if isinstance(x, list) else x.shape }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Joten, BERT-kerros palauttaa joukon hyödyllisiä tuloksia:\n",
    "* `pooled_output` on tulos, joka saadaan keskiarvoistamalla kaikki sekvenssin tokenit. Voit ajatella sen älykkäänä semanttisena upotuksena koko verkosta. Se vastaa `GlobalAveragePooling1D`-kerroksen tulosta aiemmassa mallissamme.\n",
    "* `sequence_output` on viimeisen transformer-kerroksen tulos (vastaa `TransformerBlock`-kerroksen tulosta yllä olevassa mallissamme).\n",
    "* `encoder_outputs` ovat kaikkien transformer-kerrosten tulokset. Koska olemme ladanneet 4-kerroksisen BERT-mallin (kuten nimestä, joka sisältää `4_H`, voi päätellä), siinä on 4 tensoria. Viimeinen niistä on sama kuin `sequence_output`.\n",
    "\n",
    "Nyt määrittelemme end-to-end-luokittelumallin. Käytämme *funktionaalista mallin määrittelyä*, jossa määritämme mallin syötteen ja annamme sitten joukon lausekkeita sen tuloksen laskemiseksi. Teemme myös BERT-mallin painot ei-opetettaviksi ja opetamme vain lopullisen luokittelijan:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer (KerasLayer)        {'input_type_ids': ( 0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer_1 (KerasLayer)      {'pooled_output': (N 4782465     keras_layer[0][0]                \n",
      "                                                                 keras_layer[0][1]                \n",
      "                                                                 keras_layer[0][2]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 128)          0           keras_layer_1[0][5]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 4)            516         dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 4,782,981\n",
      "Trainable params: 516\n",
      "Non-trainable params: 4,782,465\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inp = keras.Input(shape=(),dtype=tf.string)\n",
    "x = vectorizer(inp)\n",
    "x = bert(x)\n",
    "x = keras.layers.Dropout(0.1)(x['pooled_output'])\n",
    "out = keras.layers.Dense(4,activation='softmax')(x)\n",
    "model = keras.models.Model(inp,out)\n",
    "bert.trainable = False\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 528s 559ms/step - loss: 0.8056 - acc: 0.6983 - val_loss: 0.5953 - val_acc: 0.7888\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9bb1e36d00>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'], optimizer='adam')\n",
    "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vaikka koulutettavia parametreja on vähän, prosessi on melko hidas, koska BERT-ominaisuuksien uuttaja on laskennallisesti raskas. Näyttää siltä, ettemme onnistuneet saavuttamaan kohtuullista tarkkuutta, joko koulutuksen puutteen tai mallin parametrien vähyyden vuoksi.\n",
    "\n",
    "Yritetään avata BERT-painot ja kouluttaa sitä myös. Tämä vaatii erittäin pienen oppimisnopeuden sekä tarkemman koulutusstrategian, jossa käytetään **warmupia** ja **AdamW**-optimointialgoritmia. Käytämme `tf-models-official`-pakettia luodaksemme optimointialgoritmin:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer (KerasLayer)        {'input_type_ids': ( 0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer_1 (KerasLayer)      {'pooled_output': (N 4782465     keras_layer[0][0]                \n",
      "                                                                 keras_layer[0][1]                \n",
      "                                                                 keras_layer[0][2]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 128)          0           keras_layer_1[0][5]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 4)            516         dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 4,782,981\n",
      "Trainable params: 4,782,980\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n",
      "938/938 [==============================] - 629s 664ms/step - loss: 0.6344 - acc: 0.7658 - val_loss: 0.4876 - val_acc: 0.8247\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9bb0bd0070>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from official.nlp import optimization \n",
    "bert.trainable=True\n",
    "model.summary()\n",
    "epochs = 3\n",
    "opt = optimization.create_optimizer(\n",
    "    init_lr=3e-5,\n",
    "    num_train_steps=epochs*len(ds_train),\n",
    "    num_warmup_steps=0.1*epochs*len(ds_train),\n",
    "    optimizer_type='adamw')\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'], optimizer=opt)\n",
    "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kuten huomaat, koulutus etenee melko hitaasti – mutta saatat haluta kokeilla ja kouluttaa mallia muutaman epochin ajan (5–10) ja katsoa, voitko saada parhaan tuloksen verrattuna aiemmin käyttämiimme lähestymistapoihin.\n",
    "\n",
    "## Huggingface Transformers -kirjasto\n",
    "\n",
    "Toinen hyvin yleinen (ja hieman yksinkertaisempi) tapa käyttää Transformer-malleja on [HuggingFace-paketti](https://github.com/huggingface/), joka tarjoaa yksinkertaisia rakennuspalikoita erilaisiin NLP-tehtäviin. Se on saatavilla sekä Tensorflow- että PyTorch-ympäristöihin, jotka ovat muita erittäin suosittuja neuroverkkojen kehysalustoja.\n",
    "\n",
    "> **Huom**: Jos et ole kiinnostunut näkemään, miten Transformers-kirjasto toimii – voit hypätä tämän muistikirjan loppuun, koska et tule näkemään mitään olennaisesti erilaista kuin mitä olemme tehneet aiemmin. Toistamme samat vaiheet BERT-mallin kouluttamisessa käyttäen eri kirjastoa ja huomattavasti suurempaa mallia. Näin ollen prosessi sisältää melko pitkää koulutusta, joten saatat haluta vain selata koodin läpi.\n",
    "\n",
    "Katsotaanpa, miten ongelmamme voidaan ratkaista käyttämällä [Huggingface Transformers](http://huggingface.co).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensimmäinen asia, joka meidän täytyy tehdä, on valita käytettävä malli. Huggingfacella on sisäänrakennettujen mallien lisäksi [verkkopohjainen mallivarasto](https://huggingface.co/models), josta löydät paljon lisää yhteisön valmiiksi kouluttamia malleja. Kaikki nämä mallit voidaan ladata ja käyttää pelkästään antamalla mallin nimi. Kaikki tarvittavat binääritiedostot mallia varten ladataan automaattisesti.\n",
    "\n",
    "Joissain tilanteissa sinun täytyy ladata omia mallejasi. Tällöin voit määrittää hakemiston, joka sisältää kaikki tarvittavat tiedostot, kuten tokenisoijan parametrit, `config.json`-tiedoston mallin parametreilla, binääripainot jne.\n",
    "\n",
    "Mallin nimen perusteella voimme luoda sekä mallin että tokenisoijan. Aloitetaan tokenisoijasta:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "# To load the model from Internet repository using model name. \n",
    "# Use this if you are running from your own copy of the notebooks\n",
    "bert_model = 'bert-base-uncased' \n",
    "\n",
    "# To load the model from the directory on disk. Use this for Microsoft Learn module, because we have\n",
    "# prepared all required files for you.\n",
    "#bert_model = './bert'\n",
    "\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(bert_model)\n",
    "\n",
    "MAX_SEQ_LEN = 128\n",
    "PAD_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "UNK_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tokenizer`-objekti sisältää `encode`-funktion, jota voidaan käyttää suoraan tekstin koodaamiseen:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 23435, 12314, 2003, 1037, 2307, 7705, 2005, 17953, 2361, 102]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('Tensorflow is a great framework for NLP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voimme myös käyttää tokenisoijaa koodataksemme sekvenssin tavalla, joka sopii mallille välittämiseen, eli sisältäen `token_ids`, `input_mask` kentät jne. Voimme myös määrittää, että haluamme Tensorflow-tensoreita antamalla `return_tensors='tf'` argumentin:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[ 101, 7592, 1010, 2045,  102]], dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[0, 0, 0, 0, 0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[1, 1, 1, 1, 1]], dtype=int32)>}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(['Hello, there'],return_tensors='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tässä tapauksessa käytämme valmiiksi koulutettua BERT-mallia nimeltä `bert-base-uncased`. *Uncased* tarkoittaa, että malli ei erota isoja ja pieniä kirjaimia.\n",
    "\n",
    "Kun mallia koulutetaan, meidän täytyy antaa syötteeksi tokenisoitu sekvenssi, joten suunnittelemme datankäsittelyputken. Koska `tokenizer.encode` on Python-funktio, käytämme samaa lähestymistapaa kuin edellisessä osiossa kutsumalla sitä `py_function`-toiminnolla:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(x):\n",
    "    return tokenizer.encode(x.numpy().decode('utf-8'),return_tensors='tf',padding='max_length',max_length=MAX_SEQ_LEN,truncation=True)[0]\n",
    "\n",
    "def process_fn(x):\n",
    "    s = x['title']+' '+x['description']\n",
    "    e = tf.py_function(process,inp=[s],Tout=(tf.int32))\n",
    "    e.set_shape(MAX_SEQ_LEN)\n",
    "    return e,x['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nyt voimme ladata varsinaisen mallin käyttämällä `BertForSequenceClassification`-pakettia. Tämä varmistaa, että mallillamme on jo tarvittava luokitteluun tarkoitettu arkkitehtuuri, mukaan lukien lopullinen luokitin. Näet varoitusviestin, jossa todetaan, että lopullisen luokittimen painot eivät ole alustettuja ja malli vaatii esikoulutusta - tämä on täysin normaalia, sillä juuri sitä olemme tekemässä!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = transformers.TFBertForSequenceClassification.from_pretrained(bert_model,num_labels=4,output_attentions=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  109482240 \n",
      "_________________________________________________________________\n",
      "dropout_75 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  3076      \n",
      "=================================================================\n",
      "Total params: 109,485,316\n",
      "Trainable params: 109,485,316\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kuten näet `summary()`-funktiosta, malli sisältää lähes 110 miljoonaa parametria! Oletettavasti, jos haluamme yksinkertaisen luokittelutehtävän suhteellisen pienellä datalla, emme halua kouluttaa BERT-peruskerrosta:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  109482240 \n",
      "_________________________________________________________________\n",
      "dropout_75 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  3076      \n",
      "=================================================================\n",
      "Total params: 109,485,316\n",
      "Trainable params: 3,076\n",
      "Non-trainable params: 109,482,240\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.layers[0].trainable = False\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nyt voimme aloittaa mallin koulutuksen!\n",
    "\n",
    "> **Huomio**: Täysimittaisen BERT-mallin kouluttaminen voi olla erittäin aikaa vievää! Siksi koulutamme sitä vain ensimmäiset 32 erää. Tämä on vain esimerkki siitä, miten mallin koulutus asetetaan. Jos haluat kokeilla täysimittaista koulutusta - poista vain `steps_per_epoch` ja `validation_steps` -parametrit, ja varaudu odottamaan!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 142s 4s/step - loss: 1.3896 - acc: 0.2500 - val_loss: 1.3863 - val_acc: 0.2480\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f1d40a4b6a0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile('adam','sparse_categorical_crossentropy',['acc'])\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "model.fit(ds_train.map(process_fn).batch(32),validation_data=ds_test.map(process_fn).batch(32),steps_per_epoch=32,validation_steps=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jos lisäät iteraatioiden määrää ja odotat tarpeeksi kauan sekä koulutat useita epookkeja, voit odottaa, että BERT-luokittelu antaa meille parhaan tarkkuuden! Tämä johtuu siitä, että BERT ymmärtää jo varsin hyvin kielen rakenteen, ja meidän tarvitsee vain hienosäätää lopullista luokittelijaa. Kuitenkin, koska BERT on suuri malli, koko koulutusprosessi vie paljon aikaa ja vaatii merkittävää laskentatehoa! (GPU, ja mieluiten useampi kuin yksi).\n",
    "\n",
    "> **Huom:** Esimerkissämme olemme käyttäneet yhtä pienimmistä valmiiksi koulutetuista BERT-malleista. On olemassa suurempia malleja, jotka todennäköisesti tuottavat parempia tuloksia.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yhteenveto\n",
    "\n",
    "Tässä osiossa olemme tutustuneet hyvin uusiin mallirakenteisiin, jotka perustuvat **transformereihin**. Olemme soveltaneet niitä tekstiluokittelutehtäväämme, mutta vastaavasti BERT-malleja voidaan käyttää esimerkiksi entiteettien tunnistamiseen, kysymysten vastaamiseen ja muihin NLP-tehtäviin.\n",
    "\n",
    "Transformer-mallit edustavat NLP:n tämänhetkistä huipputasoa, ja useimmissa tapauksissa niiden tulisi olla ensimmäinen ratkaisu, jota kokeilet, kun toteutat räätälöityjä NLP-ratkaisuja. Kuitenkin, jos haluat rakentaa kehittyneitä neuroverkkomalleja, on äärimmäisen tärkeää ymmärtää tämän moduulin yhteydessä käsitellyt perusperiaatteet toistuvista neuroverkoista.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Vastuuvapauslauseke**:  \nTämä asiakirja on käännetty käyttämällä tekoälypohjaista käännöspalvelua [Co-op Translator](https://github.com/Azure/co-op-translator). Vaikka pyrimme tarkkuuteen, huomioithan, että automaattiset käännökset voivat sisältää virheitä tai epätarkkuuksia. Alkuperäistä asiakirjaa sen alkuperäisellä kielellä tulisi pitää ensisijaisena lähteenä. Kriittisen tiedon osalta suositellaan ammattimaista ihmiskäännöstä. Emme ole vastuussa väärinkäsityksistä tai virhetulkinnoista, jotka johtuvat tämän käännöksen käytöstä.\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0cb620c6d4b9f7a635928804c26cf22403d89d98d79684e4529119355ee6d5a5"
  },
  "kernelspec": {
   "display_name": "py38_tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "coopTranslator": {
   "original_hash": "ab59c532409774988ab875f2260e8e53",
   "translation_date": "2025-08-28T21:41:06+00:00",
   "source_file": "lessons/5-NLP/18-Transformers/TransformersTF.ipynb",
   "language_code": "fi"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}