# Generatiiviset vastakkaiset verkot

Edellisess√§ osiossa opimme **generatiivisista malleista**: malleista, jotka voivat luoda uusia kuvia, jotka muistuttavat koulutusdatan kuvia. VAE oli hyv√§ esimerkki generatiivisesta mallista.

## [Ennakkokysely](https://ff-quizzes.netlify.app/en/ai/quiz/19)

Kuitenkin, jos yrit√§mme luoda jotain todella merkityksellist√§, kuten maalauksen kohtuullisella resoluutiolla, VAE:lla huomaamme, ett√§ koulutus ei etene hyvin. T√§t√§ k√§ytt√∂tarkoitusta varten meid√§n tulisi oppia toinen arkkitehtuuri, joka on erityisesti suunnattu generatiivisiin malleihin - **Generatiiviset vastakkaiset verkot**, eli GANit.

GANin p√§√§idea on k√§ytt√§√§ kahta neuroverkkoa, jotka koulutetaan toisiaan vastaan:

<img src="../../../../../translated_images/fi/gan_architecture.8f3a5ab62b8d5d69.webp" width="70%"/>

> Kuva: [Dmitry Soshnikov](http://soshnikov.com)

> ‚úÖ Pieni sanasto:
> * **Generaattori** on verkko, joka ottaa satunnaisen vektorin ja tuottaa kuvan tuloksena.
> * **Diskriminaattori** on verkko, joka ottaa kuvan ja pyrkii m√§√§ritt√§m√§√§n, onko se oikea kuva (koulutusdatasta) vai generaattorin tuottama. Se on pohjimmiltaan kuvien luokittelija.

### Diskriminaattori

Diskriminaattorin arkkitehtuuri ei eroa tavallisesta kuvien luokitteluun tarkoitetusta verkosta. Yksinkertaisimmillaan se voi olla t√§ysin yhdistetty luokittelija, mutta todenn√§k√∂isimmin se on [konvoluutioverkko](../07-ConvNets/README.md).

> ‚úÖ GAN, joka perustuu konvoluutioverkkoihin, kutsutaan nimell√§ [DCGAN](https://arxiv.org/pdf/1511.06434.pdf)

CNN-diskriminaattori koostuu seuraavista kerroksista: useita konvoluutioita+poolauksia (joilla pienennet√§√§n spatiaalista kokoa) ja yksi tai useampi t√§ysin yhdistetty kerros, joka tuottaa "piirrevektorin", lopuksi bin√§√§riluokittelija.

> ‚úÖ 'Poolaus' t√§ss√§ yhteydess√§ tarkoittaa tekniikkaa, joka pienent√§√§ kuvan kokoa. "Poolauskerrokset pienent√§v√§t datan dimensioita yhdist√§m√§ll√§ yhden kerroksen neuroniryhmien tulokset yhdeksi neuroniksi seuraavassa kerroksessa." - [l√§hde](https://wikipedia.org/wiki/Convolutional_neural_network#Pooling_layers)

### Generaattori

Generaattori on hieman monimutkaisempi. Voit ajatella sen olevan k√§√§nteinen diskriminaattori. Aloittaen latenttivektorista (piirrevektorin sijaan), siin√§ on t√§ysin yhdistetty kerros, joka muuntaa sen vaadittuun kokoon/muotoon, jota seuraa dekonvoluutio+skaalaus. T√§m√§ on samanlainen kuin [autokooderin](../09-Autoencoders/README.md) *dekooderi*-osa.

> ‚úÖ Koska konvoluutiokerros toteutetaan lineaarisena suodattimena, joka kulkee kuvan l√§pi, dekonvoluutio on pohjimmiltaan samanlainen kuin konvoluutio ja voidaan toteuttaa samalla kerroslogiikalla.

<img src="../../../../../translated_images/fi/gan_arch_detail.46b95fd366f8e543.webp" width="70%"/>

> Kuva: [Dmitry Soshnikov](http://soshnikov.com)

### GANin koulutus

GANeja kutsutaan **vastakkaisiksi**, koska generaattorin ja diskriminaattorin v√§lill√§ on jatkuva kilpailu. T√§m√§n kilpailun aikana sek√§ generaattori ett√§ diskriminaattori parantavat suoritustaan, jolloin verkko oppii tuottamaan yh√§ parempia kuvia.

Koulutus tapahtuu kahdessa vaiheessa:

* **Diskriminaattorin koulutus**. T√§m√§ teht√§v√§ on melko suoraviivainen: generaattori tuottaa er√§n kuvia, jotka merkit√§√§n 0:ksi (v√§√§rennetty kuva), ja otetaan er√§ kuvia sy√∂tt√∂datasta (merkittyn√§ 1:ksi, oikea kuva). Saamme jonkin *diskriminaattorih√§vi√∂n* ja suoritamme takaisinkulun.
* **Generaattorin koulutus**. T√§m√§ on hieman monimutkaisempaa, koska emme tied√§ generaattorin odotettua tulosta suoraan. Otamme koko GAN-verkon, joka koostuu generaattorista ja diskriminaattorista, sy√∂t√§mme siihen satunnaisia vektoreita ja odotamme tuloksen olevan 1 (vastaa oikeita kuvia). Sitten j√§√§dyt√§mme diskriminaattorin parametrit (emme halua sen kouluttavan t√§ss√§ vaiheessa) ja suoritamme takaisinkulun.

T√§m√§n prosessin aikana sek√§ generaattorin ett√§ diskriminaattorin h√§vi√∂t eiv√§t laske merkitt√§v√§sti. Ihannetilanteessa niiden tulisi v√§r√§hdell√§, mik√§ vastaa molempien verkkojen suorituskyvyn paranemista.

## ‚úçÔ∏è Harjoitukset: GANit

* [GAN-muistikirja TensorFlow/Kerasilla](GANTF.ipynb)
* [GAN-muistikirja PyTorchilla](GANPyTorch.ipynb)

### GANin koulutuksen ongelmat

GANit ovat tunnetusti erityisen vaikeita kouluttaa. T√§ss√§ muutamia ongelmia:

* **Mode Collapse**. T√§ll√§ termill√§ tarkoitetaan, ett√§ generaattori oppii tuottamaan yhden onnistuneen kuvan, joka huijaa diskriminaattoria, mutta ei monipuolista valikoimaa erilaisia kuvia.
* **Herkk√§ hyperparametreille**. Usein voi n√§hd√§, ett√§ GAN ei konvergoi ollenkaan, ja sitten oppimisnopeuden √§killinen lasku johtaa konvergenssiin.
* **Tasapainon yll√§pit√§minen** generaattorin ja diskriminaattorin v√§lill√§. Monissa tapauksissa diskriminaattorin h√§vi√∂ voi pudota nollaan suhteellisen nopeasti, mik√§ johtaa siihen, ett√§ generaattori ei pysty en√§√§ kouluttautumaan. T√§m√§n voittamiseksi voimme yritt√§√§ asettaa eri oppimisnopeudet generaattorille ja diskriminaattorille tai ohittaa diskriminaattorin koulutuksen, jos h√§vi√∂ on jo liian alhainen.
* Korkean resoluution **koulutus**. Sama ongelma kuin autokoodereilla, t√§m√§ ongelma syntyy, koska liian monen konvoluutiokerroksen rekonstruointi johtaa artefakteihin. T√§m√§ ongelma ratkaistaan tyypillisesti niin sanotulla **progressiivisella kasvulla**, jossa ensin muutama kerros koulutetaan matalaresoluutioisilla kuvilla, ja sitten kerrokset "avataan" tai lis√§t√§√§n. Toinen ratkaisu olisi lis√§t√§ ylim√§√§r√§isi√§ yhteyksi√§ kerrosten v√§lill√§ ja kouluttaa useita resoluutioita samanaikaisesti - katso t√§m√§ [Multi-Scale Gradient GANs -paperi](https://arxiv.org/abs/1903.06048) saadaksesi lis√§tietoja.

## Tyylinsiirto

GANit ovat erinomainen tapa luoda taiteellisia kuvia. Toinen mielenkiintoinen tekniikka on niin sanottu **tyylinsiirto**, joka ottaa yhden **sis√§lt√∂kuvan** ja piirt√§√§ sen uudelleen eri tyylill√§, soveltaen suodattimia **tyylikuvasta**.

N√§in se toimii:
* Aloitamme satunnaisesta kohinakuvaasta (tai sis√§lt√∂kuvasta, mutta ymm√§rt√§misen vuoksi on helpompi aloittaa satunnaisesta kohinasta).
* Tavoitteenamme on luoda sellainen kuva, joka olisi l√§hell√§ sek√§ sis√§lt√∂kuvaa ett√§ tyylikuvaa. T√§m√§ m√§√§ritet√§√§n kahdella h√§vi√∂funktiolla:
   - **Sis√§lt√∂h√§vi√∂** lasketaan CNN:n tietyill√§ kerroksilla nykyisest√§ kuvasta ja sis√§lt√∂kuvasta saatujen piirteiden perusteella.
   - **Tyylih√§vi√∂** lasketaan nykyisen kuvan ja tyylikuvan v√§lill√§ √§lykk√§√§ll√§ tavalla k√§ytt√§m√§ll√§ Gram-matriiseja (lis√§tietoja [esimerkkimuistikirjassa](StyleTransfer.ipynb)).
* Jotta kuva olisi tasaisempi ja poistaisi kohinaa, lis√§√§mme my√∂s **Variation loss** -h√§vi√∂n, joka laskee keskim√§√§r√§isen et√§isyyden vierekk√§isten pikselien v√§lill√§.
* P√§√§optimointisilmukka s√§√§t√§√§ nykyist√§ kuvaa k√§ytt√§m√§ll√§ gradienttilaskentaa (tai jotain muuta optimointialgoritmia) minimoidakseen kokonaish√§vi√∂n, joka on kaikkien kolmen h√§vi√∂n painotettu summa.

## ‚úçÔ∏è Esimerkki: [Tyylinsiirto](StyleTransfer.ipynb)

## [J√§lkikysely](https://ff-quizzes.netlify.app/en/ai/quiz/20)

## Yhteenveto

T√§ss√§ oppitunnissa opit GANeista ja niiden kouluttamisesta. Opit my√∂s erityisist√§ haasteista, joita t√§m√§ neuroverkkojen tyyppi voi kohdata, sek√§ strategioista niiden voittamiseksi.

## üöÄ Haaste

K√§y l√§pi [Tyylinsiirto-muistikirja](StyleTransfer.ipynb) k√§ytt√§en omia kuviasi.

## Kertaus ja itseopiskelu

Lis√§tietoja GANeista l√∂yd√§t n√§ist√§ l√§hteist√§:

* Marco Pasini, [10 Lessons I Learned Training GANs for one Year](https://towardsdatascience.com/10-lessons-i-learned-training-generative-adversarial-networks-gans-for-a-year-c9071159628)
* [StyleGAN](https://en.wikipedia.org/wiki/StyleGAN), *de facto* GAN-arkkitehtuuri harkittavaksi
* [Creating Generative Art using GANs on Azure ML](https://soshnikov.com/scienceart/creating-generative-art-using-gan-on-azureml/)

## Teht√§v√§

Palaa johonkin t√§m√§n oppitunnin kahdesta muistikirjasta ja kouluta GAN omilla kuvillasi. Mit√§ voit luoda?

---

