{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# కార్ట్పోల్ బ్యాలెన్సింగ్ కోసం RL శిక్షణ\n",
    "\n",
    "ఈ నోట్‌బుక్ [AI for Beginners Curriculum](http://aka.ms/ai-beginners) భాగం. ఇది [అధికారిక PyTorch ట్యుటోరియల్](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html) మరియు [ఈ కార్ట్పోల్ PyTorch అమలు](https://github.com/yc930401/Actor-Critic-pytorch) నుండి ప్రేరణ పొందింది.\n",
    "\n",
    "ఈ ఉదాహరణలో, మేము RL ఉపయోగించి ఒక మోడల్‌ను శిక్షణ ఇస్తాము, ఇది ఒక కార్ట్‌పై ఉన్న కంబళిని సమతుల్యం చేయగలదు, ఆ కార్ట్ ఎడమ మరియు కుడి వైపుకు ఆడవచ్చు. కంబళిని సిమ్యులేట్ చేయడానికి మేము [OpenAI Gym](https://www.gymlibrary.ml/) వాతావరణాన్ని ఉపయోగిస్తాము.\n",
    "\n",
    "> **గమనిక**: మీరు ఈ పాఠం కోడ్‌ను స్థానికంగా (ఉదా: Visual Studio Code నుండి) నడపవచ్చు, అప్పుడు సిమ్యులేషన్ కొత్త విండోలో తెరుచుకుంటుంది. ఆన్‌లైన్‌లో కోడ్ నడిపేటప్పుడు, కొంత మార్పులు చేయాల్సి రావచ్చు, వివరాలు [ఇక్కడ](https://towardsdatascience.com/rendering-openai-gym-envs-on-binder-and-google-colab-536f99391cc7) ఉన్నాయి.\n",
    "\n",
    "ముందుగా Gym ఇన్‌స్టాల్ అయిందో లేదో చూసుకుందాం:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ఇప్పుడు మనం CartPole వాతావరణాన్ని సృష్టించి దానిపై ఎలా ఆపరేట్ చేయాలో చూద్దాం. ఒక వాతావరణానికి క్రింది లక్షణాలు ఉంటాయి:\n",
    "\n",
    "* **Action space** అనేది సిమ్యులేషన్ ప్రతి దశలో మనం చేయగలిగే సాధ్యమైన చర్యల సమాహారం\n",
    "* **Observation space** అనేది మనం చేయగలిగే పరిశీలనల స్థలం\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "print(f\"Action space: {env.action_space}\")\n",
    "print(f\"Observation space: {env.observation_space}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "సిమ్యులేషన్ ఎలా పనిచేస్తుందో చూద్దాం. క్రింది లూప్ సిమ్యులేషన్‌ను నడిపిస్తుంది, `env.step` ముగింపు ఫ్లాగ్ `done` ను తిరిగి ఇవ్వకపోవడం వరకు. మనం యాదృచ్ఛికంగా చర్యలను ఎంచుకుంటాము `env.action_space.sample()` ఉపయోగించి, అంటే ప్రయోగం చాలా త్వరగా విఫలమవుతుంది (CartPole వాతావరణం CartPole వేగం, దాని స్థానం లేదా కోణం కొన్ని పరిమితుల వెలుపల ఉన్నప్పుడు ముగుస్తుంది).\n",
    "\n",
    "> సిమ్యులేషన్ కొత్త విండోలో తెరుస్తుంది. మీరు కోడ్‌ను అనేక సార్లు నడిపించి దాని ప్రవర్తనను చూడవచ్చు.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "\n",
    "done = False\n",
    "total_reward = 0\n",
    "while not done:\n",
    "   env.render()\n",
    "   obs, rew, done, info = env.step(env.action_space.sample())\n",
    "   total_reward += rew\n",
    "   print(f\"{obs} -> {rew}\")\n",
    "print(f\"Total reward: {total_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "మీరు గమనించవచ్చు, ఆబ్జర్వేషన్లు 4 సంఖ్యలను కలిగి ఉంటాయి. అవి:\n",
    "- కార్ట్ యొక్క స్థానం\n",
    "- కార్ట్ యొక్క వేగం\n",
    "- ధ్రువం యొక్క కోణం\n",
    "- ధ్రువం యొక్క తిరుగుడు వేగం\n",
    "\n",
    "`rew` అనేది ప్రతి దశలో మేము పొందే రివార్డు. CartPole పరిసరంలో మీరు ప్రతి సిమ్యులేషన్ దశకు 1 పాయింట్ రివార్డు పొందుతారని మీరు చూడవచ్చు, మరియు లక్ష్యం మొత్తం రివార్డును గరిష్టం చేయడం, అంటే CartPole పడకుండా సమతుల్యం నిలబెట్టగలిగే సమయం.\n",
    "\n",
    "పునరుద్ధరణ అభ్యాస సమయంలో, మా లక్ష్యం ఒక **పాలసీ** $\\pi$ ను శిక్షణ ఇవ్వడం, ఇది ప్రతి స్థితి $s$ కోసం ఏ చర్య $a$ తీసుకోవాలో చెప్పుతుంది, అంటే $a = \\pi(s)$.\n",
    "\n",
    "మీరు ప్రాబబిలిస్టిక్ పరిష్కారాన్ని కోరుకుంటే, పాలసీని ప్రతి చర్యకు సంబంధించిన ప్రాబబిలిటీల సమితిని ఇచ్చే విధంగా భావించవచ్చు, అంటే $\\pi(a|s)$ అనగా స్థితి $s$ వద్ద చర్య $a$ తీసుకోవాల్సిన ప్రాబబిలిటీ.\n",
    "\n",
    "## పాలసీ గ్రేడియంట్ పద్ధతి\n",
    "\n",
    "సరళమైన RL అల్గోరిథంలో, దీనిని **పాలసీ గ్రేడియంట్** అంటారు, మేము తదుపరి చర్యను అంచనా వేయడానికి ఒక న్యూరల్ నెట్‌వర్క్‌ను శిక్షణ ఇస్తాము.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "num_inputs = 4\n",
    "num_actions = 2\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(num_inputs, 128, bias=False, dtype=torch.float32),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(128, num_actions, bias = False, dtype=torch.float32),\n",
    "    torch.nn.Softmax(dim=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "మేము నెట్‌వర్క్‌ను అనేక ప్రయోగాలు నిర్వహించి శిక్షణ ఇస్తాము, మరియు ప్రతి రన్ తర్వాత మా నెట్‌వర్క్‌ను నవీకరిస్తాము. ప్రయోగాన్ని నిర్వహించి ఫలితాలను (అంటే **ట్రేస్**) తిరిగి ఇచ్చే ఫంక్షన్‌ను నిర్వచిద్దాం - అన్ని స్థితులు, చర్యలు (మరియు వాటి సిఫారసు చేసిన సంభావ్యతలు), మరియు బహుమతులు:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(max_steps_per_episode = 10000,render=False):    \n",
    "    states, actions, probs, rewards = [],[],[],[]\n",
    "    state = env.reset()\n",
    "    for _ in range(max_steps_per_episode):\n",
    "        if render:\n",
    "            env.render()\n",
    "        action_probs = model(torch.from_numpy(np.expand_dims(state,0)))[0]\n",
    "        action = np.random.choice(num_actions, p=np.squeeze(action_probs.detach().numpy()))\n",
    "        nstate, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            break\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        probs.append(action_probs.detach().numpy())\n",
    "        rewards.append(reward)\n",
    "        state = nstate\n",
    "    return np.vstack(states), np.vstack(actions), np.vstack(probs), np.vstack(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "మీరు ఒక ఎపిసోడ్‌ను శిక్షణ పొందని నెట్‌వర్క్‌తో నడిపించి, మొత్తం బహుమతి (అంటే ఎపిసోడ్ పొడవు) చాలా తక్కువగా ఉన్నదని గమనించవచ్చు:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s, a, p, r = run_episode()\n",
    "print(f\"Total reward: {np.sum(r)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "పాలసీ గ్రాడియెంట్ అల్గోరిథం యొక్క ఒక క్లిష్టమైన అంశం **డిస్కౌంటెడ్ రివార్డ్స్** ఉపయోగించడం. ఆలోచన ఏమిటంటే, ఆట యొక్క ప్రతి దశలో మొత్తం రివార్డ్స్ వెక్టర్‌ను గణించటం, మరియు ఈ ప్రక్రియలో మేము ప్రారంభ రివార్డ్స్‌ను కొన్ని గుణకం $gamma$ ఉపయోగించి డిస్కౌంట్ చేస్తాము. మేము ఫలితంగా వచ్చిన వెక్టర్‌ను సాధారణీకరించ also, ఎందుకంటే దీన్ని మా శిక్షణను ప్రభావితం చేయడానికి బరువుగా ఉపయోగిస్తాము:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 0.0001\n",
    "\n",
    "def discounted_rewards(rewards,gamma=0.99,normalize=True):\n",
    "    ret = []\n",
    "    s = 0\n",
    "    for r in rewards[::-1]:\n",
    "        s = r + gamma * s\n",
    "        ret.insert(0, s)\n",
    "    if normalize:\n",
    "        ret = (ret-np.mean(ret))/(np.std(ret)+eps)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ఇప్పుడు నిజమైన శిక్షణ ప్రారంభిద్దాం! మనం 300 ఎపిసోడ్లు నడిపించబోతున్నాము, ప్రతి ఎపిసోడ్‌లో మనం ఈ క్రింది పనులు చేస్తాము:\n",
    "\n",
    "1. ప్రయోగాన్ని నడిపించి ట్రేస్ సేకరించండి  \n",
    "2. తీసుకున్న చర్యలు మరియు అంచనా probabilities మధ్య తేడా (`gradients`) లెక్కించండి. తేడా తక్కువగా ఉంటే, మనం సరైన చర్య తీసుకున్నామని ఎక్కువ నమ్మకం ఉంటుంది.  \n",
    "3. డిస్కౌంటెడ్ రివార్డ్స్ లెక్కించి gradients ను వాటితో గుణించండి - ఇది ఎక్కువ రివార్డ్ ఉన్న దశలు తక్కువ రివార్డ్ ఉన్న దశల కంటే ఫలితంపై ఎక్కువ ప్రభావం చూపుతాయని నిర్ధారిస్తుంది  \n",
    "4. మన న్యూరల్ నెట్‌వర్క్ కోసం ఆశించిన లక్ష్య చర్యలు, రన్ సమయంలో అంచనా probabilities నుండి కొంత భాగం మరియు లెక్కించిన gradients నుండి కొంత భాగం తీసుకుంటాయి. gradients మరియు రివార్డ్స్‌ను ఎంత మేరకు పరిగణలోకి తీసుకోవాలో నిర్ణయించడానికి `alpha` పారామీటర్ ఉపయోగిస్తాము - దీనిని రీఫోర్స్‌మెంట్ అల్గోరిథం యొక్క *లెర్నింగ్ రేట్* అంటారు.  \n",
    "5. చివరగా, మనం స్టేట్స్ మరియు ఆశించిన చర్యలపై నెట్‌వర్క్‌ను శిక్షణ ఇస్తాము, మరియు ఈ ప్రక్రియను పునరావృతం చేస్తాము.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "def train_on_batch(x, y):\n",
    "    x = torch.from_numpy(x)\n",
    "    y = torch.from_numpy(y)\n",
    "    optimizer.zero_grad()\n",
    "    predictions = model(x)\n",
    "    loss = -torch.mean(torch.log(predictions) * y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1e-4\n",
    "\n",
    "history = []\n",
    "for epoch in range(300):\n",
    "    states, actions, probs, rewards = run_episode()\n",
    "    one_hot_actions = np.eye(2)[actions.T][0]\n",
    "    gradients = one_hot_actions-probs\n",
    "    dr = discounted_rewards(rewards)\n",
    "    gradients *= dr\n",
    "    target = alpha*np.vstack([gradients])+probs\n",
    "    train_on_batch(states,target)\n",
    "    history.append(np.sum(rewards))\n",
    "    if epoch%100==0:\n",
    "        print(f\"{epoch} -> {np.sum(rewards)}\")\n",
    "\n",
    "plt.plot(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ఇప్పుడు ఫలితాన్ని చూడటానికి రేండరింగ్‌తో ఎపిసోడ్‌ను నడపుదాం:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = run_episode(render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ఆశిస్తున్నాను, మీరు ఇప్పుడు పొల్ బాగా సంతులనం అవుతుందని చూడగలుగుతారు!\n",
    "\n",
    "## యాక్టర్-క్రిటిక్ మోడల్\n",
    "\n",
    "యాక్టర్-క్రిటిక్ మోడల్ అనేది పాలసీ గ్రాడియెంట్ల మరింత అభివృద్ధి, ఇందులో మనం పాలసీ మరియు అంచనా వేయబడిన రివార్డులను నేర్చుకునే న్యూరల్ నెట్‌వర్క్‌ను నిర్మిస్తాము. నెట్‌వర్క్‌కు రెండు అవుట్‌పుట్లు ఉంటాయి (లేదా మీరు దీన్ని రెండు వేర్వేరు నెట్‌వర్క్‌లుగా చూడవచ్చు):\n",
    "* **యాక్టర్** పాలసీ గ్రాడియెంట్ మోడల్‌లో ఉన్నట్లుగా, మనకు స్టేట్ ప్రాబబిలిటీ పంపిణీని ఇచ్చి తీసుకోవాల్సిన చర్యను సిఫార్సు చేస్తుంది\n",
    "* **క్రిటిక్** ఆ చర్యల నుండి రివార్డ్ ఎంత ఉంటుందో అంచనా వేస్తుంది. ఇది ఇచ్చిన స్టేట్‌లో భవిష్యత్తులో మొత్తం అంచనా వేయబడిన రివార్డులను తిరిగి ఇస్తుంది.\n",
    "\n",
    "ఇలాంటి మోడల్‌ను నిర్వచిద్దాం:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import count\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "lr = 0.0001\n",
    "\n",
    "class Actor(torch.nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Actor, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.linear1 = torch.nn.Linear(self.state_size, 128)\n",
    "        self.linear2 = torch.nn.Linear(128, 256)\n",
    "        self.linear3 = torch.nn.Linear(256, self.action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        output = F.relu(self.linear1(state))\n",
    "        output = F.relu(self.linear2(output))\n",
    "        output = self.linear3(output)\n",
    "        distribution = torch.distributions.Categorical(F.softmax(output, dim=-1))\n",
    "        return distribution\n",
    "\n",
    "\n",
    "class Critic(torch.nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Critic, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.linear1 = torch.nn.Linear(self.state_size, 128)\n",
    "        self.linear2 = torch.nn.Linear(128, 256)\n",
    "        self.linear3 = torch.nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        output = F.relu(self.linear1(state))\n",
    "        output = F.relu(self.linear2(output))\n",
    "        value = self.linear3(output)\n",
    "        return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "మనం మా `discounted_rewards` మరియు `run_episode` ఫంక్షన్లను కొంచెం మార్చుకోవాలి:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discounted_rewards(next_value, rewards, masks, gamma=0.99):\n",
    "    R = next_value\n",
    "    returns = []\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        R = rewards[step] + gamma * R * masks[step]\n",
    "        returns.insert(0, R)\n",
    "    return returns\n",
    "\n",
    "def run_episode(actor, critic, n_iters):\n",
    "    optimizerA = torch.optim.Adam(actor.parameters())\n",
    "    optimizerC = torch.optim.Adam(critic.parameters())\n",
    "    for iter in range(n_iters):\n",
    "        state = env.reset()\n",
    "        log_probs = []\n",
    "        values = []\n",
    "        rewards = []\n",
    "        masks = []\n",
    "        entropy = 0\n",
    "        env.reset()\n",
    "\n",
    "        for i in count():\n",
    "            env.render()\n",
    "            state = torch.FloatTensor(state).to(device)\n",
    "            dist, value = actor(state), critic(state)\n",
    "\n",
    "            action = dist.sample()\n",
    "            next_state, reward, done, _ = env.step(action.cpu().numpy())\n",
    "\n",
    "            log_prob = dist.log_prob(action).unsqueeze(0)\n",
    "            entropy += dist.entropy().mean()\n",
    "\n",
    "            log_probs.append(log_prob)\n",
    "            values.append(value)\n",
    "            rewards.append(torch.tensor([reward], dtype=torch.float, device=device))\n",
    "            masks.append(torch.tensor([1-done], dtype=torch.float, device=device))\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                print('Iteration: {}, Score: {}'.format(iter, i))\n",
    "                break\n",
    "\n",
    "\n",
    "        next_state = torch.FloatTensor(next_state).to(device)\n",
    "        next_value = critic(next_state)\n",
    "        returns = discounted_rewards(next_value, rewards, masks)\n",
    "\n",
    "        log_probs = torch.cat(log_probs)\n",
    "        returns = torch.cat(returns).detach()\n",
    "        values = torch.cat(values)\n",
    "\n",
    "        advantage = returns - values\n",
    "\n",
    "        actor_loss = -(log_probs * advantage.detach()).mean()\n",
    "        critic_loss = advantage.pow(2).mean()\n",
    "\n",
    "        optimizerA.zero_grad()\n",
    "        optimizerC.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        critic_loss.backward()\n",
    "        optimizerA.step()\n",
    "        optimizerC.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ఇప్పుడు మనం ప్రధాన శిక్షణ లూప్‌ను నడిపించబోతున్నాము. సరైన నష్ట ఫంక్షన్లను లెక్కించి మరియు నెట్‌వర్క్ పారామితులను నవీకరించడం ద్వారా మాన్యువల్ నెట్‌వర్క్ శిక్షణ ప్రక్రియను ఉపయోగిస్తాము:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "actor = Actor(state_size, action_size).to(device)\n",
    "critic = Critic(state_size, action_size).to(device)\n",
    "run_episode(actor, critic, n_iters=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "చివరికి, మనం వాతావరణాన్ని మూసివేయండి.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ముఖ్యాంశాలు\n",
    "\n",
    "ఈ డెమోలో మనం రెండు RL అల్గోరిథమ్స్ చూశాము: సింపుల్ పాలసీ గ్రేడియంట్ మరియు మరింత సున్నితమైన యాక్టర్-క్రిటిక్. ఆ అల్గోరిథమ్స్ స్టేట్, యాక్షన్ మరియు రివార్డ్ అనే సారాంశ భావాలతో పనిచేస్తాయని మీరు చూడవచ్చు - అందువల్ల అవి చాలా భిన్నమైన వాతావరణాల్లో కూడా వర్తించవచ్చు.\n",
    "\n",
    "రీఫోర్స్‌మెంట్ లెర్నింగ్ మనకు సమస్యను పరిష్కరించడానికి ఉత్తమ వ్యూహాన్ని చివరి రివార్డ్‌ను చూసి నేర్చుకునే అవకాశం ఇస్తుంది. లేబుల్ చేయబడిన డేటాసెట్‌ల అవసరం లేకపోవడం వల్ల మనం సిమ్యులేషన్లను ఎన్నో సార్లు పునరావృతం చేసి మన మోడల్స్‌ను మెరుగుపరచుకోవచ్చు. అయితే, RLలో ఇంకా అనేక సవాళ్లు ఉన్నాయి, మీరు ఈ ఆసక్తికరమైన AI రంగంపై మరింత దృష్టి పెట్టాలనుకుంటే అవి తెలుసుకోవచ్చు.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n<!-- CO-OP TRANSLATOR DISCLAIMER START -->\n**అస్పష్టత**:  \nఈ పత్రాన్ని AI అనువాద సేవ [Co-op Translator](https://github.com/Azure/co-op-translator) ఉపయోగించి అనువదించబడింది. మేము ఖచ్చితత్వానికి ప్రయత్నించినప్పటికీ, ఆటోమేటెడ్ అనువాదాల్లో పొరపాట్లు లేదా తప్పిదాలు ఉండవచ్చు. మూల పత్రం దాని స్వదేశీ భాషలో అధికారిక మూలంగా పరిగణించాలి. ముఖ్యమైన సమాచారానికి, ప్రొఫెషనల్ మానవ అనువాదం సిఫార్సు చేయబడుతుంది. ఈ అనువాదం వాడకంలో ఏర్పడిన ఏవైనా అపార్థాలు లేదా తప్పుదారితీసే అర్థాలు కోసం మేము బాధ్యత వహించము.\n<!-- CO-OP TRANSLATOR DISCLAIMER END -->\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  },
  "coopTranslator": {
   "original_hash": "04f8d9978cd11281d81dd037cbf6ce20",
   "translation_date": "2025-11-26T02:38:57+00:00",
   "source_file": "lessons/6-Other/22-DeepRL/CartPole-RL-PyTorch.ipynb",
   "language_code": "te"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}