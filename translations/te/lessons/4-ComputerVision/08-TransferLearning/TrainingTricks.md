# డీప్ లెర్నింగ్ శిక్షణ చిట్కాలు

న్యూరల్ నెట్‌వర్క్లు లోతుగా మారుతున్న కొద్దీ, వాటి శిక్షణ ప్రక్రియ మరింత కష్టతరమవుతుంది. ఒక ప్రధాన సమస్యగా పిలవబడేది [వానిషింగ్ గ్రాడియెంట్స్](https://en.wikipedia.org/wiki/Vanishing_gradient_problem) లేదా [ఎక్స్‌ప్లోడింగ్ గ్రాడియెంట్స్](https://deepai.org/machine-learning-glossary-and-terms/exploding-gradient-problem#:~:text=Exploding%20gradients%20are%20a%20problem,updates%20are%20small%20and%20controlled.) సమస్య. [ఈ పోస్ట్](https://towardsdatascience.com/the-vanishing-exploding-gradient-problem-in-deep-neural-networks-191358470c11) ఆ సమస్యల గురించి మంచి పరిచయాన్ని ఇస్తుంది.

డీప్ నెట్‌వర్క్ల శిక్షణను మరింత సమర్థవంతంగా చేయడానికి కొన్ని సాంకేతికతలు ఉపయోగించవచ్చు.

## విలువలను సరైన పరిధిలో ఉంచడం

సంఖ్యాత్మక లెక్కింపులు స్థిరంగా ఉండాలంటే, మన న్యూరల్ నెట్‌వర్క్‌లోని అన్ని విలువలు సాధారణంగా [-1..1] లేదా [0..1] పరిధిలో ఉండేలా చూసుకోవాలి. ఇది కఠినమైన నియమం కాదు, కానీ ఫ్లోటింగ్ పాయింట్ లెక్కింపుల స్వభావం ప్రకారం, విభిన్న పరిమాణాల విలువలను ఖచ్చితంగా కలిపి నిర్వహించడం కష్టం. ఉదాహరణకు, 10<sup>-10</sup> మరియు 10<sup>10</sup> ను కలిపితే, మనం సాధారణంగా 10<sup>10</sup> ను పొందుతాము, ఎందుకంటే చిన్న విలువ పెద్దదైన ఆర్డర్‌కు మార్చబడుతుంది, అందువల్ల మాంటిస్సా (mantissa) పోతుంది.

అధిక భాగం యాక్టివేషన్ ఫంక్షన్లు [-1..1] పరిధిలో నాన్-లీనియర్ ఉంటాయి, కాబట్టి అన్ని ఇన్‌పుట్ డేటాను [-1..1] లేదా [0..1] పరిధిలో స్కేల్ చేయడం మంచిది.

## ప్రారంభ బరువు ప్రారంభీకరణ

సిద్ధాంతంగా, నెట్‌వర్క్ లేయర్ల ద్వారా విలువలు ఒకే పరిధిలో ఉండాలి. కాబట్టి, విలువల పంపిణీని నిలుపుకునే విధంగా బరువులను ప్రారంభీకరించడం ముఖ్యం.

సాధారణ పంపిణీ **N(0,1)** మంచిది కాదు, ఎందుకంటే మనకు *n* ఇన్‌పుట్లు ఉంటే, అవుట్‌పుట్ యొక్క స్టాండర్డ్ డివియేషన్ *n* అవుతుంది, మరియు విలువలు [0..1] పరిధి నుండి బయటకు వెళ్లే అవకాశం ఉంటుంది.

క్రింది ప్రారంభీకరణలు తరచుగా ఉపయోగిస్తారు:

 * యూనిఫార్మ్ పంపిణీ -- `uniform`
 * **N(0,1/n)** -- `gaussian`
 * **N(0,1/&radic;n_in)** ఇది ఇన్‌పుట్లు సున్నా సగటు మరియు 1 స్టాండర్డ్ డివియేషన్ ఉన్నప్పుడు అదే సగటు/స్టాండర్డ్ డివియేషన్ నిలుపుతుంది
 * **N(0,&radic;2/(n_in+n_out))** -- దీనిని **Xavier initialization** (`glorot`) అంటారు, ఇది ఫార్వర్డ్ మరియు బ్యాక్‌వర్డ్ ప్రొపగేషన్ సమయంలో సిగ్నల్స్‌ను పరిధిలో ఉంచడంలో సహాయపడుతుంది

## బ్యాచ్ నార్మలైజేషన్

సరైన బరువు ప్రారంభీకరణ ఉన్నప్పటికీ, శిక్షణ సమయంలో బరువులు అనియంత్రితంగా పెద్దవిగా లేదా చిన్నవిగా మారవచ్చు, అవి సిగ్నల్స్‌ను సరైన పరిధి నుండి బయటకు తీసుకెళ్తాయి. మనం సిగ్నల్స్‌ను తిరిగి తీసుకురావడానికి **నార్మలైజేషన్** సాంకేతికతలను ఉపయోగించవచ్చు. వాటిలో కొన్ని ఉన్నాయి (బరువు నార్మలైజేషన్, లేయర్ నార్మలైజేషన్), కానీ ఎక్కువగా ఉపయోగించేది బ్యాచ్ నార్మలైజేషన్.

**బ్యాచ్ నార్మలైజేషన్** భావన ఏమిటంటే, మినీబ్యాచ్‌లోని అన్ని విలువలను పరిగణలోకి తీసుకుని, ఆ విలువల ఆధారంగా నార్మలైజేషన్ (అంటే సగటు తీసి, స్టాండర్డ్ డివియేషన్‌తో భాగించడం) చేయడం. ఇది ఒక నెట్‌వర్క్ లేయర్‌గా అమలు చేయబడుతుంది, ఇది బరువులు వర్తింపజేసిన తర్వాత, యాక్టివేషన్ ఫంక్షన్ ముందు ఈ నార్మలైజేషన్ చేస్తుంది. ఫలితంగా, మనం ఎక్కువ తుది ఖచ్చితత్వం మరియు వేగవంతమైన శిక్షణను చూడగలుగుతాము.

ఇది బ్యాచ్ నార్మలైజేషన్ పై [మూల పత్రం](https://arxiv.org/pdf/1502.03167.pdf), [వికీపీడియా వివరణ](https://en.wikipedia.org/wiki/Batch_normalization), మరియు [మంచి పరిచయ బ్లాగ్ పోస్ట్](https://towardsdatascience.com/batch-normalization-in-3-levels-of-understanding-14c2da90a338) (మరియు [రష్యన్‌లో](https://habrahabr.ru/post/309302/)).

## డ్రాప్‌అవుట్

**డ్రాప్‌అవుట్** అనేది శిక్షణ సమయంలో యాదృచ్ఛికంగా కొన్ని శాతం న్యూరాన్లను తొలగించే ఆసక్తికర సాంకేతికత. ఇది ఒక లేయర్‌గా అమలు చేయబడుతుంది, దీనికి ఒక పారామీటర్ ఉంటుంది (తొలగించాల్సిన న్యూరాన్ల శాతం, సాధారణంగా 10%-50%), మరియు శిక్షణ సమయంలో ఇది ఇన్‌పుట్ వెక్టర్‌లో యాదృచ్ఛిక అంశాలను జీరో చేస్తుంది, తద్వారా తదుపరి లేయర్‌కు పంపుతుంది.

ఇది విచిత్రమైన ఆలోచనగా అనిపించవచ్చు, కానీ మీరు [`Dropout.ipynb`](Dropout.ipynb) నోట్బుక్‌లో MNIST అంకెల వర్గీకరణ శిక్షణపై డ్రాప్‌అవుట్ ప్రభావాన్ని చూడవచ్చు. ఇది శిక్షణ వేగాన్ని పెంచుతుంది మరియు తక్కువ శిక్షణ ఎపోక్స్‌లో ఎక్కువ ఖచ్చితత్వాన్ని సాధించడానికి సహాయపడుతుంది.

ఈ ప్రభావాన్ని కొన్ని విధాలుగా వివరించవచ్చు:

 * ఇది మోడల్‌కు యాదృచ్ఛిక షాక్‌లా భావించవచ్చు, ఇది ఆప్టిమైజేషన్‌ను లోకల్ మినిమమ్ నుండి బయటకు తీస్తుంది
 * ఇది *అప్రత్యక్ష మోడల్ సగటు*గా భావించవచ్చు, ఎందుకంటే డ్రాప్‌అవుట్ సమయంలో మనం కొంచెం భిన్నమైన మోడల్‌ను శిక్షణ ఇస్తున్నట్లు అనుకోవచ్చు

> *కొంతమంది చెబుతారు, మద్యం తాగిన వ్యక్తి ఏదైనా నేర్చుకోవడానికి ప్రయత్నిస్తే, అతను మత్తులో లేని వ్యక్తి కంటే మరుసటి ఉదయం ఆ విషయం మెరుగ్గా గుర్తుంచుకుంటాడని, ఎందుకంటే కొంతమంది న్యూరాన్లు సరిగ్గా పనిచేయకపోవడం వల్ల మెదడు అర్థం చేసుకోవడానికి మెరుగ్గా అనుకూలమవుతుందని. మనం దీన్ని స్వయంగా పరీక్షించలేదు ఇది నిజమో కాదో.*

## ఓవర్‌ఫిట్టింగ్ నివారణ

డీప్ లెర్నింగ్‌లో ఒక ముఖ్యమైన అంశం [ఓవర్‌ఫిట్టింగ్](../../3-NeuralNetworks/05-Frameworks/Overfitting.md) నివారించడం. చాలా శక్తివంతమైన న్యూరల్ నెట్‌వర్క్ మోడల్ ఉపయోగించాలనిపించినా, మోడల్ పారామీటర్ల సంఖ్యను శిక్షణ నమూనాల సంఖ్యతో సమతుల్యం చేయాలి.

> మనం ముందుగా పరిచయం చేసిన [ఓవర్‌ఫిట్టింగ్](../../3-NeuralNetworks/05-Frameworks/Overfitting.md) భావనను మీరు బాగా అర్థం చేసుకోవాలి!

ఓవర్‌ఫిట్టింగ్ నివారించడానికి కొన్ని మార్గాలు ఉన్నాయి:

 * ఎర్లీ స్టాప్పింగ్ -- వాలిడేషన్ సెట్ లో లోపాన్ని నిరంతరం పర్యవేక్షించి, వాలిడేషన్ లోపం పెరగడం ప్రారంభించినప్పుడు శిక్షణను ఆపడం.
 * స్పష్టమైన బరువు డికే / రెగ్యులరైజేషన్ -- బరువుల అధిక పరిమాణాలకు లాస్ ఫంక్షన్‌కు అదనపు శిక్షణా జరిమానా జోడించడం, ఇది మోడల్‌ను అస్థిర ఫలితాల నుండి రక్షిస్తుంది
 * మోడల్ సగటు -- అనేక మోడల్స్‌ను శిక్షణ ఇస్తూ, ఫలితాలను సగటు చేయడం. ఇది వైవిధ్యాన్ని తగ్గించడంలో సహాయపడుతుంది.
 * డ్రాప్‌అవుట్ (అప్రత్యక్ష మోడల్ సగటు)

## ఆప్టిమైజర్లు / శిక్షణ అల్గోరిథమ్స్

శిక్షణలో మరో ముఖ్య అంశం మంచి శిక్షణ అల్గోరిథమ్ ఎంచుకోవడం. సాంప్రదాయ **గ్రాడియెంట్ డిసెంట్** సరైన ఎంపిక అయినప్పటికీ, అది కొన్నిసార్లు చాలా నెమ్మదిగా ఉండవచ్చు లేదా ఇతర సమస్యలకు దారితీస్తుంది.

డీప్ లెర్నింగ్‌లో, మనం **స్టోకాస్టిక్ గ్రాడియెంట్ డిసెంట్** (SGD) ఉపయోగిస్తాము, ఇది శిక్షణ సెట్ నుండి యాదృచ్ఛికంగా ఎంచుకున్న మినీబ్యాచ్‌లపై వర్తించే గ్రాడియెంట్ డిసెంట్. బరువులు ఈ సూత్రం ద్వారా సవరించబడతాయి:

w<sup>t+1</sup> = w<sup>t</sup> - &eta;&nabla;&lagran;

### మోమెంటం

**మోమెంటం SGD**లో, మనం గత దశల నుండి గ్రాడియెంట్ యొక్క ఒక భాగాన్ని నిలుపుకుంటాము. ఇది మనం ఇనర్షియాతో ఎక్కడో కదులుతున్నప్పుడు, విభిన్న దిశలో పంచ్ వచ్చినా, మన మార్గం తక్షణమే మారదు, కానీ మొదటి కదలిక యొక్క కొంత భాగాన్ని కొనసాగిస్తుంది. ఇక్కడ మనం *వేగం* సూచించడానికి మరో వెక్టర్ v ను పరిచయం చేస్తాము:

* v<sup>t+1</sup> = &gamma; v<sup>t</sup> - &eta;&nabla;&lagran;
* w<sup>t+1</sup> = w<sup>t</sup> + v<sup>t+1</sup>

ఇక్కడ &gamma; పారామీటర్ ఇనర్షియాను ఎంత మేర పరిగణలోకి తీసుకుంటుందో సూచిస్తుంది: &gamma;=0 అంటే సాంప్రదాయ SGD; &gamma;=1 అంటే శుద్ధమైన మోషన్ సమీకరణం.

### ఆడమ్, అడాగ్రాడ్, మొదలైనవి

ప్రతి లేయర్‌లో మనం సిగ్నల్స్‌ను W<sub>i</sub> అనే మ్యాట్రిక్స్‌తో గుణిస్తాము, ||W<sub>i</sub>|| ఆధారంగా, గ్రాడియెంట్ 0కి దగ్గరగా తగ్గిపోవచ్చు లేదా ఎప్పటికీ పెరిగిపోవచ్చు. ఇదే Exploding/Vanishing Gradients సమస్య యొక్క మూలం.

ఈ సమస్యకు ఒక పరిష్కారం గ్రాడియెంట్ దిశను మాత్రమే ఉపయోగించడం, మరియు పరిమాణాన్ని పరిగణలోకి తీసుకోకపోవడం, అంటే:

w<sup>t+1</sup> = w<sup>t</sup> - &eta;(&nabla;&lagran;/||&nabla;&lagran;||), ఇక్కడ ||&nabla;&lagran;|| = &radic;&sum;(&nabla;&lagran;)<sup>2</sup>

ఈ అల్గోరిథమ్‌ను **Adagrad** అంటారు. అదే భావనను ఉపయోగించే ఇతర అల్గోరిథమ్స్: **RMSProp**, **Adam**

> **Adam** అనేది అనేక అప్లికేషన్లకు చాలా సమర్థవంతమైన అల్గోరిథమ్‌గా పరిగణించబడుతుంది, కాబట్టి మీరు ఏది ఉపయోగించాలో తెలియకపోతే - Adam ఉపయోగించండి.

### గ్రాడియెంట్ క్లిప్పింగ్

గ్రాడియెంట్ క్లిప్పింగ్ పై చెప్పిన ఆలోచనకు విస్తరణ. ||&nabla;&lagran;|| &le; &theta; అయితే, మేము అసలు గ్రాడియెంట్‌ను బరువు ఆప్టిమైజేషన్‌లో పరిగణిస్తాము, మరియు ||&nabla;&lagran;|| > &theta; అయితే, గ్రాడియెంట్‌ను దాని నార్మ్‌తో భాగిస్తాము. ఇక్కడ &theta; ఒక పారామీటర్, ఎక్కువ సందర్భాల్లో &theta;=1 లేదా &theta;=10 తీసుకోవచ్చు.

### లెర్నింగ్ రేట్ డికే

శిక్షణ విజయానికి &eta; అనే లెర్నింగ్ రేట్ పారామీటర్ చాలా ముఖ్యం. పెద్ద &eta; విలువలు వేగవంతమైన శిక్షణకు దారితీస్తాయని భావించడం తార్కికం, ఇది శిక్షణ ప్రారంభంలో మనం సాధారణంగా కోరుకునేది, తరువాత చిన్న &eta; విలువ నెట్‌వర్క్‌ను సున్నితంగా సర్దుబాటు చేయడానికి సహాయపడుతుంది. కాబట్టి, ఎక్కువ సందర్భాల్లో శిక్షణ సమయంలో &eta; ను తగ్గించడం మంచిది.

దీన్ని ప్రతి ఎపోక్ తర్వాత &eta; ను కొంత సంఖ్య (ఉదా. 0.98) తో గుణించడం ద్వారా చేయవచ్చు, లేదా మరింత క్లిష్టమైన **లెర్నింగ్ రేట్ షెడ్యూల్** ఉపయోగించవచ్చు.

## వివిధ నెట్‌వర్క్ ఆర్కిటెక్చర్లు

మీ సమస్యకు సరైన నెట్‌వర్క్ ఆర్కిటెక్చర్ ఎంచుకోవడం కష్టం కావచ్చు. సాధారణంగా, మనం మన ప్రత్యేక పనికి (లేదా దానికి సమానమైన పనికి) పనిచేసిన ఆర్కిటెక్చర్‌ను ఎంచుకుంటాము. కంప్యూటర్ విజన్ కోసం న్యూరల్ నెట్‌వర్క్ ఆర్కిటెక్చర్లపై [మంచి సమీక్ష](https://www.topbots.com/a-brief-history-of-neural-network-architectures/) ఇది.

> మన దగ్గర ఉన్న శిక్షణ నమూనాల సంఖ్యకు తగినంత శక్తివంతమైన ఆర్కిటెక్చర్ ఎంచుకోవడం ముఖ్యం. చాలా శక్తివంతమైన మోడల్ ఎంచుకోవడం [ఓవర్‌ఫిట్టింగ్](../../3-NeuralNetworks/05-Frameworks/Overfitting.md) కు దారితీస్తుంది.

మరొక మంచి మార్గం అవసరమైన సంక్లిష్టతకు ఆటోమేటిక్‌గా సర్దుబాటు అయ్యే ఆర్కిటెక్చర్ ఉపయోగించడం. కొంత మేరకు, **ResNet** ఆర్కిటెక్చర్ మరియు **Inception** స్వయంచాలకంగా సర్దుబాటు అవుతాయి. [కంప్యూటర్ విజన్ ఆర్కిటెక్చర్లపై మరింత](../07-ConvNets/CNN_Architectures.md) సమాచారం.

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**అస్పష్టత**:  
ఈ పత్రాన్ని AI అనువాద సేవ [Co-op Translator](https://github.com/Azure/co-op-translator) ఉపయోగించి అనువదించబడింది. మేము ఖచ్చితత్వానికి ప్రయత్నించినప్పటికీ, ఆటోమేటెడ్ అనువాదాల్లో పొరపాట్లు లేదా తప్పిదాలు ఉండవచ్చు. మూల పత్రం దాని స్వదేశీ భాషలో అధికారిక మూలంగా పరిగణించాలి. ముఖ్యమైన సమాచారానికి, ప్రొఫెషనల్ మానవ అనువాదం సిఫార్సు చేయబడుతుంది. ఈ అనువాదం వాడకంలో ఏర్పడిన ఏవైనా అపార్థాలు లేదా తప్పుదారితీసే అర్థాలు కోసం మేము బాధ్యత వహించము.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->