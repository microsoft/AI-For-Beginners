{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# అటెన్షన్ మెకానిజమ్స్ మరియు ట్రాన్స్‌ఫార్మర్స్\n",
    "\n",
    "ఒక ప్రధాన లోపం రికరెంట్ నెట్‌వర్క్స్‌లో ఏమిటంటే, ఒక సీక్వెన్స్‌లోని అన్ని పదాలు ఫలితంపై సమాన ప్రభావం చూపుతాయి. ఇది సీక్వెన్స్-టు-సీక్వెన్స్ పనుల కోసం సాధారణ LSTM ఎంకోడర్-డీకోడర్ మోడల్స్‌లో సబ్ఆప్టిమల్ పనితీరుకు దారితీస్తుంది, ఉదాహరణకు నేమ్డ్ ఎంటిటీ రికగ్నిషన్ మరియు మెషీన్ ట్రాన్స్‌లేషన్. వాస్తవానికి, ఇన్‌పుట్ సీక్వెన్స్‌లోని కొన్ని ప్రత్యేక పదాలు ఇతరాల కంటే సీక్వెన్షియల్ అవుట్‌పుట్స్‌పై ఎక్కువ ప్రభావం చూపుతాయి.\n",
    "\n",
    "మెషీన్ ట్రాన్స్‌లేషన్ వంటి సీక్వెన్స్-టు-సీక్వెన్స్ మోడల్‌ను పరిగణించండి. ఇది రెండు రికరెంట్ నెట్‌వర్క్స్ ద్వారా అమలు చేయబడుతుంది, ఒక నెట్‌వర్క్ (**ఎంకోడర్**) ఇన్‌పుట్ సీక్వెన్స్‌ను హిడెన్ స్టేట్‌గా సంకుచితం చేస్తుంది, మరొకటి, **డీకోడర్**, ఆ హిడెన్ స్టేట్‌ను అనువాద ఫలితంగా విస్తరించుతుంది. ఈ విధానంలో సమస్య ఏమిటంటే, నెట్‌వర్క్ యొక్క తుది స్థితి వాక్య ప్రారంభాన్ని గుర్తుంచుకోవడంలో కష్టపడుతుంది, అందువల్ల పొడవైన వాక్యాలలో మోడల్ నాణ్యత తక్కువగా ఉంటుంది.\n",
    "\n",
    "**అటెన్షన్ మెకానిజమ్స్** RNN యొక్క ప్రతి అవుట్‌పుట్ అంచనాపై ప్రతి ఇన్‌పుట్ వెక్టర్ యొక్క సందర్భాత్మక ప్రభావాన్ని బరువు వేయడానికి ఒక మార్గాన్ని అందిస్తాయి. ఇది అమలు చేయబడే విధానం ఏమిటంటే, ఇన్‌పుట్ RNN యొక్క మధ్యస్థితుల మరియు అవుట్‌పుట్ RNN మధ్య షార్ట్‌కట్స్ సృష్టించడం. ఈ విధంగా, అవుట్‌పుట్ సింబల్ $y_t$ ఉత్పత్తి చేస్తున్నప్పుడు, అన్ని ఇన్‌పుట్ హిడెన్ స్టేట్స్ $h_i$ ను వివిధ బరువు గుణకాలు $\\alpha_{t,i}$ తో పరిగణలోకి తీసుకుంటాము.\n",
    "\n",
    "![Image showing an encoder/decoder model with an additive attention layer](../../../../../translated_images/te/encoder-decoder-attention.7a726296894fb567.webp)\n",
    "*అడిటివ్ అటెన్షన్ మెకానిజం ఉన్న ఎంకోడర్-డీకోడర్ మోడల్ [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) నుండి, [ఈ బ్లాగ్ పోస్ట్](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html) నుండి సేకరించబడింది*\n",
    "\n",
    "అటెన్షన్ మ్యాట్రిక్స్ $\\{\\alpha_{i,j}\\}$ ఒక నిర్దిష్ట అవుట్‌పుట్ పదం ఉత్పత్తిలో కొన్ని ఇన్‌పుట్ పదాలు ఎంత భాగం పోషిస్తున్నాయో సూచిస్తుంది. క్రింద అలాంటి మ్యాట్రిక్స్ ఉదాహరణ ఉంది:\n",
    "\n",
    "![Image showing a sample alignment found by RNNsearch-50, taken from Bahdanau - arviz.org](../../../../../translated_images/te/bahdanau-fig3.09ba2d37f202a6af.webp)\n",
    "\n",
    "*[Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) నుండి తీసుకున్న చిత్రం (ఫిగర్ 3)*\n",
    "\n",
    "అటెన్షన్ మెకానిజమ్స్ ప్రస్తుత లేదా సమీప భవిష్యత్తులో నేచురల్ లాంగ్వేజ్ ప్రాసెసింగ్‌లో ఉన్న అత్యాధునిక స్థితికి కారణమవుతున్నాయి. అయితే, అటెన్షన్ జోడించడం వల్ల మోడల్ పరిమాణాలు గణనీయంగా పెరుగుతాయి, ఇది RNNలతో స్కేలింగ్ సమస్యలకు దారితీస్తుంది. RNNలను స్కేలు చేయడంలో ప్రధాన పరిమితి ఏమిటంటే, మోడల్స్ యొక్క రికరెంట్ స్వభావం కారణంగా ట్రైనింగ్‌ను బ్యాచ్ చేయడం మరియు ప్యారలలైజ్ చేయడం కష్టమవుతుంది. RNNలో సీక్వెన్స్‌లోని ప్రతి అంశాన్ని వరుసగా ప్రాసెస్ చేయాలి, అందువల్ల ఇది సులభంగా ప్యారలలైజ్ చేయలేము.\n",
    "\n",
    "ఈ పరిమితితో కలిపి అటెన్షన్ మెకానిజమ్స్ స్వీకరణ, మనం ఇప్పుడు తెలుసుకున్న మరియు ఉపయోగిస్తున్న అత్యాధునిక ట్రాన్స్‌ఫార్మర్ మోడల్స్ సృష్టికి దారితీసింది, BERT నుండి OpenGPT3 వరకు.\n",
    "\n",
    "## ట్రాన్స్‌ఫార్మర్ మోడల్స్\n",
    "\n",
    "ప్రతి గత అంచనాపై ఆధారపడి తదుపరి అంచనా దశకు సందర్భాన్ని పంపడం బదులు, **ట్రాన్స్‌ఫార్మర్ మోడల్స్** **పోసిషనల్ ఎంకోడింగ్స్** మరియు అటెన్షన్ ఉపయోగించి ఇచ్చిన విండోలోని ఇన్‌పుట్ సందర్భాన్ని పట్టుకుంటాయి. క్రింది చిత్రం పోసిషనల్ ఎంకోడింగ్స్ మరియు అటెన్షన్ ఎలా ఇచ్చిన విండోలో సందర్భాన్ని పట్టుకుంటాయో చూపిస్తుంది.\n",
    "\n",
    "![Animated GIF showing how the evaluations are performed in transformer models.](../../../../../lessons/5-NLP/18-Transformers/images/transformer-animated-explanation.gif)\n",
    "\n",
    "ప్రతి ఇన్‌పుట్ పొజిషన్ స్వతంత్రంగా ప్రతి అవుట్‌పుట్ పొజిషన్‌కు మ్యాప్ చేయబడినందున, ట్రాన్స్‌ఫార్మర్స్ RNNల కంటే మెరుగ్గా ప్యారలలైజ్ చేయగలవు, ఇది చాలా పెద్ద మరియు వ్యక్తీకరణాత్మక భాషా మోడల్స్‌ను సాధ్యమవుతుంది. ప్రతి అటెన్షన్ హెడ్ పదాల మధ్య వేరే సంబంధాలను నేర్చుకోవడానికి ఉపయోగపడుతుంది, ఇది తరువాతి నేచురల్ లాంగ్వేజ్ ప్రాసెసింగ్ పనులను మెరుగుపరుస్తుంది.\n",
    "\n",
    "**BERT** (Bidirectional Encoder Representations from Transformers) అనేది చాలా పెద్ద బహుళ-లేయర్ ట్రాన్స్‌ఫార్మర్ నెట్‌వర్క్, *BERT-base* కోసం 12 లేయర్లు, *BERT-large* కోసం 24 లేయర్లు కలిగి ఉంటుంది. ఈ మోడల్ మొదట పెద్ద టెక్స్ట్ డేటా కార్పస్ (వికీపీడియా + పుస్తకాలు) పై అనుసూచిత శిక్షణ (unsupervised training) ద్వారా ప్రీ-ట్రెయిన్ చేయబడుతుంది (వాక్యంలో మాస్క్ చేసిన పదాలను అంచనా వేయడం). ప్రీ-ట్రెయినింగ్ సమయంలో మోడల్ భాషా అవగాహనను గణనీయంగా గ్రహిస్తుంది, దీన్ని తరువాత ఇతర డేటాసెట్లతో ఫైన్-ట్యూనింగ్ ద్వారా ఉపయోగించవచ్చు. ఈ ప్రక్రియను **ట్రాన్స్‌ఫర్ లెర్నింగ్** అంటారు.\n",
    "\n",
    "![picture from http://jalammar.github.io/illustrated-bert/](../../../../../translated_images/te/jalammarBERT-language-modeling-masked-lm.34f113ea5fec4362.webp)\n",
    "\n",
    "ట్రాన్స్‌ఫార్మర్ ఆర్కిటెక్చర్స్‌లో అనేక వేరియేషన్లు ఉన్నాయి, వాటిలో BERT, DistilBERT, BigBird, OpenGPT3 మరియు మరిన్ని ఉన్నాయి, వీటిని ఫైన్-ట్యూన్ చేయవచ్చు. [HuggingFace ప్యాకేజ్](https://github.com/huggingface/) PyTorchతో ఈ ఆర్కిటెక్చర్స్‌లో చాలా మోడల్స్ శిక్షణ కోసం రిపాజిటరీని అందిస్తుంది.\n",
    "\n",
    "## టెక్స్ట్ క్లాసిఫికేషన్ కోసం BERT ఉపయోగించడం\n",
    "\n",
    "మనం ప్రీ-ట్రెయిన్ చేసిన BERT మోడల్‌ను సాంప్రదాయమైన పని: సీక్వెన్స్ క్లాసిఫికేషన్ కోసం ఎలా ఉపయోగించాలో చూద్దాం. మనం మా అసలు AG News డేటాసెట్‌ను క్లాసిఫై చేస్తాము.\n",
    "\n",
    "ముందుగా, HuggingFace లైబ్రరీ మరియు మన డేటాసెట్‌ను లోడ్ చేద్దాం:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Building vocab...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "from torchnlp import *\n",
    "import transformers\n",
    "train_dataset, test_dataset, classes, vocab = load_dataset()\n",
    "vocab_len = len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "మనం ప్రీ-ట్రెయిన్డ్ BERT మోడల్ ఉపయోగించబోతున్నందున, ప్రత్యేక టోకనైజర్ అవసరం అవుతుంది. మొదట, ప్రీ-ట్రెయిన్డ్ BERT మోడల్‌కు సంబంధించిన టోకనైజర్‌ను లోడ్ చేస్తాము.\n",
    "\n",
    "HuggingFace లైబ్రరీలో ప్రీ-ట్రెయిన్డ్ మోడల్స్ యొక్క రిపాజిటరీ ఉంటుంది, వాటి పేర్లను `from_pretrained` ఫంక్షన్లకు ఆర్గ్యుమెంట్లుగా ఇచ్చి మీరు సులభంగా ఉపయోగించుకోవచ్చు. మోడల్‌కు అవసరమైన అన్ని బైనరీ ఫైళ్లు ఆటోమేటిక్‌గా డౌన్లోడ్ అవుతాయి.\n",
    "\n",
    "కానీ, కొన్ని సందర్భాల్లో మీరు మీ స్వంత మోడల్స్‌ను లోడ్ చేయవలసి వస్తుంది, అప్పుడు టోకనైజర్ కోసం పరామితులు, మోడల్ పరామితులతో కూడిన `config.json` ఫైల్, బైనరీ వెయిట్స్ మొదలైన అన్ని సంబంధిత ఫైళ్లు ఉన్న డైరెక్టరీని మీరు స్పెసిఫై చేయవచ్చు.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load the model from Internet repository using model name. \n",
    "# Use this if you are running from your own copy of the notebooks\n",
    "bert_model = 'bert-base-uncased' \n",
    "\n",
    "# To load the model from the directory on disk. Use this for Microsoft Learn module, because we have\n",
    "# prepared all required files for you.\n",
    "bert_model = './bert'\n",
    "\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(bert_model)\n",
    "\n",
    "MAX_SEQ_LEN = 128\n",
    "PAD_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "UNK_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tokenizer` ఆబ్జెక్ట్‌లో `encode` ఫంక్షన్ ఉంటుంది, దీన్ని టెక్స్ట్‌ను నేరుగా ఎన్‌కోడ్ చేయడానికి ఉపయోగించవచ్చు:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 1052, 22123, 2953, 2818, 2003, 1037, 2307, 7705, 2005, 17953, 2361, 102]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('PyTorch is a great framework for NLP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "అప్పుడు, మనం ట్రైనింగ్ సమయంలో డేటాను యాక్సెస్ చేయడానికి ఉపయోగించే ఇటరేటర్లను సృష్టిద్దాం. BERT తన స్వంత ఎంకోడింగ్ ఫంక్షన్ ఉపయోగిస్తుండగా, మనం ముందుగా నిర్వచించిన `padify` లాంటి ప్యాడింగ్ ఫంక్షన్‌ను నిర్వచించాల్సి ఉంటుంది:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_bert(b):\n",
    "    # b is the list of tuples of length batch_size\n",
    "    #   - first element of a tuple = label, \n",
    "    #   - second = feature (text sequence)\n",
    "    # build vectorized sequence\n",
    "    v = [tokenizer.encode(x[1]) for x in b]\n",
    "    # compute max length of a sequence in this minibatch\n",
    "    l = max(map(len,v))\n",
    "    return ( # tuple of two tensors - labels and features\n",
    "        torch.LongTensor([t[0] for t in b]),\n",
    "        torch.stack([torch.nn.functional.pad(torch.tensor(t),(0,l-len(t)),mode='constant',value=0) for t in v])\n",
    "    )\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=8, collate_fn=pad_bert, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=8, collate_fn=pad_bert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "మన కేసులో, మేము `bert-base-uncased` అనే ప్రీ-ట్రెయిన్డ్ BERT మోడల్‌ను ఉపయోగించబోతున్నాము. మోడల్‌ను `BertForSequenceClassification` ప్యాకేజీ ఉపయోగించి లోడ్ చేద్దాం. ఇది మా మోడల్ ఇప్పటికే క్లాసిఫికేషన్ కోసం అవసరమైన ఆర్కిటెక్చర్ కలిగి ఉందని నిర్ధారిస్తుంది, ఫైనల్ క్లాసిఫైయర్ సహా. మీరు ఒక హెచ్చరిక సందేశం చూడవచ్చు, ఇది ఫైనల్ క్లాసిఫైయర్ యొక్క వెయిట్స్ ప్రారంభించబడలేదని మరియు మోడల్ ప్రీ-ట్రెయినింగ్ అవసరం అని సూచిస్తుంది - ఇది పూర్తిగా సరే, ఎందుకంటే అదే మేము చేయబోతున్నాం!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./bert were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./bert and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = transformers.BertForSequenceClassification.from_pretrained(bert_model,num_labels=4).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ఇప్పుడు మేము శిక్షణ ప్రారంభించడానికి సిద్ధంగా ఉన్నాము! BERT ఇప్పటికే ప్రీ-ట్రెయిన్ చేయబడినందున, ప్రారంభ బరువులను నాశనం చేయకుండా ఉండేందుకు తక్కువ లెర్నింగ్ రేట్‌తో ప్రారంభించాలనుకుంటున్నాము.\n",
    "\n",
    "అన్ని కష్టపడి చేసిన పని `BertForSequenceClassification` మోడల్ చేస్తుంది. మేము శిక్షణ డేటాపై మోడల్‌ను పిలిచినప్పుడు, అది ఇన్‌పుట్ మినీబ్యాచ్ కోసం లాస్ మరియు నెట్‌వర్క్ అవుట్‌పుట్ రెండింటినీ ఇస్తుంది. మేము పారామీటర్ ఆప్టిమైజేషన్ కోసం లాస్‌ను ఉపయోగిస్తాము (`loss.backward()` బ్యాక్‌వర్డ్ పాస్ చేస్తుంది), మరియు శిక్షణ ఖచ్చితత్వం లెక్కించడానికి `out` ను ఉపయోగిస్తాము, అందులో పొందిన లేబుల్స్ `labs` (`argmax` ఉపయోగించి లెక్కించబడినవి) ను ఆశించిన `labels` తో పోల్చుతాము.\n",
    "\n",
    "ప్రక్రియను నియంత్రించడానికి, మేము లాస్ మరియు ఖచ్చితత్వాన్ని కొన్ని ఇటరేషన్లలో సేకరిస్తూ, ప్రతి `report_freq` శిక్షణ చక్రాల్లో వాటిని ప్రింట్ చేస్తాము.\n",
    "\n",
    "ఈ శిక్షణ చాలా సమయం తీసుకోవచ్చు, కాబట్టి మేము ఇటరేషన్ల సంఖ్యను పరిమితం చేస్తాము.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 1.1254194641113282, Accuracy = 0.585\n",
      "Loss = 0.6194715118408203, Accuracy = 0.83\n",
      "Loss = 0.46665248870849607, Accuracy = 0.8475\n",
      "Loss = 0.4309701919555664, Accuracy = 0.8575\n",
      "Loss = 0.35427074432373046, Accuracy = 0.8825\n",
      "Loss = 0.3306886291503906, Accuracy = 0.8975\n",
      "Loss = 0.30340143203735354, Accuracy = 0.8975\n",
      "Loss = 0.26139299392700194, Accuracy = 0.915\n",
      "Loss = 0.26708646774291994, Accuracy = 0.9225\n",
      "Loss = 0.3667240524291992, Accuracy = 0.8675\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
    "\n",
    "report_freq = 50\n",
    "iterations = 500 # make this larger to train for longer time!\n",
    "\n",
    "model.train()\n",
    "\n",
    "i,c = 0,0\n",
    "acc_loss = 0\n",
    "acc_acc = 0\n",
    "\n",
    "for labels,texts in train_loader:\n",
    "    labels = labels.to(device)-1 # get labels in the range 0-3         \n",
    "    texts = texts.to(device)\n",
    "    loss, out = model(texts, labels=labels)[:2]\n",
    "    labs = out.argmax(dim=1)\n",
    "    acc = torch.mean((labs==labels).type(torch.float32))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    acc_loss += loss\n",
    "    acc_acc += acc\n",
    "    i+=1\n",
    "    c+=1\n",
    "    if i%report_freq==0:\n",
    "        print(f\"Loss = {acc_loss.item()/c}, Accuracy = {acc_acc.item()/c}\")\n",
    "        c = 0\n",
    "        acc_loss = 0\n",
    "        acc_acc = 0\n",
    "    iterations-=1\n",
    "    if not iterations:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "మీరు చూడవచ్చు (ప్రత్యేకంగా మీరు పునరావృతాల సంఖ్యను పెంచి సరిపడా సమయం వేచి ఉంటే) BERT వర్గీకరణ మనకు మంచి ఖచ్చితత్వం ఇస్తుంది! ఇది ఎందుకంటే BERT ఇప్పటికే భాష యొక్క నిర్మాణాన్ని బాగా అర్థం చేసుకుంటుంది, మరియు మనం కేవలం తుది వర్గీకర్తను ఫైన్-ట్యూన్ చేయాలి. అయితే, BERT ఒక పెద్ద మోడల్ కావడంతో, మొత్తం శిక్షణ ప్రక్రియ చాలా సమయం తీసుకుంటుంది, మరియు గంభీరమైన కంప్యూటింగ్ శక్తిని అవసరం చేస్తుంది! (GPU, మరియు సాధ్యమైతే ఒకటి కంటే ఎక్కువ).\n",
    "\n",
    "> **Note:** మన ఉదాహరణలో, మనం చిన్న ప్రీ-ట్రెయిన్డ్ BERT మోడల్స్ లో ఒకటిని ఉపయోగిస్తున్నాము. మెరుగైన ఫలితాలు ఇచ్చే పెద్ద మోడల్స్ కూడా ఉన్నాయి.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## మోడల్ పనితీరు మూల్యాంకనం\n",
    "\n",
    "ఇప్పుడు మనం టెస్ట్ డేటాసెట్‌పై మన మోడల్ పనితీరును మూల్యాంకనం చేయవచ్చు. మూల్యాంకన లూప్ శిక్షణ లూప్‌కు చాలా సమానంగా ఉంటుంది, కానీ మోడల్‌ను మూల్యాంకన మోడ్‌లోకి మార్చడం కోసం `model.eval()` ను పిలవడం మర్చిపోకూడదు.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy: 0.9047029702970297\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "iterations = 100\n",
    "acc = 0\n",
    "i = 0\n",
    "for labels,texts in test_loader:\n",
    "    labels = labels.to(device)-1      \n",
    "    texts = texts.to(device)\n",
    "    _, out = model(texts, labels=labels)[:2]\n",
    "    labs = out.argmax(dim=1)\n",
    "    acc += torch.mean((labs==labels).type(torch.float32))\n",
    "    i+=1\n",
    "    if i>iterations: break\n",
    "        \n",
    "print(f\"Final accuracy: {acc.item()/i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ముఖ్యాంశాలు\n",
    "\n",
    "ఈ యూనిట్‌లో, **transformers** లైబ్రరీ నుండి ముందుగా శిక్షణ పొందిన భాషా మోడల్‌ను ఎలా సులభంగా తీసుకుని మన టెక్స్ట్ వర్గీకరణ పనికి అనుకూలపరచుకోవచ్చో చూశాం. అదే విధంగా, BERT మోడల్స్‌ను ఎంటిటీ ఎక్స్‌ట్రాక్షన్, ప్రశ్నలకు సమాధానం ఇవ్వడం, మరియు ఇతర NLP పనుల కోసం ఉపయోగించవచ్చు.\n",
    "\n",
    "ట్రాన్స్‌ఫార్మర్ మోడల్స్ NLPలో ప్రస్తుత అత్యాధునిక స్థితిని ప్రతిబింబిస్తాయి, మరియు చాలా సందర్భాల్లో మీరు కస్టమ్ NLP పరిష్కారాలను అమలు చేయడం ప్రారంభించినప్పుడు మొదటి పరిష్కారంగా ఇవే ప్రయత్నించాలి. అయితే, ఈ మాడ్యూల్‌లో చర్చించిన రికరెంట్ న్యూరల్ నెట్‌వర్క్స్ యొక్క ప్రాథమిక సూత్రాలను అర్థం చేసుకోవడం అత్యంత ముఖ్యమైనది, మీరు అధునాతన న్యూరల్ మోడల్స్ నిర్మించాలనుకుంటే.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n<!-- CO-OP TRANSLATOR DISCLAIMER START -->\n**అస్పష్టత**:  \nఈ పత్రాన్ని AI అనువాద సేవ [Co-op Translator](https://github.com/Azure/co-op-translator) ఉపయోగించి అనువదించబడింది. మేము ఖచ్చితత్వానికి ప్రయత్నించినప్పటికీ, ఆటోమేటెడ్ అనువాదాల్లో పొరపాట్లు లేదా తప్పిదాలు ఉండవచ్చు. మూల పత్రం దాని స్వదేశీ భాషలో అధికారిక మూలంగా పరిగణించాలి. ముఖ్యమైన సమాచారానికి, ప్రొఫెషనల్ మానవ అనువాదం సిఫార్సు చేయబడుతుంది. ఈ అనువాదం వాడకంలో ఏర్పడిన ఏవైనా అపార్థాలు లేదా తప్పుదారుల కోసం మేము బాధ్యత వహించము.\n<!-- CO-OP TRANSLATOR DISCLAIMER END -->\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37_pytorch",
   "language": "python",
   "name": "conda-env-py37_pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "coopTranslator": {
   "original_hash": "753865967678a92dbce7d7efbd36d980",
   "translation_date": "2025-11-26T02:19:27+00:00",
   "source_file": "lessons/5-NLP/18-Transformers/TransformersPyTorch.ipynb",
   "language_code": "te"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}