{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# అటెన్షన్ మెకానిజమ్స్ మరియు ట్రాన్స్‌ఫార్మర్స్\n",
    "\n",
    "రెకరెంట్ నెట్‌వర్క్స్ యొక్క ఒక ప్రధాన లోపం ఏమిటంటే, ఒక సీక్వెన్స్‌లోని అన్ని పదాలు ఫలితంపై సమాన ప్రభావం చూపుతాయి. ఇది నేమ్డ్ ఎంటిటీ రికగ్నిషన్ మరియు మెషీన్ ట్రాన్స్‌లేషన్ వంటి సీక్వెన్స్ టు సీక్వెన్స్ టాస్క్‌ల కోసం స్టాండర్డ్ LSTM ఎంకోడర్-డీకోడర్ మోడల్స్‌లో సబ్ఆప్టిమల్ పనితీరుకు దారితీస్తుంది. వాస్తవానికి, ఇన్‌పుట్ సీక్వెన్స్‌లోని కొన్ని ప్రత్యేక పదాలు ఇతరాల కంటే సీక్వెన్షియల్ అవుట్‌పుట్స్‌పై ఎక్కువ ప్రభావం చూపుతాయి.\n",
    "\n",
    "మెషీన్ ట్రాన్స్‌లేషన్ వంటి సీక్వెన్స్-టు-సీక్వెన్స్ మోడల్‌ను పరిగణించండి. ఇది రెండు రెకరెంట్ నెట్‌వర్క్స్ ద్వారా అమలు చేయబడుతుంది, ఒక నెట్‌వర్క్ (**ఎంకోడర్**) ఇన్‌పుట్ సీక్వెన్స్‌ను హిడెన్ స్టేట్‌గా కుదింప చేస్తుంది, మరొకటి, **డీకోడర్**, ఈ హిడెన్ స్టేట్‌ను అనువాద ఫలితంగా విస్తరిస్తుంది. ఈ విధానంలో సమస్య ఏమిటంటే, నెట్‌వర్క్ యొక్క తుది స్థితి వాక్య ప్రారంభాన్ని గుర్తుంచుకోవడంలో కష్టపడుతుంది, అందువల్ల పొడవైన వాక్యాలపై మోడల్ నాణ్యత తక్కువగా ఉంటుంది.\n",
    "\n",
    "**అటెన్షన్ మెకానిజమ్స్** RNN యొక్క ప్రతి అవుట్‌పుట్ అంచనాపై ప్రతి ఇన్‌పుట్ వెక్టర్ యొక్క సందర్భాత్మక ప్రభావాన్ని బరువు వేయడానికి ఒక మార్గాన్ని అందిస్తాయి. ఇది అమలు చేయబడే విధానం ఏమిటంటే, ఇన్‌పుట్ RNN యొక్క మధ్యస్థితుల మరియు అవుట్‌పుట్ RNN మధ్య షార్ట్‌కట్స్ సృష్టించడం. ఈ విధంగా, అవుట్‌పుట్ సింబల్ $y_t$ ను ఉత్పత్తి చేస్తున్నప్పుడు, మేము అన్ని ఇన్‌పుట్ హిడెన్ స్టేట్స్ $h_i$ ను వివిధ బరువు గుణకాలు $\\alpha_{t,i}$ తో పరిగణలోకి తీసుకుంటాము.\n",
    "\n",
    "![Image showing an encoder/decoder model with an additive attention layer](../../../../../translated_images/te/encoder-decoder-attention.7a726296894fb567.webp)\n",
    "*అడిటివ్ అటెన్షన్ మెకానిజం ఉన్న ఎంకోడర్-డీకోడర్ మోడల్ [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) నుండి, [ఈ బ్లాగ్ పోస్ట్](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html) నుండి సైట్ చేయబడింది*\n",
    "\n",
    "అటెన్షన్ మ్యాట్రిక్స్ $\\{\\alpha_{i,j}\\}$ ఒక నిర్దిష్ట అవుట్‌పుట్ సీక్వెన్స్ పదం ఉత్పత్తిలో కొన్ని ఇన్‌పుట్ పదాలు ఎంత భాగం పోషిస్తున్నాయో సూచిస్తుంది. క్రింద అలాంటి మ్యాట్రిక్స్ ఉదాహరణ ఉంది:\n",
    "\n",
    "![Image showing a sample alignment found by RNNsearch-50, taken from Bahdanau - arviz.org](../../../../../translated_images/te/bahdanau-fig3.09ba2d37f202a6af.webp)\n",
    "\n",
    "*[Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) నుండి తీసుకున్న చిత్రం (ఫిగర్ 3)*\n",
    "\n",
    "అటెన్షన్ మెకానిజమ్స్ ప్రస్తుత లేదా సమీప భవిష్యత్తులో నేచురల్ లాంగ్వేజ్ ప్రాసెసింగ్‌లో ఉన్న అత్యుత్తమ స్థితికి కారణమవుతున్నాయి. అయితే, అటెన్షన్ జోడించడం వల్ల మోడల్ పరిమాణాలు గణనీయంగా పెరుగుతాయి, ఇది RNNలతో స్కేలింగ్ సమస్యలకు దారితీసింది. RNNలను స్కేలు చేయడంలో ఒక ముఖ్యమైన పరిమితి ఏమిటంటే, మోడల్స్ యొక్క రికరెంట్ స్వభావం కారణంగా ట్రైనింగ్‌ను బ్యాచ్ చేయడం మరియు ప్యారలలైజ్ చేయడం కష్టమవుతుంది. RNNలో సీక్వెన్స్ యొక్క ప్రతి అంశం వరుసగా ప్రాసెస్ చేయబడాలి, అందువల్ల ఇది సులభంగా ప్యారలలైజ్ చేయలేము.\n",
    "\n",
    "ఈ పరిమితితో కలిపి అటెన్షన్ మెకానిజమ్స్ ఆమోదం, మనం ఇప్పుడు తెలుసుకునే మరియు ఉపయోగించే బర్ట్ నుండి OpenGPT3 వరకు ఉన్న స్టేట్ ఆఫ్ ది ఆర్ట్ ట్రాన్స్‌ఫార్మర్ మోడల్స్ సృష్టికి దారితీసింది.\n",
    "\n",
    "## ట్రాన్స్‌ఫార్మర్ మోడల్స్\n",
    "\n",
    "ప్రతి గత అంచనాపై ఆధారపడి కంటెక్స్ట్‌ను తదుపరి అంచనా దశకు పంపించడమునకు బదులుగా, **ట్రాన్స్‌ఫార్మర్ మోడల్స్** **పోసిషనల్ ఎంకోడింగ్స్** మరియు **అటెన్షన్** ఉపయోగించి ఇచ్చిన విండోలోని ఇన్‌పుట్ కంటెక్స్ట్‌ను పట్టుకుంటాయి. క్రింది చిత్రం పోసిషనల్ ఎంకోడింగ్స్ మరియు అటెన్షన్ ఎలా ఇచ్చిన విండోలో కంటెక్స్ట్‌ను పట్టుకుంటాయో చూపిస్తుంది.\n",
    "\n",
    "![Animated GIF showing how the evaluations are performed in transformer models.](../../../../../lessons/5-NLP/18-Transformers/images/transformer-animated-explanation.gif)\n",
    "\n",
    "ప్రతి ఇన్‌పుట్ పొజిషన్‌ను స్వతంత్రంగా ప్రతి అవుట్‌పుట్ పొజిషన్‌కు మ్యాప్ చేయడం వల్ల, ట్రాన్స్‌ఫార్మర్స్ RNNల కంటే మెరుగ్గా ప్యారలలైజ్ చేయగలవు, ఇది చాలా పెద్ద మరియు వ్యక్తీకరణాత్మక భాషా మోడల్స్‌ను సాధ్యమవుతుంది. ప్రతి అటెన్షన్ హెడ్ వేర్వేరు పదాల మధ్య సంబంధాలను నేర్చుకోవడానికి ఉపయోగించబడుతుంది, ఇది దిగువన ఉన్న నేచురల్ లాంగ్వేజ్ ప్రాసెసింగ్ టాస్క్‌లను మెరుగుపరుస్తుంది.\n",
    "\n",
    "## సింపుల్ ట్రాన్స్‌ఫార్మర్ మోడల్ నిర్మాణం\n",
    "\n",
    "కేరాస్‌లో బిల్ట్-ఇన్ ట్రాన్స్‌ఫార్మర్ లేయర్ లేదు, కానీ మనం మన సొంతం నిర్మించవచ్చు. మునుపటి విధంగా, మనం AG News డేటాసెట్ యొక్క టెక్స్ట్ క్లాసిఫికేషన్‌పై దృష్టి సారిస్తాము, కానీ ట్రాన్స్‌ఫార్మర్ మోడల్స్ కఠినమైన NLP టాస్క్‌లలో ఉత్తమ ఫలితాలు చూపిస్తాయని చెప్పడం అవసరం.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "\n",
    "ds_train, ds_test = tfds.load('ag_news_subset').values()\n",
    "\n",
    "def extract_text(x):\n",
    "    return x['title']+' '+x['description']\n",
    "\n",
    "def tupelize(x):\n",
    "    return (extract_text(x),x['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "కేరాస్‌లో కొత్త లేయర్లు `Layer` క్లాస్‌ను సబ్‌క్లాస్ చేయాలి, మరియు `call` మెథడ్‌ను అమలు చేయాలి. మనం **పోసిషనల్ ఎంబెడ్డింగ్** లేయర్‌తో ప్రారంభిద్దాం. మనం [అధికారిక కేరాస్ డాక్యుమెంటేషన్ నుండి కొంత కోడ్](https://keras.io/examples/nlp/text_classification_with_transformer/) ఉపయోగించబోతున్నాము. మనం అన్ని ఇన్‌పుట్ సీక్వెన్సులను `maxlen` పొడవు వరకు ప్యాడ్ చేస్తామని అనుకుంటున్నాము.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(keras.layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = keras.layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "        self.maxlen = maxlen\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = self.maxlen\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x+positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ఈ లేయర్ రెండు `Embedding` లేయర్లతో కూడి ఉంటుంది: టోకెన్లను ఎంబెడ్ చేయడానికి (ముందుగా చర్చించిన విధంగా) మరియు టోకెన్ స్థానాల కోసం. టోకెన్ స్థానాలు 0 నుండి `maxlen` వరకు సహజ సంఖ్యల శ్రేణిగా `tf.range` ఉపయోగించి సృష్టించబడతాయి, ఆపై ఎంబెడ్డింగ్ లేయర్ ద్వారా పంపబడతాయి. రెండు ఫలిత ఎంబెడ్డింగ్ వెక్టార్లు కలిపి, `maxlen`$\\times$`embed_dim` ఆకారంలో స్థానికంగా ఎంబెడ్ చేయబడిన ఇన్‌పుట్ ప్రాతినిధ్యాన్ని ఉత్పత్తి చేస్తాయి.\n",
    "\n",
    "<img src=\"../../../../../translated_images/te/pos-embedding.e41ce9b6cf6078af.webp\" width=\"40%\"/>\n",
    "\n",
    "ఇప్పుడు, ట్రాన్స్‌ఫార్మర్ బ్లాక్‌ను అమలు చేద్దాం. ఇది ముందుగా నిర్వచించిన ఎంబెడ్డింగ్ లేయర్ అవుట్‌పుట్‌ను తీసుకుంటుంది:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim, name='attn')\n",
    "        self.ffn = keras.Sequential(\n",
    "            [keras.layers.Dense(ff_dim, activation=\"relu\"), keras.layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = keras.layers.Dropout(rate)\n",
    "        self.dropout2 = keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer స్థానికంగా సంకేతీకరించిన ఇన్‌పుట్‌పై `MultiHeadAttention` ను వర్తింపజేస్తుంది, దీని ద్వారా `maxlen`$\\times$`embed_dim` పరిమాణం గల అటెన్షన్ వెక్టర్ ఉత్పత్తి అవుతుంది, ఇది తరువాత ఇన్‌పుట్‌తో కలిపి `LayerNormalization` ఉపయోగించి సాధారణీకరించబడుతుంది.\n",
    "\n",
    "> **Note**: `LayerNormalization` అనేది ఈ అభ్యాస మార్గంలో *కంప్యూటర్ విజన్* భాగంలో చర్చించిన `BatchNormalization` కు సమానమైనది, కానీ ఇది ప్రతి శిక్షణ నమూనా కోసం గత లేయర్ అవుట్పుట్‌లను స్వతంత్రంగా సాధారణీకరించి, వాటిని [-1..1] పరిధిలోకి తీసుకువస్తుంది.\n",
    "\n",
    "ఈ లేయర్ అవుట్పుట్ తరువాత `Dense` నెట్‌వర్క్ (మన సందర్భంలో - రెండు లేయర్ పెర్సెప్ట్రాన్) ద్వారా పంపబడుతుంది, మరియు ఫలితం తుది అవుట్పుట్‌కు జోడించబడుతుంది (దీన్ని మళ్లీ సాధారణీకరణకు లోబెడతారు).\n",
    "\n",
    "<img src=\"../../../../../translated_images/te/transformer-layer.905e14747ca4e7d5.webp\" width=\"30%\" />\n",
    "\n",
    "ఇప్పుడు, పూర్తి ట్రాన్స్‌ఫార్మర్ మోడల్‌ను నిర్వచించడానికి మేము సిద్ధంగా ఉన్నాము:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "text_vectorization (TextVect (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "token_and_position_embedding (None, 256, 32)           648192    \n",
      "_________________________________________________________________\n",
      "transformer_block (Transform (None, 256, 32)           10656     \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 20)                660       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 84        \n",
      "=================================================================\n",
      "Total params: 659,592\n",
      "Trainable params: 659,592\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 32  # Embedding size for each token\n",
    "num_heads = 2  # Number of attention heads\n",
    "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
    "maxlen = 256\n",
    "vocab_size = 20000\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.experimental.preprocessing.TextVectorization(max_tokens=vocab_size,output_sequence_length=maxlen, input_shape=(1,)),\n",
    "    TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim),\n",
    "    TransformerBlock(embed_dim, num_heads, ff_dim),\n",
    "    keras.layers.GlobalAveragePooling1D(),\n",
    "    keras.layers.Dropout(0.1),\n",
    "    keras.layers.Dense(20, activation=\"relu\"),\n",
    "    keras.layers.Dropout(0.1),\n",
    "    keras.layers.Dense(4, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokenizer\n",
      "938/938 [==============================] - 45s 39ms/step - loss: 0.4978 - acc: 0.8068 - val_loss: 0.2808 - val_acc: 0.9124\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9c2427a0d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Training tokenizer')\n",
    "model.layers[0].adapt(ds_train.map(extract_text))\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'], optimizer='adam')\n",
    "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT ట్రాన్స్‌ఫార్మర్ మోడల్స్\n",
    "\n",
    "**BERT** (Bidirectional Encoder Representations from Transformers) అనేది చాలా పెద్ద బహుళ పొరల ట్రాన్స్‌ఫార్మర్ నెట్‌వర్క్, *BERT-base* కోసం 12 పొరలు, *BERT-large* కోసం 24 పొరలు కలిగి ఉంటుంది. ఈ మోడల్ మొదట పెద్ద టెక్స్ట్ డేటా సేకరణ (వికీపీడియా + పుస్తకాలు) పై అనుసూచిత శిక్షణ (వాక్యంలో మాస్క్ చేసిన పదాలను అంచనా వేయడం) ద్వారా ప్రీ-ట్రెయిన్ చేయబడుతుంది. ప్రీ-ట్రెయినింగ్ సమయంలో మోడల్ భాషా అర్థం చేసుకోవడంలో గణనీయమైన స్థాయిని పొందుతుంది, దీన్ని తర్వాత ఇతర డేటాసెట్‌లతో ఫైన్ ట్యూనింగ్ ద్వారా ఉపయోగించవచ్చు. ఈ ప్రక్రియను **ట్రాన్స్‌ఫర్ లెర్నింగ్** అంటారు.\n",
    "\n",
    "![picture from http://jalammar.github.io/illustrated-bert/](../../../../../translated_images/te/jalammarBERT-language-modeling-masked-lm.34f113ea5fec4362.webp)\n",
    "\n",
    "BERT, DistilBERT, BigBird, OpenGPT3 మరియు మరిన్ని వంటి అనేక ట్రాన్స్‌ఫార్మర్ వేరియేషన్లు ఉన్నాయి, వీటిని ఫైన్ ట్యూన్ చేయవచ్చు.\n",
    "\n",
    "మనం ప్రీ-ట్రెయిన్ చేసిన BERT మోడల్‌ను సంప్రదాయ సీక్వెన్స్ క్లాసిఫికేషన్ సమస్యను పరిష్కరించడానికి ఎలా ఉపయోగించాలో చూద్దాం. ఈ ఆలోచన మరియు కొంత కోడ్ [అధికారిక డాక్యుమెంటేషన్](https://www.tensorflow.org/text/tutorials/classify_text_with_bert) నుండి తీసుకుంటాము.\n",
    "\n",
    "ప్రీ-ట్రెయిన్ చేసిన మోడల్స్‌ను లోడ్ చేయడానికి, మనం **Tensorflow hub** ఉపయోగిస్తాము. ముందుగా, BERT-స్పెసిఫిక్ వెక్టరైజర్‌ను లోడ్ చేద్దాం:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_41180/4216669875.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow_text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow_hub\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mhub\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhub\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mKerasLayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_text'"
     ]
    }
   ],
   "source": [
    "import tensorflow_text \n",
    "import tensorflow_hub as hub\n",
    "vectorizer = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_type_ids': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       " array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "       dtype=int32)>,\n",
       " 'input_word_ids': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       " array([[  101,  1045,  2293, 19081,   102,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]], dtype=int32)>,\n",
       " 'input_mask': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       " array([[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "       dtype=int32)>}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer(['I love transformers'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "మూల నెట్‌వర్క్ శిక్షణ పొందిన అదే వెక్టరైజర్‌ను ఉపయోగించడం చాలా ముఖ్యం. అలాగే, BERT వెక్టరైజర్ మూడు భాగాలను ఇస్తుంది:\n",
    "* `input_word_ids`, ఇది ఇన్‌పుట్ వాక్యానికి టోకెన్ సంఖ్యల క్రమం\n",
    "* `input_mask`, ఇది క్రమంలో ఏ భాగం నిజమైన ఇన్‌పుట్‌ను కలిగి ఉందో, ఏది ప్యాడింగ్ అనేదో చూపిస్తుంది. ఇది `Masking` లేయర్ ద్వారా ఉత్పత్తి అయ్యే మాస్క్‌కు సమానంగా ఉంటుంది\n",
    "* `input_type_ids` భాషా మోడలింగ్ పనుల కోసం ఉపయోగిస్తారు, మరియు ఒక క్రమంలో రెండు ఇన్‌పుట్ వాక్యాలను నిర్దేశించడానికి అనుమతిస్తుంది.\n",
    "\n",
    "తర్వాత, మనం BERT ఫీచర్ ఎక్స్‌ట్రాక్టర్‌ను సృష్టించవచ్చు:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = hub.KerasLayer('https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pooled_output -> (1, 128)\n",
      "encoder_outputs -> 4\n",
      "sequence_output -> (1, 128, 128)\n",
      "default -> (1, 128)\n"
     ]
    }
   ],
   "source": [
    "z = bert(vectorizer(['I love transformers']))\n",
    "for i,x in z.items():\n",
    "    print(f\"{i} -> { len(x) if isinstance(x, list) else x.shape }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "కాబట్టి, BERT లేయర్ కొన్ని ఉపయోగకరమైన ఫలితాలను ఇస్తుంది:\n",
    "* `pooled_output` అనేది సీక్వెన్స్‌లోని అన్ని టోకెన్లను సగటు తీసుకున్న ఫలితం. దీన్ని మొత్తం నెట్‌వర్క్ యొక్క తెలివైన సేమాంటిక్ ఎంబెడ్డింగ్‌గా చూడవచ్చు. ఇది మన గత మోడల్‌లోని `GlobalAveragePooling1D` లేయర్ అవుట్పుట్‌కు సమానం.\n",
    "* `sequence_output` అనేది చివరి ట్రాన్స్‌ఫార్మర్ లేయర్ అవుట్పుట్ (మన మోడల్‌లోని `TransformerBlock` అవుట్పుట్‌కు సరిపోతుంది)\n",
    "* `encoder_outputs` అనేవి అన్ని ట్రాన్స్‌ఫార్మర్ లేయర్ల అవుట్పుట్లు. మనం 4-లేయర్ BERT మోడల్‌ను లోడ్ చేసుకున్నాము (పేరు నుండి మీరు అర్థం చేసుకోవచ్చు, ఇందులో `4_H` ఉంది), అందువల్ల 4 టెన్సర్లు ఉన్నాయి. చివరి టెన్సర్ `sequence_output`తో సమానం.\n",
    "\n",
    "ఇప్పుడు మనం ఎండ్-టు-ఎండ్ క్లాసిఫికేషన్ మోడల్‌ను నిర్వచిస్తాము. మోడల్ ఇన్‌పుట్‌ను నిర్వచించి, దాని అవుట్పుట్‌ను లెక్కించడానికి ఒక సిరీస్ ఎక్స్‌ప్రెషన్స్‌ను ఉపయోగించే *ఫంక్షనల్ మోడల్ నిర్వచనం* ఉపయోగిస్తాము. BERT మోడల్ వెయిట్స్‌ను ట్రెయిన్ చేయలేని విధంగా సెట్ చేసి, కేవలం చివరి క్లాసిఫైయర్‌ను మాత్రమే ట్రెయిన్ చేస్తాము:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer (KerasLayer)        {'input_type_ids': ( 0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer_1 (KerasLayer)      {'pooled_output': (N 4782465     keras_layer[0][0]                \n",
      "                                                                 keras_layer[0][1]                \n",
      "                                                                 keras_layer[0][2]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 128)          0           keras_layer_1[0][5]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 4)            516         dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 4,782,981\n",
      "Trainable params: 516\n",
      "Non-trainable params: 4,782,465\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inp = keras.Input(shape=(),dtype=tf.string)\n",
    "x = vectorizer(inp)\n",
    "x = bert(x)\n",
    "x = keras.layers.Dropout(0.1)(x['pooled_output'])\n",
    "out = keras.layers.Dense(4,activation='softmax')(x)\n",
    "model = keras.models.Model(inp,out)\n",
    "bert.trainable = False\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 528s 559ms/step - loss: 0.8056 - acc: 0.6983 - val_loss: 0.5953 - val_acc: 0.7888\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9bb1e36d00>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'], optimizer='adam')\n",
    "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "శిక్షణ పొందగల పరిమాణాలు తక్కువగా ఉన్నప్పటికీ, ప్రక్రియ చాలా నెమ్మదిగా ఉంటుంది, ఎందుకంటే BERT ఫీచర్ ఎక్స్‌ట్రాక్టర్ గణనాత్మకంగా భారంగా ఉంటుంది. సరైన శిక్షణ లేకపోవడం లేదా మోడల్ పరిమాణాల లోపం కారణంగా మనం తగినంత ఖచ్చితత్వం సాధించలేకపోయినట్లు కనిపిస్తోంది.\n",
    "\n",
    "BERT వెయిట్స్‌ను అన్‌ఫ్రీజ్ చేసి దానిని కూడా శిక్షణ ఇవ్వడానికి ప్రయత్నిద్దాం. దీనికి చాలా తక్కువ లెర్నింగ్ రేట్ అవసరం, అలాగే **వార్మప్**తో కూడిన మరింత జాగ్రత్తగా శిక్షణ వ్యూహం అవసరం, **AdamW** ఆప్టిమైజర్ ఉపయోగించి. ఆప్టిమైజర్ సృష్టించడానికి `tf-models-official` ప్యాకేజీని ఉపయోగిస్తాము:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer (KerasLayer)        {'input_type_ids': ( 0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer_1 (KerasLayer)      {'pooled_output': (N 4782465     keras_layer[0][0]                \n",
      "                                                                 keras_layer[0][1]                \n",
      "                                                                 keras_layer[0][2]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 128)          0           keras_layer_1[0][5]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 4)            516         dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 4,782,981\n",
      "Trainable params: 4,782,980\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n",
      "938/938 [==============================] - 629s 664ms/step - loss: 0.6344 - acc: 0.7658 - val_loss: 0.4876 - val_acc: 0.8247\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9bb0bd0070>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from official.nlp import optimization \n",
    "bert.trainable=True\n",
    "model.summary()\n",
    "epochs = 3\n",
    "opt = optimization.create_optimizer(\n",
    "    init_lr=3e-5,\n",
    "    num_train_steps=epochs*len(ds_train),\n",
    "    num_warmup_steps=0.1*epochs*len(ds_train),\n",
    "    optimizer_type='adamw')\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'], optimizer=opt)\n",
    "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "మీరు చూడగలిగినట్లుగా, శిక్షణ చాలా నెమ్మదిగా జరుగుతుంది - కానీ మీరు ప్రయోగాలు చేసి, మోడల్‌ను కొన్ని epochs (5-10) పాటు శిక్షణ ఇవ్వాలని అనుకోవచ్చు మరియు మునుపటి పద్ధతులతో పోల్చితే ఉత్తమ ఫలితాన్ని పొందగలరా అని చూడండి.\n",
    "\n",
    "## Huggingface Transformers లైబ్రరీ\n",
    "\n",
    "మరొక చాలా సాధారణ (మరియు కొంచెం సులభమైన) మార్గం Transformer మోడల్స్ ఉపయోగించడానికి [HuggingFace ప్యాకేజ్](https://github.com/huggingface/) ఇది, ఇది వివిధ NLP పనుల కోసం సులభమైన నిర్మాణ భాగాలను అందిస్తుంది. ఇది Tensorflow మరియు PyTorch రెండింటికీ అందుబాటులో ఉంది, ఇది మరొక చాలా ప్రాచుర్యం పొందిన న్యూరల్ నెట్‌వర్క్ ఫ్రేమ్‌వర్క్.\n",
    "\n",
    "> **Note**: మీరు Transformers లైబ్రరీ ఎలా పనిచేస్తుందో చూడటంలో ఆసక్తి లేకపోతే - మీరు ఈ నోట్‌బుక్ చివరికి వెళ్లవచ్చు, ఎందుకంటే మేము పైగా చేసిన వాటి నుండి మీరు ఏవైనా ముఖ్యమైన భిన్నతలు చూడరు. మేము వేరే లైబ్రరీ ఉపయోగించి మరియు గణనీయంగా పెద్ద మోడల్ ఉపయోగించి BERT మోడల్ శిక్షణ ఇచ్చే అదే దశలను పునరావృతం చేస్తాము. కాబట్టి, ఈ ప్రక్రియ కొంతకాలం పాటు శిక్షణ అవసరం, కాబట్టి మీరు కేవలం కోడ్‌ను చూసుకోవచ్చు.\n",
    "\n",
    "మనం [Huggingface Transformers](http://huggingface.co) ఉపయోగించి మా సమస్యను ఎలా పరిష్కరించవచ్చో చూద్దాం.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "మొదట మనం చేయవలసింది మనం ఉపయోగించబోయే మోడల్‌ను ఎంచుకోవడం. కొన్ని బిల్ట్-ఇన్ మోడల్స్ తో పాటు, Huggingface లో ఒక [ఆన్లైన్ మోడల్ రిపాజిటరీ](https://huggingface.co/models) ఉంది, అక్కడ మీరు కమ్యూనిటీ ద్వారా తయారైన మరిన్ని ప్రీ-ట్రెయిన్డ్ మోడల్స్‌ను కనుగొనవచ్చు. ఆ మోడల్స్ అన్నీ కేవలం మోడల్ పేరు ఇచ్చి లోడ్ చేసి ఉపయోగించవచ్చు. మోడల్‌కు అవసరమైన అన్ని బైనరీ ఫైళ్లు ఆటోమేటిక్‌గా డౌన్లోడ్ అవుతాయి.\n",
    "\n",
    "కొన్నిసార్లు మీరు మీ స్వంత మోడల్స్‌ను లోడ్ చేయవలసి వస్తుంది, అప్పుడు మీరు టోకనైజర్ కోసం పరామితులు, మోడల్ పరామితులతో కూడిన `config.json` ఫైల్, బైనరీ వెయిట్స్ మొదలైన అన్ని సంబంధిత ఫైళ్లు ఉన్న డైరెక్టరీని స్పెసిఫై చేయవచ్చు.\n",
    "\n",
    "మోడల్ పేరుతో, మేము మోడల్ మరియు టోకనైజర్ రెండింటినీ ఇన్స్టాన్షియేట్ చేయవచ్చు. టోకనైజర్‌తో ప్రారంభిద్దాం:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "# To load the model from Internet repository using model name. \n",
    "# Use this if you are running from your own copy of the notebooks\n",
    "bert_model = 'bert-base-uncased' \n",
    "\n",
    "# To load the model from the directory on disk. Use this for Microsoft Learn module, because we have\n",
    "# prepared all required files for you.\n",
    "#bert_model = './bert'\n",
    "\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(bert_model)\n",
    "\n",
    "MAX_SEQ_LEN = 128\n",
    "PAD_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "UNK_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tokenizer` ఆబ్జెక్ట్‌లో టెక్స్ట్‌ను నేరుగా ఎన్‌కోడ్ చేయడానికి ఉపయోగించగల `encode` ఫంక్షన్ ఉంటుంది:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 23435, 12314, 2003, 1037, 2307, 7705, 2005, 17953, 2361, 102]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('Tensorflow is a great framework for NLP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "మేము టోకనైజర్‌ను కూడా ఉపయోగించి ఒక సీక్వెన్స్‌ను మోడల్‌కు పంపడానికి అనుకూలమైన విధంగా ఎన్‌కోడ్ చేయవచ్చు, అంటే `token_ids`, `input_mask` ఫీల్డ్స్ మొదలైనవి. మేము `return_tensors='tf'` ఆర్గ్యుమెంట్‌ను అందించడం ద్వారా Tensorflow టెన్సార్లను కావాలని కూడా పేర్కొనవచ్చు:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[ 101, 7592, 1010, 2045,  102]], dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[0, 0, 0, 0, 0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[1, 1, 1, 1, 1]], dtype=int32)>}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(['Hello, there'],return_tensors='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "మన సందర్భంలో, మేము `bert-base-uncased` అనే ప్రీ-ట్రెయిన్డ్ BERT మోడల్‌ను ఉపయోగించబోతున్నాము. *Uncased* అంటే మోడల్ కేస్-సెన్సిటివ్ కాదు అని అర్థం. \n",
    "\n",
    "మోడల్‌ను ట్రెయిన్ చేయడానికి, టోకనైజ్ చేసిన సీక్వెన్స్‌ను ఇన్‌పుట్‌గా ఇవ్వాలి, అందుకే మేము డేటా ప్రాసెసింగ్ పైప్‌లైన్‌ను డిజైన్ చేస్తాము. `tokenizer.encode` ఒక Python ఫంక్షన్ కావడంతో, గత యూనిట్‌లో చేసినట్లే `py_function` ఉపయోగించి దీన్ని కాల్ చేయడం ద్వారా అదే విధానాన్ని అనుసరిస్తాము:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(x):\n",
    "    return tokenizer.encode(x.numpy().decode('utf-8'),return_tensors='tf',padding='max_length',max_length=MAX_SEQ_LEN,truncation=True)[0]\n",
    "\n",
    "def process_fn(x):\n",
    "    s = x['title']+' '+x['description']\n",
    "    e = tf.py_function(process,inp=[s],Tout=(tf.int32))\n",
    "    e.set_shape(MAX_SEQ_LEN)\n",
    "    return e,x['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ఇప్పుడు మనం `BertForSequenceClassfication` ప్యాకేజీ ఉపయోగించి అసలు మోడల్‌ను లోడ్ చేయవచ్చు. ఇది మన మోడల్ ఇప్పటికే క్లాసిఫికేషన్ కోసం అవసరమైన నిర్మాణాన్ని కలిగి ఉందని నిర్ధారిస్తుంది, చివరి క్లాసిఫైయర్ సహా. మీరు ఒక హెచ్చరిక సందేశం చూడగలరు, అది చివరి క్లాసిఫైయర్ యొక్క వెయిట్లు ప్రారంభించబడలేదని మరియు మోడల్ ప్రీ-ట్రైనింగ్ అవసరం అని చెబుతుంది - ఇది పూర్తిగా సరే, ఎందుకంటే అదే మనం చేయబోతున్నాం!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = transformers.TFBertForSequenceClassification.from_pretrained(bert_model,num_labels=4,output_attentions=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  109482240 \n",
      "_________________________________________________________________\n",
      "dropout_75 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  3076      \n",
      "=================================================================\n",
      "Total params: 109,485,316\n",
      "Trainable params: 109,485,316\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`summary()` నుండి మీరు చూడగలిగినట్లుగా, మోడల్ సుమారు 110 మిలియన్ల పరామితులను కలిగి ఉంది! సాధారణంగా, తక్కువ పరిమాణం గల డేటాసెట్ పై సులభమైన వర్గీకరణ పనిని చేయాలనుకుంటే, BERT బేస్ లేయర్ ను శిక్షణ ఇవ్వాలని మనం కోరుకోము:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  109482240 \n",
      "_________________________________________________________________\n",
      "dropout_75 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  3076      \n",
      "=================================================================\n",
      "Total params: 109,485,316\n",
      "Trainable params: 3,076\n",
      "Non-trainable params: 109,482,240\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.layers[0].trainable = False\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ఇప్పుడు మనం శిక్షణ ప్రారంభించడానికి సిద్ధంగా ఉన్నాము!\n",
    "\n",
    "> **Note**: పూర్తి స్థాయి BERT మోడల్ శిక్షణ చాలా సమయం తీసుకోవచ్చు! అందుకే మనం మొదటి 32 బ్యాచ్లకు మాత్రమే శిక్షణ ఇస్తాము. ఇది మోడల్ శిక్షణ ఎలా అమర్చబడిందో చూపించడానికి మాత్రమే. మీరు పూర్తి స్థాయి శిక్షణ ప్రయత్నించాలనుకుంటే - `steps_per_epoch` మరియు `validation_steps` పారామీటర్లను తీసివేసి, వేచి ఉండడానికి సిద్ధంగా ఉండండి!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 142s 4s/step - loss: 1.3896 - acc: 0.2500 - val_loss: 1.3863 - val_acc: 0.2480\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f1d40a4b6a0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile('adam','sparse_categorical_crossentropy',['acc'])\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "model.fit(ds_train.map(process_fn).batch(32),validation_data=ds_test.map(process_fn).batch(32),steps_per_epoch=32,validation_steps=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "మీరు iterations సంఖ్యను పెంచి, సరిపడా సమయం వేచి, మరియు అనేక epochs పాటు శిక్షణ ఇస్తే, BERT వర్గీకరణ మాకు ఉత్తమ ఖచ్చితత్వాన్ని ఇస్తుందని ఆశించవచ్చు! కారణం ఏమిటంటే BERT ఇప్పటికే భాష యొక్క నిర్మాణాన్ని బాగా అర్థం చేసుకున్నది, కాబట్టి మేము కేవలం తుది వర్గీకరణకర్తను ఫైన్-ట్యూన్ చేయాలి. అయితే, BERT ఒక పెద్ద మోడల్ కావడంతో, మొత్తం శిక్షణ ప్రక్రియ చాలా సమయం తీసుకుంటుంది మరియు గంభీరమైన కంప్యూటింగ్ శక్తిని అవసరం చేస్తుంది! (GPU, మరియు సాధ్యమైతే ఒకటి కంటే ఎక్కువ).\n",
    "\n",
    "> **Note:** మా ఉదాహరణలో, మేము అత్యల్ప ప్రీ-ట్రెయిన్ చేసిన BERT మోడల్స్‌లో ఒకటిని ఉపయోగిస్తున్నాము. మెరుగైన ఫలితాలు ఇచ్చే పెద్ద మోడల్స్ కూడా ఉన్నాయి.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ముఖ్యాంశాలు\n",
    "\n",
    "ఈ యూనిట్‌లో, మనం **ట్రాన్స్‌ఫార్మర్స్** ఆధారిత అత్యంత తాజా మోడల్ నిర్మాణాలను చూశాము. మనం వాటిని టెక్స్ట్ వర్గీకరణ పనికి ఉపయోగించాము, కానీ అదే విధంగా BERT మోడల్స్‌ను ఎంటిటీ ఎక్స్‌ట్రాక్షన్, ప్రశ్నలకు సమాధానం ఇవ్వడం మరియు ఇతర NLP పనుల కోసం కూడా ఉపయోగించవచ్చు.\n",
    "\n",
    "ట్రాన్స్‌ఫార్మర్ మోడల్స్ NLPలో ప్రస్తుత అత్యుత్తమ స్థాయిని సూచిస్తాయి, మరియు చాలా సందర్భాల్లో మీరు కస్టమ్ NLP పరిష్కారాలను అమలు చేయడం ప్రారంభించినప్పుడు మొదటి పరిష్కారంగా ఇవే ప్రయత్నించాలి. అయితే, ఈ మాడ్యూల్‌లో చర్చించిన రికరెంట్ న్యూరల్ నెట్‌వర్క్స్ యొక్క ప్రాథమిక సూత్రాలను అర్థం చేసుకోవడం అత్యంత ముఖ్యమైనది, మీరు అధునాతన న్యూరల్ మోడల్స్‌ను నిర్మించాలనుకుంటే.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n<!-- CO-OP TRANSLATOR DISCLAIMER START -->\n**అస్పష్టత**:  \nఈ పత్రాన్ని AI అనువాద సేవ [Co-op Translator](https://github.com/Azure/co-op-translator) ఉపయోగించి అనువదించబడింది. మేము ఖచ్చితత్వానికి ప్రయత్నించినప్పటికీ, ఆటోమేటెడ్ అనువాదాల్లో పొరపాట్లు లేదా తప్పిదాలు ఉండవచ్చు. మూల పత్రం దాని స్వదేశీ భాషలో అధికారిక మూలంగా పరిగణించాలి. ముఖ్యమైన సమాచారానికి, ప్రొఫెషనల్ మానవ అనువాదం సిఫార్సు చేయబడుతుంది. ఈ అనువాదం వాడకంలో ఏర్పడిన ఏవైనా అపార్థాలు లేదా తప్పుదారితీసే అర్థాలు కోసం మేము బాధ్యత వహించము.\n<!-- CO-OP TRANSLATOR DISCLAIMER END -->\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0cb620c6d4b9f7a635928804c26cf22403d89d98d79684e4529119355ee6d5a5"
  },
  "kernelspec": {
   "display_name": "py38_tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "coopTranslator": {
   "original_hash": "ab59c532409774988ab875f2260e8e53",
   "translation_date": "2025-11-26T02:24:26+00:00",
   "source_file": "lessons/5-NLP/18-Transformers/TransformersTF.ipynb",
   "language_code": "te"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}