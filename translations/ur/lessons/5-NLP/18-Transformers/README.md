# ุชูุฌ ฺฉ ุทุฑู ฺฉุงุฑ ุงูุฑ ูนุฑุงูุณูุงุฑูุฑุฒ

## [ูฺฉฺุฑ ุณ ูพู ฺฉุง ฺฉูุฆุฒ](https://ff-quizzes.netlify.app/en/ai/quiz/35)

NLP ฺฉ ุดุนุจ ูฺบ ุณุจ ุณ ุงู ูุณุงุฆู ูฺบ ุณ ุงฺฉ **ูุดู ุชุฑุฌู** ุ ุฌู ฺฉ ุงฺฉ ุจูุงุฏ ฺฉุงู  ุฌู ฺฏูฺฏู ูนุฑุงูุณููน ุฌุณ ูนููุฒ ฺฉ ุจูุงุฏ  ุงุณ ุณฺฉุดู ูฺบุ ู ูุดู ุชุฑุฌู ูพุฑ ุชูุฌ ูุฑฺฉูุฒ ฺฉุฑฺบ ฺฏุ ุง ุฒุงุฏ ุนููู ุทูุฑ ูพุฑุ ฺฉุณ ุจฺพ *sequence-to-sequence* ฺฉุงู ูพุฑ (ุฌุณ **sentence transduction** ุจฺพ ฺฉุง ุฌุงุชุง )

RNNs ฺฉ ุณุงุชฺพุ sequence-to-sequence ุฏู recurrent ููน ูุฑฺฉุณ ฺฉ ุฐุฑุน ูุงูุฐ ฺฉุง ุฌุงุชุง ุ ุฌุงฺบ ุงฺฉ ููน ูุฑฺฉุ **encoder**ุ ุงู ูพูน ุณฺฉูุฆูุณ ฺฉู ุงฺฉ hidden state ูฺบ ุชุจุฏู ฺฉุฑุชุง ุ ุฌุจฺฉ ุฏูุณุฑุง ููน ูุฑฺฉุ **decoder**ุ ุงุณ hidden state ฺฉู ุงฺฉ ุชุฑุฌู ุดุฏ ูุชุฌ ูฺบ ุชุจุฏู ฺฉุฑุชุง  ุงุณ ุทุฑู ฺฉุงุฑ ฺฉ ุณุงุชฺพ ฺฉฺฺพ ูุณุงุฆู ฺบ:

* encoder ููน ูุฑฺฉ ฺฉ ุขุฎุฑ ุญุงูุช ุฌูู ฺฉ ุขุบุงุฒ ฺฉู ุงุฏ ุฑฺฉฺพู ูฺบ ูุดฺฉู ูุญุณูุณ ฺฉุฑุช ุ ุฌุณ ฺฉ ูุฌ ุณ ููุจ ุฌูููฺบ ฺฉ ู ูุงฺู ฺฉุง ูุนุงุฑ ุฎุฑุงุจ ู ุฌุงุชุง 
* ุณฺฉูุฆูุณ ูฺบ ููุฌูุฏ ุชูุงู ุงููุงุธ ฺฉุง ูุชุฌ ูพุฑ ฺฉุณุงฺบ ุงุซุฑ ูุชุง  ุญููุช ูฺบุ ุชุงูุ ุงู ูพูน ุณฺฉูุฆูุณ ูฺบ ููุฌูุฏ ูุฎุตูุต ุงููุงุธ ฺฉุง sequential outputs ูพุฑ ุฏูุณุฑูฺบ ฺฉ ููุงุจู ูฺบ ุฒุงุฏ ุงุซุฑ ูุชุง 

**ุชูุฌ ฺฉ ุทุฑู ฺฉุงุฑ** RNN ฺฉ ุฑ ุขุคูน ูพูน ูพุด ฺฏูุฆ ูพุฑ ุฑ ุงู ูพูน ูฺฉูนุฑ ฺฉ ุณุงู ู ุณุจุงู ฺฉ ุงุซุฑ ฺฉู ูุฒู ุฏู ฺฉุง ุงฺฉ ุฐุฑุน ูุฑุงู ฺฉุฑุช ฺบ ุงุณ ฺฉู ูุงูุฐ ฺฉุฑู ฺฉุง ุทุฑู   ฺฉ ุงู ูพูน RNN ฺฉ ุฏุฑูุงู ุญุงูุชูฺบ ุงูุฑ ุขุคูน ูพูน RNN ฺฉ ุฏุฑูุงู ุดุงุฑูน ฺฉูนุณ ุจูุงุฆ ุฌุงุฆฺบ ุงุณ ุทุฑุญุ ุฌุจ ุขุคูน ูพูน ุนูุงูุช y<sub>t</sub> ูพุฏุง ฺฉ ุฌุง ุฑ ูุ ู ุชูุงู ุงู ูพูน hidden states h<sub>i</sub> ฺฉู ูุฎุชูู ูุฒู ฺฉ coefficients &alpha;<sub>t,i</sub> ฺฉ ุณุงุชฺพ ูุฏูุธุฑ ุฑฺฉฺพฺบ ฺฏ

![ุชุตูุฑ ุฌู encoder/decoder ูุงฺู ฺฉู additive attention layer ฺฉ ุณุงุชฺพ ุฏฺฉฺพุง ุฑ ](../../../../../translated_images/ur/encoder-decoder-attention.7a726296894fb567.webp)

> [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) ูฺบ additive attention mechanism ฺฉ ุณุงุชฺพ encoder-decoder ูุงฺูุ [ุงุณ ุจูุงฺฏ ูพูุณูน](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html) ุณ ูุง ฺฏุง

ุชูุฌ ููนุฑฺฉุณ {&alpha;<sub>i,j</sub>} ุงุณ ุญุฏ ฺฉู ุธุงุฑ ฺฉุฑ ฺฏ ฺฉ ุงู ูพูน ุณฺฉูุฆูุณ ูฺบ ููุฌูุฏ ูุฎุตูุต ุงููุงุธ ุขุคูน ูพูน ุณฺฉูุฆูุณ ูฺบ ุฏ ฺฏุฆ ููุธ ฺฉ ุชุฎูู ูฺบ ฺฉุชูุง ฺฉุฑุฏุงุฑ ุงุฏุง ฺฉุฑุช ฺบ ูฺ ุงฺฉ ูุซุงู ุฏ ฺฏุฆ :

![ุชุตูุฑ ุฌู RNNsearch-50 ฺฉ ุฐุฑุน ูพุงุฆ ฺฏุฆ ุงฺฉ ูููู alignment ฺฉู ุฏฺฉฺพุง ุฑ ุ Bahdanau - arviz.org ุณ ู ฺฏุฆ](../../../../../translated_images/ur/bahdanau-fig3.09ba2d37f202a6af.webp)

> [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) ุณ ุชุตูุฑ (Fig.3)

ุชูุฌ ฺฉ ุทุฑู ฺฉุงุฑ NLP ูฺบ ููุฌูุฏ ุง ูุฑุจ ููุฌูุฏ state of the art ฺฉ ู ุจุช ุญุฏ ุชฺฉ ุฐู ุฏุงุฑ ฺบ ุชุงูุ ุชูุฌ ุดุงูู ฺฉุฑู ุณ ูุงฺู ฺฉ ูพุฑุงููนุฑุฒ ฺฉ ุชุนุฏุงุฏ ูฺบ ุจุช ุฒุงุฏ ุงุถุงู ูุชุง ุ ุฌุณ ฺฉ ูุฌ ุณ RNNs ฺฉ ุณุงุชฺพ scaling ฺฉ ูุณุงุฆู ูพุฏุง ูุฆ RNNs ฺฉู scale ฺฉุฑู ฺฉ ุงฺฉ ุงู ุฑฺฉุงููน   ฺฉ ูุงฺูุฒ ฺฉ recurrent ููุนุช ุชุฑุจุช ฺฉู batch ุงูุฑ parallelize ฺฉุฑูุง ูุดฺฉู ุจูุง ุฏุช  RNN ูฺบ ุณฺฉูุฆูุณ ฺฉ ุฑ ุนูุตุฑ ฺฉู ุชุฑุชุจ ูุงุฑ ูพุฑูุณุณ ฺฉุฑูุง ุถุฑูุฑ ูุชุง ุ ุฌุณ ฺฉุง ูุทูุจ  ฺฉ ุงุณ ุขุณุงู ุณ parallelize ูฺบ ฺฉุง ุฌุง ุณฺฉุชุง

![Encoder Decoder with Attention](../../../../../lessons/5-NLP/18-Transformers/images/EncDecAttention.gif)

> [ฺฏูฺฏู ฺฉ ุจูุงฺฏ](https://research.googleblog.com/2016/09/a-neural-network-for-machine.html) ุณ ุชุตูุฑ

ุชูุฌ ฺฉ ุทุฑู ฺฉุงุฑ ฺฉู ุงูพูุงู ุงูุฑ ุงุณ ุฑฺฉุงููน ฺฉ ุงูุชุฒุงุฌ ู ุขุฌ ฺฉ state of the art Transformer Models ฺฉ ุชุฎูู ฺฉ ุฑุง ููุงุฑ ฺฉุ ุฌุณ BERT ุงูุฑ Open-GPT3

## ูนุฑุงูุณูุงุฑูุฑ ูุงฺูุฒ

ูนุฑุงูุณูุงุฑูุฑุฒ ฺฉ ูพฺฺพ ุงฺฉ ุงู ุฎุงู   ฺฉ RNNs ฺฉ ุชุฑุชุจ ูุงุฑ ููุนุช ุณ ุจฺุง ุฌุงุฆ ุงูุฑ ุงฺฉ ุงุณุง ูุงฺู ุจูุงุง ุฌุงุฆ ุฌู ุชุฑุจุช ฺฉ ุฏูุฑุงู parallelizable ู  ุฏู ุฎุงูุงุช ฺฉู ูุงูุฐ ฺฉุฑ ฺฉ ุญุงุตู ฺฉุง ุฌุงุชุง :

* positional encoding
* RNNs (ุง CNNs) ฺฉ ุจุฌุงุฆ patterns ฺฉู capture ฺฉุฑู ฺฉ ู self-attention mechanism ฺฉุง ุงุณุชุนูุงู (ุงุณ ู ู ููุงู ุฌุณ ู ูนุฑุงูุณูุงุฑูุฑุฒ ฺฉู ูุชุนุงุฑู ฺฉุฑุงุง ุงุณ *[Attention is all you need](https://arxiv.org/abs/1706.03762)* ฺฉุง ุฌุงุชุง )

### Positional Encoding/Embedding

positional encoding ฺฉุง ุฎุงู ุฏุฑุฌ ุฐู :
1. RNNs ฺฉุง ุงุณุชุนูุงู ฺฉุฑุช ููุชุ tokens ฺฉ relative position steps ฺฉ ุชุนุฏุงุฏ ุณ ุธุงุฑ ูุช ุ ุงูุฑ ุงุณ ู ุงุณ ูุงุถุญ ุทูุฑ ูพุฑ ุธุงุฑ ฺฉุฑู ฺฉ ุถุฑูุฑุช ูฺบ ูุช
2. ุชุงูุ ุฌุจ ู ุชูุฌ ูพุฑ ุณูุฆฺ ฺฉุฑุช ฺบุ ุชู ูฺบ ุณฺฉูุฆูุณ ฺฉ ุงูุฏุฑ tokens ฺฉ relative positions ูุนููู ูู ฺุง
3. positional encoding ุญุงุตู ฺฉุฑู ฺฉ ูุ ู ุงูพู tokens ฺฉ ุณฺฉูุฆูุณ ฺฉู ุณฺฉูุฆูุณ ูฺบ token positions ฺฉ ุณฺฉูุฆูุณ ฺฉ ุณุงุชฺพ ุจฺฺพุงุช ฺบ (ุนูุ 0,1, ... ฺฉ ุชุนุฏุงุฏ ฺฉุง ุงฺฉ ุณฺฉูุฆูุณ)
4. ูพฺพุฑ ู token position ฺฉู token embedding vector ฺฉ ุณุงุชฺพ mix ฺฉุฑุช ฺบ position (integer) ฺฉู vector ูฺบ ุชุจุฏู ฺฉุฑู ฺฉ ูุ ู ูุฎุชูู ุทุฑู ุงุณุชุนูุงู ฺฉุฑ ุณฺฉุช ฺบ:

* Trainable embeddingุ token embedding ฺฉ ุทุฑุญ  ู ุทุฑู  ุฌุณ ูพุฑ ู ุงฺบ ุบูุฑ ฺฉุฑุช ฺบ ู tokens ุงูุฑ ุงู ฺฉ positions ุฏูููฺบ ูพุฑ embedding layers ูฺฏุงุช ฺบุ ุฌุณ ฺฉ ูุชุฌ ูฺบ ุงฺฉ  dimensions ฺฉ embedding vectors ุญุงุตู ูุช ฺบุ ุฌูฺบ ู ูพฺพุฑ ุงฺฉ ุณุงุชฺพ ุฌูฺุช ฺบ
* Fixed position encoding functionุ ุฌุณุง ฺฉ ุงุตู ููุงู ูฺบ ุชุฌูุฒ ฺฉุง ฺฏุง 

<img src="../../../../../translated_images/ur/pos-embedding.e41ce9b6cf6078af.webp" width="50%"/>

> ุชุตูุฑ ูุตูู ฺฉ ุทุฑู ุณ

ุฌู ูุชุฌ ูฺบ positional embedding ฺฉ ุณุงุชฺพ ููุชุง  ู ุงุตู token ุงูุฑ ุงุณ ฺฉ ุณฺฉูุฆูุณ ฺฉ ุงูุฏุฑ position ุฏูููฺบ ฺฉู embed ฺฉุฑุชุง 

### Multi-Head Self-Attention

ุงฺฏูุงุ ูฺบ ุงูพู ุณฺฉูุฆูุณ ฺฉ ุงูุฏุฑ ฺฉฺฺพ patterns ฺฉู capture ฺฉุฑู ฺฉ ุถุฑูุฑุช  ุงุณุง ฺฉุฑู ฺฉ ูุ ูนุฑุงูุณูุงุฑูุฑุฒ **self-attention** mechanism ุงุณุชุนูุงู ฺฉุฑุช ฺบุ ุฌู ุจูุงุฏ ุทูุฑ ูพุฑ ู ุชูุฌ  ุฌู ุงู ูพูน ุงูุฑ ุขุคูน ูพูน ฺฉ ุทูุฑ ูพุฑ ุงฺฉ  ุณฺฉูุฆูุณ ูพุฑ ูุงฺฏู ูุช  self-attention ฺฉู ูุงฺฏู ฺฉุฑู ุณ ูฺบ ุฌูู ฺฉ ุงูุฏุฑ **context** ฺฉู ูุฏูุธุฑ ุฑฺฉฺพู ฺฉ ุงุฌุงุฒุช ููุช ุ ุงูุฑ  ุฏฺฉฺพู ฺฉ ุงุฌุงุฒุช ููุช  ฺฉ ฺฉูู ุณ ุงููุงุธ ุขูพุณ ูฺบ ุฌฺ ูุฆ ฺบ ูุซุงู ฺฉ ุทูุฑ ูพุฑุ  ูฺบ  ุฏฺฉฺพู ฺฉ ุงุฌุงุฒุช ุฏุชุง  ฺฉ ฺฉูู ุณ ุงููุงุธ coreferences ุฌุณ *it* ฺฉ ุฐุฑุน ุญูุงู ุฏ ฺฏุฆ ฺบุ ุงูุฑ ุณุงู ู ุณุจุงู ฺฉู ุจฺพ ูุฏูุธุฑ ุฑฺฉฺพุชุง :

![](../../../../../translated_images/ur/CoreferenceResolution.861924d6d384a7d6.webp)

> [ฺฏูฺฏู ุจูุงฺฏ](https://research.googleblog.com/2017/08/transformer-novel-neural-network.html) ุณ ุชุตูุฑ

ูนุฑุงูุณูุงุฑูุฑุฒ ูฺบุ ู **Multi-Head Attention** ุงุณุชุนูุงู ฺฉุฑุช ฺบ ุชุงฺฉ ููน ูุฑฺฉ ฺฉู ูุฎุชูู ูุณู ฺฉ dependencies ฺฉู capture ฺฉุฑู ฺฉ ุทุงูุช ุฏ ุฌุง ุณฺฉุ ุฌุณ ฺฉ long-term ุจููุงุจู short-term word relationsุ co-reference ุจููุงุจู ฺฉฺฺพ ุงูุฑุ ูุบุฑ

[TensorFlow Notebook](TransformersTF.ipynb) ูนุฑุงูุณูุงุฑูุฑ layers ฺฉ ููุงุฐ ูพุฑ ูุฒุฏ ุชูุตูุงุช ูุฑุงู ฺฉุฑุชุง 

### Encoder-Decoder Attention

ูนุฑุงูุณูุงุฑูุฑุฒ ูฺบุ ุชูุฌ ุฏู ุฌฺฏูฺบ ูพุฑ ุงุณุชุนูุงู ูุช :

* ุงู ูพูน ูุชู ฺฉ ุงูุฏุฑ patterns ฺฉู self-attention ฺฉ ุฐุฑุน capture ฺฉุฑู ฺฉ ู
* ุณฺฉูุฆูุณ ุชุฑุฌู ฺฉุฑู ฺฉ ู -  encoder ุงูุฑ decoder ฺฉ ุฏุฑูุงู ุชูุฌ ฺฉ layer 

Encoder-decoder attention RNNs ูฺบ ุงุณุชุนูุงู ูู ูุงู ุชูุฌ ฺฉ ุทุฑู ฺฉุงุฑ ุณ ุจุช ููุช ุฌูุช ุ ุฌุณุง ฺฉ ุงุณ ุณฺฉุดู ฺฉ ุขุบุงุฒ ูฺบ ุจุงู ฺฉุง ฺฏุง   ูุชุญุฑฺฉ ุฎุงฺฉ encoder-decoder attention ฺฉ ฺฉุฑุฏุงุฑ ฺฉู ูุงุถุญ ฺฉุฑุชุง 

![Animated GIF showing how the evaluations are performed in transformer models.](../../../../../lessons/5-NLP/18-Transformers/images/transformer-animated-explanation.gif)

ฺููฺฉ ุฑ ุงู ูพูน position ฺฉู ุขุฒุงุฏุงู ุทูุฑ ูพุฑ ุฑ ุขุคูน ูพูน position ูพุฑ map ฺฉุง ุฌุงุชุง ุ ูนุฑุงูุณูุงุฑูุฑุฒ RNNs ฺฉ ููุงุจู ูฺบ ุจุชุฑ parallelize ฺฉุฑ ุณฺฉุช ฺบุ ุฌู ฺฉ ุจุช ุจฺ ุงูุฑ ุฒุงุฏ expressive language models ฺฉู ูุงุจู ุจูุงุชุง  ุฑ attention head ฺฉู ุงููุงุธ ฺฉ ุฏุฑูุงู ูุฎุชูู ุชุนููุงุช ุณฺฉฺพู ฺฉ ู ุงุณุชุนูุงู ฺฉุง ุฌุง ุณฺฉุชุง  ุฌู ูฺ ุฏ ฺฏุฆ Natural Language Processing ฺฉุงููฺบ ฺฉู ุจุชุฑ ุจูุงุชุง 

## BERT

**BERT** (Bidirectional Encoder Representations from Transformers) ุงฺฉ ุจุช ุจฺุง multi-layer transformer ููน ูุฑฺฉ  ุฌุณ ูฺบ *BERT-base* ฺฉ ู 12 layers ฺบุ ุงูุฑ *BERT-large* ฺฉ ู 24 layers ฺบ ูุงฺู ฺฉู ูพู ุงฺฉ ุจฺ text data corpus (WikiPedia + books) ูพุฑ unsupervised training (ุฌูู ูฺบ masked words ฺฉ ูพุด ฺฏูุฆ) ฺฉุง ุงุณุชุนูุงู ฺฉุฑุช ูุฆ pre-train ฺฉุง ุฌุงุชุง  pre-training ฺฉ ุฏูุฑุงู ูุงฺู ุฒุจุงู ฺฉ ุณูุฌฺพ ฺฉ ุงู ุณุทุญูฺบ ฺฉู ุฌุฐุจ ฺฉุฑุชุง ุ ุฌูฺบ ูพฺพุฑ ุฏฺฏุฑ datasets ฺฉ ุณุงุชฺพ fine tuning ฺฉ ุฐุฑุน ุงุณุชุนูุงู ฺฉุง ุฌุง ุณฺฉุชุง  ุงุณ ุนูู ฺฉู **transfer learning** ฺฉุง ุฌุงุชุง 

![ุชุตูุฑ http://jalammar.github.io/illustrated-bert/ ุณ](../../../../../translated_images/ur/jalammarBERT-language-modeling-masked-lm.34f113ea5fec4362.webp)

> ุชุตูุฑ [ูุงุฎุฐ](http://jalammar.github.io/illustrated-bert/)

## โ๏ธ ูุดูฺบ: ูนุฑุงูุณูุงุฑูุฑุฒ

ุงูพู ุชุนูู ฺฉู ุฏุฑุฌ ุฐู ูููน ุจฺฉุณ ูฺบ ุฌุงุฑ ุฑฺฉฺพฺบ:

* [PyTorch ูฺบ ูนุฑุงูุณูุงุฑูุฑุฒ](TransformersPyTorch.ipynb)
* [TensorFlow ูฺบ ูนุฑุงูุณูุงุฑูุฑุฒ](TransformersTF.ipynb)

## ูุชุฌ

ุงุณ ุณุจู ูฺบ ุขูพ ู ูนุฑุงูุณูุงุฑูุฑุฒ ุงูุฑ ุชูุฌ ฺฉ ุทุฑู ฺฉุงุฑ ฺฉ ุจุงุฑ ูฺบ ุณฺฉฺพุงุ ุฌู NLP ฺฉ ูนูู ุจุงฺฉุณ ูฺบ ุถุฑูุฑ ุขูุงุช ฺบ ูนุฑุงูุณูุงุฑูุฑ ุขุฑฺฉูนฺฉฺุฑุฒ ฺฉ ุจุช ุณ ูุฎุชูู ุญุงูุชฺบ ฺบุ ุฌู ูฺบ BERTุ DistilBERTุ BigBirdุ OpenGPT3 ุงูุฑ ูุฒุฏ ุดุงูู ฺบุ ุฌูฺบ fine tune ฺฉุง ุฌุง ุณฺฉุชุง  [HuggingFace ูพฺฉุฌ](https://github.com/huggingface/) PyTorch ุงูุฑ TensorFlow ุฏูููฺบ ฺฉ ุณุงุชฺพ ุงู ุขุฑฺฉูนฺฉฺุฑุฒ ูฺบ ุณ ุจุช ุณ ฺฉู ุชุฑุจุช ุฏู ฺฉ ู repository ูุฑุงู ฺฉุฑุชุง 

## ๐ ฺููุฌ

## [ูฺฉฺุฑ ฺฉ ุจุนุฏ ฺฉุง ฺฉูุฆุฒ](https://ff-quizzes.netlify.app/en/ai/quiz/36)

## ุฌุงุฆุฒ ุงูุฑ ุฎูุฏ ูุทุงูุน

* [ุจูุงฺฏ ูพูุณูน](https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/)ุ ุฌู ฺฉูุงุณฺฉู [Attention is all you need](https://arxiv.org/abs/1706.03762) ููุงู ฺฉู ูนุฑุงูุณูุงุฑูุฑุฒ ูพุฑ ูุถุงุญุช ฺฉุฑุช 
* [ุจูุงฺฏ ูพูุณูนุณ ฺฉ ุงฺฉ ุณุฑุฒ](https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452) ูนุฑุงูุณูุงุฑูุฑุฒ ูพุฑุ ุฌู ุขุฑฺฉูนฺฉฺุฑ ฺฉู ุชูุตู ุณ ูุถุงุญุช ฺฉุฑุช 

## [Assignment](assignment.md)

---

