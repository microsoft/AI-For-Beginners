{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ایمبیڈنگز\n",
    "\n",
    "پچھلی مثال میں، ہم نے `vocab_size` کی لمبائی والے ہائی ڈائمینشنل بیگ آف ورڈز ویکٹرز پر کام کیا، اور کم ڈائمینشنل پوزیشنل ریپریزنٹیشن ویکٹرز کو واضح طور پر اسپارس ون-ہاٹ ریپریزنٹیشن میں تبدیل کیا۔ یہ ون-ہاٹ ریپریزنٹیشن میموری کے لحاظ سے مؤثر نہیں ہے۔ اس کے علاوہ، ہر لفظ کو دوسرے الفاظ سے آزاد سمجھا جاتا ہے، اس لیے ون-ہاٹ انکوڈڈ ویکٹرز الفاظ کے درمیان معنوی مشابہت کو ظاہر نہیں کرتے۔\n",
    "\n",
    "اس یونٹ میں، ہم **نیوز اے جی** ڈیٹاسیٹ کو مزید دریافت کریں گے۔ شروع کرنے کے لیے، آئیے ڈیٹا لوڈ کرتے ہیں اور پچھلے یونٹ سے کچھ تعریفیں حاصل کرتے ہیں۔\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "\n",
    "ds_train, ds_test = tfds.load('ag_news_subset').values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ایمبیڈنگ کیا ہے؟\n",
    "\n",
    "**ایمبیڈنگ** کا تصور یہ ہے کہ الفاظ کو کم جہتی گھنے ویکٹرز کے ذریعے ظاہر کیا جائے جو لفظ کے معنوی مفہوم کو ظاہر کرتے ہیں۔ ہم بعد میں یہ بات کریں گے کہ بامعنی لفظی ایمبیڈنگز کیسے بنائی جائیں، لیکن فی الحال ایمبیڈنگز کو ایک ایسے طریقے کے طور پر سوچیں جو لفظی ویکٹر کی جہت کو کم کرتا ہے۔\n",
    "\n",
    "ایمبیڈنگ لیئر ایک لفظ کو ان پٹ کے طور پر لیتی ہے اور ایک مخصوص `embedding_size` کے آؤٹ پٹ ویکٹر کو پیدا کرتی ہے۔ ایک لحاظ سے، یہ `Dense` لیئر سے بہت مشابہ ہے، لیکن یہ ایک-ہاٹ انکوڈڈ ویکٹر لینے کے بجائے، ایک لفظی نمبر کو ان پٹ کے طور پر لے سکتی ہے۔\n",
    "\n",
    "جب ہم اپنے نیٹ ورک میں پہلی لیئر کے طور پر ایمبیڈنگ لیئر کا استعمال کرتے ہیں، تو ہم بیگ-آف-ورڈز ماڈل سے **ایمبیڈنگ بیگ** ماڈل میں منتقل ہو سکتے ہیں، جہاں ہم پہلے اپنے متن کے ہر لفظ کو اس کے متعلقہ ایمبیڈنگ میں تبدیل کرتے ہیں، اور پھر ان تمام ایمبیڈنگز پر کوئی مجموعی فنکشن جیسے کہ `sum`، `average` یا `max` کا حساب لگاتے ہیں۔\n",
    "\n",
    "![پانچ سیکوئنس الفاظ کے لیے ایمبیڈنگ کلاسیفائر کی تصویر۔](../../../../../translated_images/ur/embedding-classifier-example.b77f021a7ee67eee.webp)\n",
    "\n",
    "ہمارے کلاسیفائر نیورل نیٹ ورک میں درج ذیل لیئرز شامل ہیں:\n",
    "\n",
    "* `TextVectorization` لیئر، جو ایک سٹرنگ کو ان پٹ کے طور پر لیتی ہے اور ٹوکن نمبرز کا ایک ٹینسر پیدا کرتی ہے۔ ہم ایک معقول `vocab_size` مقرر کریں گے اور کم استعمال ہونے والے الفاظ کو نظرانداز کریں گے۔ ان پٹ کی شکل 1 ہوگی، اور آؤٹ پٹ کی شکل $n$ ہوگی، کیونکہ نتیجے میں ہمیں $n$ ٹوکنز ملیں گے، جن میں سے ہر ایک میں 0 سے `vocab_size` تک کے نمبر ہوں گے۔\n",
    "* `Embedding` لیئر، جو $n$ نمبرز لیتی ہے اور ہر نمبر کو ایک دی گئی لمبائی (ہمارے مثال میں 100) کے گھنے ویکٹر میں کم کرتی ہے۔ اس طرح، $n$ کی شکل کا ان پٹ ٹینسر $n\\times 100$ کی شکل کے ٹینسر میں تبدیل ہو جائے گا۔\n",
    "* ایگریگیشن لیئر، جو اس ٹینسر کا پہلے محور کے ساتھ اوسط نکالتی ہے، یعنی یہ مختلف الفاظ کے مطابق تمام $n$ ان پٹ ٹینسرز کا اوسط نکالے گی۔ اس لیئر کو نافذ کرنے کے لیے، ہم ایک `Lambda` لیئر استعمال کریں گے اور اس میں اوسط نکالنے کا فنکشن پاس کریں گے۔ آؤٹ پٹ کی شکل 100 ہوگی، اور یہ پورے ان پٹ سیکوئنس کی عددی نمائندگی ہوگی۔\n",
    "* آخری `Dense` لکیری کلاسیفائر۔\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization (TextVec  (None, None)             0         \n",
      " torization)                                                     \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, None, 100)         3000000   \n",
      "                                                                 \n",
      " lambda (Lambda)             (None, 100)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4)                 404       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,000,404\n",
      "Trainable params: 3,000,404\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 30000\n",
    "batch_size = 128\n",
    "\n",
    "vectorizer = keras.layers.experimental.preprocessing.TextVectorization(max_tokens=vocab_size,input_shape=(1,))\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    vectorizer,    \n",
    "    keras.layers.Embedding(vocab_size,100),\n",
    "    keras.layers.Lambda(lambda x: tf.reduce_mean(x,axis=1)),\n",
    "    keras.layers.Dense(4, activation='softmax')\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`summary` کے پرنٹ آؤٹ میں، **output shape** کالم میں پہلے ٹینسر ڈائمینشن `None` مائنی بیچ کے سائز کو ظاہر کرتا ہے، اور دوسرا ٹوکن سیکوئنس کی لمبائی کو ظاہر کرتا ہے۔ مائنی بیچ میں موجود تمام ٹوکن سیکوئنسز کی لمبائیاں مختلف ہوتی ہیں۔ ہم اگلے سیکشن میں اس سے نمٹنے کے طریقے پر بات کریں گے۔\n",
    "\n",
    "اب نیٹ ورک کو ٹرین کرتے ہیں:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training vectorizer\n",
      "938/938 [==============================] - 20s 20ms/step - loss: 0.7891 - acc: 0.8155 - val_loss: 0.4470 - val_acc: 0.8642\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22255515100>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_text(x):\n",
    "    return x['title']+' '+x['description']\n",
    "\n",
    "def tupelize(x):\n",
    "    return (extract_text(x),x['label'])\n",
    "\n",
    "print(\"Training vectorizer\")\n",
    "vectorizer.adapt(ds_train.take(500).map(extract_text))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'])\n",
    "model.fit(ds_train.map(tupelize).batch(batch_size),validation_data=ds_test.map(tupelize).batch(batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "**نوٹ** کہ ہم ڈیٹا کے ایک ذیلی حصے کی بنیاد پر ویکٹرائزر بنا رہے ہیں۔ یہ عمل کو تیز کرنے کے لیے کیا جاتا ہے، اور اس کے نتیجے میں ایسی صورتحال پیدا ہو سکتی ہے جب ہمارے متن کے تمام ٹوکنز لغت میں موجود نہ ہوں۔ اس صورت میں، ان ٹوکنز کو نظر انداز کر دیا جائے گا، جو تھوڑی کم درستگی کا باعث بن سکتا ہے۔ تاہم، حقیقی زندگی میں متن کا ایک ذیلی حصہ اکثر لغت کا اچھا تخمینہ فراہم کرتا ہے۔\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### متغیر ترتیب کے سائز سے نمٹنا\n",
    "\n",
    "آئیے سمجھتے ہیں کہ منی بیچز میں تربیت کیسے ہوتی ہے۔ اوپر دیے گئے مثال میں، ان پٹ ٹینسر کا طول 1 ہے، اور ہم 128-لمبے منی بیچز استعمال کرتے ہیں، اس طرح ٹینسر کا اصل سائز $128 \\times 1$ ہوتا ہے۔ تاہم، ہر جملے میں ٹوکنز کی تعداد مختلف ہوتی ہے۔ اگر ہم `TextVectorization` لیئر کو ایک انفرادی ان پٹ پر لاگو کریں، تو واپس کیے گئے ٹوکنز کی تعداد مختلف ہوگی، اس بات پر منحصر ہے کہ متن کو کیسے ٹوکنائز کیا گیا ہے:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 1 45], shape=(2,), dtype=int64)\n",
      "tf.Tensor([ 112 1271    1    3 1747  158], shape=(6,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer('Hello, world!'))\n",
    "print(vectorizer('I am glad to meet you!'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "تاہم، جب ہم ویکٹرائزر کو کئی سلسلوں پر لاگو کرتے ہیں، تو اسے مستطیل شکل کا ٹینسر پیدا کرنا ہوتا ہے، اس لیے یہ غیر استعمال شدہ عناصر کو پیڈ ٹوکن (جو ہمارے معاملے میں صفر ہے) سے بھر دیتا ہے۔\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 6), dtype=int64, numpy=\n",
       "array([[   1,   45,    0,    0,    0,    0],\n",
       "       [ 112, 1271,    1,    3, 1747,  158]], dtype=int64)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer(['Hello, world!','I am glad to meet you!'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "یہاں ہم ایمبیڈنگز دیکھ سکتے ہیں:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 1.53059261e-02,  6.80514947e-02,  3.14026810e-02, ...,\n",
       "         -8.92002955e-02,  1.52911525e-04, -5.65562584e-02],\n",
       "        [ 2.57456154e-01,  2.79364467e-01, -2.03605562e-01, ...,\n",
       "         -2.07474351e-01,  8.31158683e-02, -2.03911960e-01],\n",
       "        [ 3.98201384e-02, -8.03454965e-03,  2.39790026e-02, ...,\n",
       "         -7.18549127e-04,  2.66963355e-02, -4.30646613e-02],\n",
       "        [ 3.98201384e-02, -8.03454965e-03,  2.39790026e-02, ...,\n",
       "         -7.18549127e-04,  2.66963355e-02, -4.30646613e-02],\n",
       "        [ 3.98201384e-02, -8.03454965e-03,  2.39790026e-02, ...,\n",
       "         -7.18549127e-04,  2.66963355e-02, -4.30646613e-02],\n",
       "        [ 3.98201384e-02, -8.03454965e-03,  2.39790026e-02, ...,\n",
       "         -7.18549127e-04,  2.66963355e-02, -4.30646613e-02]],\n",
       "\n",
       "       [[ 1.89674050e-01,  2.61548996e-01, -3.67433839e-02, ...,\n",
       "         -2.07366899e-01, -1.05442435e-01, -2.36952081e-01],\n",
       "        [ 6.16133213e-02,  1.80511594e-01,  9.77298319e-02, ...,\n",
       "         -5.46628237e-02, -1.07340455e-01, -1.06589928e-01],\n",
       "        [ 1.53059261e-02,  6.80514947e-02,  3.14026810e-02, ...,\n",
       "         -8.92002955e-02,  1.52911525e-04, -5.65562584e-02],\n",
       "        [-4.84890305e-02, -8.41715634e-02,  1.51529670e-01, ...,\n",
       "          1.28192469e-01, -7.77286515e-02,  1.26041949e-01],\n",
       "        [-4.17212099e-02, -5.60694858e-02,  4.08860669e-02, ...,\n",
       "          8.70475471e-02,  8.92383084e-02,  1.67974353e-01],\n",
       "        [ 2.85779923e-01,  4.57767487e-01,  4.52292450e-02, ...,\n",
       "         -1.97419018e-01, -2.04659685e-01, -2.79758364e-01]]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[1](vectorizer(['Hello, world!','I am glad to meet you!'])).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **نوٹ**: پیڈنگ کی مقدار کو کم کرنے کے لیے، بعض صورتوں میں یہ مناسب ہوتا ہے کہ ڈیٹا سیٹ میں تمام سلسلوں کو بڑھتی ہوئی لمبائی (یا زیادہ درست طور پر، ٹوکنز کی تعداد) کے ترتیب میں رکھا جائے۔ یہ یقینی بنائے گا کہ ہر منی بیچ میں ایک جیسی لمبائی کے سلسلے شامل ہوں۔\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## سیمینٹک ایمبیڈنگز: ورڈ2ویک\n",
    "\n",
    "پچھلی مثال میں، ایمبیڈنگ لیئر نے الفاظ کو ویکٹر ریپریزنٹیشنز میں میپ کرنا سیکھا، لیکن ان ریپریزنٹیشنز میں سیمینٹک معنی نہیں تھے۔ یہ بہتر ہوگا کہ ایک ایسا ویکٹر ریپریزنٹیشن سیکھا جائے جس میں مشابہ الفاظ یا مترادفات ایسے ویکٹرز کے قریب ہوں جو کسی ویکٹر فاصلے (مثلاً یُوکلیڈین فاصلے) کے لحاظ سے قریب ہوں۔\n",
    "\n",
    "اس مقصد کے لیے، ہمیں اپنے ایمبیڈنگ ماڈل کو ایک بڑی ٹیکسٹ کلیکشن پر پہلے سے تربیت دینا ہوگا، جیسے کہ [Word2Vec](https://en.wikipedia.org/wiki/Word2vec) تکنیک کا استعمال کرتے ہوئے۔ یہ دو بنیادی آرکیٹیکچرز پر مبنی ہے جو الفاظ کی ڈسٹریبیوٹڈ ریپریزنٹیشن پیدا کرنے کے لیے استعمال ہوتے ہیں:\n",
    "\n",
    " - **کنٹینیوس بیگ آف ورڈز** (CBoW)، جہاں ماڈل کو ارد گرد کے کانٹیکسٹ سے ایک لفظ کی پیش گوئی کرنے کے لیے تربیت دی جاتی ہے۔ دیے گئے این گرام $(W_{-2},W_{-1},W_0,W_1,W_2)$ میں، ماڈل کا مقصد $W_0$ کو $(W_{-2},W_{-1},W_1,W_2)$ سے پیش گوئی کرنا ہے۔\n",
    " - **کنٹینیوس اسکیپ-گرام** CBoW کے برعکس ہے۔ ماڈل موجودہ لفظ کی پیش گوئی کے لیے ارد گرد کے کانٹیکسٹ الفاظ کی ونڈو استعمال کرتا ہے۔\n",
    "\n",
    "CBoW تیز ہے، جبکہ اسکیپ-گرام سست ہے، لیکن یہ کم استعمال ہونے والے الفاظ کی بہتر نمائندگی کرتا ہے۔\n",
    "\n",
    "![تصویر جو CBoW اور اسکیپ-گرام الگورتھمز کو الفاظ کو ویکٹرز میں تبدیل کرنے کے لیے دکھا رہی ہے۔](../../../../../translated_images/ur/example-algorithms-for-converting-words-to-vectors.fbe9207a726922f6.webp)\n",
    "\n",
    "گوگل نیوز ڈیٹاسیٹ پر پری ٹرینڈ Word2Vec ایمبیڈنگ کے ساتھ تجربہ کرنے کے لیے، ہم **gensim** لائبریری استعمال کر سکتے ہیں۔ نیچے ہم 'neural' کے سب سے مشابہ الفاظ تلاش کرتے ہیں۔\n",
    "\n",
    "> **نوٹ:** جب آپ پہلی بار ویکٹرز بناتے ہیں، تو انہیں ڈاؤنلوڈ کرنے میں کچھ وقت لگ سکتا ہے!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "w2v = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neuronal -> 0.7804799675941467\n",
      "neurons -> 0.7326500415802002\n",
      "neural_circuits -> 0.7252851724624634\n",
      "neuron -> 0.7174385190010071\n",
      "cortical -> 0.6941086649894714\n",
      "brain_circuitry -> 0.6923246383666992\n",
      "synaptic -> 0.6699118614196777\n",
      "neural_circuitry -> 0.6638563275337219\n",
      "neurochemical -> 0.6555314064025879\n",
      "neuronal_activity -> 0.6531826257705688\n"
     ]
    }
   ],
   "source": [
    "for w,p in w2v.most_similar('neural'):\n",
    "    print(f\"{w} -> {p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ہم لفظ سے ویکٹر ایمبیڈنگ بھی نکال سکتے ہیں، جسے درجہ بندی ماڈل کی تربیت میں استعمال کیا جا سکتا ہے۔ ایمبیڈنگ میں 300 اجزاء ہوتے ہیں، لیکن یہاں وضاحت کے لیے ہم صرف ویکٹر کے پہلے 20 اجزاء دکھاتے ہیں:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01226807,  0.06225586,  0.10693359,  0.05810547,  0.23828125,\n",
       "        0.03686523,  0.05151367, -0.20703125,  0.01989746,  0.10058594,\n",
       "       -0.03759766, -0.1015625 , -0.15820312, -0.08105469, -0.0390625 ,\n",
       "       -0.05053711,  0.16015625,  0.2578125 ,  0.10058594, -0.25976562],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v['play'][:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "زبردست بات سیمینٹک ایمبیڈنگز کے بارے میں یہ ہے کہ آپ سیمینٹکس کی بنیاد پر ویکٹر انکوڈنگ کو تبدیل کر سکتے ہیں۔ مثال کے طور پر، ہم ایک ایسا لفظ تلاش کرنے کے لئے کہہ سکتے ہیں جس کی ویکٹر نمائندگی *king* اور *woman* کے الفاظ کے جتنا قریب ہو سکے، اور *man* کے لفظ سے جتنا دور ہو سکے:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('queen', 0.7118192911148071)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar(positive=['king','woman'],negative=['man'])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "ایک مثال اوپر کچھ اندرونی GenSym جادو استعمال کرتی ہے، لیکن بنیادی منطق حقیقت میں کافی سادہ ہے۔ ایمبیڈنگز کے بارے میں ایک دلچسپ بات یہ ہے کہ آپ ایمبیڈنگ ویکٹرز پر عام ویکٹر آپریشنز انجام دے سکتے ہیں، اور یہ آپریشنز الفاظ کے **معانی** پر اثر انداز ہوں گے۔ اوپر دی گئی مثال کو ویکٹر آپریشنز کی شرائط میں بیان کیا جا سکتا ہے: ہم **KING-MAN+WOMAN** کے مطابق ویکٹر کا حساب لگاتے ہیں (آپریشنز `+` اور `-` متعلقہ الفاظ کے ویکٹر نمائندگیوں پر انجام دیے جاتے ہیں)، اور پھر اس ویکٹر کے قریب ترین لفظ کو لغت میں تلاش کرتے ہیں۔\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'queen'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the vector corresponding to kind-man+woman\n",
    "qvec = w2v['king']-1.7*w2v['man']+1.7*w2v['woman']\n",
    "# find the index of the closest embedding vector \n",
    "d = np.sum((w2v.vectors-qvec)**2,axis=1)\n",
    "min_idx = np.argmin(d)\n",
    "# find the corresponding word\n",
    "w2v.index_to_key[min_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **نوٹ**: ہمیں *مرد* اور *عورت* کے ویکٹرز میں ایک چھوٹا سا عدد شامل کرنا پڑا - اسے ہٹا کر دیکھیں کہ کیا ہوتا ہے۔\n",
    "\n",
    "قریب ترین ویکٹر تلاش کرنے کے لیے، ہم TensorFlow کے آلات استعمال کرتے ہیں تاکہ ہمارے ویکٹر اور لغت میں موجود تمام ویکٹرز کے درمیان فاصلوں کا ایک ویکٹر حساب کریں، اور پھر `argmin` کا استعمال کرتے ہوئے کم سے کم لفظ کا انڈیکس تلاش کریں۔\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "جبکہ Word2Vec الفاظ کے معنی کو ظاہر کرنے کا ایک بہترین طریقہ لگتا ہے، اس کے کئی نقصانات ہیں، جن میں شامل ہیں:\n",
    "\n",
    "* CBoW اور skip-gram ماڈلز دونوں **predictive embeddings** ہیں، اور یہ صرف مقامی سیاق و سباق کو مدنظر رکھتے ہیں۔ Word2Vec عالمی سیاق و سباق کا فائدہ نہیں اٹھاتا۔\n",
    "* Word2Vec الفاظ کی **morphology** کو مدنظر نہیں رکھتا، یعنی یہ حقیقت کہ کسی لفظ کا مطلب اس کے مختلف حصوں، جیسے جڑ، پر منحصر ہو سکتا ہے۔\n",
    "\n",
    "**FastText** دوسرے نقصان کو دور کرنے کی کوشش کرتا ہے اور Word2Vec پر مبنی ہے، ہر لفظ اور اس کے اندر موجود کردار n-grams کے لیے ویکٹر نمائندگی سیکھ کر۔ ان نمائندگیوں کی قدریں ہر تربیتی مرحلے پر ایک ویکٹر میں اوسط کی جاتی ہیں۔ اگرچہ یہ پری ٹریننگ میں اضافی حساب کتاب شامل کرتا ہے، یہ لفظ embeddings کو ذیلی لفظ معلومات کو انکوڈ کرنے کے قابل بناتا ہے۔\n",
    "\n",
    "ایک اور طریقہ، **GloVe**، لفظ embeddings کے لیے ایک مختلف نقطہ نظر استعمال کرتا ہے، جو لفظ-سیاق و سباق میٹرکس کی factorization پر مبنی ہے۔ پہلے، یہ ایک بڑا میٹرکس بناتا ہے جو مختلف سیاق و سباق میں لفظ کے وقوعات کی تعداد شمار کرتا ہے، اور پھر یہ کوشش کرتا ہے کہ اس میٹرکس کو کم جہتوں میں اس طرح ظاہر کرے کہ reconstruction loss کم سے کم ہو۔\n",
    "\n",
    "gensim لائبریری ان لفظ embeddings کی حمایت کرتی ہے، اور آپ اوپر دیے گئے ماڈل لوڈنگ کوڈ کو تبدیل کرکے ان کے ساتھ تجربہ کر سکتے ہیں۔\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## کیراس میں پہلے سے تربیت یافتہ ایمبیڈنگز کا استعمال\n",
    "\n",
    "ہم اوپر دیے گئے مثال کو اس طرح تبدیل کر سکتے ہیں کہ اپنی ایمبیڈنگ لیئر کے میٹرکس کو معنوی ایمبیڈنگز، جیسے کہ Word2Vec، سے پہلے سے بھر دیں۔ پہلے سے تربیت یافتہ ایمبیڈنگ اور ٹیکسٹ کارپس کے ذخیرہ الفاظ ممکنہ طور پر ایک جیسے نہیں ہوں گے، اس لیے ہمیں ایک کا انتخاب کرنا ہوگا۔ یہاں ہم دو ممکنہ اختیارات کا جائزہ لیتے ہیں: ٹوکنائزر کے ذخیرہ الفاظ کا استعمال، اور Word2Vec ایمبیڈنگز کے ذخیرہ الفاظ کا استعمال۔\n",
    "\n",
    "### ٹوکنائزر کے ذخیرہ الفاظ کا استعمال\n",
    "\n",
    "جب ٹوکنائزر کے ذخیرہ الفاظ کا استعمال کیا جاتا ہے، تو ذخیرہ الفاظ کے کچھ الفاظ کے لیے Word2Vec ایمبیڈنگز موجود ہوں گی، اور کچھ غائب ہوں گے۔ فرض کریں کہ ہمارے ذخیرہ الفاظ کا سائز `vocab_size` ہے، اور Word2Vec ایمبیڈنگ ویکٹر کی لمبائی `embed_size` ہے، تو ایمبیڈنگ لیئر ایک وزن میٹرکس کی شکل میں ظاہر ہوگی جس کا سائز `vocab_size`$\\times$`embed_size` ہوگا۔ ہم اس میٹرکس کو ذخیرہ الفاظ کے ذریعے بھر کر مکمل کریں گے:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding size: 300\n",
      "Populating matrix, this will take some time...Done, found 4551 words, 784 words missing\n"
     ]
    }
   ],
   "source": [
    "embed_size = len(w2v.get_vector('hello'))\n",
    "print(f'Embedding size: {embed_size}')\n",
    "\n",
    "vocab = vectorizer.get_vocabulary()\n",
    "W = np.zeros((vocab_size,embed_size))\n",
    "print('Populating matrix, this will take some time...',end='')\n",
    "found, not_found = 0,0\n",
    "for i,w in enumerate(vocab):\n",
    "    try:\n",
    "        W[i] = w2v.get_vector(w)\n",
    "        found+=1\n",
    "    except:\n",
    "        # W[i] = np.random.normal(0.0,0.3,size=(embed_size,))\n",
    "        not_found+=1\n",
    "\n",
    "print(f\"Done, found {found} words, {not_found} words missing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ورڈ2ویک کے ذخیرہ الفاظ میں موجود نہ ہونے والے الفاظ کے لیے، ہم یا تو انہیں صفر کے طور پر چھوڑ سکتے ہیں، یا ایک تصادفی ویکٹر بنا سکتے ہیں۔\n",
    "\n",
    "اب ہم پری ٹرینڈ وزن کے ساتھ ایک ایمبیڈنگ لیئر تعریف کر سکتے ہیں:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = keras.layers.Embedding(vocab_size,embed_size,weights=[W],trainable=False)\n",
    "model = keras.models.Sequential([\n",
    "    vectorizer, emb,\n",
    "    keras.layers.Lambda(lambda x: tf.reduce_mean(x,axis=1)),\n",
    "    keras.layers.Dense(4, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 10s 10ms/step - loss: 1.1075 - acc: 0.7822 - val_loss: 0.9134 - val_acc: 0.8175\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2220226ef10>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'])\n",
    "model.fit(ds_train.map(tupelize).batch(batch_size),\n",
    "          validation_data=ds_test.map(tupelize).batch(batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **نوٹ**: دھیان دیں کہ ہم نے `Embedding` بناتے وقت `trainable=False` سیٹ کیا ہے، جس کا مطلب ہے کہ ہم Embedding لیئر کو دوبارہ تربیت نہیں دے رہے ہیں۔ اس کی وجہ سے درستگی تھوڑی کم ہو سکتی ہے، لیکن یہ تربیت کے عمل کو تیز کر دیتا ہے۔\n",
    "\n",
    "### ایمبیڈنگ کے ذخیرہ الفاظ کا استعمال\n",
    "\n",
    "پچھلے طریقے کے ساتھ ایک مسئلہ یہ ہے کہ TextVectorization اور Embedding میں استعمال ہونے والے ذخیرہ الفاظ مختلف ہیں۔ اس مسئلے کو حل کرنے کے لیے، ہم درج ذیل میں سے کوئی ایک حل استعمال کر سکتے ہیں:\n",
    "* Word2Vec ماڈل کو ہمارے ذخیرہ الفاظ پر دوبارہ تربیت دیں۔\n",
    "* ہمارے ڈیٹا سیٹ کو پہلے سے تربیت یافتہ Word2Vec ماڈل کے ذخیرہ الفاظ کے ساتھ لوڈ کریں۔ ڈیٹا سیٹ کو لوڈ کرتے وقت استعمال ہونے والے ذخیرہ الفاظ کو لوڈنگ کے دوران مخصوص کیا جا سکتا ہے۔\n",
    "\n",
    "دوسرا طریقہ زیادہ آسان لگتا ہے، تو آئیے اسے نافذ کرتے ہیں۔ سب سے پہلے، ہم Word2Vec ایمبیڈنگ سے لیے گئے مخصوص ذخیرہ الفاظ کے ساتھ ایک `TextVectorization` لیئر بنائیں گے:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(w2v.vocab.keys())\n",
    "vectorizer = keras.layers.experimental.preprocessing.TextVectorization(input_shape=(1,))\n",
    "vectorizer.set_vocabulary(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "لائبریری gensim لفظ embeddings میں ایک آسان فنکشن `get_keras_embeddings` شامل ہے، جو خود بخود آپ کے لئے متعلقہ Keras embeddings layer تخلیق کرے گا۔\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "938/938 [==============================] - 20s 14ms/step - loss: 1.3377 - acc: 0.4978 - val_loss: 1.2995 - val_acc: 0.5647\n",
      "Epoch 2/5\n",
      "938/938 [==============================] - 10s 10ms/step - loss: 1.2587 - acc: 0.5722 - val_loss: 1.2339 - val_acc: 0.5842\n",
      "Epoch 3/5\n",
      "938/938 [==============================] - 10s 10ms/step - loss: 1.1980 - acc: 0.5884 - val_loss: 1.1826 - val_acc: 0.5954\n",
      "Epoch 4/5\n",
      "938/938 [==============================] - 12s 13ms/step - loss: 1.1503 - acc: 0.6002 - val_loss: 1.1417 - val_acc: 0.6018\n",
      "Epoch 5/5\n",
      "938/938 [==============================] - 11s 12ms/step - loss: 1.1120 - acc: 0.6097 - val_loss: 1.1083 - val_acc: 0.6104\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2220ccb81c0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    vectorizer, \n",
    "    w2v.get_keras_embedding(train_embeddings=False),\n",
    "    keras.layers.Lambda(lambda x: tf.reduce_mean(x,axis=1)),\n",
    "    keras.layers.Dense(4, activation='softmax')\n",
    "])\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'])\n",
    "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128),epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ہماری ڈیٹاسیٹ کے کچھ الفاظ پری ٹرینڈ گلوو وکیبلری میں موجود نہیں ہیں، اور اسی وجہ سے انہیں بنیادی طور پر نظرانداز کیا جاتا ہے۔ اس مسئلے کو حل کرنے کے لیے، ہم اپنی ڈیٹاسیٹ کی بنیاد پر اپنی ایمبیڈنگز کو ٹرین کر سکتے ہیں۔\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## سیاقی ایمبیڈنگز\n",
    "\n",
    "روایتی پری ٹرینڈ ایمبیڈنگز جیسے Word2Vec کی ایک اہم حد یہ ہے کہ، اگرچہ وہ کسی لفظ کے کچھ معنی کو سمجھ سکتے ہیں، وہ مختلف معنوں کے درمیان فرق نہیں کر سکتے۔ یہ نیچے والے ماڈلز میں مسائل پیدا کر سکتا ہے۔\n",
    "\n",
    "مثال کے طور پر، لفظ 'play' کے مختلف معنی ہیں ان دو جملوں میں:\n",
    "- میں تھیٹر میں ایک **play** دیکھنے گیا۔\n",
    "- جان اپنے دوستوں کے ساتھ **play** کرنا چاہتا ہے۔\n",
    "\n",
    "ہم نے جن پری ٹرینڈ ایمبیڈنگز کی بات کی وہ لفظ 'play' کے دونوں معنوں کو ایک ہی ایمبیڈنگ میں ظاہر کرتی ہیں۔ اس حد کو دور کرنے کے لیے، ہمیں **زبان ماڈل** کی بنیاد پر ایمبیڈنگز بنانی ہوں گی، جو ایک بڑے متن کے مجموعے پر تربیت یافتہ ہوتا ہے اور *جانتا ہے* کہ الفاظ کو مختلف سیاق و سباق میں کیسے جوڑا جا سکتا ہے۔ سیاقی ایمبیڈنگز پر تفصیل سے بات کرنا اس سبق کے دائرہ کار سے باہر ہے، لیکن ہم ان پر واپس آئیں گے جب اگلے یونٹ میں زبان ماڈلز پر بات کریں گے۔\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**ڈسکلیمر**:  \nیہ دستاویز AI ترجمہ سروس [Co-op Translator](https://github.com/Azure/co-op-translator) کا استعمال کرتے ہوئے ترجمہ کی گئی ہے۔ ہم درستگی کے لیے کوشش کرتے ہیں، لیکن براہ کرم آگاہ رہیں کہ خودکار ترجمے میں غلطیاں یا غیر درستیاں ہو سکتی ہیں۔ اصل دستاویز کو اس کی اصل زبان میں مستند ذریعہ سمجھا جانا چاہیے۔ اہم معلومات کے لیے، پیشہ ور انسانی ترجمہ کی سفارش کی جاتی ہے۔ ہم اس ترجمے کے استعمال سے پیدا ہونے والی کسی بھی غلط فہمی یا غلط تشریح کے ذمہ دار نہیں ہیں۔\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0cb620c6d4b9f7a635928804c26cf22403d89d98d79684e4529119355ee6d5a5"
  },
  "kernel_info": {
   "name": "conda-env-py37_tensorflow-py"
  },
  "kernelspec": {
   "display_name": "py37_tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "coopTranslator": {
   "original_hash": "b859482be7f61d1eadc2c6a2720a37e4",
   "translation_date": "2025-08-28T04:30:22+00:00",
   "source_file": "lessons/5-NLP/14-Embeddings/EmbeddingsTF.ipynb",
   "language_code": "ur"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}