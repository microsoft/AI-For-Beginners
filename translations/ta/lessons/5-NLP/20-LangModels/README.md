# முன்பே பயிற்சி பெற்ற பெரிய மொழி மாதிரிகள்

முந்தைய பணிகளில், குறிப்பிட்ட பணியைச் செய்ய நரம்பியல் வலையமைப்பை லேபிள் செய்யப்பட்ட தரவுத்தொகுப்பைப் பயன்படுத்தி பயிற்சி செய்தோம். BERT போன்ற பெரிய டிரான்ஸ்ஃபார்மர் மாதிரிகளைப் பயன்படுத்தி, மொழி மாதிரியை உருவாக்க சுய-மேற்பார்வை முறையில் மொழி மாதிரியாக்கத்தைப் பயன்படுத்துகிறோம், பின்னர் குறிப்பிட்ட துறைக்கு உரிய பயிற்சியுடன் குறிப்பிட்ட பணிக்காக சிறப்பம்சமாக்கப்படுகிறது. ஆனால், பெரிய மொழி மாதிரிகள் எந்த துறைக்கு உரிய பயிற்சியின்றியும் பல பணிகளைத் தீர்க்க முடியும் என்று நிரூபிக்கப்பட்டுள்ளது. இதைச் செய்யும் திறன் கொண்ட மாதிரிகளின் குடும்பம் **GPT**: Generative Pre-Trained Transformer என்று அழைக்கப்படுகிறது.

## [முன்-வகுப்பு வினாடி வினா](https://ff-quizzes.netlify.app/en/ai/quiz/39)

## உரை உருவாக்கம் மற்றும் Perplexity

கீழ்மட்ட பயிற்சியின்றி பொதுப் பணிகளைச் செய்யும் நரம்பியல் வலையமைப்பின் யோசனை [Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) என்ற ஆவணத்தில் முன்வைக்கப்பட்டுள்ளது. முக்கிய யோசனை என்னவென்றால், பல பணிகளை **உரை உருவாக்கம்** மூலம் மாதிரியாக்க முடியும், ஏனெனில் உரையைப் புரிந்துகொள்வது அதைப் உருவாக்கும் திறனை கொண்டிருப்பதைக் குறிக்கிறது. மாதிரி மனித அறிவை உள்ளடக்கிய மிகப்பெரிய உரை தொகுப்பில் பயிற்சி பெறுவதால், இது பல்வேறு தலைப்புகள் பற்றிய அறிவை கொண்டதாகவும் மாறுகிறது.

> உரையைப் புரிந்துகொள்வதும் அதை உருவாக்கும் திறனை கொண்டிருப்பதும் உலகத்தைப் பற்றிய சிலவற்றை அறிந்திருப்பதையும் குறிக்கிறது. மனிதர்கள் பெரும்பாலும் வாசிப்பதன் மூலம் கற்றுக்கொள்கிறார்கள், GPT வலையமைப்பும் இதேபோன்றது.

உரை உருவாக்க வலையமைப்புகள் $$P(w_N)$$ என்ற அடுத்த வார்த்தையின் சாத்தியத்தை கணிக்கின்றன. ஆனால், அடுத்த வார்த்தையின் நிபந்தனையற்ற சாத்தியம் உரை தொகுப்பில் அந்த வார்த்தையின் அடிக்கடி தோன்றும் அளவுக்கு சமமாக இருக்கும். GPT, முந்தைய வார்த்தைகளைத் தரப்பட்ட நிலையில் அடுத்த வார்த்தையின் **நிபந்தனை சாத்தியத்தை** வழங்குகிறது: $$P(w_N | w_{n-1}, ..., w_0)$$

> சாத்தியங்களைப் பற்றிய மேலும் தகவல்களை [Data Science for Beginners Curriculum](https://github.com/microsoft/Data-Science-For-Beginners/tree/main/1-Introduction/04-stats-and-probability) இல் படிக்கலாம்.

மொழி உருவாக்க மாதிரியின் தரத்தை **perplexity** மூலம் வரையறுக்கலாம். இது எந்த பணிக்குறிப்பிட்ட தரவுத்தொகுப்பின்றியும் மாதிரியின் தரத்தை அளவிட உதவும் உள்ளக அளவீடு. இது *ஒரு வாக்கியத்தின் சாத்தியம்* என்ற கருத்தின் அடிப்படையில் உள்ளது - மாதிரி உண்மையானதாக இருக்கக்கூடிய ஒரு வாக்கியத்திற்கு அதிக சாத்தியத்தை அளிக்கிறது (அதாவது, மாதிரி அதனால் **perplexed** ஆகவில்லை), மற்றும் குறைவாக அர்த்தமுள்ள வாக்கியங்களுக்கு குறைந்த சாத்தியத்தை அளிக்கிறது (எ.கா. *Can it does what?*). மாதிரிக்கு உண்மையான உரை தொகுப்பிலிருந்து வாக்கியங்களை வழங்கும்போது, அவை அதிக சாத்தியத்தை, மற்றும் குறைந்த **perplexity** ஐ கொண்டிருக்க வேண்டும் என்று எதிர்பார்க்கிறோம். கணித ரீதியாக, இது சோதனை தொகுப்பின் சாத்தியத்தின் மாறுபட்ட எதிர்மாறான மதிப்பாக வரையறுக்கப்படுகிறது:
$$
\mathrm{Perplexity}(W) = \sqrt[N]{1\over P(W_1,...,W_N)}
$$ 

**[Hugging Face](https://transformer.huggingface.co/doc/gpt2-large) இல் GPT-இயக்கப்பட்ட உரை தொகுப்பை பயன்படுத்தி உரை உருவாக்கத்தை பரிசோதிக்கலாம்**. இந்த தொகுப்பில், உங்கள் உரையை எழுதத் தொடங்குங்கள், மற்றும் **[TAB]** அழுத்துவதன் மூலம் பல முடிவு விருப்பங்களைப் பெறலாம். அவை மிகவும் குறுகியதாக இருந்தால், அல்லது நீங்கள் திருப்தியடையவில்லை என்றால் - [TAB] ஐ மீண்டும் அழுத்தவும், மேலும் விருப்பங்களைப் பெறலாம், அதில் நீண்ட உரை துண்டுகளும் அடங்கும்.

## GPT என்பது ஒரு குடும்பம்

GPT என்பது ஒரு மாதிரி மட்டுமல்ல, மாறாக [OpenAI](https://openai.com) மூலம் உருவாக்கப்பட்ட மற்றும் பயிற்சி பெற்ற மாதிரிகளின் தொகுப்பாகும்.

GPT மாதிரிகளின் கீழ், நமக்கு உள்ளவை:

| [GPT-2](https://huggingface.co/docs/transformers/model_doc/gpt2#openai-gpt2) | [GPT 3](https://openai.com/research/language-models-are-few-shot-learners) | [GPT-4](https://openai.com/gpt-4) |
| -- | -- | -- |
|1.5 பில்லியன் அளவுகோல்களுடன் மொழி மாதிரி | 175 பில்லியன் அளவுகோல்களுடன் மொழி மாதிரி | 100T அளவுகோல்கள் மற்றும் படங்கள் மற்றும் உரை உள்ளீடுகளை ஏற்கிறது மற்றும் உரை வெளியீடுகளை வழங்குகிறது. |

GPT-3 மற்றும் GPT-4 மாதிரிகள் [Microsoft Azure இல் ஒரு cognitive service ஆக](https://azure.microsoft.com/en-us/services/cognitive-services/openai-service/#overview?WT.mc_id=academic-77998-cacaste) மற்றும் [OpenAI API](https://openai.com/api/) ஆக கிடைக்கின்றன.

## Prompt Engineering

GPT மிகப்பெரிய அளவிலான தரவுகளில் மொழி மற்றும் குறியீட்டை புரிந்துகொள்ள பயிற்சி பெற்றதால், அவை உள்ளீடுகளுக்கு பதிலளிக்கின்றன (prompts). Prompts என்பது GPT இன் உள்ளீடுகள் அல்லது கேள்விகள், இதில் ஒருவர் மாதிரிகளுக்கு பணிகளை முடிக்க வழிகாட்டுதல்களை வழங்குகிறார். விரும்பிய முடிவை பெற, மிகச் சிறந்த prompt தேவைப்படுகிறது, இது சரியான வார்த்தைகள், வடிவங்கள், சொற்றொடர்கள் அல்லது கூட சின்னங்களைத் தேர்ந்தெடுப்பதை உள்ளடக்கியது. இந்த அணுகுமுறை [Prompt Engineering](https://learn.microsoft.com/en-us/shows/ai-show/the-basics-of-prompt-engineering-with-azure-openai-service?WT.mc_id=academic-77998-bethanycheum) என்று அழைக்கப்படுகிறது.

[இந்த ஆவணம்](https://learn.microsoft.com/en-us/semantic-kernel/prompt-engineering/?WT.mc_id=academic-77998-bethanycheum) prompt engineering பற்றிய மேலும் தகவல்களை வழங்குகிறது.

## ✍️ உதாரண நோட்புக்: [OpenAI-GPT உடன் விளையாடுதல்](GPT-PyTorch.ipynb)

பின்வரும் நோட்புக்குகளில் உங்கள் கற்றலை தொடருங்கள்:

* [OpenAI-GPT மற்றும் Hugging Face Transformers உடன் உரை உருவாக்கம்](GPT-PyTorch.ipynb)

## முடிவு

புதிய பொதுப் பயிற்சி பெற்ற மொழி மாதிரிகள் மொழி அமைப்பை மாதிரியாக்குவதுடன் மட்டுமல்லாமல், இயற்கை மொழியின் மிகப்பெரிய அளவையும் கொண்டுள்ளன. எனவே, அவை சில NLP பணிகளை zero-shot அல்லது few-shot அமைப்புகளில் திறமையாக பயன்படுத்தப்படலாம்.

## [பின்-வகுப்பு வினாடி வினா](https://ff-quizzes.netlify.app/en/ai/quiz/40)

---

**குறிப்பு**:  
இந்த ஆவணம் [Co-op Translator](https://github.com/Azure/co-op-translator) என்ற AI மொழிபெயர்ப்பு சேவையைப் பயன்படுத்தி மொழிபெயர்க்கப்பட்டுள்ளது. நாங்கள் துல்லியத்திற்காக முயற்சிக்கின்றோம், ஆனால் தானியங்கி மொழிபெயர்ப்புகளில் பிழைகள் அல்லது தவறான தகவல்கள் இருக்கக்கூடும் என்பதை தயவுசெய்து கவனத்தில் கொள்ளவும். அதன் தாய்மொழியில் உள்ள மூல ஆவணம் அதிகாரப்பூர்வ ஆதாரமாக கருதப்பட வேண்டும். முக்கியமான தகவல்களுக்கு, தொழில்முறை மனித மொழிபெயர்ப்பு பரிந்துரைக்கப்படுகிறது. இந்த மொழிபெயர்ப்பைப் பயன்படுத்துவதால் ஏற்படும் எந்த தவறான புரிதல்கள் அல்லது தவறான விளக்கங்களுக்கு நாங்கள் பொறுப்பல்ல.