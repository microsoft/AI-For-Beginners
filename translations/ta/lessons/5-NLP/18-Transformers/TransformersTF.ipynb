{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# கவனிப்பு முறைமைகள் மற்றும் டிரான்ஸ்ஃபார்மர்கள்\n",
    "\n",
    "மீளச்சுழற்சி நெட்வொர்க்குகளின் முக்கிய குறைபாடுகளில் ஒன்று, ஒரு வரிசையில் உள்ள அனைத்து சொற்களும் முடிவில் ஒரே அளவு தாக்கத்தை ஏற்படுத்தும். இது பெயரிடப்பட்ட பொருள் அடையாளம் மற்றும் மெஷின் மொழிபெயர்ப்பு போன்ற வரிசை-வரிசை பணிகளுக்கு வழக்கமான LSTM குறியாக்க-குறியாக்க மாடல்களுடன் குறைந்த செயல்திறனை ஏற்படுத்துகிறது. உண்மையில், உள்ளீட்டு வரிசையில் உள்ள குறிப்பிட்ட சொற்கள் மற்றவற்றை விட தொடர்ச்சியான வெளியீடுகளில் அதிக தாக்கத்தை ஏற்படுத்துகின்றன.\n",
    "\n",
    "மெஷின் மொழிபெயர்ப்பு போன்ற வரிசை-வரிசை மாடலை எடுத்துக்கொள்ளுங்கள். இது இரண்டு மீளச்சுழற்சி நெட்வொர்க்குகளால் செயல்படுத்தப்படுகிறது, இதில் ஒரு நெட்வொர்க் (**குறியாக்கி**) உள்ளீட்டு வரிசையை மறைமாநிலமாக சுருக்குகிறது, மற்றொன்று (**குறியாக்கி**) இந்த மறைமாநிலத்தை மொழிபெயர்க்கப்பட்ட முடிவாக விரிக்கிறது. இந்த அணுகுமுறையின் பிரச்சினை என்னவென்றால், நெட்வொர்க்கின் இறுதி நிலை ஒரு வாக்கியத்தின் தொடக்கத்தை நினைவில் வைத்துக்கொள்ள கடினமாக இருக்கும், இதனால் நீண்ட வாக்கியங்களில் மாடலின் தரம் குறைகிறது.\n",
    "\n",
    "**கவனிப்பு முறைமைகள்** RNN இன் ஒவ்வொரு வெளியீட்டு கணிப்பில் உள்ளீட்டு வெக்டரின் சூழலியல் தாக்கத்தை எடுக்கும் ஒரு வழியை வழங்குகின்றன. இது செயல்படுத்தப்படும் விதம் என்னவென்றால், உள்ளீட்டு RNN இன் இடைநிலை நிலைகளுக்கும் வெளியீட்டு RNN க்கும் இடையில் குறுக்குவழிகளை உருவாக்குவதன் மூலம். இந்த முறையில், $y_t$ என்ற வெளியீட்டு சின்னத்தை உருவாக்கும்போது, ​​வித்தியாசமான எடை குணகங்கள் $\\alpha_{t,i}$ உடன் அனைத்து உள்ளீட்டு மறைமாநிலங்களை $h_i$ கணக்கில் எடுத்துக்கொள்வோம்.\n",
    "\n",
    "![கூட்டல் கவனிப்பு அடுக்கு கொண்ட குறியாக்கி/குறியாக்கி மாடலைக் காட்டும் படம்](../../../../../translated_images/ta/encoder-decoder-attention.7a726296894fb567.webp)\n",
    "*[Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) இல் உள்ள கூட்டல் கவனிப்பு முறைமையுடன் குறியாக்கி-குறியாக்கி மாடல், [இந்த வலைப்பதிவு](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html) இடமிருந்து மேற்கோள்]*\n",
    "\n",
    "கவனிப்பு அட்டவணை $\\{\\alpha_{i,j}\\}$ ஒரு குறிப்பிட்ட உள்ளீட்டு சொற்கள் வெளியீட்டு வரிசையில் ஒரு கொடுக்கப்பட்ட சொல்லை உருவாக்குவதில் எந்த அளவுக்கு பங்கு வகிக்கின்றன என்பதை பிரதிநிதித்துவப்படுத்தும். கீழே அத்தகைய அட்டவணையின் உதாரணம் உள்ளது:\n",
    "\n",
    "![Bahdanau - arviz.org இல் இருந்து எடுத்துக்கொள்ளப்பட்ட RNNsearch-50 கண்ட ஒரு மாதAlignment-ஐக் காட்டும் படம்](../../../../../translated_images/ta/bahdanau-fig3.09ba2d37f202a6af.webp)\n",
    "\n",
    "*[Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) (Fig.3) இல் இருந்து எடுத்த படம்]*\n",
    "\n",
    "கவனிப்பு முறைமைகள் இயற்கை மொழி செயலாக்கத்தில் தற்போதைய அல்லது தற்போதைய நிலைமையை உருவாக்குவதற்கு பெரும்பாலும் பொறுப்பாக உள்ளன. கவனிப்பைச் சேர்ப்பது மாடல் அளவுருக்களின் எண்ணிக்கையை பெரிதும் அதிகரிக்கிறது, இது RNN களுடன் அளவீட்டு சிக்கல்களை ஏற்படுத்தியது. RNN களை அளவீடு செய்வதற்கான முக்கிய கட்டுப்பாடு என்னவென்றால், மாடல்களின் மீளச்சுழற்சி தன்மை பயிற்சியை தொகுதி மற்றும் இணைபார்க்க சிக்கலாக ஆக்குகிறது. RNN இல் ஒரு வரிசையின் ஒவ்வொரு கூறும் வரிசைப்படி செயல்படுத்தப்பட வேண்டும், இது எளிதாக இணைபார்க்க முடியாது என்பதைக் குறிக்கிறது.\n",
    "\n",
    "கவனிப்பு முறைமைகளை ஏற்றுக்கொள்வது மற்றும் இந்த கட்டுப்பாட்டுடன் இணைந்து, இன்று நாம் BERT முதல் OpenGPT3 வரை அறிந்தும் பயன்படுத்தும் தற்போதைய நிலைமையை உருவாக்கும் டிரான்ஸ்ஃபார்மர் மாடல்களை உருவாக்கியது.\n",
    "\n",
    "## டிரான்ஸ்ஃபார்மர் மாடல்கள்\n",
    "\n",
    "ஒவ்வொரு முந்தைய கணிப்பின் சூழலை அடுத்த மதிப்பீட்டு படியில் அனுப்புவதற்கு பதிலாக, **டிரான்ஸ்ஃபார்மர் மாடல்கள்** **இடதிகதிகள் குறியீடுகள்** மற்றும் **கவனிப்பு** ஆகியவற்றைப் பயன்படுத்தி கொடுக்கப்பட்ட உர சாளரத்தில் உள்ள ஒரு கொடுக்கப்பட்ட உள்ளீட்டின் சூழலைப் பிடிக்கின்றன. கீழே உள்ள படம், கவனிப்புடன் இடதிகதிகள் குறியீடுகள் ஒரு கொடுக்கப்பட்ட சாளரத்தில் சூழலை எவ்வாறு பிடிக்க முடியும் என்பதை காட்டுகிறது.\n",
    "\n",
    "![டிரான்ஸ்ஃபார்மர் மாடல்களில் மதிப்பீடுகள் எவ்வாறு செய்யப்படுகின்றன என்பதை விளக்கும் அனிமேஷன் GIF.](../../../../../lessons/5-NLP/18-Transformers/images/transformer-animated-explanation.gif)\n",
    "\n",
    "ஒவ்வொரு உள்ளீட்டு நிலை ஒவ்வொரு வெளியீட்டு நிலைக்கு தனித்தனியாக வரைபடம் செய்யப்படுவதால், டிரான்ஸ்ஃபார்மர்கள் RNN களை விட சிறப்பாக இணைபார்க்க முடியும், இது மிகப்பெரிய மற்றும் வெளிப்படையான மொழி மாடல்களை இயக்குகிறது. ஒவ்வொரு கவனிப்பு தலைவும் சொற்களுக்கிடையேயான வித்தியாசமான உறவுகளை கற்றுக்கொள்வதற்கு பயன்படுத்தப்படலாம், இது கீழ்மட்ட இயற்கை மொழி செயலாக்க பணிகளை மேம்படுத்துகிறது.\n",
    "\n",
    "## எளிய டிரான்ஸ்ஃபார்மர் மாடலை உருவாக்குதல்\n",
    "\n",
    "Keras இல் உள்ளடக்கப்பட்ட டிரான்ஸ்ஃபார்மர் அடுக்கு இல்லை, ஆனால் நாங்கள் நம்முடையதை உருவாக்கலாம். முன்பை போலவே, AG News தரவுத்தொகுப்பின் உர வகைப்படுத்தலின் மீது கவனம் செலுத்துவோம், ஆனால் டிரான்ஸ்ஃபார்மர் மாடல்கள் மிகவும் கடினமான NLP பணிகளில் சிறந்த முடிவுகளை காட்டுகின்றன என்பதை குறிப்பிடுவது முக்கியம்.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "\n",
    "ds_train, ds_test = tfds.load('ag_news_subset').values()\n",
    "\n",
    "def extract_text(x):\n",
    "    return x['title']+' '+x['description']\n",
    "\n",
    "def tupelize(x):\n",
    "    return (extract_text(x),x['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "கேராஸ் இல் புதிய அடுக்குகள் `Layer` வகுப்பை துணை வகுப்பாகக் கொள்ள வேண்டும் மற்றும் `call` முறைமையை செயல்படுத்த வேண்டும். **நிலையமைவு எம்பெடிங்** அடுக்குடன் தொடங்குவோம். [கேராஸ் அதிகாரப்பூர்வ ஆவணங்களில் உள்ள சில குறியீடுகளை](https://keras.io/examples/nlp/text_classification_with_transformer/) பயன்படுத்துவோம். அனைத்து உள்ளீட்டு வரிசைகளையும் `maxlen` நீளத்திற்கு பதம் செய்கிறோம் என்று கருதுவோம்.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(keras.layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = keras.layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "        self.maxlen = maxlen\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = self.maxlen\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x+positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "இந்த அடுக்கு இரண்டு `Embedding` அடுக்குகளை கொண்டுள்ளது: டோக்கன்களை எம்பெடிங் செய்ய (முன்னதாக நாம் விவாதித்த முறையில்) மற்றும் டோக்கன் இடங்களை. டோக்கன் இடங்கள் `tf.range` பயன்படுத்தி 0 முதல் `maxlen` வரை இயற்கை எண்களின் வரிசையாக உருவாக்கப்படுகின்றன, பின்னர் எம்பெடிங் அடுக்கில் அனுப்பப்படுகின்றன. இரண்டு எம்பெடிங் வெக்டர்கள் சேர்க்கப்பட்டு, உள்ளீட்டின் இடமாற்ற எம்பெடிங் பிரதிநிதித்துவத்தை உருவாக்குகின்றன, இதன் வடிவம் `maxlen`$\\times$`embed_dim`.\n",
    "\n",
    "<img src=\"../../../../../translated_images/ta/pos-embedding.e41ce9b6cf6078af.webp\" width=\"40%\"/>\n",
    "\n",
    "இப்போது, டிரான்ஸ்ஃபார்மர் பிளாக்கை செயல்படுத்துவோம். இது முன்பு வரையறுக்கப்பட்ட எம்பெடிங் அடுக்கின் வெளியீட்டை எடுக்கும்:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim, name='attn')\n",
    "        self.ffn = keras.Sequential(\n",
    "            [keras.layers.Dense(ff_dim, activation=\"relu\"), keras.layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = keras.layers.Dropout(rate)\n",
    "        self.dropout2 = keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer `MultiHeadAttention`-ஐ நிலை குறிக்கப்பட்ட உள்ளீட்டில் பயன்படுத்தி, `maxlen`$\\times$`embed_dim` பரிமாணத்துடன் கவனத்தை உருவாக்குகிறது, இது பின்னர் உள்ளீட்டுடன் கலக்கப்பட்டு `LayerNormalization` மூலம் சீரமைக்கப்படுகிறது.\n",
    "\n",
    "> **குறிப்பு**: `LayerNormalization` என்பது *கணினி பார்வை* பகுதியின் போது விவாதிக்கப்பட்ட `BatchNormalization` போன்றது, ஆனால் இது ஒவ்வொரு பயிற்சி மாதிரிக்கும் முந்தைய அடுக்கு வெளியீடுகளை தனித்தனியாக சீரமைக்கிறது, அவற்றை [-1..1] வரம்பிற்குள் கொண்டு வர.\n",
    "\n",
    "இந்த அடுக்கின் வெளியீடு பின்னர் `Dense` நெட்வொர்க்கில் (எங்கள் வழக்கில் - இரண்டு அடுக்கு perceptron) அனுப்பப்படுகிறது, மற்றும் முடிவில் உள்ள வெளியீட்டுடன் சேர்க்கப்படுகிறது (மீண்டும் சீரமைக்கப்படுகிறது).\n",
    "\n",
    "<img src=\"../../../../../translated_images/ta/transformer-layer.905e14747ca4e7d5.webp\" width=\"30%\" />\n",
    "\n",
    "இப்போது, முழுமையான transformer மாதிரியை வரையறுக்க தயாராக உள்ளோம்:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "text_vectorization (TextVect (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "token_and_position_embedding (None, 256, 32)           648192    \n",
      "_________________________________________________________________\n",
      "transformer_block (Transform (None, 256, 32)           10656     \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 20)                660       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 84        \n",
      "=================================================================\n",
      "Total params: 659,592\n",
      "Trainable params: 659,592\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 32  # Embedding size for each token\n",
    "num_heads = 2  # Number of attention heads\n",
    "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
    "maxlen = 256\n",
    "vocab_size = 20000\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.experimental.preprocessing.TextVectorization(max_tokens=vocab_size,output_sequence_length=maxlen, input_shape=(1,)),\n",
    "    TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim),\n",
    "    TransformerBlock(embed_dim, num_heads, ff_dim),\n",
    "    keras.layers.GlobalAveragePooling1D(),\n",
    "    keras.layers.Dropout(0.1),\n",
    "    keras.layers.Dense(20, activation=\"relu\"),\n",
    "    keras.layers.Dropout(0.1),\n",
    "    keras.layers.Dense(4, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokenizer\n",
      "938/938 [==============================] - 45s 39ms/step - loss: 0.4978 - acc: 0.8068 - val_loss: 0.2808 - val_acc: 0.9124\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9c2427a0d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Training tokenizer')\n",
    "model.layers[0].adapt(ds_train.map(extract_text))\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'], optimizer='adam')\n",
    "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT மாற்றி மாடல்கள்\n",
    "\n",
    "**BERT** (Bidirectional Encoder Representations from Transformers) என்பது மிகப்பெரிய, பல அடுக்கு மாற்றி நெட்வொர்க் ஆகும், இதில் *BERT-base* க்கு 12 அடுக்குகள் மற்றும் *BERT-large* க்கு 24 அடுக்குகள் உள்ளன. இந்த மாடல் முதலில் பெரிய உரை தரவுத்தொகுப்பில் (WikiPedia + புத்தகங்கள்) கண்காணிக்கப்படாத பயிற்சியை (ஒரு வாக்கியத்தில் மறைக்கப்பட்ட வார்த்தைகளை கணிக்க) பயன்படுத்தி முன்பயிற்சி செய்யப்படுகிறது. முன்பயிற்சியின் போது, மாடல் முக்கியமான மொழி புரிதலை உறிஞ்சுகிறது, இதை பிற தரவுத்தொகுப்புகளுடன் நன்றாகச் சீரமைத்து பயன்படுத்தலாம். இந்த செயல்முறை **மாற்றக் கற்றல்** என்று அழைக்கப்படுகிறது.\n",
    "\n",
    "![http://jalammar.github.io/illustrated-bert/ இல் இருந்து படம்](../../../../../translated_images/ta/jalammarBERT-language-modeling-masked-lm.34f113ea5fec4362.webp)\n",
    "\n",
    "BERT, DistilBERT, BigBird, OpenGPT3 மற்றும் பலவற்றைச் சேர்த்து நன்றாகச் சீரமைக்கக்கூடிய பல மாற்றி கட்டமைப்புகளின் மாறுபாடுகள் உள்ளன.\n",
    "\n",
    "முன்பயிற்சி செய்யப்பட்ட BERT மாடலை எவ்வாறு பயன்படுத்தி பாரம்பரிய வரிசை வகைப்படுத்தல் பிரச்சினையை தீர்க்கலாம் என்பதை பார்ப்போம். [அதிகாரப்பூர்வ ஆவணத்தில்](https://www.tensorflow.org/text/tutorials/classify_text_with_bert) இருந்து சில யோசனைகள் மற்றும் குறியீடுகளை எடுத்துக்கொள்வோம்.\n",
    "\n",
    "முன்பயிற்சி செய்யப்பட்ட மாடல்களை ஏற்ற, **Tensorflow hub** ஐ பயன்படுத்துவோம். முதலில், BERT-க்கு குறிப்பிட்ட வெக்டரைசரை ஏற்றுவோம்:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_41180/4216669875.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow_text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow_hub\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mhub\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhub\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mKerasLayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_text'"
     ]
    }
   ],
   "source": [
    "import tensorflow_text \n",
    "import tensorflow_hub as hub\n",
    "vectorizer = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_type_ids': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       " array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "       dtype=int32)>,\n",
       " 'input_word_ids': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       " array([[  101,  1045,  2293, 19081,   102,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]], dtype=int32)>,\n",
       " 'input_mask': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       " array([[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "       dtype=int32)>}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer(['I love transformers'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "மூல நெட்வொர்க்கில் பயிற்சி அளிக்கப்பட்ட அதே vectorizer-ஐ நீங்கள் பயன்படுத்துவது மிகவும் முக்கியம். மேலும், BERT vectorizer மூன்று கூறுகளை திருப்பி வழங்குகிறது:\n",
    "* `input_word_ids`, இது உள்ளீட்டு வாக்கியத்திற்கான டோக்கன் எண்களின் வரிசையாகும்\n",
    "* `input_mask`, இது வரிசையின் எந்த பகுதி உண்மையான உள்ளீட்டை கொண்டுள்ளது, மற்றும் எந்த பகுதி padding ஆகும் என்பதை காட்டுகிறது. இது `Masking` அடுக்கு உருவாக்கும் mask-க்கு ஒத்ததாக உள்ளது\n",
    "* `input_type_ids` மொழி மாதிரி உருவாக்கும் பணிகளுக்கு பயன்படுத்தப்படுகிறது, மேலும் ஒரு வரிசையில் இரண்டு உள்ளீட்டு வாக்கியங்களை குறிப்பிட அனுமதிக்கிறது.\n",
    "\n",
    "பின்னர், BERT அம்சங்களை எடுக்கக்கூடிய extractor-ஐ உருவாக்கலாம்:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = hub.KerasLayer('https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pooled_output -> (1, 128)\n",
      "encoder_outputs -> 4\n",
      "sequence_output -> (1, 128, 128)\n",
      "default -> (1, 128)\n"
     ]
    }
   ],
   "source": [
    "z = bert(vectorizer(['I love transformers']))\n",
    "for i,x in z.items():\n",
    "    print(f\"{i} -> { len(x) if isinstance(x, list) else x.shape }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT அடுக்கு பல பயனுள்ள முடிவுகளை வழங்குகிறது:\n",
    "* `pooled_output` என்பது வரிசையில் உள்ள அனைத்து டோக்கன்களையும் சராசரி செய்து பெறப்படும் முடிவு. இதை முழு நெட்வொர்க்கின் புத்திசாலியான அர்த்தமான எம்பெடிங் எனக் காணலாம். இது எங்கள் முந்தைய மாதிரியில் உள்ள `GlobalAveragePooling1D` அடுக்கின் வெளியீட்டுக்கு சமமானது.\n",
    "* `sequence_output` என்பது கடைசி டிரான்ஸ்ஃபார்மர் அடுக்கின் வெளியீடு (மேலே உள்ள எங்கள் மாதிரியில் உள்ள `TransformerBlock` வெளியீட்டுக்கு இணையானது).\n",
    "* `encoder_outputs` என்பது அனைத்து டிரான்ஸ்ஃபார்மர் அடுக்குகளின் வெளியீடுகள். நாமும் 4-அடுக்கு BERT மாதிரியை ஏற்றியுள்ளோம் (அதன் பெயரில் உள்ள `4_H` மூலம் நீங்கள் இதை ஊகிக்கலாம்), எனவே இது 4 டென்சர்களைக் கொண்டுள்ளது. கடைசி ஒன்று `sequence_output` உடன் ஒரே மாதிரியானது.\n",
    "\n",
    "இப்போது முழுமையான வகைப்படுத்தல் மாதிரியை வரையறுப்போம். *செயல்பாட்டு மாதிரி வரையறை* முறையைப் பயன்படுத்துவோம், அதில் மாதிரி உள்ளீட்டை வரையறுத்து, அதன் வெளியீட்டை கணக்கிட ஒரு தொடர் வெளிப்பாடுகளை வழங்குவோம். மேலும், BERT மாதிரி எடைகளை பயிற்சி செய்ய முடியாதவையாக மாற்றி, இறுதி வகைப்படுத்தியை மட்டுமே பயிற்சி செய்வோம்:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer (KerasLayer)        {'input_type_ids': ( 0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer_1 (KerasLayer)      {'pooled_output': (N 4782465     keras_layer[0][0]                \n",
      "                                                                 keras_layer[0][1]                \n",
      "                                                                 keras_layer[0][2]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 128)          0           keras_layer_1[0][5]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 4)            516         dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 4,782,981\n",
      "Trainable params: 516\n",
      "Non-trainable params: 4,782,465\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inp = keras.Input(shape=(),dtype=tf.string)\n",
    "x = vectorizer(inp)\n",
    "x = bert(x)\n",
    "x = keras.layers.Dropout(0.1)(x['pooled_output'])\n",
    "out = keras.layers.Dense(4,activation='softmax')(x)\n",
    "model = keras.models.Model(inp,out)\n",
    "bert.trainable = False\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 528s 559ms/step - loss: 0.8056 - acc: 0.6983 - val_loss: 0.5953 - val_acc: 0.7888\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9bb1e36d00>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'], optimizer='adam')\n",
    "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "பயிற்சி செய்யக்கூடிய அளவுக்கு குறைந்த அளவு அளவுருக்கள் இருந்தாலும், செயல்முறை மிகவும் மெதுவாக உள்ளது, ஏனெனில் BERT அம்சம் எடுக்கும் செயலி கணினி வளங்களை அதிகமாக பயன்படுத்துகிறது. போதுமான பயிற்சி இல்லாமையால் அல்லது மாதிரி அளவுருக்கள் இல்லாமையால், நியாயமான துல்லியத்தை அடைய முடியவில்லை என்று தோன்றுகிறது.\n",
    "\n",
    "BERT எடைகளைக் குளிர்வடிக்காமல் (unfreeze) செய்து அதை பயிற்சி செய்ய முயற்சிப்போம். இதற்கு மிகவும் குறைந்த கற்றல் வீதம் தேவைப்படும், மேலும் **warmup** உடன் கூடிய கவனமாக உள்ள பயிற்சி உத்தியை **AdamW** ஆப்டிமைசரை பயன்படுத்தி மேற்கொள்ள வேண்டும். ஆப்டிமைசரை உருவாக்க `tf-models-official` தொகுப்பைப் பயன்படுத்துவோம்:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer (KerasLayer)        {'input_type_ids': ( 0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer_1 (KerasLayer)      {'pooled_output': (N 4782465     keras_layer[0][0]                \n",
      "                                                                 keras_layer[0][1]                \n",
      "                                                                 keras_layer[0][2]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 128)          0           keras_layer_1[0][5]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 4)            516         dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 4,782,981\n",
      "Trainable params: 4,782,980\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n",
      "938/938 [==============================] - 629s 664ms/step - loss: 0.6344 - acc: 0.7658 - val_loss: 0.4876 - val_acc: 0.8247\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9bb0bd0070>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from official.nlp import optimization \n",
    "bert.trainable=True\n",
    "model.summary()\n",
    "epochs = 3\n",
    "opt = optimization.create_optimizer(\n",
    "    init_lr=3e-5,\n",
    "    num_train_steps=epochs*len(ds_train),\n",
    "    num_warmup_steps=0.1*epochs*len(ds_train),\n",
    "    optimizer_type='adamw')\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'], optimizer=opt)\n",
    "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "பயிற்சி மிகவும் மெதுவாக நடைபெறுகிறது என்பதை நீங்கள் காணலாம் - ஆனால் நீங்கள் சில எபோக்கள் (5-10) வரை மாடலை பயிற்சி செய்து, இதற்கு முன்பு பயன்படுத்திய அணுகுமுறைகளுடன் ஒப்பிடும்போது சிறந்த முடிவை பெற முடியுமா என்று பரிசோதிக்க விரும்பலாம்.\n",
    "\n",
    "## Huggingface Transformers Library\n",
    "\n",
    "Transformer மாடல்களை பயன்படுத்துவதற்கான மற்றொரு பொதுவான (மற்றும் சற்று எளிய) வழி [HuggingFace package](https://github.com/huggingface/) ஆகும், இது பல்வேறு NLP பணிகளுக்கான எளிய கட்டமைப்புகளை வழங்குகிறது. இது Tensorflow மற்றும் PyTorch ஆகிய இரண்டிற்கும் கிடைக்கிறது, இது மற்றொரு மிகவும் பிரபலமான நரம்பியல் நெட்வொர்க் கட்டமைப்பாகும்.\n",
    "\n",
    "> **Note**: Transformers library எப்படி வேலை செய்கிறது என்பதை பார்க்க நீங்கள் ஆர்வமாக இல்லையெனில் - இந்த நோட்புக் முடிவுக்கு நீங்கள் நேரடியாக செல்லலாம், ஏனெனில் மேலே செய்ததிலிருந்து வேறுபட்டதாக எதுவும் நீங்கள் காணமாட்டீர்கள். Huggingface Transformers நூலகத்தைப் பயன்படுத்தி BERT மாடலை பயிற்சி செய்வதற்கான அதே படிகளை மீண்டும் செய்யப்போகிறோம், ஆனால் இது மிகவும் பெரிய மாடலை உள்ளடக்கியது. எனவே, இந்த செயல்முறை நீண்ட நேர பயிற்சியை உள்ளடக்கியது, எனவே நீங்கள் கோடுகளை மட்டும் பார்ப்பதற்கு விரும்பலாம்.\n",
    "\n",
    "நாம் [Huggingface Transformers](http://huggingface.co) பயன்படுத்தி எவ்வாறு எங்கள் பிரச்சினையை தீர்க்க முடியும் என்பதை பார்ப்போம்.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "முதலில் நாம் பயன்படுத்த போகும் மாடலை தேர்வு செய்ய வேண்டும். சில உள்ளமைக்கப்பட்ட மாடல்களுடன், Huggingface ஒரு [ஆன்லைன் மாடல் களஞ்சியத்தை](https://huggingface.co/models) கொண்டுள்ளது, அங்கு சமூகத்தால் உருவாக்கப்பட்ட மேலும் பல முன்பே பயிற்சி பெற்ற மாடல்களை நீங்கள் காணலாம். அந்த மாடல்களை எல்லாம் மாடல் பெயரை வழங்குவதன் மூலம் ஏற்றவும் பயன்படுத்தவும் முடியும். மாடலுக்கான தேவையான அனைத்து பைனரி கோப்புகளும் தானாகவே பதிவிறக்கம் செய்யப்படும்.\n",
    "\n",
    "சில நேரங்களில், உங்கள் சொந்த மாடல்களை ஏற்ற வேண்டும், அப்போது டோக்கனைசர், `config.json` கோப்பு போன்ற மாடல் அளவுருக்கள், பைனரி எடை கோப்புகள் உள்ளிட்ட அனைத்து தொடர்புடைய கோப்புகளையும் கொண்டுள்ள அடைவை நீங்கள் குறிப்பிடலாம்.\n",
    "\n",
    "மாடல் பெயரிலிருந்து, நாம் மாடலையும் டோக்கனைசரையும் உருவாக்கலாம். முதலில் டோக்கனைசருடன் தொடங்குவோம்:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "# To load the model from Internet repository using model name. \n",
    "# Use this if you are running from your own copy of the notebooks\n",
    "bert_model = 'bert-base-uncased' \n",
    "\n",
    "# To load the model from the directory on disk. Use this for Microsoft Learn module, because we have\n",
    "# prepared all required files for you.\n",
    "#bert_model = './bert'\n",
    "\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(bert_model)\n",
    "\n",
    "MAX_SEQ_LEN = 128\n",
    "PAD_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "UNK_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tokenizer` பொருள் `encode` செயல்பாட்டை கொண்டுள்ளது, இது நேரடியாக உரையை குறியாக்கம் செய்ய பயன்படுத்தப்படலாம்:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 23435, 12314, 2003, 1037, 2307, 7705, 2005, 17953, 2361, 102]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('Tensorflow is a great framework for NLP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "நாம் டோக்கனைசரை பயன்படுத்தி ஒரு வரிசையை மாடலுக்கு அனுப்புவதற்கு ஏற்றவாறு குறியாக்கம் செய்யலாம், அதாவது `token_ids`, `input_mask` புலங்கள் போன்றவற்றை உள்ளடக்கி. மேலும், `return_tensors='tf'` என்ற வாதத்தை வழங்குவதன் மூலம் நாம் டென்சர்ஃப்ளோ டென்சர்களை விரும்புகிறோம் என்பதை குறிப்பிடலாம்:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[ 101, 7592, 1010, 2045,  102]], dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[0, 0, 0, 0, 0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[1, 1, 1, 1, 1]], dtype=int32)>}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(['Hello, there'],return_tensors='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "எங்கள் வழக்கில், `bert-base-uncased` எனப்படும் முன்பே பயிற்சி பெற்ற BERT மாடலை பயன்படுத்தப் போகிறோம். *Uncased* என்பது மாடல் எழுத்து பெரியதா, சிறியதா என்பதை பொருட்படுத்தாது என்பதைக் குறிக்கிறது.\n",
    "\n",
    "மாடலைப் பயிற்றுவிக்கும் போது, டோக்கன்களாக மாற்றிய வரிசையை உள்ளீடாக வழங்க வேண்டும், எனவே தரவுகளை செயலாக்கும் குழாயமைப்பை வடிவமைக்கப் போகிறோம். `tokenizer.encode` என்பது Python செயல்பாடு என்பதால், கடந்த யூனிட்டில் பயன்படுத்திய அணுகுமுறையைப் போலவே, `py_function` மூலம் இதைப் பயன்படுத்தப் போகிறோம்:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(x):\n",
    "    return tokenizer.encode(x.numpy().decode('utf-8'),return_tensors='tf',padding='max_length',max_length=MAX_SEQ_LEN,truncation=True)[0]\n",
    "\n",
    "def process_fn(x):\n",
    "    s = x['title']+' '+x['description']\n",
    "    e = tf.py_function(process,inp=[s],Tout=(tf.int32))\n",
    "    e.set_shape(MAX_SEQ_LEN)\n",
    "    return e,x['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "இப்போது `BertForSequenceClassification` தொகுப்பைப் பயன்படுத்தி உண்மையான மாதிரியை ஏற்றலாம். இது வகைப்படுத்தலுக்கான தேவையான கட்டமைப்பை, இறுதி வகைப்படுத்தியை உட்பட, ஏற்கனவே மாதிரியில் உள்ளதைக் உறுதிப்படுத்துகிறது. இறுதி வகைப்படுத்தியின் எடைகள் தொடங்கப்படவில்லை, மேலும் மாதிரி முன்பயிற்சி தேவைப்படும் என்று கூறும் எச்சரிக்கை செய்தியை நீங்கள் காண்பீர்கள் - அது முழுமையாக சரியே, ஏனெனில் அதுதான் நாம் செய்யப் போகிறோம்!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = transformers.TFBertForSequenceClassification.from_pretrained(bert_model,num_labels=4,output_attentions=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  109482240 \n",
      "_________________________________________________________________\n",
      "dropout_75 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  3076      \n",
      "=================================================================\n",
      "Total params: 109,485,316\n",
      "Trainable params: 109,485,316\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`summary()`-இல் இருந்து நீங்கள் காணக்கூடியது போல, மாடல் சுமார் 110 மில்லியன் அளவிலான பராமேட்டர்களைக் கொண்டுள்ளது! பொதுவாக, சிறிய தரவுத்தொகுப்பில் எளிய வகைப்படுத்தல் பணியைச் செய்ய விரும்பினால், BERT அடிப்படை அடுக்கு பயிற்சி செய்ய விரும்பமாட்டோம்:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  109482240 \n",
      "_________________________________________________________________\n",
      "dropout_75 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  3076      \n",
      "=================================================================\n",
      "Total params: 109,485,316\n",
      "Trainable params: 3,076\n",
      "Non-trainable params: 109,482,240\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.layers[0].trainable = False\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "இப்போது நாம் பயிற்சியை தொடங்க தயாராக இருக்கிறோம்!\n",
    "\n",
    "> **குறிப்பு**: முழுமையான அளவிலான BERT மாடலைப் பயிற்றுவிப்பது மிகவும் நேரம் பிடிக்கும்! எனவே, முதல் 32 தொகுதிகளுக்கு மட்டுமே அதை பயிற்றுவிப்போம். இது மாடல் பயிற்சியை எப்படி அமைக்க வேண்டும் என்பதை காட்டுவதற்காக மட்டுமே. முழுமையான அளவிலான பயிற்சியை முயற்சிக்க விரும்பினால் - `steps_per_epoch` மற்றும் `validation_steps` அளவுருக்களை நீக்கி, காத்திருக்க தயாராகுங்கள்!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 142s 4s/step - loss: 1.3896 - acc: 0.2500 - val_loss: 1.3863 - val_acc: 0.2480\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f1d40a4b6a0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile('adam','sparse_categorical_crossentropy',['acc'])\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "model.fit(ds_train.map(process_fn).batch(32),validation_data=ds_test.map(process_fn).batch(32),steps_per_epoch=32,validation_steps=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "நீங்கள் மீளமுறை எண்ணிக்கையை அதிகரித்து, போதுமான நேரம் காத்திருந்து, பல காலகட்டங்களுக்கு பயிற்சி செய்தால், BERT வகைப்பாட்டில் மிகச்சிறந்த துல்லியத்தை பெறலாம்! இதற்குக் காரணம், BERT மொழியின் அமைப்பை ஏற்கனவே மிகவும் நன்றாக புரிந்துகொள்கிறது, மேலும் இறுதி வகைப்பாட்டை நன்றாகச் சீரமைக்க வேண்டும். ஆனால், BERT ஒரு பெரிய மாதிரியாக இருப்பதால், முழு பயிற்சி செயல்முறை மிகவும் நீண்ட நேரம் எடுக்கும், மேலும் கணினி சக்தி அதிகமாக தேவைப்படும்! (GPU, மேலும் ஒரே ஒரு GPU-வுக்கு மேல் இருப்பது சிறந்தது).\n",
    "\n",
    "> **Note:** எங்கள் எடுத்துக்காட்டில், நாம் மிகச் சிறிய முன்பே பயிற்சி செய்யப்பட்ட BERT மாதிரிகளைப் பயன்படுத்தி வந்தோம். பெரிய மாதிரிகள் உள்ளன, அவை சிறந்த முடிவுகளை வழங்க வாய்ப்பு உள்ளது.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## முக்கிய கருத்து\n",
    "\n",
    "இந்த பிரிவில், **transformers** அடிப்படையிலான சமீபத்திய மாடல் கட்டமைப்புகளை பார்த்தோம். அவற்றை நமது உரை வகைப்படுத்தல் பணிக்காக பயன்படுத்தியுள்ளோம், ஆனால் அதேபோல், BERT மாடல்கள் entity extraction, கேள்வி பதில் மற்றும் பிற NLP பணிகளுக்கு பயன்படுத்தப்படலாம்.\n",
    "\n",
    "Transformer மாடல்கள் NLP இல் தற்போதைய state-of-the-art ஆகும், மேலும் பெரும்பாலான சந்தர்ப்பங்களில் தனிப்பயன் NLP தீர்வுகளை செயல்படுத்த தொடங்கும்போது முதலில் முயற்சிக்க வேண்டிய தீர்வாக இருக்கும். இருப்பினும், இந்த தொகுதியில் விவாதிக்கப்பட்ட மறு நிகழ்நிலை நரம்பியல் வலையமைப்புகளின் அடிப்படை அடிப்படைகளை புரிந்துகொள்வது, மேம்பட்ட நரம்பியல் மாடல்களை உருவாக்க விரும்பினால் மிகவும் முக்கியமானது.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**குறிப்பு**:  \nஇந்த ஆவணம் [Co-op Translator](https://github.com/Azure/co-op-translator) என்ற AI மொழிபெயர்ப்பு சேவையைப் பயன்படுத்தி மொழிபெயர்க்கப்பட்டுள்ளது. எங்கள் தரச்சிறப்பிற்காக முயற்சிப்பதுடன், தானியங்கி மொழிபெயர்ப்புகளில் பிழைகள் அல்லது தவறுகள் இருக்கக்கூடும் என்பதை கவனத்தில் கொள்ளவும். அதன் தாய்மொழியில் உள்ள மூல ஆவணம் அதிகாரப்பூர்வ ஆதாரமாக கருதப்பட வேண்டும். முக்கியமான தகவல்களுக்கு, தொழில்முறை மனித மொழிபெயர்ப்பு பரிந்துரைக்கப்படுகிறது. இந்த மொழிபெயர்ப்பைப் பயன்படுத்துவதால் ஏற்படும் எந்த தவறான புரிதல்கள் அல்லது தவறான விளக்கங்களுக்கு நாங்கள் பொறுப்பல்ல.\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0cb620c6d4b9f7a635928804c26cf22403d89d98d79684e4529119355ee6d5a5"
  },
  "kernelspec": {
   "display_name": "py38_tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "coopTranslator": {
   "original_hash": "ab59c532409774988ab875f2260e8e53",
   "translation_date": "2025-10-11T12:38:37+00:00",
   "source_file": "lessons/5-NLP/18-Transformers/TransformersTF.ipynb",
   "language_code": "ta"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}