{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# கவனிப்பு முறைமைகள் மற்றும் டிரான்ஸ்ஃபார்மர்கள்\n",
    "\n",
    "மீளச்சுழற்சி நெட்வொர்க்குகளின் முக்கிய குறைபாடுகளில் ஒன்று, ஒரு வரிசையில் உள்ள அனைத்து சொற்களும் முடிவில் ஒரே அளவு தாக்கத்தை ஏற்படுத்தும். இது பெயரிடப்பட்ட பொருள் அடையாளம் மற்றும் மெஷின் மொழிபெயர்ப்பு போன்ற வரிசை-வரிசை பணிகளுக்கு வழக்கமான LSTM குறியாக்க-குறியாக்க மாடல்களுடன் குறைந்த செயல்திறனை ஏற்படுத்துகிறது. உண்மையில், உள்ளீட்டு வரிசையில் உள்ள குறிப்பிட்ட சொற்கள் மற்றவற்றைவிட தொடர்ச்சியான வெளியீடுகளில் அதிக தாக்கத்தை ஏற்படுத்துகின்றன.\n",
    "\n",
    "மெஷின் மொழிபெயர்ப்பு போன்ற வரிசை-வரிசை மாடலை எடுத்துக்கொள்ளுங்கள். இது இரண்டு மீளச்சுழற்சி நெட்வொர்க்குகளால் செயல்படுத்தப்படுகிறது, இதில் ஒரு நெட்வொர்க் (**குறியாக்கி**) உள்ளீட்டு வரிசையை மறைமறைநிலை நிலைக்கு சுருக்குகிறது, மற்றொன்று (**குறியாக்கி**) இந்த மறைமறைநிலை நிலையை மொழிபெயர்க்கப்பட்ட முடிவாக விரிக்கிறது. இந்த அணுகுமுறையின் பிரச்சினை என்னவென்றால், நெட்வொர்க்கின் இறுதி நிலை ஒரு வாக்கியத்தின் தொடக்கத்தை நினைவில் வைத்துக்கொள்ள கடினமாக இருக்கும், இதனால் நீண்ட வாக்கியங்களில் மாடலின் தரம் குறைகிறது.\n",
    "\n",
    "**கவனிப்பு முறைமைகள்** RNN இன் ஒவ்வொரு வெளியீட்டு கணிப்பில் உள்ளீட்டு வெக்டரின் சூழலியல் தாக்கத்தை எடுக்கும் ஒரு வழியை வழங்குகின்றன. இது செயல்படுத்தப்படும் விதம் என்னவென்றால், உள்ளீட்டு RNN இன் இடைநிலை நிலைகள் மற்றும் வெளியீட்டு RNN இடையே குறுக்குவழிகளை உருவாக்குவதன் மூலம். இந்த முறையில், $y_t$ என்ற வெளியீட்டு சின்னத்தை உருவாக்கும்போது, ​​வித்தியாசமான எடை குணகங்கள் $\\alpha_{t,i}$ உடன் அனைத்து உள்ளீட்டு மறைமறைநிலை நிலைகள் $h_i$ ஐ கருத்தில் கொள்ளுவோம்.\n",
    "\n",
    "![குறியாக்கி/குறியாக்கி மாடல் மற்றும் சேர்க்கை கவனிப்பு அடுக்கு](../../../../../translated_images/ta/encoder-decoder-attention.7a726296894fb567.webp)\n",
    "*[Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) இல் சேர்க்கை கவனிப்பு முறைமையுடன் குறியாக்கி-குறியாக்கி மாடல், [இந்த வலைப்பதிவு](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html) இடமிருந்து மேற்கோள்]*\n",
    "\n",
    "கவனிப்பு அணி $\\{\\alpha_{i,j}\\}$ என்பது ஒரு குறிப்பிட்ட உள்ளீட்டு சொற்கள் வெளியீட்டு வரிசையில் ஒரு குறிப்பிட்ட சொல்லை உருவாக்குவதில் விளையாடும் அளவை பிரதிநிதித்துவப்படுத்தும். கீழே அத்தகைய அணியின் உதாரணம் உள்ளது:\n",
    "\n",
    "![Bahdanau - arviz.org இல் இருந்து எடுத்த RNNsearch-50 மூலம் கண்டுபிடிக்கப்பட்ட மாதAlignment](../../../../../translated_images/ta/bahdanau-fig3.09ba2d37f202a6af.webp)\n",
    "\n",
    "*[Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) (Fig.3) இல் இருந்து எடுத்த படம்]*\n",
    "\n",
    "கவனிப்பு முறைமைகள் இயற்கை மொழி செயலாக்கத்தில் தற்போதைய அல்லது தற்போதைய நிலைமையை அடைய முக்கிய காரணமாக உள்ளன. கவனிப்பை சேர்ப்பது மாடல் அளவுருக்களின் எண்ணிக்கையை பெரிதும் அதிகரிக்கிறது, இது RNN களுடன் அளவீட்டு சிக்கல்களை ஏற்படுத்தியது. RNN களை அளவீட்டில் ஒரு முக்கிய கட்டுப்பாடு என்னவென்றால், மாடல்களின் மீளச்சுழற்சி தன்மை பயிற்சியை தொகுதி மற்றும் இணைபார்க்க சவாலாக மாற்றுகிறது. RNN இல் ஒரு வரிசையின் ஒவ்வொரு கூறும் வரிசையாக செயல்படுத்தப்பட வேண்டும், இது அதை எளிதாக இணைபார்க்க முடியாது என்பதைக் குறிக்கிறது.\n",
    "\n",
    "கவனிப்பு முறைமைகளை ஏற்றுக்கொள்வது மற்றும் இந்த கட்டுப்பாட்டுடன் இணைந்து, இன்று நாம் BERT முதல் OpenGPT3 வரை அறிந்தும் பயன்படுத்தும் தற்போதைய நிலைமையை அடைந்த டிரான்ஸ்ஃபார்மர் மாடல்களின் உருவாக்கத்துக்கு வழிவகுத்தது.\n",
    "\n",
    "## டிரான்ஸ்ஃபார்மர் மாடல்கள்\n",
    "\n",
    "ஒவ்வொரு முந்தைய கணிப்பின் சூழலை அடுத்த மதிப்பீட்டு படியில் அனுப்புவதற்கு பதிலாக, **டிரான்ஸ்ஃபார்மர் மாடல்கள்** **இடதிகதிக குறியீடுகள்** மற்றும் கவனிப்பைப் பயன்படுத்தி கொடுக்கப்பட்ட உர சாளரத்தில் உள்ள ஒரு குறிப்பிட்ட உள்ளீட்டின் சூழலைப் பிடிக்கின்றன. positional encodings மற்றும் கவனிப்பு ஒரு கொடுக்கப்பட்ட சாளரத்தில் சூழலை எவ்வாறு பிடிக்க முடியும் என்பதை கீழே உள்ள படம் காட்டுகிறது.\n",
    "\n",
    "![டிரான்ஸ்ஃபார்மர் மாடல்களில் மதிப்பீடுகள் எவ்வாறு செயல்படுகின்றன என்பதை விளக்கும் அனிமேஷன் GIF](../../../../../lessons/5-NLP/18-Transformers/images/transformer-animated-explanation.gif)\n",
    "\n",
    "ஒவ்வொரு உள்ளீட்டு நிலை ஒவ்வொரு வெளியீட்டு நிலைக்கு தனித்தனியாக வரைபடப்படுவதால், டிரான்ஸ்ஃபார்மர்கள் RNN களை விட சிறப்பாக இணைபார்க்க முடியும், இது மிகப்பெரிய மற்றும் வெளிப்படையான மொழி மாடல்களை இயக்குகிறது. ஒவ்வொரு கவனிப்பு தலைவும் சொற்களுக்கிடையேயான வித்தியாசமான உறவுகளை கற்றுக்கொள்வதற்கு பயன்படுத்தப்படலாம், இது கீழ்மட்ட இயற்கை மொழி செயலாக்க பணிகளை மேம்படுத்துகிறது.\n",
    "\n",
    "**BERT** (Bidirectional Encoder Representations from Transformers) என்பது *BERT-base* க்கு 12 அடுக்குகள் மற்றும் *BERT-large* க்கு 24 அடுக்குகளுடன் மிகப்பெரிய பன்மடங்கு டிரான்ஸ்ஃபார்மர் நெட்வொர்க் ஆகும். இந்த மாடல் முதலில் பெரிய உர தரவுத்தொகுப்பில் (WikiPedia + புத்தகங்கள்) மேற்பயிற்சி செய்யப்படுகிறது, இது மேற்பார்வையற்ற பயிற்சியை (ஒரு வாக்கியத்தில் மறைக்கப்பட்ட சொற்களை கணிக்க) பயன்படுத்துகிறது. மேற்பயிற்சியின் போது, ​​மாடல் முக்கியமான அளவிலான மொழி புரிதலை உறிஞ்சுகிறது, இது பிற தரவுத்தொகுப்புகளுடன் நன்றாக அமைத்துக்கொள்ள முடியும். இந்த செயல்முறை **மாற்றக் கற்றல்** என்று அழைக்கப்படுகிறது.\n",
    "\n",
    "![http://jalammar.github.io/illustrated-bert/ இல் இருந்து படம்](../../../../../translated_images/ta/jalammarBERT-language-modeling-masked-lm.34f113ea5fec4362.webp)\n",
    "\n",
    "BERT, DistilBERT, BigBird, OpenGPT3 மற்றும் பலவற்றை fine-tune செய்யக்கூடிய டிரான்ஸ்ஃபார்மர் கட்டமைப்புகளின் பல மாறுபாடுகள் உள்ளன. [HuggingFace package](https://github.com/huggingface/) PyTorch உடன் இந்த கட்டமைப்புகளில் பலவற்றை பயிற்சி செய்ய ஒரு களஞ்சியத்தை வழங்குகிறது.\n",
    "\n",
    "## BERT ஐ உர வகைப்படுத்தலுக்கு பயன்படுத்துதல்\n",
    "\n",
    "நாம் முன்பே பயிற்சி பெற்ற BERT மாடலை எவ்வாறு பயன்படுத்தி எங்கள் பாரம்பரிய பணியை தீர்க்கலாம் என்பதை பார்ப்போம்: வரிசை வகைப்படுத்தல். நாங்கள் எங்கள் அசல் AG News தரவுத்தொகுப்பை வகைப்படுத்துவோம்.\n",
    "\n",
    "முதலில், HuggingFace நூலகத்தையும் எங்கள் தரவுத்தொகுப்பையும் ஏற்றுவோம்:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Building vocab...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "from torchnlp import *\n",
    "import transformers\n",
    "train_dataset, test_dataset, classes, vocab = load_dataset()\n",
    "vocab_len = len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "நாம் முன்பே பயிற்சி பெற்ற BERT மாடலைப் பயன்படுத்த இருப்பதால், குறிப்பிட்ட tokenizer-ஐ பயன்படுத்த வேண்டும். முதலில், முன்பே பயிற்சி பெற்ற BERT மாடலுடன் தொடர்புடைய tokenizer-ஐ ஏற்றுவோம்.\n",
    "\n",
    "HuggingFace நூலகத்தில் முன்பே பயிற்சி பெற்ற மாடல்களின் களஞ்சியம் உள்ளது, இதை நீங்கள் `from_pretrained` செயல்பாடுகளுக்கு வாதங்களாக அவற்றின் பெயர்களை குறிப்பிடுவதன் மூலம் பயன்படுத்தலாம். மாடலுக்கான தேவையான அனைத்து பைனரி கோப்புகளும் தானாகவே பதிவிறக்கம் செய்யப்படும்.\n",
    "\n",
    "ஆனால், சில நேரங்களில் உங்கள் சொந்த மாடல்களை ஏற்ற வேண்டும், அப்போது tokenizer-க்கு தேவையான அளவுருக்கள், மாடல் அளவுருக்கள் கொண்ட `config.json` கோப்பு, பைனரி எடைகள் போன்ற அனைத்து தொடர்புடைய கோப்புகளையும் கொண்டுள்ள அடைவை நீங்கள் குறிப்பிடலாம்.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load the model from Internet repository using model name. \n",
    "# Use this if you are running from your own copy of the notebooks\n",
    "bert_model = 'bert-base-uncased' \n",
    "\n",
    "# To load the model from the directory on disk. Use this for Microsoft Learn module, because we have\n",
    "# prepared all required files for you.\n",
    "bert_model = './bert'\n",
    "\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(bert_model)\n",
    "\n",
    "MAX_SEQ_LEN = 128\n",
    "PAD_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "UNK_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tokenizer` பொருள் `encode` செயல்பாட்டை கொண்டுள்ளது, இது நேரடியாக உரையை குறியாக்கம் செய்ய பயன்படுத்தப்படலாம்:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 1052, 22123, 2953, 2818, 2003, 1037, 2307, 7705, 2005, 17953, 2361, 102]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('PyTorch is a great framework for NLP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "அப்போது, பயிற்சியின் போது தரவுகளை அணுக நாம் பயன்படுத்தும் இடைநிலைகளை உருவாக்கலாம். BERT தனது சொந்த குறியாக்க செயல்பாட்டைப் பயன்படுத்துவதால், நாம் முன்பு வரையறுத்த `padify` போன்ற ஒரு பூர்த்தி செயல்பாட்டை வரையறுக்க வேண்டும்:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_bert(b):\n",
    "    # b is the list of tuples of length batch_size\n",
    "    #   - first element of a tuple = label, \n",
    "    #   - second = feature (text sequence)\n",
    "    # build vectorized sequence\n",
    "    v = [tokenizer.encode(x[1]) for x in b]\n",
    "    # compute max length of a sequence in this minibatch\n",
    "    l = max(map(len,v))\n",
    "    return ( # tuple of two tensors - labels and features\n",
    "        torch.LongTensor([t[0] for t in b]),\n",
    "        torch.stack([torch.nn.functional.pad(torch.tensor(t),(0,l-len(t)),mode='constant',value=0) for t in v])\n",
    "    )\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=8, collate_fn=pad_bert, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=8, collate_fn=pad_bert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "எங்கள் வழக்கில், நாங்கள் `bert-base-uncased` எனப்படும் முன்பே பயிற்சியளிக்கப்பட்ட BERT மாடலை பயன்படுத்தப் போகிறோம். `BertForSequenceClassfication` தொகுப்பைப் பயன்படுத்தி மாடலை ஏற்றுவோம். இது வகைப்படுத்தலுக்கான தேவையான கட்டமைப்பை, இறுதி வகைப்படுத்தியை உட்பட, ஏற்கனவே மாடலுக்கு உள்ளடக்கியதை உறுதிப்படுத்துகிறது. இறுதி வகைப்படுத்தியின் எடைகள் ஆரம்பிக்கப்படவில்லை, மேலும் மாடலுக்கு முன்பயிற்சி தேவைப்படும் என்று எச்சரிக்கை செய்தி நீங்கள் காண்பீர்கள் - இது முழுமையாக சரியானது, ஏனெனில் அதுதான் நாங்கள் செய்யப் போகிறோம்!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./bert were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./bert and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = transformers.BertForSequenceClassification.from_pretrained(bert_model,num_labels=4).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "இப்போது நாம் பயிற்சியைத் தொடங்க தயாராக இருக்கிறோம்! ஏனெனில் BERT ஏற்கனவே முன்பதிவுசெய்யப்பட்டுள்ளது, ஆரம்ப எடைகளை அழிக்காமல் இருக்க சிறிய கற்றல் விகிதத்துடன் தொடங்க விரும்புகிறோம்.\n",
    "\n",
    "`BertForSequenceClassification` மாடல் அனைத்து கடினமான வேலைகளையும் செய்கிறது. பயிற்சி தரவுகளில் மாடலை அழைக்கும் போது, அது இழப்பையும் நெட்வொர்க் வெளியீட்டையும் (input minibatch க்கான) திருப்புகிறது. பராமேற்றங்களை மேம்படுத்த இழப்பை (`loss.backward()` பின்செலுத்தலைச் செய்கிறது) பயன்படுத்துகிறோம், மற்றும் `out` ஐ பயிற்சி துல்லியத்தை கணக்கிட பயன்படுத்துகிறோம், பெறப்பட்ட லேபிள்களை `labs` (அது `argmax` ஐப் பயன்படுத்தி கணக்கிடப்படுகிறது) எதிர்பார்க்கப்பட்ட `labels` உடன் ஒப்பிடுவதன் மூலம்.\n",
    "\n",
    "செயல்முறையை கட்டுப்படுத்த, பல முறைமுறைகளில் இழப்பையும் துல்லியத்தையும் சேர்த்துக் கணக்கிடுகிறோம், மற்றும் ஒவ்வொரு `report_freq` பயிற்சி சுழற்சிகளிலும் அவற்றை அச்சிடுகிறோம்.\n",
    "\n",
    "இந்த பயிற்சி மிகவும் நீண்ட நேரம் எடுக்கும் வாய்ப்பு உள்ளது, எனவே முறைமுறைகளின் எண்ணிக்கையை வரையறுக்கிறோம்.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 1.1254194641113282, Accuracy = 0.585\n",
      "Loss = 0.6194715118408203, Accuracy = 0.83\n",
      "Loss = 0.46665248870849607, Accuracy = 0.8475\n",
      "Loss = 0.4309701919555664, Accuracy = 0.8575\n",
      "Loss = 0.35427074432373046, Accuracy = 0.8825\n",
      "Loss = 0.3306886291503906, Accuracy = 0.8975\n",
      "Loss = 0.30340143203735354, Accuracy = 0.8975\n",
      "Loss = 0.26139299392700194, Accuracy = 0.915\n",
      "Loss = 0.26708646774291994, Accuracy = 0.9225\n",
      "Loss = 0.3667240524291992, Accuracy = 0.8675\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
    "\n",
    "report_freq = 50\n",
    "iterations = 500 # make this larger to train for longer time!\n",
    "\n",
    "model.train()\n",
    "\n",
    "i,c = 0,0\n",
    "acc_loss = 0\n",
    "acc_acc = 0\n",
    "\n",
    "for labels,texts in train_loader:\n",
    "    labels = labels.to(device)-1 # get labels in the range 0-3         \n",
    "    texts = texts.to(device)\n",
    "    loss, out = model(texts, labels=labels)[:2]\n",
    "    labs = out.argmax(dim=1)\n",
    "    acc = torch.mean((labs==labels).type(torch.float32))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    acc_loss += loss\n",
    "    acc_acc += acc\n",
    "    i+=1\n",
    "    c+=1\n",
    "    if i%report_freq==0:\n",
    "        print(f\"Loss = {acc_loss.item()/c}, Accuracy = {acc_acc.item()/c}\")\n",
    "        c = 0\n",
    "        acc_loss = 0\n",
    "        acc_acc = 0\n",
    "    iterations-=1\n",
    "    if not iterations:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "நீங்கள் (முக்கியமாக iteration எண்ணிக்கையை அதிகரித்து, போதுமான நேரம் காத்திருந்தால்) BERT வகைப்படுத்தல் நமக்கு மிகவும் நல்ல துல்லியத்தை வழங்குவதை காணலாம்! இதற்குக் காரணம், BERT மொழியின் அமைப்பை ஏற்கனவே மிகவும் நன்றாக புரிந்துகொள்கிறது, மேலும் நமக்கு இறுதி வகைப்படுத்தியை fine-tune செய்யவே தேவையாக உள்ளது. ஆனால், BERT ஒரு பெரிய மாதிரி என்பதால், முழு பயிற்சி செயல்முறை மிகவும் நீண்ட நேரம் எடுக்கும், மேலும் மிகுந்த கணினி சக்தி தேவைப்படும்! (GPU, மேலும் ஒரே ஒரு GPU-வுக்கு மேல் இருக்க வேண்டும்).\n",
    "\n",
    "> **Note:** எங்கள் எடுத்துக்காட்டில், நாங்கள் மிகச் சிறிய pre-trained BERT மாதிரிகளில் ஒன்றைப் பயன்படுத்தி வந்தோம். மேலும் பெரிய மாதிரிகள் உள்ளன, அவை சிறந்த முடிவுகளை வழங்க வாய்ப்பு உள்ளது.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## மாதிரி செயல்திறனை மதிப்பீடு\n",
    "\n",
    "இப்போது, சோதனை தரவுத்தொகுப்பில் எங்கள் மாதிரியின் செயல்திறனை மதிப்பீடு செய்யலாம். மதிப்பீட்டு சுழற்சி பயிற்சி சுழற்சியைப் போன்றதே, ஆனால் `model.eval()` என்பதை அழைத்து மாதிரியை மதிப்பீட்டு முறையில் மாற்றுவது மறக்கக்கூடாது.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy: 0.9047029702970297\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "iterations = 100\n",
    "acc = 0\n",
    "i = 0\n",
    "for labels,texts in test_loader:\n",
    "    labels = labels.to(device)-1      \n",
    "    texts = texts.to(device)\n",
    "    _, out = model(texts, labels=labels)[:2]\n",
    "    labs = out.argmax(dim=1)\n",
    "    acc += torch.mean((labs==labels).type(torch.float32))\n",
    "    i+=1\n",
    "    if i>iterations: break\n",
    "        \n",
    "print(f\"Final accuracy: {acc.item()/i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## முக்கிய குறிப்புகள்\n",
    "\n",
    "இந்த பிரிவில், **transformers** நூலகத்தில் இருந்து முன்பே பயிற்சி பெற்ற மொழி மாதிரியை எடுத்து, அதை எங்கள் உரை வகைப்படுத்தல் பணிக்குத் தழுவுவது எவ்வளவு எளிதானது என்பதை பார்த்தோம். இதேபோல், BERT மாதிரிகள் பொருள் எடுக்கும், கேள்வி-பதில் மற்றும் பிற NLP பணிகளுக்கு பயன்படுத்தப்படலாம்.\n",
    "\n",
    "Transformer மாதிரிகள் NLP துறையில் தற்போதைய முன்னணி நிலையை பிரதிநிதித்துவப்படுத்துகின்றன, மேலும் பெரும்பாலான சந்தர்ப்பங்களில் தனிப்பயன் NLP தீர்வுகளை செயல்படுத்தும்போது நீங்கள் முதலில் பரிசோதனை செய்ய தொடங்க வேண்டிய தீர்வாக இருக்க வேண்டும். ஆனால், இந்த தொகுதியில் விவாதிக்கப்பட்ட மறு நிகழ்வு நரம்பியல் வலையமைப்புகளின் அடிப்படை அடிப்படைகளை புரிந்துகொள்வது, மேம்பட்ட நரம்பியல் மாதிரிகளை உருவாக்க விரும்பினால் மிகவும் முக்கியமானது.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**குறிப்பு**:  \nஇந்த ஆவணம் [Co-op Translator](https://github.com/Azure/co-op-translator) என்ற AI மொழிபெயர்ப்பு சேவையை பயன்படுத்தி மொழிபெயர்க்கப்பட்டுள்ளது. எங்கள் தரச்செயல்முறையை உறுதிப்படுத்த முயற்சிக்கிறோம், ஆனால் தானியங்கி மொழிபெயர்ப்புகளில் பிழைகள் அல்லது தவறுகள் இருக்கக்கூடும் என்பதை கவனத்தில் கொள்ளவும். அதன் தாய்மொழியில் உள்ள மூல ஆவணம் அதிகாரப்பூர்வ ஆதாரமாக கருதப்பட வேண்டும். முக்கியமான தகவல்களுக்கு, தொழில்முறை மனித மொழிபெயர்ப்பு பரிந்துரைக்கப்படுகிறது. இந்த மொழிபெயர்ப்பைப் பயன்படுத்துவதால் ஏற்படும் எந்த தவறான புரிதல்கள் அல்லது தவறான விளக்கங்களுக்கு நாங்கள் பொறுப்பல்ல.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37_pytorch",
   "language": "python",
   "name": "conda-env-py37_pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "coopTranslator": {
   "original_hash": "753865967678a92dbce7d7efbd36d980",
   "translation_date": "2025-10-11T12:36:54+00:00",
   "source_file": "lessons/5-NLP/18-Transformers/TransformersPyTorch.ipynb",
   "language_code": "ta"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}