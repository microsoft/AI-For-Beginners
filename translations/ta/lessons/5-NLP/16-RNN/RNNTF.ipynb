{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# மீண்டும் நிகழும் நரம்பியல் வலைகள்\n",
    "\n",
    "முந்தைய தொகுதியில், உரையின் செறிந்த அர்த்த பிரதிநிதிகளைப் பற்றி கற்றுக்கொண்டோம். நாம் பயன்படுத்திய கட்டமைப்பு ஒரு வாக்கியத்தில் உள்ள சொற்களின் ஒருங்கிணைந்த அர்த்தத்தைப் பிடிக்கிறது, ஆனால் அது சொற்களின் **வரிசையை** கணக்கில் எடுத்துக்கொள்ளவில்லை, ஏனெனில் எம்பெடிங்கிற்குப் பிறகு வரும் ஒருங்கிணைப்பு செயல்பாடு இந்த தகவலை மூல உரையிலிருந்து நீக்குகிறது. இந்த மாதிரிகள் சொற்களின் வரிசையை பிரதிநிதித்துவப்படுத்த முடியாததால், உரை உருவாக்கம் அல்லது கேள்வி பதில் போன்ற சிக்கலான அல்லது குழப்பமான பணிகளைத் தீர்க்க முடியாது.\n",
    "\n",
    "ஒரு உரை வரிசையின் அர்த்தத்தைப் பிடிக்க, **மீண்டும் நிகழும் நரம்பியல் வலை**, அல்லது RNN எனப்படும் நரம்பியல் வலை கட்டமைப்பைப் பயன்படுத்துவோம். RNN பயன்படுத்தும்போது, நாங்கள் எங்கள் வாக்கியத்தை வலைகளின் வழியாக ஒரு டோக்கன் ஒன்றாக அனுப்புகிறோம், மற்றும் வலைகள் சில **நிலை** உருவாக்குகிறது, அதை அடுத்த டோக்கனுடன் மீண்டும் வலைகளுக்கு அனுப்புகிறோம்.\n",
    "\n",
    "![மீண்டும் நிகழும் நரம்பியல் வலை உருவாக்கத்தின் உதாரணத்தை காட்டும் படம்.](../../../../../translated_images/ta/rnn.27f5c29c53d727b5.webp)\n",
    "\n",
    "$X_0,\\dots,X_n$ என்ற உள்ளீட்டு டோக்கன் வரிசையைத் தரும்போது, RNN நரம்பியல் வலைகள் வரிசையை உருவாக்குகிறது, மற்றும் இந்த வரிசையை முடிவுக்கு கொண்டு செல்ல ஒரு பின்செலுத்தல் செயல்பாட்டைப் பயன்படுத்தி பயிற்சி செய்கிறது. ஒவ்வொரு வலைகள் தொகுதியும் $(X_i,S_i)$ என்ற ஜோடியை உள்ளீடாக எடுத்து, $S_{i+1}$ என்ற முடிவை உருவாக்குகிறது. இறுதி நிலை $S_n$ அல்லது வெளியீடு $Y_n$ ஒரு நேரியல் வகைப்பாட்டாளருக்கு செல்கிறது முடிவை உருவாக்க. அனைத்து வலைகள் தொகுதிகளும் ஒரே எடைகளைப் பகிர்ந்து கொள்கின்றன, மற்றும் ஒரு பின்செலுத்தல் செயல்பாட்டைப் பயன்படுத்தி முடிவுக்கு பயிற்சி செய்யப்படுகின்றன.\n",
    "\n",
    "> மேலே உள்ள படம் மீண்டும் நிகழும் நரம்பியல் வலைகளை விரிவாக்கப்பட்ட வடிவத்தில் (இடது பக்கம்) மற்றும் சுருக்கமான மீண்டும் நிகழும் பிரதிநிதித்துவத்தில் (வலது பக்கம்) காட்டுகிறது. அனைத்து RNN செல்கள் ஒரே **பகிரக்கூடிய எடைகளை** கொண்டிருப்பதை உணருவது முக்கியம்.\n",
    "\n",
    "நிலை வெக்டர்கள் $S_0,\\dots,S_n$ வலைகளின் வழியாக அனுப்பப்படுவதால், RNN சொற்களுக்கிடையேயான வரிசை சார்ந்த சார்புகளை கற்றுக்கொள்ள முடியும். உதாரணமாக, *not* என்ற சொல் வரிசையில் எங்காவது தோன்றும்போது, அது நிலை வெக்டருக்குள் குறிப்பிட்ட கூறுகளை மறுக்க கற்றுக்கொள்ள முடியும்.\n",
    "\n",
    "உள்ளே, ஒவ்வொரு RNN செல்களும் இரண்டு எடை மடிக்கோவைகள் $W_H$ மற்றும் $W_I$, மற்றும் பைஸ் $b$ கொண்டிருக்கும். ஒவ்வொரு RNN படியில், $X_i$ என்ற உள்ளீடு மற்றும் $S_i$ என்ற உள்ளீட்டு நிலை கொடுக்கப்பட்டால், வெளியீட்டு நிலை $S_{i+1} = f(W_H\\times S_i + W_I\\times X_i+b)$ என கணக்கிடப்படும், இங்கு $f$ ஒரு செயல்பாட்டு செயல்பாடு (அடிக்கடி $\\tanh$).\n",
    "\n",
    "> உரை உருவாக்கம் (அடுத்த அலகில் நாம் கவரப்போகிறோம்) அல்லது இயந்திர மொழிபெயர்ப்பு போன்ற பிரச்சினைகளுக்கு, ஒவ்வொரு RNN படியிலும் சில வெளியீட்டு மதிப்பை பெற விரும்புகிறோம். இந்த நிலையில், மேலும் ஒரு மடிக்கோவை $W_O$ இருக்கும், மற்றும் வெளியீடு $Y_i=f(W_O\\times S_i+b_O)$ என கணக்கிடப்படும்.\n",
    "\n",
    "நாம் எங்கள் செய்தி தரவுத்தொகுப்பை வகைப்படுத்த மீண்டும் நிகழும் நரம்பியல் வலைகள் எவ்வாறு உதவ முடியும் என்பதை பார்ப்போம்.\n",
    "\n",
    "> சாண்ட்பாக்ஸ் சூழலுக்காக, தேவையான நூலகம் நிறுவப்பட்டு, தரவுகள் முன்கூட்டியே பெறப்பட்டுள்ளதா என்பதை உறுதிப்படுத்த கீழே உள்ள செல்களை இயக்க வேண்டும். நீங்கள் உள்ளூர் முறையில் இயக்கினால், கீழே உள்ள செல்களை தவிர்க்கலாம்.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install --quiet tensorflow_datasets==4.4.0\n",
    "!cd ~ && wget -q -O - https://mslearntensorflowlp.blob.core.windows.net/data/tfds-ag-news.tgz | tar xz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "\n",
    "# We are going to be training pretty large models. In order not to face errors, we need\n",
    "# to set tensorflow option to grow GPU memory allocation when required\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "if len(physical_devices)>0:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "ds_train, ds_test = tfds.load('ag_news_subset').values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "பெரிய மாதிரிகளை பயிற்சி செய்யும்போது, GPU நினைவக ஒதுக்கீடு ஒரு பிரச்சனையாக இருக்கலாம். மேலும், தரவுகள் GPU நினைவகத்தில் பொருந்தும் வகையில், ஆனால் பயிற்சி போதுமான வேகமாக இருக்கும் வகையில், மினிபேட்ச் அளவுகளை மாற்றி முயற்சிக்க வேண்டிய அவசியம் இருக்கலாம். நீங்கள் இந்த குறியீட்டை உங்கள் சொந்த GPU இயந்திரத்தில் இயக்கினால், பயிற்சியின் வேகத்தை அதிகரிக்க மினிபேட்ச் அளவுகளை சரிசெய்ய முயற்சிக்கலாம்.\n",
    "\n",
    "> **Note**: NVidia டிரைவர்கள் சில பதிப்புகள் மாதிரியை பயிற்சி செய்த பிறகு நினைவகத்தை வெளியிடாமல் இருக்கலாம் என்று அறியப்பட்டுள்ளது. இந்த நோட்புக்கில் பல உதாரணங்களை இயக்குகிறோம், இது குறிப்பிட்ட அமைப்புகளில் நினைவகம் முடிவடைவதற்கு காரணமாக இருக்கலாம், குறிப்பாக நீங்கள் இதே நோட்புக்கில் உங்கள் சொந்த முயற்சிகளைச் செய்கிறீர்கள் என்றால். மாதிரியை பயிற்சி செய்யத் தொடங்கும்போது சில விசித்திரமான பிழைகள் ஏற்பட்டால், நீங்கள் நோட்புக் கர்னலை மீண்டும் தொடங்க விரும்பலாம்.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "embed_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## எளிய RNN வகைப்பாடு\n",
    "\n",
    "எளிய RNN இன் பட்சத்தில், ஒவ்வொரு மீள்நிலை அலகும் ஒரு எளிய நேரியல் நெட்வொர்க் ஆகும், இது உள்ளீட்டு வெக்டர் மற்றும் நிலை வெக்டரை எடுத்துக்கொண்டு, புதிய நிலை வெக்டரை உருவாக்குகிறது. Keras இல், இதை `SimpleRNN` அடுக்கு மூலம் பிரதிநிதித்துவப்படுத்தலாம்.\n",
    "\n",
    "நாம் ஒரே-ஹாட் குறியீட்டப்பட்ட டோக்கன்களை நேரடியாக RNN அடுக்கு நோக்கி அனுப்ப முடியும், ஆனால் அவற்றின் அதிக பரிமாணத்தால் இது நல்ல யோசனை அல்ல. எனவே, வார்த்தை வெக்டர்களின் பரிமாணத்தை குறைக்க ஒரு எம்பெட்டிங் அடுக்கை பயன்படுத்தி, அதற்குப் பிறகு RNN அடுக்கு மற்றும் இறுதியில் ஒரு `Dense` வகைப்பாட்டாளரை பயன்படுத்துவோம்.\n",
    "\n",
    "> **குறிப்பு**: பரிமாணம் அதிகமாக இல்லாத சூழல்களில், உதாரணமாக எழுத்து-நிலை குறியீட்டாக்கத்தைப் பயன்படுத்தும்போது, ஒரே-ஹாட் குறியீட்டப்பட்ட டோக்கன்களை நேரடியாக RNN செலுக்கு அனுப்புவது பொருத்தமாக இருக்கலாம்.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "text_vectorization (TextVect (None, None)              0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, None, 64)          1280000   \n",
      "_________________________________________________________________\n",
      "simple_rnn (SimpleRNN)       (None, 16)                1296      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 4)                 68        \n",
      "=================================================================\n",
      "Total params: 1,281,364\n",
      "Trainable params: 1,281,364\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 20000\n",
    "\n",
    "vectorizer = keras.layers.experimental.preprocessing.TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    input_shape=(1,))\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    vectorizer,\n",
    "    keras.layers.Embedding(vocab_size, embed_size),\n",
    "    keras.layers.SimpleRNN(16),\n",
    "    keras.layers.Dense(4,activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **குறிப்பு:** எளிமைக்காக இங்கு பயிற்சி செய்யப்படாத embedding layer ஐ பயன்படுத்துகிறோம், ஆனால் சிறந்த முடிவுகளுக்காக முந்தைய அலகில் விவரிக்கப்பட்ட Word2Vec ஐ பயன்படுத்தி முன்பே பயிற்சி செய்யப்பட்ட embedding layer ஐ பயன்படுத்தலாம். இந்த குறியீட்டை முன்பே பயிற்சி செய்யப்பட்ட embedding களுடன் வேலை செய்ய மாற்றுவது உங்களுக்கு ஒரு நல்ல பயிற்சியாக இருக்கும்.\n",
    "\n",
    "இப்போது நமது RNN ஐ பயிற்சி செய்யலாம். பொதுவாக RNN களை பயிற்சி செய்வது மிகவும் கடினமானது, ஏனெனில் RNN செல்கள் வரிசை நீளத்துடன் unrolled செய்யப்படும் போது, பின்னடைவு பரப்பில் ஈடுபடும் அடுக்கு எண்ணிக்கை மிகவும் அதிகமாக இருக்கும். எனவே சிறிய learning rate ஐ தேர்வு செய்ய வேண்டும், மேலும் நல்ல முடிவுகளை உருவாக்க பெரிய தரவுத்தொகுப்பில் நெட்வொர்க்கை பயிற்சி செய்ய வேண்டும். இது மிகவும் நீண்ட நேரம் எடுக்கக்கூடும், எனவே GPU ஐ பயன்படுத்துவது விரும்பத்தக்கது.\n",
    "\n",
    "வேகத்தை அதிகரிக்க, நாங்கள் RNN மாதிரியை செய்தி தலைப்புகளில் மட்டுமே பயிற்சி செய்ய உள்ளோம், விளக்கத்தை தவிர்க்கிறோம். விளக்கத்துடன் பயிற்சி செய்ய முயற்சிக்கலாம், மேலும் மாதிரியை பயிற்சி செய்ய முடிகிறதா என்று பார்க்கலாம்.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training vectorizer\n"
     ]
    }
   ],
   "source": [
    "def extract_title(x):\n",
    "    return x['title']\n",
    "\n",
    "def tupelize_title(x):\n",
    "    return (extract_title(x),x['label'])\n",
    "\n",
    "print('Training vectorizer')\n",
    "vectorizer.adapt(ds_train.take(2000).map(extract_title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 82s 11ms/step - loss: 0.6629 - acc: 0.7623 - val_loss: 0.5559 - val_acc: 0.7995\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f3e0030d350>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'], optimizer='adam')\n",
    "model.fit(ds_train.map(tupelize_title).batch(batch_size),validation_data=ds_test.map(tupelize_title).batch(batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "> **குறிப்பு**: இங்கு துல்லியம் குறைவாக இருக்க வாய்ப்பு உள்ளது, ஏனெனில் நாங்கள் செய்தி தலைப்புகளில் மட்டுமே பயிற்சி பெறுகிறோம்.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## மாறி வரிசைகளை மீண்டும் பார்வையிடுதல்\n",
    "\n",
    "`TextVectorization` அடுக்கு மினிபேட்சில் மாறுபடும் நீள வரிசைகளை தானாகவே பாட் டோக்கன்களுடன் நிரப்பும் என்பதை நினைவில் கொள்ளுங்கள். அந்த டோக்கன்கள் பயிற்சியில் பங்கேற்கின்றன, மேலும் அவை மாதிரியின் ஒருங்கிணைப்பை சிக்கலாக்கலாம்.\n",
    "\n",
    "பாட் டோக்கன்களின் அளவை குறைக்க பல அணுகுமுறைகளை நாம் எடுத்துக்கொள்ளலாம். அவற்றில் ஒன்று, தரவுத்தொகுப்பை வரிசை நீளத்தால் மறுசீரமைத்து, அனைத்து வரிசைகளையும் அளவின்படி குழுவாக அமைப்பது. இதை `tf.data.experimental.bucket_by_sequence_length` செயல்பாட்டைப் பயன்படுத்தி செய்யலாம் (பாருங்கள் [ஆவணங்கள்](https://www.tensorflow.org/api_docs/python/tf/data/experimental/bucket_by_sequence_length)).\n",
    "\n",
    "மற்றொரு அணுகுமுறை **மாஸ்கிங்** பயன்படுத்துவது. Keras-இல், சில அடுக்குகள் கூடுதல் உள்ளீட்டை ஆதரிக்கின்றன, இது எந்த டோக்கன்கள் பயிற்சியில் கணக்கில் எடுத்துக்கொள்ளப்பட வேண்டும் என்பதை காட்டுகிறது. மாஸ்கிங்கை நமது மாதிரியில் சேர்க்க, தனி `Masking` அடுக்கை சேர்க்கலாம் ([ஆவணங்கள்](https://keras.io/api/layers/core_layers/masking/)), அல்லது `Embedding` அடுக்கின் `mask_zero=True` அளவுருவை குறிப்பிடலாம்.\n",
    "\n",
    "> **Note**: இந்த பயிற்சி முழு தரவுத்தொகுப்பில் ஒரு எபோக்கை முடிக்க சுமார் 5 நிமிடங்கள் ஆகும். பொறுமை இழந்தால், பயிற்சியை எந்த நேரத்திலும் நிறுத்தலாம். மேலும், பயிற்சிக்காக பயன்படுத்தப்படும் தரவின் அளவை வரையறுக்க `.take(...)` பிரிவை `ds_train` மற்றும் `ds_test` தரவுத்தொகுப்புகளுக்கு பிறகு சேர்க்கலாம்.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 371s 49ms/step - loss: 0.5401 - acc: 0.8079 - val_loss: 0.3780 - val_acc: 0.8822\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f3dec118850>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_text(x):\n",
    "    return x['title']+' '+x['description']\n",
    "\n",
    "def tupelize(x):\n",
    "    return (extract_text(x),x['label'])\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    vectorizer,\n",
    "    keras.layers.Embedding(vocab_size,embed_size,mask_zero=True),\n",
    "    keras.layers.SimpleRNN(16),\n",
    "    keras.layers.Dense(4,activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'], optimizer='adam')\n",
    "model.fit(ds_train.map(tupelize).batch(batch_size),validation_data=ds_test.map(tupelize).batch(batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "இப்போது நாங்கள் மாஸ்கிங் பயன்படுத்துகிறோம், எனவே தலைப்புகள் மற்றும் விளக்கங்களின் முழு தரவுத்தொகுப்பில் மாடலை பயிற்சி செய்யலாம்.\n",
    "\n",
    "> **குறிப்பு**: நீங்கள் கவனித்தீர்களா, நாங்கள் செய்தி தலைப்புகளில் பயிற்சி செய்யப்பட்ட vectorizer ஐ பயன்படுத்தி வருகிறோம், ஆனால் கட்டுரையின் முழு உடல değil? இது சில tokens புறக்கணிக்கப்பட காரணமாக இருக்கலாம், எனவே vectorizer ஐ மீண்டும் பயிற்சி செய்வது நல்லது. ஆனால், இது மிகச் சிறிய தாக்கத்தை மட்டுமே ஏற்படுத்தக்கூடும், எனவே எளிமைக்காக முந்தைய pre-trained vectorizer ஐ பயன்படுத்துவோம்.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM: நீண்ட குறுகிய கால நினைவகம்\n",
    "\n",
    "RNNகளின் முக்கிய பிரச்சனைகளில் ஒன்று **மங்கும் சாய்வு** (vanishing gradients) ஆகும். RNNகள் மிகவும் நீளமாக இருக்கலாம், மேலும் பின்னோக்குச் சாய்வு (backpropagation) செயல்பாட்டின் போது சாய்வுகளை நெட்வொர்க்கின் முதல் அடுக்கு வரை பரப்புவதில் சிரமம் ஏற்படலாம். இது நிகழும் போது, நெட்வொர்க் தூரத்தில் உள்ள டோக்கன்களுக்கிடையிலான தொடர்புகளை கற்றுக்கொள்ள முடியாது. இந்த பிரச்சனையைத் தவிர்க்க ஒரு வழி **கேடுகள்** (gates) மூலம் **வெளிப்படையான நிலை மேலாண்மை** (explicit state management) அறிமுகப்படுத்துவது ஆகும். கேடுகளை அறிமுகப்படுத்தும் இரண்டு பொதுவான கட்டமைப்புகள் **நீண்ட குறுகிய கால நினைவகம்** (LSTM) மற்றும் **கேடட் ரிலே யூனிட்** (GRU) ஆகும். இங்கு LSTMகளைப் பற்றி பேசுவோம்.\n",
    "\n",
    "![நீண்ட குறுகிய கால நினைவக செலின் ஒரு உதாரணத்தை காட்டும் படம்](../../../../../lessons/5-NLP/16-RNN/images/long-short-term-memory-cell.svg)\n",
    "\n",
    "LSTM நெட்வொர்க் RNN போலவே அமைக்கப்பட்டுள்ளது, ஆனால் இரண்டு நிலைகள் அடுக்கு முதல் அடுக்கு வரை அனுப்பப்படுகின்றன: உண்மையான நிலை $c$, மற்றும் மறைக்கப்பட்ட வெக்டர் $h$. ஒவ்வொரு யூனிடிலும், மறைக்கப்பட்ட வெக்டர் $h_{t-1}$ மற்றும் உள்ளீடு $x_t$ இணைக்கப்பட்டு, அவை **கேடுகள்** மூலம் நிலை $c_t$ மற்றும் வெளியீடு $h_{t}$ என்ன ஆக வேண்டும் என்பதை கட்டுப்படுத்துகின்றன. ஒவ்வொரு கேடுக்கும் சிக்மாய்டு செயல்பாடு (sigmoid activation) உள்ளது (அதன் வெளியீடு $[0,1]$ வரம்பில் இருக்கும்), இது நிலை வெக்டருடன் பெருக்கப்படும் போது பிட்வைஸ் மாஸ்க் (bitwise mask) போல கருதலாம். LSTMகளில் பின்வரும் கேடுகள் உள்ளன (மேலே உள்ள படத்தில் இடது பக்கம் முதல் வலது பக்கம் வரை):\n",
    "* **மறக்கக் கேடு** (forget gate): இது $c_{t-1}$ வெக்டரின் எந்த கூறுகளை மறக்க வேண்டும், மற்றும் எந்தவற்றை வழியாக அனுப்ப வேண்டும் என்பதைத் தீர்மானிக்கிறது.\n",
    "* **உள்ளீட்டு கேடு** (input gate): இது உள்ளீட்டு வெக்டர் மற்றும் முந்தைய மறைக்கப்பட்ட வெக்டர் எந்த அளவுக்கு நிலை வெக்டரில் சேர்க்கப்பட வேண்டும் என்பதைத் தீர்மானிக்கிறது.\n",
    "* **வெளியீட்டு கேடு** (output gate): இது புதிய நிலை வெக்டரை எடுத்து, அதன் எந்த கூறுகள் புதிய மறைக்கப்பட்ட வெக்டர் $h_t$ உருவாக்க பயன்படுத்தப்பட வேண்டும் என்பதை முடிவு செய்கிறது.\n",
    "\n",
    "நிலை $c$ கூறுகளை ஒளி/ஆஃப் செய்யக்கூடிய கொடிகள் (flags) போல கருதலாம். உதாரணமாக, வரிசையில் *Alice* என்ற பெயரை சந்திக்கும் போது, அது ஒரு பெண்ணை குறிக்கிறது என்று நாங்கள் ஊகிக்கிறோம், மேலும் வாக்கியத்தில் ஒரு பெண் பெயர்ச்சொல் உள்ளது என்று கூறும் கொடியை நிலையில் உயர்த்துகிறோம். பின்னர் *and Tom* என்ற வார்த்தைகளை சந்திக்கும் போது, பலவினை பெயர்ச்சொல் உள்ளது என்று கூறும் கொடியை உயர்த்துவோம். இவ்வாறு நிலையை மாற்றுவதன் மூலம், வாக்கியத்தின் இலக்கண பண்புகளை கண்காணிக்க முடியும்.\n",
    "\n",
    "> **குறிப்பு**: LSTMகளின் உள்ளமைவைப் புரிந்துகொள்ள ஒரு சிறந்த வளம்: [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) by Christopher Olah.\n",
    "\n",
    "LSTM செலின் உள்ளமைப்பு சிக்கலாக தோன்றினாலும், Keras இந்த செயல்பாட்டை `LSTM` அடுக்கின் (layer) உள்ளே மறைக்கிறது, எனவே மேலே உள்ள உதாரணத்தில் நமக்கு செய்ய வேண்டியது ஒரே ஒன்று: மீள்நோக்கு அடுக்கை மாற்றுவது:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15000/15000 [==============================] - 188s 13ms/step - loss: 0.5692 - acc: 0.7916 - val_loss: 0.3441 - val_acc: 0.8870\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f3d6af5c350>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    vectorizer,\n",
    "    keras.layers.Embedding(vocab_size, embed_size),\n",
    "    keras.layers.LSTM(8),\n",
    "    keras.layers.Dense(4,activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'], optimizer='adam')\n",
    "model.fit(ds_train.map(tupelize).batch(8),validation_data=ds_test.map(tupelize).batch(8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **குறிப்பு** LSTMகளை பயிற்சி செய்வது மிகவும் மெதுவாக இருக்கும், மேலும் பயிற்சியின் தொடக்கத்தில் துல்லியத்தில் அதிக உயர்வு காணப்படாமல் இருக்கலாம். நல்ல துல்லியத்தை அடைய நீங்கள் சில நேரம் தொடர்ந்து பயிற்சி செய்ய வேண்டியிருக்கும்.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## இரு திசை மற்றும் பல அடுக்கு RNNகள்\n",
    "\n",
    "இ bisher உள்ள எங்கள் எடுத்துக்காட்டுகளில், மீள்நோக்கு நெட்வொர்க்குகள் ஒரு வரிசையின் தொடக்கத்திலிருந்து முடிவுவரை செயல்படுகின்றன. இது இயல்பாகவே எங்களுக்கு உணரப்படுகிறது, ஏனெனில் இது நாம் படிக்கும் அல்லது பேச்சைக் கேட்கும் திசையில் நடக்கிறது. ஆனால், உள்ளீட்டு வரிசையை சீரற்ற அணுகல் தேவைப்படும் சூழல்களில், மீள்நோக்கு கணக்கீட்டை இரு திசைகளிலும் இயக்குவது பொருத்தமாக இருக்கும். இரு திசைகளிலும் கணக்கீடுகளை அனுமதிக்கும் RNNகள் **இரு திசை RNNகள்** என்று அழைக்கப்படுகின்றன, மேலும் அவற்றை `Bidirectional` என்ற சிறப்பு அடுக்கு மூலம் மீள்நோக்கு அடுக்கை சுற்றி உருவாக்கலாம்.\n",
    "\n",
    "> **குறிப்பு**: `Bidirectional` அடுக்கு அதன் உள்ளே அடுக்கின் இரண்டு பிரதிகளை உருவாக்குகிறது, மேலும் அந்த பிரதிகளில் ஒன்றின் `go_backwards` பண்பை `True` ஆக அமைக்கிறது, இதனால் அது வரிசையின் எதிர் திசையில் செல்கிறது.\n",
    "\n",
    "மீள்நோக்கு நெட்வொர்க்குகள், ஒருதிசை அல்லது இருதிசை, ஒரு வரிசையின் உள்ளமைப்புகளைப் பிடித்து, அவற்றை நிலை வெக்டர்களில் சேமிக்கின்றன அல்லது அவற்றை வெளியீடாக திருப்பி விடுகின்றன. குவால்வோல்யூஷன் நெட்வொர்க்குகளின் போல், முதல் அடுக்கால் எடுக்கப்பட்ட கீழ்நிலை அமைப்புகளிலிருந்து மேல்நிலை அமைப்புகளைப் பிடிக்க, முதல் அடுக்கைத் தொடர்ந்து மற்றொரு மீள்நோக்கு அடுக்கை உருவாக்கலாம். இது **பல அடுக்கு RNN** என்ற கருத்துக்கு வழிவகுக்கிறது, இது இரண்டு அல்லது அதற்கு மேற்பட்ட மீள்நோக்கு நெட்வொர்க்குகளை கொண்டுள்ளது, இதில் முந்தைய அடுக்கின் வெளியீடு அடுத்த அடுக்கிற்கு உள்ளீடாக அனுப்பப்படுகிறது.\n",
    "\n",
    "![பல அடுக்கு நீண்ட-குறுகிய-கால நினைவக RNN-ஐ காட்டும் படம்](../../../../../translated_images/ta/multi-layer-lstm.dd975e29bb2a59fe.webp)\n",
    "\n",
    "*[Fernando López எழுதிய இந்த அற்புதமான பதிவிலிருந்து](https://towardsdatascience.com/from-a-lstm-cell-to-a-multilayer-lstm-network-with-pytorch-2899eb5696f3) படம்.*\n",
    "\n",
    "Keras இவற்றை உருவாக்குவதைக் குறைந்த முயற்சியுடன் எளிதாக்குகிறது, ஏனெனில் நீங்கள் மாடலுக்கு மேலும் மீள்நோக்கு அடுக்குகளைச் சேர்க்க வேண்டும். கடைசி அடுக்கைத் தவிர அனைத்து அடுக்குகளுக்கும், `return_sequences=True` என்ற அளவுருவை குறிப்பிட வேண்டும், ஏனெனில் மீள்நோக்கு கணக்கீட்டின் இறுதி நிலையை மட்டுமல்லாமல் அனைத்து இடைநிலை நிலைகளையும் அடுக்கு திருப்பி வழங்க வேண்டும்.\n",
    "\n",
    "நாம் எங்கள் வகைப்படுத்தல் பிரச்சினைக்காக இரண்டு அடுக்கு இருதிசை LSTM ஒன்றை உருவாக்குவோம்.\n",
    "\n",
    "> **குறிப்பு**: இந்த குறியீடு மீண்டும் நிறைவடைய மிகவும் நீண்ட நேரம் எடுக்கும், ஆனால் இது இதுவரை நாம் கண்டுள்ள மிக உயர்ந்த துல்லியத்தை வழங்குகிறது. எனவே, காத்திருந்து முடிவைப் பார்க்கலாம்.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5044/7500 [===================>..........] - ETA: 2:33 - loss: 0.3709 - acc: 0.8706\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r5045/7500 [===================>..........] - ETA: 2:33 - loss: 0.3709 - acc: 0.8706"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    vectorizer,\n",
    "    keras.layers.Embedding(vocab_size, 128, mask_zero=True),\n",
    "    keras.layers.Bidirectional(keras.layers.LSTM(64,return_sequences=True)),\n",
    "    keras.layers.Bidirectional(keras.layers.LSTM(64)),    \n",
    "    keras.layers.Dense(4,activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'], optimizer='adam')\n",
    "model.fit(ds_train.map(tupelize).batch(batch_size),\n",
    "          validation_data=ds_test.map(tupelize).batch(batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNNகள் மற்ற பணிகளுக்கு\n",
    "\n",
    "இப்போது வரை, RNNகளை உரை வரிசைகளை வகைப்படுத்த பயன்படுத்துவதில் கவனம் செலுத்தியுள்ளோம். ஆனால் அவை மேலும் பல பணிகளைச் செய்ய முடியும், உதாரணமாக உரை உருவாக்கம் மற்றும் இயந்திர மொழிபெயர்ப்பு &mdash; இந்த பணிகளை அடுத்த அலகில் பார்க்கப்போவோம்.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**குறிப்பு**:  \nஇந்த ஆவணம் [Co-op Translator](https://github.com/Azure/co-op-translator) என்ற AI மொழிபெயர்ப்பு சேவையைப் பயன்படுத்தி மொழிபெயர்க்கப்பட்டுள்ளது. நாங்கள் துல்லியத்திற்காக முயற்சிக்கின்றோம், ஆனால் தானியக்க மொழிபெயர்ப்புகளில் பிழைகள் அல்லது தவறான தகவல்கள் இருக்கக்கூடும் என்பதை தயவுசெய்து கவனத்தில் கொள்ளுங்கள். அதன் தாய்மொழியில் உள்ள மூல ஆவணம் அதிகாரப்பூர்வ ஆதாரமாக கருதப்பட வேண்டும். முக்கியமான தகவல்களுக்கு, தொழில்முறை மனித மொழிபெயர்ப்பு பரிந்துரைக்கப்படுகிறது. இந்த மொழிபெயர்ப்பைப் பயன்படுத்துவதால் ஏற்படும் எந்த தவறான புரிதல்கள் அல்லது தவறான விளக்கங்களுக்கு நாங்கள் பொறுப்பல்ல.\n"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "conda-env-py37_tensorflow-py"
  },
  "kernelspec": {
   "display_name": "py37_tensorflow",
   "language": "python",
   "name": "conda-env-py37_tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "coopTranslator": {
   "original_hash": "81351e61f619b432ff51010a4f993194",
   "translation_date": "2025-10-11T12:54:15+00:00",
   "source_file": "lessons/5-NLP/16-RNN/RNNTF.ipynb",
   "language_code": "ta"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}