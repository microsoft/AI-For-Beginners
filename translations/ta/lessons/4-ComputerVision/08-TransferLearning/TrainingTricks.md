# டீப் லெர்னிங் பயிற்சி முறைகள்

நரம்பியல் நெட்வொர்க்குகள் ஆழமாக ஆழமாக செல்லும்போது, அவற்றின் பயிற்சி செயல்முறை மேலும் மேலும் சவாலாக மாறுகிறது. முக்கியமான பிரச்சினைகளில் ஒன்று [வெனிஷிங் கிரேடியண்ட்ஸ்](https://en.wikipedia.org/wiki/Vanishing_gradient_problem) அல்லது [எக்ஸ்ப்ளோடிங் கிரேடியண்ட்ஸ்](https://deepai.org/machine-learning-glossary-and-terms/exploding-gradient-problem#:~:text=Exploding%20gradients%20are%20a%20problem,updates%20are%20small%20and%20controlled.) எனப்படும் பிரச்சினைகள். [இந்த பதிவு](https://towardsdatascience.com/the-vanishing-exploding-gradient-problem-in-deep-neural-networks-191358470c11) இந்த பிரச்சினைகளுக்கு ஒரு நல்ல அறிமுகத்தை வழங்குகிறது.

ஆழமான நெட்வொர்க்குகளை பயிற்சி செய்ய எளிதாக்க, சில தொழில்நுட்பங்களை பயன்படுத்தலாம்.

## மதிப்புகளை நியாயமான இடைவெளியில் வைத்திருத்தல்

எண் கணக்கீடுகளை நிலையானதாக வைத்திருக்க, நமது நரம்பியல் நெட்வொர்க்கில் உள்ள அனைத்து மதிப்புகளும் பொதுவாக [-1..1] அல்லது [0..1] அளவுகளில் இருக்க வேண்டும். இது ஒரு கடுமையான தேவையல்ல, ஆனால் மிதவை புள்ளி கணக்கீடுகளின் இயல்பே அப்படி. வெவ்வேறு அளவுகளில் உள்ள மதிப்புகளை ஒன்றாக சரியாக கையாள முடியாது. உதாரணமாக, 10<sup>-10</sup> மற்றும் 10<sup>10</sup> ஐ சேர்த்தால், 10<sup>10</sup> கிடைக்கும், ஏனெனில் சிறிய மதிப்பு பெரியதுடன் ஒத்திசைக்கப்படும், இதனால் மான்டிசா (mantissa) இழக்கப்படும்.

பெரும்பாலான செயல்பாட்டு செயல்பாடுகள் [-1..1] இடைவெளியில் அல்லாத நேர்கோட்டுத்தன்மைகளை கொண்டுள்ளன, எனவே அனைத்து உள்ளீட்டு தரவுகளையும் [-1..1] அல்லது [0..1] இடைவெளிக்கு அளவிடுவது நியாயமானது.

## ஆரம்ப எடை தொடக்கத்தொகுப்பு

இடைவெளி மதிப்புகள் நெட்வொர்க் அடுக்குகள் வழியாக செல்லும் போது ஒரே அளவிலிருப்பதை நாம் விரும்புகிறோம். எனவே, மதிப்புகளின் விநியோகத்தை பாதுகாக்கும் வகையில் எடைகளை தொடங்குவது முக்கியம்.

**N(0,1)** போன்ற சாதாரண விநியோகம் நல்ல தேர்வாகாது, ஏனெனில் *n* உள்ளீடுகள் இருந்தால், வெளியீட்டின் நிலைமாறுபாடு *n* ஆக இருக்கும், மேலும் மதிப்புகள் [0..1] இடைவெளியை மீறக்கூடும்.

பின்வரும் தொடக்க முறைகள் பொதுவாக பயன்படுத்தப்படுகின்றன:

- ஒரே மாதிரியான விநியோகம் -- `uniform`
- **N(0,1/n)** -- `gaussian`
- **N(0,1/&radic;n_in)** -- இது, சுழல்நிலை 0 மற்றும் நிலைமாறுபாடு 1 கொண்ட உள்ளீடுகளுக்கு அதே சுழல்நிலை/நிலைமாறுபாட்டை பராமரிக்கிறது.
- **N(0,&radic;2/(n_in+n_out))** -- **Xavier Initialization** (`glorot`) எனப்படும் இது, முன்னோக்கி மற்றும் பின்னோக்கி பரவலின் போது சிக்னல்களை இடைவெளியில் வைத்திருக்க உதவுகிறது.

## பேட்ச் நார்மலைசேஷன்

சரியான எடை தொடக்கத்தொகுப்புடன் கூட, பயிற்சியின் போது எடைகள் மிகப்பெரியதாகவோ அல்லது மிகச்சிறியதாகவோ மாறலாம், இது சிக்னல்களை சரியான இடைவெளியை மீறச் செய்யும். நார்மலைசேஷன் தொழில்நுட்பங்களைப் பயன்படுத்தி சிக்னல்களை மீண்டும் சரியான இடைவெளிக்கு கொண்டு வரலாம். பல நார்மலைசேஷன் முறைகள் உள்ளன (எடை நார்மலைசேஷன், அடுக்கு நார்மலைசேஷன்), ஆனால் பொதுவாக பயன்படுத்தப்படுவது பேட்ச் நார்மலைசேஷன்.

**பேட்ச் நார்மலைசேஷன்** என்ற கருத்து மினிபேட்ச் முழுவதும் உள்ள மதிப்புகளை கணக்கில் எடுத்துக்கொண்டு, அவற்றின் அடிப்படையில் நார்மலைசேஷன் (உதா: சராசரியை கழித்து, நிலைமாறுபாட்டால் வகுத்தல்) செய்யும். இது எடைகளைப் பயன்படுத்திய பிறகு, ஆனால் செயல்பாட்டு செயல்பாட்டுக்கு முன் செயல்படும் ஒரு நெட்வொர்க் அடுக்காக செயல்படுகிறது. இதன் விளைவாக, அதிக இறுதி துல்லியத்தையும், வேகமான பயிற்சியையும் காணலாம்.

இது பற்றிய [அசல் ஆய்வு](https://arxiv.org/pdf/1502.03167.pdf), [விக்கிபீடியா விளக்கம்](https://en.wikipedia.org/wiki/Batch_normalization), மற்றும் [ஒரு நல்ல அறிமுக பதிவை](https://towardsdatascience.com/batch-normalization-in-3-levels-of-understanding-14c2da90a338) காணலாம் (அதேபோல [ரஷியன் மொழியில்](https://habrahabr.ru/post/309302/)).

## டிராப்அவுட்

**டிராப்அவுட்** என்பது பயிற்சியின் போது சில சதவீதம் சீரற்ற நரம்புகளை நீக்கும் ஒரு சுவாரஸ்யமான தொழில்நுட்பமாகும். இது ஒரு அடுக்காக செயல்படுகிறது, இதில் ஒரு அளவுரு (நரம்புகளை நீக்க வேண்டிய சதவீதம், பொதுவாக 10%-50%) உள்ளது, மேலும் பயிற்சியின் போது, அடுத்த அடுக்கிற்கு அனுப்புவதற்கு முன் உள்ளீட்டு வெக்டரின் சீரற்ற கூறுகளை பூஜ்யமாக்குகிறது.

இது ஒரு விசித்திரமான யோசனையாக தோன்றினாலும், [`Dropout.ipynb`](Dropout.ipynb) நோட்புக்கில் MNIST இலக்க வகைப்பாட்டாளரை பயிற்சி செய்யும் போது டிராப்அவுட்டின் விளைவுகளை காணலாம். இது பயிற்சியை வேகமாகச் செய்ய உதவுகிறது மற்றும் குறைந்த பயிற்சி சுற்றுகளில் அதிக துல்லியத்தை அடைய உதவுகிறது.

இந்த விளைவை பல வழிகளில் விளக்கலாம்:

- இது மாடலுக்கு ஒரு சீரற்ற அதிர்ச்சி காரணியாக கருதப்படலாம், இது உள்ளூர் குறைந்தபட்சத்திலிருந்து ஆப்டிமைசேஷனை வெளியே கொண்டு வருகிறது.
- இது *மறைமுக மாடல் சராசரி* ஆக கருதப்படலாம், ஏனெனில் டிராப்அவுட் போது நாம் சிறிது மாறுபட்ட மாடலை பயிற்சி செய்கிறோம்.

> *சிலர் கூறுகிறார்கள், ஒரு குடிபோதையில் உள்ள நபர் ஏதாவது ஒன்றை கற்றுக்கொள்ள முயற்சிக்கும்போது, அவர் மறுநாள் காலை இதை நன்றாக நினைவில் வைத்திருப்பார், ஒரு மது அருந்தாத நபரை ஒப்பிடும்போது. ஏனெனில் சில செயல்படாத நரம்புகளுடன் உள்ள மூளை பொருளை புரிந்துகொள்ள சிறப்பாக தழுவுகிறது. இது உண்மையா என்று நாங்கள் சோதிக்கவில்லை.*

## ஓவர்ஃபிட்டிங்கைத் தடுக்க

டீப் லெர்னிங்கின் ஒரு முக்கிய அம்சம் [ஓவர்ஃபிட்டிங்கை](../../3-NeuralNetworks/05-Frameworks/Overfitting.md) தடுக்கக் கற்றுக்கொள்வது. மிகவும் சக்திவாய்ந்த நரம்பியல் நெட்வொர்க் மாடலைப் பயன்படுத்துவது கவர்ச்சியாக இருக்கலாம், ஆனால் மாடல் அளவுருக்களின் எண்ணிக்கையையும் பயிற்சி மாதிரிகளின் எண்ணிக்கையையும் எப்போதும் சமநிலைப்படுத்த வேண்டும்.

> [ஓவர்ஃபிட்டிங்](../../3-NeuralNetworks/05-Frameworks/Overfitting.md) என்ற கருத்தை நாங்கள் முன்பு அறிமுகப்படுத்தியதை நீங்கள் நன்றாக புரிந்துகொள்ளுங்கள்!

ஓவர்ஃபிட்டிங்கைத் தடுக்க சில வழிகள் உள்ளன:

- ஆரம்பத்தில் நிறுத்துதல் -- சரிபார்ப்பு தொகுப்பில் பிழையை தொடர்ந்து கண்காணித்து, சரிபார்ப்பு பிழை அதிகரிக்கத் தொடங்கும் போது பயிற்சியை நிறுத்துதல்.
- வெளிப்படையான எடை சிதைவு / ஒழுங்குபடுத்தல் -- எடை மதிப்புகளின் அதிகமான முழுமதிப்புகளுக்கு இழப்புப் செயல்பாட்டில் கூடுதல் தண்டனை சேர்த்தல், இது மாடலை மிகவும் நிலையற்ற முடிவுகளை பெறுவதிலிருந்து தடுக்கிறது.
- மாடல் சராசரி -- பல மாடல்களை பயிற்சி செய்து அதன் முடிவுகளை சராசரி செய்தல். இது மாறுபாட்டை குறைக்க உதவுகிறது.
- டிராப்அவுட் (மறைமுக மாடல் சராசரி)

## ஆப்டிமைசர்கள் / பயிற்சி அல்காரிதம்கள்

பயிற்சியின் மற்றொரு முக்கிய அம்சம் நல்ல பயிற்சி அல்காரிதத்தைத் தேர்ந்தெடுப்பது. பாரம்பரிய **கிரேடியண்ட் டிசென்ட்** ஒரு நியாயமான தேர்வாக இருக்கலாம், ஆனால் இது சில நேரங்களில் மிகவும் மெதுவாகவோ அல்லது பிற பிரச்சினைகளை உருவாக்கவோ முடியும்.

டீப் லெர்னிங்கில், நாங்கள் **ஸ்டோகாஸ்டிக் கிரேடியண்ட் டிசென்ட்** (SGD) ஐப் பயன்படுத்துகிறோம், இது பயிற்சி தொகுப்பிலிருந்து சீரற்ற முறையில் தேர்ந்தெடுக்கப்பட்ட மினிபேட்ச்களுக்கு கிரேடியண்ட் டிசென்ட் பயன்படுத்தும் முறை. எடைகள் இந்த சமன்பாட்டைப் பயன்படுத்தி சரிசெய்யப்படுகின்றன:

w<sup>t+1</sup> = w<sup>t</sup> - &eta;&nabla;&lagran;

### மோமென்டம்

**மோமென்டம் SGD** இல், முந்தைய படிகளிலிருந்து ஒரு பகுதியை வைத்திருக்கிறோம். இது நாம் எங்காவது ஒரு திசையில் நகரும் போது, மற்றொரு திசையில் ஒரு தாக்கம் வந்தால், நமது பாதை உடனடியாக மாறாமல், ஆரம்ப இயக்கத்தின் ஒரு பகுதியைத் தக்கவைத்துக்கொள்வதைப் போன்றது. இங்கு *வேகம்* என்பதைக் குறிக்க மற்றொரு வெக்டர் v ஐ அறிமுகப்படுத்துகிறோம்:

- v<sup>t+1</sup> = &gamma; v<sup>t</sup> - &eta;&nabla;&lagran;
- w<sup>t+1</sup> = w<sup>t</sup> + v<sup>t+1</sup>

இங்கு &gamma; அளவுரு, நாங்கள் இயக்கத்தை எவ்வளவு அளவுக்கு கணக்கில் எடுத்துக்கொள்கிறோம் என்பதை குறிக்கிறது: &gamma;=0 பாரம்பரிய SGD ஐ குறிக்கிறது; &gamma;=1 என்பது ஒரு தூய இயக்க சமன்பாடு.

### ஆடம், ஆடாகிராட், மற்றும் பிறவை

ஒவ்வொரு அடுக்கிலும், நாம் சில W<sub>i</sub> மாட்ரிக்ஸ்களால் சிக்னல்களை பெருக்குகிறோம். W<sub>i</sub> இன் ||W<sub>i</sub>|| மதிப்பின் அடிப்படையில், கிரேடியண்ட் 0 க்கு அருகிலோ அல்லது அளவில்லாமல் அதிகரிக்கக்கூடியதாகவோ இருக்கலாம். இது Exploding/Vanishing Gradients பிரச்சினையின் சாரம்.

இந்த பிரச்சினைக்கு ஒரு தீர்வு, சமன்பாட்டில் கிரேடியண்டின் திசையை மட்டுமே பயன்படுத்தி, முழுமதிப்பை புறக்கணிப்பதாகும், அதாவது:

w<sup>t+1</sup> = w<sup>t</sup> - &eta;(&nabla;&lagran;/||&nabla;&lagran;||), இங்கு ||&nabla;&lagran;|| = &radic;&sum;(&nabla;&lagran;)<sup>2</sup>

இந்த அல்காரிதம் **ஆடாகிராட்** என அழைக்கப்படுகிறது. இதே யோசனையைப் பயன்படுத்தும் மற்ற அல்காரிதம்கள்: **RMSProp**, **ஆடம்**

> **ஆடம்** பல பயன்பாடுகளுக்கு மிகவும் திறமையான அல்காரிதமாக கருதப்படுகிறது, எனவே நீங்கள் எந்த ஒன்றை பயன்படுத்துவது என்று உறுதியாக இல்லாவிட்டால் - ஆடத்தை பயன்படுத்துங்கள்.

### கிரேடியண்ட் கிளிப்பிங்

கிரேடியண்ட் கிளிப்பிங் என்பது மேலே உள்ள யோசனையின் விரிவாக்கமாகும். ||&nabla;&lagran;|| &le; &theta; என்றால், எடை ஆப்டிமைசேஷனில் அசல் கிரேடியண்ட் பயன்படுத்தப்படுகிறது, மற்றும் ||&nabla;&lagran;|| > &theta; என்றால், கிரேடியண்ட் அதன் அளவால் வகுக்கப்படுகிறது. இங்கு &theta; ஒரு அளவுரு, பெரும்பாலான சந்தர்ப்பங்களில் &theta;=1 அல்லது &theta;=10 எடுக்கலாம்.

### கற்றல் வீத சிதைவு

பயிற்சி வெற்றியடைவது பெரும்பாலும் கற்றல் வீத அளவுரு &eta; மீது निर्भर. பயிற்சியின் ஆரம்பத்தில் பெரிய &eta; மதிப்புகள் வேகமான பயிற்சியை ஏற்படுத்தும், மேலும் பின்னர் சிறிய &eta; மதிப்புகள் நெட்வொர்க்கை நன்றாகச் சரிசெய்ய உதவும். எனவே, பெரும்பாலான சந்தர்ப்பங்களில் பயிற்சியின் போது &eta; ஐ குறைக்க விரும்புகிறோம்.

இது ஒவ்வொரு பயிற்சி சுற்றின் பிறகும் &eta; ஐ ஒரு எண்ணால் (உதா: 0.98) பெருக்குவதன் மூலம் அல்லது மேலும் சிக்கலான **கற்றல் வீத அட்டவணை** பயன்படுத்துவதன் மூலம் செய்யலாம்.

## வெவ்வேறு நெட்வொர்க் கட்டமைப்புகள்

உங்கள் பிரச்சினைக்கு சரியான நெட்வொர்க் கட்டமைப்பைத் தேர்ந்தெடுப்பது சிக்கலாக இருக்கலாம். பொதுவாக, நமது குறிப்பிட்ட பணிக்கான (அல்லது அதே போன்ற) ஒரு நிரூபிக்கப்பட்ட கட்டமைப்பை எடுத்துக்கொள்வோம். கணினி பார்வைக்கான நரம்பியல் நெட்வொர்க் கட்டமைப்புகளின் [ஒரு நல்ல சுருக்கத்தை](https://www.topbots.com/a-brief-history-of-neural-network-architectures/) காணலாம்.

> நமக்கு உள்ள பயிற்சி மாதிரிகளின் எண்ணிக்கைக்கு போதுமான சக்தி வாய்ந்த கட்டமைப்பைத் தேர்ந்தெடுப்பது முக்கியம். மிகவும் சக்திவாய்ந்த மாடலைத் தேர்ந்தெடுப்பது [ஓவர்ஃபிட்டிங்கை](../../3-NeuralNetworks/05-Frameworks/Overfitting.md) ஏற்படுத்தக்கூடும்.

மற்றொரு நல்ல வழி, தேவையான சிக்கலுக்கு தானாகவே சரிசெய்யக்கூடிய கட்டமைப்பை பயன்படுத்துவது. ஒரு அளவுக்கு, **ResNet** கட்டமைப்பு மற்றும் **Inception** தானாகவே சரிசெய்யக்கூடியவை. [கணினி பார்வை கட்டமைப்புகள் பற்றிய மேலும் தகவல்](../07-ConvNets/CNN_Architectures.md).

---

**குறிப்பு**:  
இந்த ஆவணம் [Co-op Translator](https://github.com/Azure/co-op-translator) என்ற AI மொழிபெயர்ப்பு சேவையை பயன்படுத்தி மொழிபெயர்க்கப்பட்டுள்ளது. எங்கள் தரத்தை உறுதிப்படுத்த முயற்சிக்கிறோம், ஆனால் தானியக்க மொழிபெயர்ப்புகளில் பிழைகள் அல்லது தவறுகள் இருக்கக்கூடும் என்பதை கவனத்தில் கொள்ளவும். அதன் தாய்மொழியில் உள்ள மூல ஆவணம் அதிகாரப்பூர்வ ஆதாரமாக கருதப்பட வேண்டும். முக்கியமான தகவல்களுக்கு, தொழில்முறை மனித மொழிபெயர்ப்பு பரிந்துரைக்கப்படுகிறது. இந்த மொழிபெயர்ப்பைப் பயன்படுத்துவதால் ஏற்படும் எந்த தவறான புரிதல்கள் அல்லது தவறான விளக்கங்களுக்கு நாங்கள் பொறுப்பல்ல.