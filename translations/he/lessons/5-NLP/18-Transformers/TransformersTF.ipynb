{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# מנגנוני קשב ומודלים של טרנספורמרים\n",
    "\n",
    "חיסרון מרכזי של רשתות חוזרות (Recurrent Networks) הוא שכל המילים ברצף משפיעות באותה מידה על התוצאה. הדבר גורם לביצועים תת-אופטימליים במודלים סטנדרטיים של LSTM עבור משימות רצף לרצף, כמו זיהוי ישויות בשם ותרגום מכונה. במציאות, למילים מסוימות ברצף הקלט יש השפעה רבה יותר על הפלט הרציף מאשר לאחרות.\n",
    "\n",
    "נבחן מודל רצף לרצף, כמו תרגום מכונה. מודל כזה מיושם באמצעות שתי רשתות חוזרות, כאשר רשת אחת (**מקודד**) דוחסת את רצף הקלט למצב מוסתר, ורשת אחרת, **מפענח**, פורסת את המצב המוסתר הזה לתוצאה מתורגמת. הבעיה בגישה זו היא שהמצב הסופי של הרשת מתקשה לזכור את תחילת המשפט, מה שגורם לאיכות ירודה של המודל במשפטים ארוכים.\n",
    "\n",
    "**מנגנוני קשב** מספקים דרך לשקלל את ההשפעה ההקשרית של כל וקטור קלט על כל תחזית פלט של ה-RNN. הדבר מיושם על ידי יצירת קיצורי דרך בין המצבים הביניים של ה-RNN של הקלט לבין ה-RNN של הפלט. כך, בעת יצירת סמל פלט $y_t$, ניקח בחשבון את כל המצבים המוסתרים של הקלט $h_i$, עם מקדמי משקל שונים $\\alpha_{t,i}$.\n",
    "\n",
    "![תמונה המציגה מודל מקודד/מפענח עם שכבת קשב אדיטיבית](../../../../../translated_images/he/encoder-decoder-attention.7a726296894fb567.webp)\n",
    "*מודל מקודד-מפענח עם מנגנון קשב אדיטיבי מתוך [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf), מצוטט מתוך [פוסט הבלוג הזה](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)*\n",
    "\n",
    "מטריצת הקשב $\\{\\alpha_{i,j}\\}$ מייצגת את המידה שבה מילים מסוימות בקלט משפיעות על יצירת מילה מסוימת ברצף הפלט. להלן דוגמה למטריצה כזו:\n",
    "\n",
    "![תמונה המציגה יישור דוגמה שנמצא על ידי RNNsearch-50, מתוך Bahdanau - arviz.org](../../../../../translated_images/he/bahdanau-fig3.09ba2d37f202a6af.webp)\n",
    "\n",
    "*תמונה מתוך [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) (איור 3)*\n",
    "\n",
    "מנגנוני קשב אחראים לחלק ניכר מהמצב הנוכחי או הקרוב למצב הנוכחי של האמנות בתחום עיבוד שפה טבעית. עם זאת, הוספת קשב מגדילה משמעותית את מספר הפרמטרים במודל, מה שהוביל לבעיות סקלאביליות עם RNNs. מגבלה מרכזית בסקלאביליות של RNNs היא שהאופי החוזר של המודלים מקשה על ביצוע אצווה (batching) והקבלה במקביל. ב-RNN, כל אלמנט ברצף צריך להיות מעובד בסדר רציף, מה שאומר שלא ניתן להקביל בקלות את התהליך.\n",
    "\n",
    "האימוץ של מנגנוני קשב בשילוב עם מגבלה זו הוביל ליצירת מודלים של טרנספורמרים, שהם כיום המצב המתקדם ביותר (State of the Art) שאנו מכירים ומשתמשים בהם, כמו BERT ו-OpenGPT3.\n",
    "\n",
    "## מודלים של טרנספורמרים\n",
    "\n",
    "במקום להעביר את ההקשר של כל תחזית קודמת לשלב ההערכה הבא, **מודלים של טרנספורמרים** משתמשים ב**קידודים מיקום** וב**קשב** כדי ללכוד את ההקשר של קלט נתון בתוך חלון טקסט מסוים. התמונה למטה מראה כיצד קידודי מיקום עם קשב יכולים ללכוד הקשר בתוך חלון נתון.\n",
    "\n",
    "![GIF מונפש המציג כיצד מתבצעות ההערכות במודלים של טרנספורמרים.](../../../../../lessons/5-NLP/18-Transformers/images/transformer-animated-explanation.gif)\n",
    "\n",
    "מכיוון שכל מיקום קלט ממופה באופן עצמאי לכל מיקום פלט, טרנספורמרים יכולים להקביל טוב יותר מ-RNNs, מה שמאפשר יצירת מודלים לשוניים גדולים ומרשימים יותר. כל ראש קשב יכול לשמש ללמידת יחסים שונים בין מילים, מה שמשפר משימות עיבוד שפה טבעית.\n",
    "\n",
    "## בניית מודל טרנספורמר פשוט\n",
    "\n",
    "Keras אינה כוללת שכבת טרנספורמר מובנית, אך ניתן לבנות אחת בעצמנו. כמו קודם, נתמקד בסיווג טקסט של מערך הנתונים AG News, אך כדאי לציין שמודלים של טרנספורמרים מציגים את התוצאות הטובות ביותר במשימות NLP מורכבות יותר.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "\n",
    "ds_train, ds_test = tfds.load('ag_news_subset').values()\n",
    "\n",
    "def extract_text(x):\n",
    "    return x['title']+' '+x['description']\n",
    "\n",
    "def tupelize(x):\n",
    "    return (extract_text(x),x['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "שכבות חדשות ב-Keras צריכות לרשת את מחלקת `Layer`, ולממש את המתודה `call`. נתחיל עם שכבת **Positional Embedding**. נשתמש [בקוד מתוך תיעוד רשמי של Keras](https://keras.io/examples/nlp/text_classification_with_transformer/). נניח שאנו מרפדים את כל רצפי הקלט לאורך `maxlen`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(keras.layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = keras.layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "        self.maxlen = maxlen\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = self.maxlen\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x+positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "שכבה זו מורכבת משתי שכבות `Embedding`: אחת להטמעת טוקנים (בדרך שדנו בה קודם) ואחת להטמעת מיקומי טוקנים. מיקומי הטוקנים נוצרים כרצף של מספרים טבעיים מ-0 עד `maxlen` באמצעות `tf.range`, ולאחר מכן מועברים דרך שכבת ההטמעה. שני וקטורי ההטמעה המתקבלים מחוברים יחד, ומייצרים ייצוג מוטמע-מיקומית של הקלט בצורת `maxlen`$\\times$`embed_dim`.\n",
    "\n",
    "עכשיו, בואו נממש את בלוק הטרנספורמר. הוא יקבל את הפלט של שכבת ההטמעה שהוגדרה קודם:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim, name='attn')\n",
    "        self.ffn = keras.Sequential(\n",
    "            [keras.layers.Dense(ff_dim, activation=\"relu\"), keras.layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = keras.layers.Dropout(rate)\n",
    "        self.dropout2 = keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "עכשיו, אנחנו מוכנים להגדיר את מודל הטרנספורמר המלא:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "text_vectorization (TextVect (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "token_and_position_embedding (None, 256, 32)           648192    \n",
      "_________________________________________________________________\n",
      "transformer_block (Transform (None, 256, 32)           10656     \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 20)                660       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 84        \n",
      "=================================================================\n",
      "Total params: 659,592\n",
      "Trainable params: 659,592\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 32  # Embedding size for each token\n",
    "num_heads = 2  # Number of attention heads\n",
    "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
    "maxlen = 256\n",
    "vocab_size = 20000\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.experimental.preprocessing.TextVectorization(max_tokens=vocab_size,output_sequence_length=maxlen, input_shape=(1,)),\n",
    "    TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim),\n",
    "    TransformerBlock(embed_dim, num_heads, ff_dim),\n",
    "    keras.layers.GlobalAveragePooling1D(),\n",
    "    keras.layers.Dropout(0.1),\n",
    "    keras.layers.Dense(20, activation=\"relu\"),\n",
    "    keras.layers.Dropout(0.1),\n",
    "    keras.layers.Dense(4, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokenizer\n",
      "938/938 [==============================] - 45s 39ms/step - loss: 0.4978 - acc: 0.8068 - val_loss: 0.2808 - val_acc: 0.9124\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9c2427a0d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Training tokenizer')\n",
    "model.layers[0].adapt(ds_train.map(extract_text))\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'], optimizer='adam')\n",
    "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## מודלים של BERT Transformer\n",
    "\n",
    "**BERT** (Bidirectional Encoder Representations from Transformers) הוא רשת טרנספורמר רב שכבתית גדולה מאוד עם 12 שכבות עבור *BERT-base*, ו-24 עבור *BERT-large*. המודל עובר תחילה אימון מוקדם על מאגר נתונים גדול של טקסט (ויקיפדיה + ספרים) באמצעות אימון לא מפוקח (ניבוי מילים מוסתרות במשפט). במהלך האימון המוקדם, המודל סופג רמה משמעותית של הבנת שפה, שניתן לאחר מכן לנצל עם מערכי נתונים אחרים באמצעות כיוונון עדין. תהליך זה נקרא **למידה מעוברת**.\n",
    "\n",
    "![תמונה מתוך http://jalammar.github.io/illustrated-bert/](../../../../../translated_images/he/jalammarBERT-language-modeling-masked-lm.34f113ea5fec4362.webp)\n",
    "\n",
    "ישנם וריאציות רבות של ארכיטקטורות טרנספורמר, כולל BERT, DistilBERT, BigBird, OpenGPT3 ועוד, שניתן לבצע עליהן כיוונון עדין.\n",
    "\n",
    "בואו נראה כיצד ניתן להשתמש במודל BERT שעבר אימון מוקדם כדי לפתור את בעיית סיווג הרצפים המסורתית שלנו. נשתמש ברעיון ובחלק מהקוד מתוך [התיעוד הרשמי](https://www.tensorflow.org/text/tutorials/classify_text_with_bert).\n",
    "\n",
    "כדי לטעון מודלים שעברו אימון מוקדם, נשתמש ב-**Tensorflow hub**. ראשית, נטען את הוקטורייזר הספציפי ל-BERT:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_41180/4216669875.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow_text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow_hub\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mhub\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhub\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mKerasLayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_text'"
     ]
    }
   ],
   "source": [
    "import tensorflow_text \n",
    "import tensorflow_hub as hub\n",
    "vectorizer = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_type_ids': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       " array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "       dtype=int32)>,\n",
       " 'input_word_ids': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       " array([[  101,  1045,  2293, 19081,   102,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]], dtype=int32)>,\n",
       " 'input_mask': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       " array([[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "       dtype=int32)>}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer(['I love transformers'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "חשוב להשתמש באותו וקטורייזר שבו הרשת המקורית אומנה. בנוסף, הוקטורייזר של BERT מחזיר שלושה רכיבים:\n",
    "* `input_word_ids`, שהוא רצף של מספרי טוקנים עבור המשפט הקלט\n",
    "* `input_mask`, שמראה איזה חלק מהרצף מכיל קלט אמיתי ואיזה חלק הוא ריפוד. זה דומה למסכה שמיוצרת על ידי השכבה `Masking`\n",
    "* `input_type_ids` משמש למשימות של מודל שפה, ומאפשר לציין שני משפטי קלט ברצף אחד.\n",
    "\n",
    "לאחר מכן, נוכל ליצור מופע של מחלץ התכונות של BERT:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = hub.KerasLayer('https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pooled_output -> (1, 128)\n",
      "encoder_outputs -> 4\n",
      "sequence_output -> (1, 128, 128)\n",
      "default -> (1, 128)\n"
     ]
    }
   ],
   "source": [
    "z = bert(vectorizer(['I love transformers']))\n",
    "for i,x in z.items():\n",
    "    print(f\"{i} -> { len(x) if isinstance(x, list) else x.shape }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "אז, שכבת BERT מחזירה מספר תוצאות שימושיות:\n",
    "* `pooled_output` הוא תוצאה של ממוצע כל הטוקנים ברצף. ניתן לראות אותו כייצוג סמנטי חכם של כל הרשת. הוא שווה ערך לפלט של שכבת `GlobalAveragePooling1D` במודל הקודם שלנו.\n",
    "* `sequence_output` הוא הפלט של שכבת הטרנספורמר האחרונה (מקביל לפלט של `TransformerBlock` במודל שלנו למעלה).\n",
    "* `encoder_outputs` הם הפלטים של כל שכבות הטרנספורמר. מכיוון שטענו מודל BERT עם 4 שכבות (כפי שניתן לנחש מהשם, שמכיל `4_H`), יש לו 4 טנסורים. האחרון מביניהם זהה ל-`sequence_output`.\n",
    "\n",
    "כעת נגדיר את מודל הסיווג מקצה לקצה. נשתמש ב-*הגדרת מודל פונקציונלית*, שבה נגדיר את הקלט של המודל, ולאחר מכן נספק סדרת ביטויים לחישוב הפלט שלו. בנוסף, נקבע את משקלי מודל ה-BERT כלא-ניתנים לאימון, ונאמן רק את מסווג הסיום:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer (KerasLayer)        {'input_type_ids': ( 0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer_1 (KerasLayer)      {'pooled_output': (N 4782465     keras_layer[0][0]                \n",
      "                                                                 keras_layer[0][1]                \n",
      "                                                                 keras_layer[0][2]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 128)          0           keras_layer_1[0][5]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 4)            516         dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 4,782,981\n",
      "Trainable params: 516\n",
      "Non-trainable params: 4,782,465\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inp = keras.Input(shape=(),dtype=tf.string)\n",
    "x = vectorizer(inp)\n",
    "x = bert(x)\n",
    "x = keras.layers.Dropout(0.1)(x['pooled_output'])\n",
    "out = keras.layers.Dense(4,activation='softmax')(x)\n",
    "model = keras.models.Model(inp,out)\n",
    "bert.trainable = False\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 528s 559ms/step - loss: 0.8056 - acc: 0.6983 - val_loss: 0.5953 - val_acc: 0.7888\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9bb1e36d00>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'], optimizer='adam')\n",
    "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "למרות שיש מעט פרמטרים שניתן לאמן, התהליך די איטי, מכיוון שמחלץ התכונות של BERT הוא כבד מבחינה חישובית. נראה שלא הצלחנו להגיע לדיוק סביר, או בגלל חוסר באימון, או בגלל חוסר בפרמטרים של המודל.\n",
    "\n",
    "בואו ננסה לשחרר את המשקלים של BERT ולאמן גם אותם. זה דורש קצב למידה מאוד קטן, וגם אסטרטגיית אימון זהירה יותר עם **חימום מוקדם**, תוך שימוש בממטב **AdamW**. נשתמש בחבילה `tf-models-official` כדי ליצור את הממטב:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer (KerasLayer)        {'input_type_ids': ( 0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer_1 (KerasLayer)      {'pooled_output': (N 4782465     keras_layer[0][0]                \n",
      "                                                                 keras_layer[0][1]                \n",
      "                                                                 keras_layer[0][2]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 128)          0           keras_layer_1[0][5]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 4)            516         dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 4,782,981\n",
      "Trainable params: 4,782,980\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n",
      "938/938 [==============================] - 629s 664ms/step - loss: 0.6344 - acc: 0.7658 - val_loss: 0.4876 - val_acc: 0.8247\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9bb0bd0070>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from official.nlp import optimization \n",
    "bert.trainable=True\n",
    "model.summary()\n",
    "epochs = 3\n",
    "opt = optimization.create_optimizer(\n",
    "    init_lr=3e-5,\n",
    "    num_train_steps=epochs*len(ds_train),\n",
    "    num_warmup_steps=0.1*epochs*len(ds_train),\n",
    "    optimizer_type='adamw')\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'], optimizer=opt)\n",
    "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "כפי שאתם יכולים לראות, תהליך האימון מתקדם לאט - אבל ייתכן שתרצו להתנסות ולאמן את המודל לכמה אפוקים (5-10) ולבדוק אם תוכלו להשיג את התוצאה הטובה ביותר בהשוואה לגישות שהשתמשנו בהן קודם.\n",
    "\n",
    "## ספריית Huggingface Transformers\n",
    "\n",
    "דרך נוספת נפוצה (וקצת פשוטה יותר) להשתמש במודלים של Transformer היא [חבילת HuggingFace](https://github.com/huggingface/), שמספקת אבני בניין פשוטות למשימות שונות בתחום עיבוד שפה טבעית (NLP). החבילה זמינה גם עבור Tensorflow וגם עבור PyTorch, מסגרת רשתות עצביות פופולרית נוספת.\n",
    "\n",
    "> **Note**: אם אינכם מעוניינים לראות כיצד ספריית Transformers עובדת - תוכלו לדלג לסוף המחברת, מכיוון שלא תראו שום דבר שונה באופן מהותי ממה שעשינו קודם. אנו נחזור על אותם שלבים של אימון מודל BERT באמצעות ספרייה שונה ומודל גדול בהרבה. לכן, התהליך כולל אימון די ממושך, כך שייתכן שתרצו פשוט לעבור על הקוד.\n",
    "\n",
    "בואו נראה כיצד ניתן לפתור את הבעיה שלנו באמצעות [Huggingface Transformers](http://huggingface.co).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "הדבר הראשון שצריך לעשות הוא לבחור את המודל שבו נשתמש. בנוסף לכמה מודלים מובנים, Huggingface מכיל [מאגר מודלים מקוון](https://huggingface.co/models), שבו ניתן למצוא הרבה מודלים מוכנים מראש שפותחו על ידי הקהילה. כל אחד מהמודלים הללו ניתן לטעון ולהשתמש בו פשוט על ידי מתן שם המודל. כל הקבצים הבינאריים הנדרשים עבור המודל יורדו באופן אוטומטי.\n",
    "\n",
    "במקרים מסוימים תצטרכו לטעון מודלים משלכם, ובמקרה כזה תוכלו לציין את הספרייה שמכילה את כל הקבצים הרלוונטיים, כולל פרמטרים עבור ה-tokenizer, קובץ `config.json` עם פרמטרי המודל, משקלים בינאריים, ועוד.\n",
    "\n",
    "מתוך שם המודל, ניתן ליצור גם את המודל וגם את ה-tokenizer. בואו נתחיל עם ה-tokenizer:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "# To load the model from Internet repository using model name. \n",
    "# Use this if you are running from your own copy of the notebooks\n",
    "bert_model = 'bert-base-uncased' \n",
    "\n",
    "# To load the model from the directory on disk. Use this for Microsoft Learn module, because we have\n",
    "# prepared all required files for you.\n",
    "#bert_model = './bert'\n",
    "\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(bert_model)\n",
    "\n",
    "MAX_SEQ_LEN = 128\n",
    "PAD_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "UNK_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "האובייקט `tokenizer` מכיל את הפונקציה `encode` שניתן להשתמש בה ישירות לקידוד טקסט:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 23435, 12314, 2003, 1037, 2307, 7705, 2005, 17953, 2361, 102]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('Tensorflow is a great framework for NLP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "אנחנו יכולים גם להשתמש במפענח כדי לקודד רצף בצורה שמתאימה להעברה למודל, כלומר כולל שדות `token_ids`, `input_mask` וכו'. אנחנו יכולים גם לציין שאנחנו רוצים טנסורים של Tensorflow על ידי מתן ארגומנט `return_tensors='tf'`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[ 101, 7592, 1010, 2045,  102]], dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[0, 0, 0, 0, 0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[1, 1, 1, 1, 1]], dtype=int32)>}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(['Hello, there'],return_tensors='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "במקרה שלנו, נשתמש במודל BERT מאומן מראש שנקרא `bert-base-uncased`. *Uncased* מציין שהמודל אינו רגיש לאותיות גדולות או קטנות.\n",
    "\n",
    "כאשר אנו מאמנים את המודל, עלינו לספק רצף מחולק לטוקנים כקלט, ולכן נעצב צינור עיבוד נתונים. מכיוון ש-`tokenizer.encode` היא פונקציה ב-Python, נשתמש באותה גישה כמו ביחידה הקודמת ונקרא לה באמצעות `py_function`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(x):\n",
    "    return tokenizer.encode(x.numpy().decode('utf-8'),return_tensors='tf',padding='max_length',max_length=MAX_SEQ_LEN,truncation=True)[0]\n",
    "\n",
    "def process_fn(x):\n",
    "    s = x['title']+' '+x['description']\n",
    "    e = tf.py_function(process,inp=[s],Tout=(tf.int32))\n",
    "    e.set_shape(MAX_SEQ_LEN)\n",
    "    return e,x['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "עכשיו אנחנו יכולים לטעון את המודל עצמו באמצעות החבילה `BertForSequenceClassification`. זה מבטיח שלמודל שלנו כבר יש את הארכיטקטורה הנדרשת לסיווג, כולל המסווג הסופי. תראו הודעת אזהרה שמציינת שהמשקלים של המסווג הסופי לא מאותחלים, והמודל ידרוש אימון מוקדם - זה לגמרי בסדר, כי זה בדיוק מה שאנחנו עומדים לעשות!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = transformers.TFBertForSequenceClassification.from_pretrained(bert_model,num_labels=4,output_attentions=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  109482240 \n",
      "_________________________________________________________________\n",
      "dropout_75 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  3076      \n",
      "=================================================================\n",
      "Total params: 109,485,316\n",
      "Trainable params: 109,485,316\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "כפי שניתן לראות מ-`summary()`, המודל מכיל כמעט 110 מיליון פרמטרים! כנראה שאם נרצה משימת סיווג פשוטה על מערך נתונים קטן יחסית, לא נרצה לאמן את שכבת הבסיס של BERT:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  109482240 \n",
      "_________________________________________________________________\n",
      "dropout_75 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  3076      \n",
      "=================================================================\n",
      "Total params: 109,485,316\n",
      "Trainable params: 3,076\n",
      "Non-trainable params: 109,482,240\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.layers[0].trainable = False\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "עכשיו אנחנו מוכנים להתחיל את האימון!\n",
    "\n",
    "> **הערה**: אימון מודל BERT בקנה מידה מלא יכול להיות מאוד זמן רב! לכן, נאמן אותו רק עבור 32 האצוות הראשונות. זה רק כדי להראות איך תהליך האימון של המודל מוגדר. אם אתם מעוניינים לנסות אימון בקנה מידה מלא - פשוט הסירו את הפרמטרים `steps_per_epoch` ו-`validation_steps`, והכינו את עצמכם להמתנה!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 142s 4s/step - loss: 1.3896 - acc: 0.2500 - val_loss: 1.3863 - val_acc: 0.2480\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f1d40a4b6a0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile('adam','sparse_categorical_crossentropy',['acc'])\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "model.fit(ds_train.map(process_fn).batch(32),validation_data=ds_test.map(process_fn).batch(32),steps_per_epoch=32,validation_steps=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "אם תגדילו את מספר האיטרציות ותמתינו מספיק זמן, ותאמנו במשך מספר אפוקים, תוכלו לצפות שמודל הסיווג של BERT ייתן לנו את הדיוק הטוב ביותר! זאת מכיוון ש-BERT כבר מבין היטב את מבנה השפה, וכל מה שנדרש הוא לכוונן את מסווג הסיום. עם זאת, מכיוון ש-BERT הוא מודל גדול, כל תהליך האימון לוקח זמן רב ודורש כוח חישובי משמעותי! (כרטיס גרפי, ורצוי יותר מאחד).\n",
    "\n",
    "> **הערה:** בדוגמה שלנו, השתמשנו באחד מהמודלים הקטנים ביותר של BERT שהוכנו מראש. קיימים מודלים גדולים יותר שסביר להניח שיניבו תוצאות טובות יותר.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## נקודות חשובות\n",
    "\n",
    "ביחידה זו, ראינו ארכיטקטורות מודלים עדכניות המבוססות על **טרנספורמרים**. יישמנו אותן במשימת סיווג הטקסט שלנו, אך באופן דומה, ניתן להשתמש במודלים של BERT גם לחילוץ ישויות, מענה על שאלות ומשימות עיבוד שפה טבעית (NLP) נוספות.\n",
    "\n",
    "מודלים מבוססי טרנספורמרים מייצגים את חוד החנית הנוכחי בתחום ה-NLP, וברוב המקרים הם צריכים להיות הפתרון הראשון שבו תתחילו להתנסות כאשר אתם מיישמים פתרונות מותאמים אישית בתחום זה. עם זאת, הבנה של העקרונות הבסיסיים של רשתות עצביות חוזרות (RNN), שנדונו במודול זה, היא חשובה ביותר אם ברצונכם לבנות מודלים עצביים מתקדמים.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**כתב ויתור**:  \nמסמך זה תורגם באמצעות שירות תרגום מבוסס בינה מלאכותית [Co-op Translator](https://github.com/Azure/co-op-translator). למרות שאנו שואפים לדיוק, יש לקחת בחשבון שתרגומים אוטומטיים עשויים להכיל שגיאות או אי דיוקים. המסמך המקורי בשפתו המקורית צריך להיחשב כמקור סמכותי. עבור מידע קריטי, מומלץ להשתמש בתרגום מקצועי על ידי אדם. איננו נושאים באחריות לאי הבנות או לפרשנויות שגויות הנובעות משימוש בתרגום זה.\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0cb620c6d4b9f7a635928804c26cf22403d89d98d79684e4529119355ee6d5a5"
  },
  "kernelspec": {
   "display_name": "py38_tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "coopTranslator": {
   "original_hash": "ab59c532409774988ab875f2260e8e53",
   "translation_date": "2025-08-28T21:43:24+00:00",
   "source_file": "lessons/5-NLP/18-Transformers/TransformersTF.ipynb",
   "language_code": "he"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}