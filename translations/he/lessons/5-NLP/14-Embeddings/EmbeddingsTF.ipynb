{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## הטמעות\n",
    "\n",
    "בדוגמה הקודמת שלנו, עבדנו עם וקטורי bag-of-words בעלי ממד גבוה באורך `vocab_size`, והמרנו באופן מפורש וקטורי ייצוג מיקום בעלי ממד נמוך לייצוג דל מסוג one-hot. ייצוג זה אינו יעיל מבחינת זיכרון. בנוסף, כל מילה מטופלת באופן עצמאי, כך שווקטורים מקודדים ב-one-hot אינם מבטאים דמיון סמנטי בין מילים.\n",
    "\n",
    "ביחידה זו, נמשיך לחקור את מערך הנתונים **News AG**. כדי להתחיל, נטען את הנתונים ונקבל כמה הגדרות מהיחידה הקודמת.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "\n",
    "ds_train, ds_test = tfds.load('ag_news_subset').values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### מהו embedding?\n",
    "\n",
    "הרעיון של **embedding** הוא לייצג מילים באמצעות וקטורים צפופים בממדים נמוכים יותר, שמשקפים את המשמעות הסמנטית של המילה. בהמשך נדון כיצד לבנות embeddings משמעותיים, אבל לעת עתה נחשוב על embeddings כדרך להפחית את הממדיות של וקטור מילה.\n",
    "\n",
    "לכן, שכבת embedding מקבלת מילה כקלט ומפיקה וקטור פלט בגודל `embedding_size` שנבחר. במובן מסוים, זה דומה מאוד לשכבת `Dense`, אבל במקום לקבל וקטור מקודד one-hot כקלט, היא יכולה לקבל מספר שמייצג מילה.\n",
    "\n",
    "על ידי שימוש בשכבת embedding כשכבה הראשונה ברשת שלנו, אנחנו יכולים לעבור ממודל bag-of-words למודל **embedding bag**, שבו תחילה אנו ממירים כל מילה בטקסט שלנו ל-embedding המתאים לה, ואז מחשבים פונקציית צבירה כלשהי על כל ה-embeddings, כמו `sum`, `average` או `max`.\n",
    "\n",
    "![תמונה המציגה מסווג embedding עבור חמישה רצפי מילים.](../../../../../translated_images/he/embedding-classifier-example.b77f021a7ee67eee.webp)\n",
    "\n",
    "רשת הנוירונים המסווגת שלנו מורכבת מהשכבות הבאות:\n",
    "\n",
    "* שכבת `TextVectorization`, שמקבלת מחרוזת כקלט ומפיקה טנזור של מספרי טוקנים. נגדיר גודל אוצר מילים סביר `vocab_size`, ונתעלם ממילים שמשתמשים בהן פחות. צורת הקלט תהיה 1, וצורת הפלט תהיה $n$, מכיוון שנקבל $n$ טוקנים כתוצאה, שכל אחד מהם מכיל מספרים בין 0 ל-`vocab_size`.\n",
    "* שכבת `Embedding`, שמקבלת $n$ מספרים ומצמצמת כל מספר לוקטור צפוף באורך נתון (100 בדוגמה שלנו). כך, טנזור קלט בצורת $n$ יומר לטנזור בצורת $n\\times 100$.\n",
    "* שכבת צבירה, שמחשבת את הממוצע של הטנזור הזה לאורך הציר הראשון, כלומר היא תחשב את הממוצע של כל $n$ הטנזורים הקלטיים שמייצגים מילים שונות. כדי ליישם שכבה זו, נשתמש בשכבת `Lambda`, ונעביר לתוכה את הפונקציה לחישוב הממוצע. הפלט יהיה בצורת 100, והוא יהיה הייצוג המספרי של כל רצף הקלט.\n",
    "* מסווג ליניארי סופי מסוג `Dense`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization (TextVec  (None, None)             0         \n",
      " torization)                                                     \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, None, 100)         3000000   \n",
      "                                                                 \n",
      " lambda (Lambda)             (None, 100)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4)                 404       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,000,404\n",
      "Trainable params: 3,000,404\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 30000\n",
    "batch_size = 128\n",
    "\n",
    "vectorizer = keras.layers.experimental.preprocessing.TextVectorization(max_tokens=vocab_size,input_shape=(1,))\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    vectorizer,    \n",
    "    keras.layers.Embedding(vocab_size,100),\n",
    "    keras.layers.Lambda(lambda x: tf.reduce_mean(x,axis=1)),\n",
    "    keras.layers.Dense(4, activation='softmax')\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "בפלט `summary`, בעמודת **output shape**, הממד הראשון של הטנזור `None` מתייחס לגודל המיני-אצווה, והממד השני מתייחס לאורך רצף הטוקנים. כל רצפי הטוקנים במיני-אצווה הם באורכים שונים. נדון כיצד להתמודד עם זה בסעיף הבא.\n",
    "\n",
    "עכשיו בואו נתחיל לאמן את הרשת:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training vectorizer\n",
      "938/938 [==============================] - 20s 20ms/step - loss: 0.7891 - acc: 0.8155 - val_loss: 0.4470 - val_acc: 0.8642\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22255515100>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_text(x):\n",
    "    return x['title']+' '+x['description']\n",
    "\n",
    "def tupelize(x):\n",
    "    return (extract_text(x),x['label'])\n",
    "\n",
    "print(\"Training vectorizer\")\n",
    "vectorizer.adapt(ds_train.take(500).map(extract_text))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'])\n",
    "model.fit(ds_train.map(tupelize).batch(batch_size),validation_data=ds_test.map(tupelize).batch(batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "**שימו לב** שאנחנו בונים וקטורייזר על בסיס תת-קבוצה של הנתונים. הדבר נעשה כדי להאיץ את התהליך, וזה עשוי להוביל למצב שבו לא כל הטוקנים מהטקסט שלנו נמצאים באוצר המילים. במקרה כזה, הטוקנים הללו ייעלמו, מה שעשוי לגרום לדיוק מעט נמוך יותר. עם זאת, בחיים האמיתיים תת-קבוצה של טקסט לעיתים קרובות מספקת הערכה טובה של אוצר המילים.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### התמודדות עם גדלים משתנים של רצפי משתנים\n",
    "\n",
    "בואו נבין איך מתבצע אימון במיניבאצ'ים. בדוגמה למעלה, למערך הקלט יש מימד 1, ואנחנו משתמשים במיניבאצ'ים באורך 128, כך שהגודל בפועל של המערך הוא $128 \\times 1$. עם זאת, מספר הטוקנים בכל משפט הוא שונה. אם ניישם את השכבה `TextVectorization` על קלט יחיד, מספר הטוקנים המוחזר יהיה שונה, בהתאם לאופן שבו הטקסט עובר תהליך טוקניזציה:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 1 45], shape=(2,), dtype=int64)\n",
      "tf.Tensor([ 112 1271    1    3 1747  158], shape=(6,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer('Hello, world!'))\n",
    "print(vectorizer('I am glad to meet you!'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "עם זאת, כאשר אנו מיישמים את הוקטורייזר על מספר רצפים, עליו לייצר טנזור בצורת מלבן, ולכן הוא ממלא אלמנטים לא בשימוש עם הטוקן PAD (שבמקרה שלנו הוא אפס):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 6), dtype=int64, numpy=\n",
       "array([[   1,   45,    0,    0,    0,    0],\n",
       "       [ 112, 1271,    1,    3, 1747,  158]], dtype=int64)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer(['Hello, world!','I am glad to meet you!'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "כאן אנו יכולים לראות את ההטמעות:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 1.53059261e-02,  6.80514947e-02,  3.14026810e-02, ...,\n",
       "         -8.92002955e-02,  1.52911525e-04, -5.65562584e-02],\n",
       "        [ 2.57456154e-01,  2.79364467e-01, -2.03605562e-01, ...,\n",
       "         -2.07474351e-01,  8.31158683e-02, -2.03911960e-01],\n",
       "        [ 3.98201384e-02, -8.03454965e-03,  2.39790026e-02, ...,\n",
       "         -7.18549127e-04,  2.66963355e-02, -4.30646613e-02],\n",
       "        [ 3.98201384e-02, -8.03454965e-03,  2.39790026e-02, ...,\n",
       "         -7.18549127e-04,  2.66963355e-02, -4.30646613e-02],\n",
       "        [ 3.98201384e-02, -8.03454965e-03,  2.39790026e-02, ...,\n",
       "         -7.18549127e-04,  2.66963355e-02, -4.30646613e-02],\n",
       "        [ 3.98201384e-02, -8.03454965e-03,  2.39790026e-02, ...,\n",
       "         -7.18549127e-04,  2.66963355e-02, -4.30646613e-02]],\n",
       "\n",
       "       [[ 1.89674050e-01,  2.61548996e-01, -3.67433839e-02, ...,\n",
       "         -2.07366899e-01, -1.05442435e-01, -2.36952081e-01],\n",
       "        [ 6.16133213e-02,  1.80511594e-01,  9.77298319e-02, ...,\n",
       "         -5.46628237e-02, -1.07340455e-01, -1.06589928e-01],\n",
       "        [ 1.53059261e-02,  6.80514947e-02,  3.14026810e-02, ...,\n",
       "         -8.92002955e-02,  1.52911525e-04, -5.65562584e-02],\n",
       "        [-4.84890305e-02, -8.41715634e-02,  1.51529670e-01, ...,\n",
       "          1.28192469e-01, -7.77286515e-02,  1.26041949e-01],\n",
       "        [-4.17212099e-02, -5.60694858e-02,  4.08860669e-02, ...,\n",
       "          8.70475471e-02,  8.92383084e-02,  1.67974353e-01],\n",
       "        [ 2.85779923e-01,  4.57767487e-01,  4.52292450e-02, ...,\n",
       "         -1.97419018e-01, -2.04659685e-01, -2.79758364e-01]]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[1](vectorizer(['Hello, world!','I am glad to meet you!'])).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**הערה**: כדי למזער את כמות הריפוד, במקרים מסוימים יש היגיון למיין את כל הרצפים במאגר הנתונים לפי סדר עולה של אורך (או, ליתר דיוק, מספר הטוקנים). כך יובטח שכל מיני-אצווה תכיל רצפים באורך דומה.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## הטמעות סמנטיות: Word2Vec\n",
    "\n",
    "בדוגמה הקודמת שלנו, שכבת ההטמעה למדה למפות מילים לייצוגים וקטוריים, אך לייצוגים הללו לא הייתה משמעות סמנטית. יהיה נחמד ללמוד ייצוג וקטורי כך שמילים דומות או מילים נרדפות יתאימו לוקטורים הקרובים זה לזה במונחים של מרחק וקטורי מסוים (לדוגמה, מרחק אוקלידי).\n",
    "\n",
    "כדי לעשות זאת, עלינו לאמן מראש את מודל ההטמעה שלנו על אוסף גדול של טקסט באמצעות טכניקה כמו [Word2Vec](https://en.wikipedia.org/wiki/Word2vec). הטכניקה מבוססת על שתי ארכיטקטורות עיקריות המשמשות ליצירת ייצוג מבוזר של מילים:\n",
    "\n",
    " - **Continuous bag-of-words** (CBoW), שבה אנו מאמנים את המודל לנבא מילה מתוך ההקשר הסובב אותה. בהינתן הנגרם $(W_{-2},W_{-1},W_0,W_1,W_2)$, מטרת המודל היא לנבא את $W_0$ מתוך $(W_{-2},W_{-1},W_1,W_2)$.\n",
    " - **Continuous skip-gram** הוא ההפך מ-CBoW. המודל משתמש בחלון ההקשר של המילים הסובבות כדי לנבא את המילה הנוכחית.\n",
    "\n",
    "CBoW מהיר יותר, בעוד ש-Skip-Gram איטי יותר, אך הוא עושה עבודה טובה יותר בייצוג מילים נדירות.\n",
    "\n",
    "![תמונה המציגה את האלגוריתמים CBoW ו-Skip-Gram להמרת מילים לוקטורים.](../../../../../translated_images/he/example-algorithms-for-converting-words-to-vectors.fbe9207a726922f6.webp)\n",
    "\n",
    "כדי להתנסות בהטמעת Word2Vec שאומנה מראש על מאגר הנתונים של Google News, ניתן להשתמש בספריית **gensim**. להלן נאתר את המילים הדומות ביותר ל'neural'.\n",
    "\n",
    "> **Note:** כשאתם יוצרים לראשונה וקטורי מילים, הורדתם עשויה לקחת זמן!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "w2v = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neuronal -> 0.7804799675941467\n",
      "neurons -> 0.7326500415802002\n",
      "neural_circuits -> 0.7252851724624634\n",
      "neuron -> 0.7174385190010071\n",
      "cortical -> 0.6941086649894714\n",
      "brain_circuitry -> 0.6923246383666992\n",
      "synaptic -> 0.6699118614196777\n",
      "neural_circuitry -> 0.6638563275337219\n",
      "neurochemical -> 0.6555314064025879\n",
      "neuronal_activity -> 0.6531826257705688\n"
     ]
    }
   ],
   "source": [
    "for w,p in w2v.most_similar('neural'):\n",
    "    print(f\"{w} -> {p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "אנחנו יכולים גם לחלץ את הטמעת הווקטור מהמילה, לשימוש באימון מודל הסיווג. לטמעה יש 300 רכיבים, אך כאן אנו מציגים רק את 20 הרכיבים הראשונים של הווקטור לצורך הבהרה:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01226807,  0.06225586,  0.10693359,  0.05810547,  0.23828125,\n",
       "        0.03686523,  0.05151367, -0.20703125,  0.01989746,  0.10058594,\n",
       "       -0.03759766, -0.1015625 , -0.15820312, -0.08105469, -0.0390625 ,\n",
       "       -0.05053711,  0.16015625,  0.2578125 ,  0.10058594, -0.25976562],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v['play'][:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "הדבר הנהדר בהטמעות סמנטיות הוא שניתן לתמרן את קידוד הווקטור בהתבסס על סמנטיקה. לדוגמה, אנו יכולים לבקש למצוא מילה שהייצוג הווקטורי שלה קרוב ככל האפשר למילים *מלך* ו-*אישה*, ורחוק ככל האפשר מהמילה *גבר*:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('queen', 0.7118192911148071)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar(positive=['king','woman'],negative=['man'])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "דוגמה לעיל משתמשת בכמה קסמים פנימיים של GenSym, אך ההיגיון הבסיסי למעשה די פשוט. דבר מעניין לגבי הטמעות הוא שניתן לבצע פעולות וקטור רגילות על וקטורי הטמעות, וזה ישקף פעולות על **משמעויות** של מילים. הדוגמה לעיל יכולה להתבטא במונחים של פעולות וקטור: אנו מחשבים את הווקטור המתאים ל-**KING-MAN+WOMAN** (פעולות `+` ו-`-` מתבצעות על ייצוגי וקטור של מילים מתאימות), ואז מוצאים את המילה הקרובה ביותר במילון לווקטור הזה:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'queen'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the vector corresponding to kind-man+woman\n",
    "qvec = w2v['king']-1.7*w2v['man']+1.7*w2v['woman']\n",
    "# find the index of the closest embedding vector \n",
    "d = np.sum((w2v.vectors-qvec)**2,axis=1)\n",
    "min_idx = np.argmin(d)\n",
    "# find the corresponding word\n",
    "w2v.index_to_key[min_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **הערה**: היינו צריכים להוסיף מקדמים קטנים לווקטורים של *man* ו-*woman* - נסו להסיר אותם ולראות מה קורה.\n",
    "\n",
    "כדי למצוא את הווקטור הקרוב ביותר, אנו משתמשים במנגנון של TensorFlow כדי לחשב וקטור של מרחקים בין הווקטור שלנו לבין כל הווקטורים באוצר המילים, ואז מוצאים את האינדקס של המילה המינימלית באמצעות `argmin`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "בעוד ש-Word2Vec נראה כמו דרך מצוינת לבטא סמנטיקה של מילים, יש לו חסרונות רבים, כולל הבאים:\n",
    "\n",
    "* גם מודלי CBoW וגם skip-gram הם **הטמעות ניבוי**, והם מתחשבים רק בהקשר המקומי. Word2Vec אינו מנצל את ההקשר הגלובלי.\n",
    "* Word2Vec אינו מתחשב ב**מורפולוגיה** של מילים, כלומר העובדה שמשמעות המילה יכולה להיות תלויה בחלקים שונים של המילה, כמו השורש.\n",
    "\n",
    "**FastText** מנסה להתגבר על המגבלה השנייה, ובונה על Word2Vec על ידי למידת ייצוגי וקטור עבור כל מילה וה-n-grams של תווים שנמצאים בתוך כל מילה. הערכים של הייצוגים ממוצעים לאחר מכן לווקטור אחד בכל שלב אימון. למרות שזה מוסיף הרבה חישוב נוסף לשלב ההכנה, זה מאפשר להטמעות המילים לקודד מידע תת-מילתי.\n",
    "\n",
    "שיטה נוספת, **GloVe**, משתמשת בגישה שונה להטמעות מילים, המבוססת על פירוק מטריצת ההקשר של המילים. ראשית, היא בונה מטריצה גדולה שסופרת את מספר ההופעות של מילים בהקשרים שונים, ולאחר מכן היא מנסה לייצג את המטריצה הזו בממדים נמוכים באופן שממזער את אובדן השחזור.\n",
    "\n",
    "ספריית gensim תומכת בהטמעות המילים הללו, ואתם יכולים להתנסות בהן על ידי שינוי קוד טעינת המודל שמופיע למעלה.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## שימוש באמבדינגים מוכנים מראש ב-Keras\n",
    "\n",
    "ניתן לשנות את הדוגמה לעיל כדי למלא מראש את המטריצה בשכבת האמבדינג שלנו עם אמבדינגים סמנטיים, כמו Word2Vec. סביר להניח שהמילונים של האמבדינג המוכן מראש ושל קורפוס הטקסט לא יתאימו זה לזה, ולכן עלינו לבחור אחד מהם. כאן נבחן את שתי האפשרויות האפשריות: שימוש במילון של הטוקנייזר, ושימוש במילון של אמבדינגים מ-Word2Vec.\n",
    "\n",
    "### שימוש במילון של הטוקנייזר\n",
    "\n",
    "כאשר משתמשים במילון של הטוקנייזר, חלק מהמילים במילון יהיו בעלות אמבדינגים תואמים מ-Word2Vec, וחלקן יהיו חסרות. בהתחשב בכך שגודל המילון שלנו הוא `vocab_size`, ואורך וקטור האמבדינג של Word2Vec הוא `embed_size`, שכבת האמבדינג תיוצג על ידי מטריצת משקל בצורת `vocab_size`$\\times$`embed_size`. נמלא את המטריצה הזו על ידי מעבר דרך המילון:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding size: 300\n",
      "Populating matrix, this will take some time...Done, found 4551 words, 784 words missing\n"
     ]
    }
   ],
   "source": [
    "embed_size = len(w2v.get_vector('hello'))\n",
    "print(f'Embedding size: {embed_size}')\n",
    "\n",
    "vocab = vectorizer.get_vocabulary()\n",
    "W = np.zeros((vocab_size,embed_size))\n",
    "print('Populating matrix, this will take some time...',end='')\n",
    "found, not_found = 0,0\n",
    "for i,w in enumerate(vocab):\n",
    "    try:\n",
    "        W[i] = w2v.get_vector(w)\n",
    "        found+=1\n",
    "    except:\n",
    "        # W[i] = np.random.normal(0.0,0.3,size=(embed_size,))\n",
    "        not_found+=1\n",
    "\n",
    "print(f\"Done, found {found} words, {not_found} words missing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "עבור מילים שאינן קיימות באוצר המילים של Word2Vec, אנחנו יכולים להשאיר אותן כאפסים, או ליצור וקטור אקראי.\n",
    "\n",
    "כעת נוכל להגדיר שכבת הטמעה עם משקלים שהוכנו מראש:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = keras.layers.Embedding(vocab_size,embed_size,weights=[W],trainable=False)\n",
    "model = keras.models.Sequential([\n",
    "    vectorizer, emb,\n",
    "    keras.layers.Lambda(lambda x: tf.reduce_mean(x,axis=1)),\n",
    "    keras.layers.Dense(4, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 10s 10ms/step - loss: 1.1075 - acc: 0.7822 - val_loss: 0.9134 - val_acc: 0.8175\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2220226ef10>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'])\n",
    "model.fit(ds_train.map(tupelize).batch(batch_size),\n",
    "          validation_data=ds_test.map(tupelize).batch(batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note**: שימו לב שהגדרנו `trainable=False` בעת יצירת ה-`Embedding`, מה שאומר שאנחנו לא מאמנים מחדש את שכבת ה-Embedding. זה עשוי לגרום לדיוק להיות מעט נמוך יותר, אך זה מאיץ את תהליך האימון.\n",
    "\n",
    "### שימוש באוצר מילים של Embedding\n",
    "\n",
    "בעיה אחת בגישה הקודמת היא שאוצר המילים המשמש ב-TextVectorization וב-Embedding שונה. כדי להתגבר על הבעיה הזו, ניתן להשתמש באחת מהפתרונות הבאים:\n",
    "* לאמן מחדש את מודל ה-Word2Vec על אוצר המילים שלנו.\n",
    "* לטעון את מערך הנתונים שלנו עם אוצר המילים ממודל ה-Word2Vec המאומן מראש. ניתן להגדיר את אוצר המילים המשמש לטעינת מערך הנתונים במהלך הטעינה.\n",
    "\n",
    "הגישה השנייה נראית פשוטה יותר, אז בואו ניישם אותה. קודם כל, ניצור שכבת `TextVectorization` עם אוצר מילים מוגדר, שנלקח מה-Word2Vec embeddings:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(w2v.vocab.keys())\n",
    "vectorizer = keras.layers.experimental.preprocessing.TextVectorization(input_shape=(1,))\n",
    "vectorizer.set_vocabulary(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ספריית ההטמעות של gensim מכילה פונקציה נוחה, `get_keras_embeddings`, אשר תיצור באופן אוטומטי את שכבת ההטמעות המתאימה של Keras עבורך.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "938/938 [==============================] - 20s 14ms/step - loss: 1.3377 - acc: 0.4978 - val_loss: 1.2995 - val_acc: 0.5647\n",
      "Epoch 2/5\n",
      "938/938 [==============================] - 10s 10ms/step - loss: 1.2587 - acc: 0.5722 - val_loss: 1.2339 - val_acc: 0.5842\n",
      "Epoch 3/5\n",
      "938/938 [==============================] - 10s 10ms/step - loss: 1.1980 - acc: 0.5884 - val_loss: 1.1826 - val_acc: 0.5954\n",
      "Epoch 4/5\n",
      "938/938 [==============================] - 12s 13ms/step - loss: 1.1503 - acc: 0.6002 - val_loss: 1.1417 - val_acc: 0.6018\n",
      "Epoch 5/5\n",
      "938/938 [==============================] - 11s 12ms/step - loss: 1.1120 - acc: 0.6097 - val_loss: 1.1083 - val_acc: 0.6104\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2220ccb81c0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    vectorizer, \n",
    "    w2v.get_keras_embedding(train_embeddings=False),\n",
    "    keras.layers.Lambda(lambda x: tf.reduce_mean(x,axis=1)),\n",
    "    keras.layers.Dense(4, activation='softmax')\n",
    "])\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'])\n",
    "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128),epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "אחת הסיבות לכך שאנו לא רואים דיוק גבוה יותר היא משום שחלק מהמילים ממאגר הנתונים שלנו חסרות באוצר המילים המוקדם של GloVe, ולכן הן למעשה מתעלמות. כדי להתגבר על כך, אנו יכולים לאמן את ההטבעות שלנו בהתבסס על מאגר הנתונים שלנו.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## הטמעות הקשריות\n",
    "\n",
    "אחת המגבלות המרכזיות של ייצוגי הטמעות מסורתיים שהוכנו מראש, כמו Word2Vec, היא העובדה שאמנם הם יכולים ללכוד חלק מהמשמעות של מילה, אך הם אינם מסוגלים להבחין בין משמעויות שונות. מגבלה זו יכולה לגרום לבעיות במודלים שמבוססים עליהם.\n",
    "\n",
    "לדוגמה, למילה 'play' יש משמעויות שונות בשני המשפטים הבאים:\n",
    "- הלכתי ל**הצגה** בתיאטרון.\n",
    "- ג'ון רוצה **לשחק** עם חבריו.\n",
    "\n",
    "ההטמעות שהוכנו מראש עליהן דיברנו מייצגות את שתי המשמעויות של המילה 'play' באותה הטמעה. כדי להתגבר על מגבלה זו, עלינו לבנות הטמעות המבוססות על **מודל שפה**, שמאומן על מאגר טקסטים גדול ו*יודע* כיצד מילים יכולות להשתלב בהקשרים שונים. דיון בהטמעות הקשריות חורג מתחום המדריך הזה, אך נחזור אליהן כשנדבר על מודלי שפה ביחידה הבאה.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**כתב ויתור**:  \nמסמך זה תורגם באמצעות שירות תרגום מבוסס בינה מלאכותית [Co-op Translator](https://github.com/Azure/co-op-translator). למרות שאנו שואפים לדיוק, יש לקחת בחשבון שתרגומים אוטומטיים עשויים להכיל שגיאות או אי דיוקים. המסמך המקורי בשפתו המקורית צריך להיחשב כמקור סמכותי. עבור מידע קריטי, מומלץ להשתמש בתרגום מקצועי על ידי אדם. איננו נושאים באחריות לאי הבנות או לפרשנויות שגויות הנובעות משימוש בתרגום זה.\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0cb620c6d4b9f7a635928804c26cf22403d89d98d79684e4529119355ee6d5a5"
  },
  "kernel_info": {
   "name": "conda-env-py37_tensorflow-py"
  },
  "kernelspec": {
   "display_name": "py37_tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "coopTranslator": {
   "original_hash": "b859482be7f61d1eadc2c6a2720a37e4",
   "translation_date": "2025-08-28T21:54:14+00:00",
   "source_file": "lessons/5-NLP/14-Embeddings/EmbeddingsTF.ipynb",
   "language_code": "he"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}