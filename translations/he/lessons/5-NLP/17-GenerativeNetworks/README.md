# רשתות גנרטיביות

## [שאלון לפני ההרצאה](https://ff-quizzes.netlify.app/en/ai/quiz/33)

רשתות עצביות חוזרות (RNNs) וגרסאותיהן עם תאים מבוקרים כמו תאי זיכרון ארוך-קצר (LSTMs) ויחידות חוזרות מבוקרות (GRUs) מספקות מנגנון למידול שפה בכך שהן יכולות ללמוד את סדר המילים ולספק תחזיות למילה הבאה ברצף. זה מאפשר לנו להשתמש ב-RNNs למשימות **גנרטיביות**, כמו יצירת טקסט רגיל, תרגום מכונה ואפילו תיאור תמונות.

> ✅ חשבו על כל הפעמים שבהן נהניתם ממשימות גנרטיביות כמו השלמת טקסט בזמן ההקלדה. חקרו את היישומים האהובים עליכם כדי לבדוק אם הם השתמשו ב-RNNs.

בארכיטקטורת RNN שדנו בה ביחידה הקודמת, כל יחידת RNN ייצרה את מצב החבוי הבא כתוצאה. עם זאת, ניתן גם להוסיף פלט נוסף לכל יחידה חוזרת, מה שיאפשר לנו להפיק **רצף** (ששווה באורכו לרצף המקורי). יתרה מכך, ניתן להשתמש ביחידות RNN שאינן מקבלות קלט בכל שלב, אלא רק לוקחות וקטור מצב התחלתי ומייצרות רצף של פלטים.

זה מאפשר ארכיטקטורות עצביות שונות, כפי שמוצג בתמונה הבאה:

![תמונה המציגה דפוסים נפוצים של רשתות עצביות חוזרות.](../../../../../translated_images/he/unreasonable-effectiveness-of-rnn.541ead816778f42d.webp)

> תמונה מתוך פוסט הבלוג [Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) מאת [Andrej Karpaty](http://karpathy.github.io/)

* **אחד-לאחד** הוא רשת עצבית מסורתית עם קלט אחד ופלט אחד
* **אחד-לרבים** הוא ארכיטקטורה גנרטיבית שמקבלת ערך קלט אחד ומייצרת רצף של ערכי פלט. לדוגמה, אם נרצה לאמן רשת **תיאור תמונות** שתפיק תיאור טקסטואלי של תמונה, נוכל לקחת תמונה כקלט, להעביר אותה דרך CNN כדי לקבל את מצב החבוי שלה, ואז שרשרת חוזרת תייצר את התיאור מילה אחר מילה
* **רבים-לאחד** מתאר את ארכיטקטורות RNN שדנו בהן ביחידה הקודמת, כמו סיווג טקסט
* **רבים-לרבים**, או **רצף-לרצף**, מתאר משימות כמו **תרגום מכונה**, שבהן RNN ראשון אוסף את כל המידע מרצף הקלט למצב החבוי, ושרשרת RNN אחרת פורסת את המצב הזה לרצף הפלט.

ביחידה זו נתמקד במודלים גנרטיביים פשוטים שעוזרים לנו ליצור טקסט. לשם פשטות, נשתמש בטוקניזציה ברמת תווים.

נאמן את ה-RNN הזה ליצירת טקסט שלב אחר שלב. בכל שלב, ניקח רצף של תווים באורך `nchars`, ונבקש מהרשת לייצר את התו הבא עבור כל תו קלט:

![תמונה המציגה דוגמה ליצירת המילה 'HELLO' באמצעות RNN.](../../../../../translated_images/he/rnn-generate.56c54afb52f9781d.webp)

בעת יצירת טקסט (בזמן הסקת מסקנות), נתחיל עם **הנחיה** כלשהי, שתועבר דרך תאי RNN כדי ליצור את מצב הביניים שלה, ואז מהמצב הזה מתחילה היצירה. ניצור תו אחד בכל פעם, ונעביר את המצב ואת התו שנוצר לתא RNN נוסף כדי ליצור את הבא, עד שניצור מספיק תווים.

<img src="../../../../../translated_images/he/rnn-generate-inf.5168dc65e0370eea.webp" width="60%"/>

> תמונה מאת המחבר

## ✍️ תרגילים: רשתות גנרטיביות

המשיכו ללמוד במחברות הבאות:

* [רשתות גנרטיביות עם PyTorch](GenerativePyTorch.ipynb)
* [רשתות גנרטיביות עם TensorFlow](GenerativeTF.ipynb)

## יצירת טקסט רכה וטמפרטורה

הפלט של כל תא RNN הוא התפלגות הסתברות של תווים. אם תמיד ניקח את התו עם ההסתברות הגבוהה ביותר כתו הבא בטקסט שנוצר, הטקסט יכול לעיתים להפוך ל"מחזורי" בין אותם רצפי תווים שוב ושוב, כמו בדוגמה הזו:

```
today of the second the company and a second the company ...
```

עם זאת, אם נבחן את התפלגות ההסתברות לתו הבא, ייתכן שההבדל בין כמה הסתברויות גבוהות אינו גדול, למשל תו אחד יכול להיות בעל הסתברות של 0.2, ותו אחר - 0.19, וכו'. לדוגמה, כאשר מחפשים את התו הבא ברצף '*play*', התו הבא יכול להיות באותה מידה רווח או **e** (כמו במילה *player*).

זה מוביל אותנו למסקנה שלא תמיד "הוגן" לבחור את התו עם ההסתברות הגבוהה ביותר, כי בחירת השני הגבוה ביותר עדיין יכולה להוביל לטקסט משמעותי. חכם יותר **לדגום** תווים מהתפלגות ההסתברות שניתנת על ידי פלט הרשת. ניתן גם להשתמש בפרמטר, **טמפרטורה**, שיאזן את התפלגות ההסתברות, אם נרצה להוסיף יותר אקראיות, או להפוך אותה לתלולה יותר, אם נרצה להיצמד יותר לתווים בעלי ההסתברות הגבוהה ביותר.

חקרו כיצד יצירת טקסט רכה מיושמת במחברות המקושרות לעיל.

## סיכום

בעוד שיצירת טקסט יכולה להיות שימושית בפני עצמה, היתרונות העיקריים מגיעים מהיכולת ליצור טקסט באמצעות RNNs מוקטור תכונות התחלתי כלשהו. לדוגמה, יצירת טקסט משמשת כחלק מתרגום מכונה (רצף-לרצף, במקרה זה וקטור מצב מ-*מקודד* משמש ליצירת או *פענוח* הודעה מתורגמת), או יצירת תיאור טקסטואלי של תמונה (במקרה זה וקטור התכונות יגיע ממחלץ CNN).

## 🚀 אתגר

קחו שיעורים ב-Microsoft Learn בנושא זה

* יצירת טקסט עם [PyTorch](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-pytorch/6-generative-networks/?WT.mc_id=academic-77998-cacaste)/[TensorFlow](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-tensorflow/5-generative-networks/?WT.mc_id=academic-77998-cacaste)

## [שאלון אחרי ההרצאה](https://ff-quizzes.netlify.app/en/ai/quiz/34)

## סקירה ולימוד עצמי

הנה כמה מאמרים להרחבת הידע שלכם

* גישות שונות ליצירת טקסט עם שרשרת מרקוב, LSTM ו-GPT-2: [פוסט בבלוג](https://towardsdatascience.com/text-generation-gpt-2-lstm-markov-chain-9ea371820e1e)
* דוגמת יצירת טקסט בתיעוד [Keras](https://keras.io/examples/generative/lstm_character_level_text_generation/)

## [מטלה](lab/README.md)

ראינו כיצד ליצור טקסט תו-אחר-תו. במעבדה, תחקור יצירת טקסט ברמת מילים.

---

