<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "d85c8b08f6d1b48fd7f35b99f93c1138",
  "translation_date": "2025-08-28T15:20:29+00:00",
  "source_file": "lessons/4-ComputerVision/11-ObjectDetection/README.md",
  "language_code": "sv"
}
-->
# Objektigenk√§nning

De bildklassificeringsmodeller vi har arbetat med hittills tog en bild och producerade ett kategoriskt resultat, som klassen 'nummer' i ett MNIST-problem. Men i m√•nga fall vill vi inte bara veta att en bild f√∂rest√§ller objekt ‚Äì vi vill kunna best√§mma deras exakta position. Det √§r precis detta som √§r syftet med **objektigenk√§nning**.

## [F√∂rquiz f√∂re f√∂rel√§sning](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/111)

![Objektigenk√§nning](../../../../../translated_images/Screen_Shot_2016-11-17_at_11.14.54_AM.b4bb3769353287be1b905373ed9c858102c054b16e4595c76ec3f7bba0feb549.sv.png)

> Bild fr√•n [YOLO v2-webbplats](https://pjreddie.com/darknet/yolov2/)

## Ett naivt tillv√§gag√•ngss√§tt f√∂r objektigenk√§nning

Anta att vi ville hitta en katt p√• en bild. Ett mycket naivt tillv√§gag√•ngss√§tt f√∂r objektigenk√§nning skulle vara f√∂ljande:

1. Dela upp bilden i ett antal rutor.
2. K√∂r bildklassificering p√• varje ruta.
3. De rutor som ger tillr√§ckligt h√∂g aktivering kan anses inneh√•lla det aktuella objektet.

![Naiv objektigenk√§nning](../../../../../translated_images/naive-detection.e7f1ba220ccd08c68a2ea8e06a7ed75c3fcc738c2372f9e00b7f4299a8659c01.sv.png)

> *Bild fr√•n [√∂vningsanteckningsboken](ObjectDetection-TF.ipynb)*

Detta tillv√§gag√•ngss√§tt √§r dock l√•ngt ifr√•n idealiskt, eftersom det bara till√•ter algoritmen att lokalisera objektets begr√§nsningsruta mycket oprecist. F√∂r att f√• en mer exakt position beh√∂ver vi k√∂ra n√•gon form av **regression** f√∂r att f√∂ruts√§ga koordinaterna f√∂r begr√§nsningsrutorna ‚Äì och f√∂r det beh√∂ver vi specifika dataset.

## Regression f√∂r objektigenk√§nning

[Detta blogginl√§gg](https://towardsdatascience.com/object-detection-with-neural-networks-a4e2c46b4491) ger en bra introduktion till att uppt√§cka former.

## Dataset f√∂r objektigenk√§nning

Du kan st√∂ta p√• f√∂ljande dataset f√∂r denna uppgift:

* [PASCAL VOC](http://host.robots.ox.ac.uk/pascal/VOC/) ‚Äì 20 klasser
* [COCO](http://cocodataset.org/#home) ‚Äì Common Objects in Context. 80 klasser, begr√§nsningsrutor och segmenteringsmasker

![COCO](../../../../../translated_images/coco-examples.71bc60380fa6cceb7caad48bd09e35b6028caabd363aa04fee89c414e0870e86.sv.jpg)

## M√§tv√§rden f√∂r objektigenk√§nning

### Intersection over Union

F√∂r bildklassificering √§r det enkelt att m√§ta hur v√§l algoritmen presterar, men f√∂r objektigenk√§nning m√•ste vi m√§ta b√•de korrektheten av klassen och precisionen i den f√∂rutsagda begr√§nsningsrutans position. F√∂r det senare anv√§nder vi den s√• kallade **Intersection over Union** (IoU), som m√§ter hur v√§l tv√• rutor (eller tv√• godtyckliga omr√•den) √∂verlappar.

![IoU](../../../../../translated_images/iou_equation.9a4751d40fff4e119ecd0a7bcca4e71ab1dc83e0d4f2a0d66ff0859736f593cf.sv.png)

> *Figur 2 fr√•n [detta utm√§rkta blogginl√§gg om IoU](https://pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/)*

Id√©n √§r enkel ‚Äì vi delar omr√•det f√∂r √∂verlappningen mellan tv√• figurer med omr√•det f√∂r deras union. F√∂r tv√• identiska omr√•den skulle IoU vara 1, medan f√∂r helt √•tskilda omr√•den blir det 0. Annars varierar det mellan 0 och 1. Vi betraktar vanligtvis endast de begr√§nsningsrutor d√§r IoU √§r √∂ver ett visst v√§rde.

### Genomsnittlig precision

Anta att vi vill m√§ta hur v√§l en viss klass av objekt $C$ k√§nns igen. F√∂r att m√§ta detta anv√§nder vi m√§tv√§rdet **Genomsnittlig precision** (Average Precision, AP), som ber√§knas enligt f√∂ljande:

1. Betrakta Precision-Recall-kurvan som visar noggrannheten beroende p√• ett detektionstr√∂skelv√§rde (fr√•n 0 till 1).
2. Beroende p√• tr√∂skeln kommer vi att f√• fler eller f√§rre objekt uppt√§ckta i bilden och olika v√§rden f√∂r precision och recall.
3. Kurvan kommer att se ut s√• h√§r:

<img src="https://github.com/shwars/NeuroWorkshop/raw/master/images/ObjDetectionPrecisionRecall.png"/>

> *Bild fr√•n [NeuroWorkshop](http://github.com/shwars/NeuroWorkshop)*

Den genomsnittliga precisionen f√∂r en given klass $C$ √§r arean under denna kurva. Mer exakt delas Recall-axeln vanligtvis in i 10 delar, och Precision medelv√§rdesbildas √∂ver alla dessa punkter:

$$
AP = {1\over11}\sum_{i=0}^{10}\mbox{Precision}(\mbox{Recall}={i\over10})
$$

### AP och IoU

Vi betraktar endast de detektioner d√§r IoU √§r √∂ver ett visst v√§rde. Till exempel antas vanligtvis $\mbox{IoU Threshold} = 0.5$ i PASCAL VOC-datasetet, medan AP i COCO m√§ts f√∂r olika v√§rden av $\mbox{IoU Threshold}$.

<img src="https://github.com/shwars/NeuroWorkshop/raw/master/images/ObjDetectionPrecisionRecallIoU.png"/>

> *Bild fr√•n [NeuroWorkshop](http://github.com/shwars/NeuroWorkshop)*

### Medelv√§rde av genomsnittlig precision ‚Äì mAP

Det huvudsakliga m√§tv√§rdet f√∂r objektigenk√§nning kallas **Medelv√§rde av genomsnittlig precision**, eller **mAP**. Det √§r v√§rdet av genomsnittlig precision, medelv√§rdesbildat √∂ver alla objektklasser och ibland √§ven √∂ver $\mbox{IoU Threshold}$. Processen f√∂r att ber√§kna **mAP** beskrivs mer detaljerat
[i detta blogginl√§gg](https://medium.com/@timothycarlen/understanding-the-map-evaluation-metric-for-object-detection-a07fe6962cf3)) och √§ven [h√§r med kodexempel](https://gist.github.com/tarlen5/008809c3decf19313de216b9208f3734).

## Olika tillv√§gag√•ngss√§tt f√∂r objektigenk√§nning

Det finns tv√• breda klasser av algoritmer f√∂r objektigenk√§nning:

* **Region Proposal Networks** (R-CNN, Fast R-CNN, Faster R-CNN). Huvudid√©n √§r att generera **Regions of Interest** (ROI) och k√∂ra CNN √∂ver dem f√∂r att leta efter maximal aktivering. Det liknar det naiva tillv√§gag√•ngss√§ttet, med undantaget att ROI genereras p√• ett mer intelligent s√§tt. En av de stora nackdelarna med s√•dana metoder √§r att de √§r l√•ngsamma, eftersom vi beh√∂ver m√•nga passeringar av CNN-klassificeraren √∂ver bilden.
* **En-pass** (YOLO, SSD, RetinaNet) metoder. I dessa arkitekturer designar vi n√§tverket f√∂r att f√∂ruts√§ga b√•de klasser och ROI i ett enda pass.

### R-CNN: Regionbaserad CNN

[R-CNN](http://islab.ulsan.ac.kr/files/announcement/513/rcnn_pami.pdf) anv√§nder [Selective Search](http://www.huppelen.nl/publications/selectiveSearchDraft.pdf) f√∂r att generera en hierarkisk struktur av ROI-regioner, som sedan skickas genom CNN-funktionsutdragare och SVM-klassificerare f√∂r att best√§mma objektklassen, och linj√§r regression f√∂r att best√§mma *begr√§nsningsrutans* koordinater. [Officiell artikel](https://arxiv.org/pdf/1506.01497v1.pdf)

![RCNN](../../../../../translated_images/rcnn1.cae407020dfb1d1fb572656e44f75cd6c512cc220591c116c506652c10e47f26.sv.png)

> *Bild fr√•n van de Sande et al. ICCV‚Äô11*

![RCNN-1](../../../../../translated_images/rcnn2.2d9530bb83516484ec65b250c22dbf37d3d23244f32864ebcb91d98fe7c3112c.sv.png)

> *Bilder fr√•n [denna blogg](https://towardsdatascience.com/r-cnn-fast-r-cnn-faster-r-cnn-yolo-object-detection-algorithms-36d53571365e)*

### F-RCNN ‚Äì Fast R-CNN

Detta tillv√§gag√•ngss√§tt liknar R-CNN, men regioner definieras efter att konvolutionslager har applicerats.

![FRCNN](../../../../../translated_images/f-rcnn.3cda6d9bb41888754037d2d9763e2298a96de5d9bc2a21db3147357aa5da9b1a.sv.png)

> Bild fr√•n [den officiella artikeln](https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Girshick_Fast_R-CNN_ICCV_2015_paper.pdf), [arXiv](https://arxiv.org/pdf/1504.08083.pdf), 2015

### Faster R-CNN

Huvudid√©n med detta tillv√§gag√•ngss√§tt √§r att anv√§nda ett neuralt n√§tverk f√∂r att f√∂ruts√§ga ROI ‚Äì s√• kallat *Region Proposal Network*. [Artikel](https://arxiv.org/pdf/1506.01497.pdf), 2016

![FasterRCNN](../../../../../translated_images/faster-rcnn.8d46c099b87ef30ab2ea26dbc4bdd85b974a57ba8eb526f65dc4cd0a4711de30.sv.png)

> Bild fr√•n [den officiella artikeln](https://arxiv.org/pdf/1506.01497.pdf)

### R-FCN: Regionbaserat fullt konvolutionellt n√§tverk

Denna algoritm √§r √§nnu snabbare √§n Faster R-CNN. Huvudid√©n √§r f√∂ljande:

1. Vi extraherar funktioner med hj√§lp av ResNet-101.
2. Funktionerna bearbetas av **Position-Sensitive Score Map**. Varje objekt fr√•n $C$ klasser delas in i $k\times k$ regioner, och vi tr√§nar f√∂r att f√∂ruts√§ga delar av objekt.
3. F√∂r varje del fr√•n $k\times k$ regioner r√∂star alla n√§tverk f√∂r objektklasser, och den objektklass med flest r√∂ster v√§ljs.

![r-fcn image](../../../../../translated_images/r-fcn.13eb88158b99a3da50fa2787a6be5cb310d47f0e9655cc93a1090dc7aab338d1.sv.png)

> Bild fr√•n [officiell artikel](https://arxiv.org/abs/1605.06409)

### YOLO ‚Äì You Only Look Once

YOLO √§r en realtidsalgoritm med ett enda pass. Huvudid√©n √§r f√∂ljande:

 * Bilden delas in i $S\times S$ regioner.
 * F√∂r varje region f√∂ruts√§ger **CNN** $n$ m√∂jliga objekt, *begr√§nsningsrutans* koordinater och *confidence*=*sannolikhet* * IoU.

 ![YOLO](../../../../../translated_images/yolo.a2648ec82ee8bb4ea27537677adb482fd4b733ca1705c561b6a24a85102dced5.sv.png)

> Bild fr√•n [officiell artikel](https://arxiv.org/abs/1506.02640)

### Andra algoritmer

* RetinaNet: [officiell artikel](https://arxiv.org/abs/1708.02002)
   - [PyTorch-implementation i Torchvision](https://pytorch.org/vision/stable/_modules/torchvision/models/detection/retinanet.html)
   - [Keras-implementation](https://github.com/fizyr/keras-retinanet)
   - [Objektigenk√§nning med RetinaNet](https://keras.io/examples/vision/retinanet/) i Keras-exempel
* SSD (Single Shot Detector): [officiell artikel](https://arxiv.org/abs/1512.02325)

## ‚úçÔ∏è √ñvningar: Objektigenk√§nning

Forts√§tt ditt l√§rande i f√∂ljande anteckningsbok:

[ObjectDetection.ipynb](ObjectDetection.ipynb)

## Slutsats

I denna lektion fick du en snabb genomg√•ng av alla olika s√§tt som objektigenk√§nning kan utf√∂ras p√•!

## üöÄ Utmaning

L√§s igenom dessa artiklar och anteckningsb√∂cker om YOLO och prova dem sj√§lv:

* [Bra blogginl√§gg](https://www.analyticsvidhya.com/blog/2018/12/practical-guide-object-detection-yolo-framewor-python/) som beskriver YOLO
 * [Officiell webbplats](https://pjreddie.com/darknet/yolo/)
 * Yolo: [Keras-implementation](https://github.com/experiencor/keras-yolo2), [steg-f√∂r-steg-anteckningsbok](https://github.com/experiencor/basic-yolo-keras/blob/master/Yolo%20Step-by-Step.ipynb)
 * Yolo v2: [Keras-implementation](https://github.com/experiencor/keras-yolo2), [steg-f√∂r-steg-anteckningsbok](https://github.com/experiencor/keras-yolo2/blob/master/Yolo%20Step-by-Step.ipynb)

## [Efterquiz efter f√∂rel√§sning](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/211)

## Granskning och sj√§lvstudier

* [Objektigenk√§nning](https://tjmachinelearning.com/lectures/1718/obj/) av Nikhil Sardana
* [En bra j√§mf√∂relse av algoritmer f√∂r objektigenk√§nning](https://lilianweng.github.io/lil-log/2018/12/27/object-detection-part-4.html)
* [Granskning av djupinl√§rningsalgoritmer f√∂r objektigenk√§nning](https://medium.com/comet-app/review-of-deep-learning-algorithms-for-object-detection-c1f3d437b852)
* [En steg-f√∂r-steg-introduktion till grundl√§ggande algoritmer f√∂r objektigenk√§nning](https://www.analyticsvidhya.com/blog/2018/10/a-step-by-step-introduction-to-the-basic-object-detection-algorithms-part-1/)
* [Implementation av Faster R-CNN i Python f√∂r objektigenk√§nning](https://www.analyticsvidhya.com/blog/2018/11/implementation-faster-r-cnn-python-object-detection/)

## [Uppgift: Objektigenk√§nning](lab/README.md)

---

**Ansvarsfriskrivning**:  
Detta dokument har √∂versatts med hj√§lp av AI-√∂vers√§ttningstj√§nsten [Co-op Translator](https://github.com/Azure/co-op-translator). √Ñven om vi str√§var efter noggrannhet, b√∂r det noteras att automatiserade √∂vers√§ttningar kan inneh√•lla fel eller brister. Det ursprungliga dokumentet p√• dess originalspr√•k b√∂r betraktas som den auktoritativa k√§llan. F√∂r kritisk information rekommenderas professionell m√§nsklig √∂vers√§ttning. Vi ansvarar inte f√∂r eventuella missf√∂rst√•nd eller feltolkningar som uppst√•r vid anv√§ndning av denna √∂vers√§ttning.