# Introduktion till neurala n√§tverk. Flerlagers perceptron

I f√∂reg√•ende avsnitt l√§rde du dig om den enklaste modellen f√∂r neurala n√§tverk ‚Äì en enlagers perceptron, en linj√§r tv√•klassklassificeringsmodell.

I detta avsnitt kommer vi att ut√∂ka denna modell till en mer flexibel ram, vilket g√∂r det m√∂jligt f√∂r oss att:

* utf√∂ra **flervalsklassificering** ut√∂ver tv√•klassklassificering  
* l√∂sa **regressionsproblem** ut√∂ver klassificering  
* separera klasser som inte √§r linj√§rt separerbara  

Vi kommer ocks√• att utveckla v√•r egen modul√§ra ram i Python som g√∂r det m√∂jligt f√∂r oss att konstruera olika arkitekturer f√∂r neurala n√§tverk.

## [F√∂rtest](https://ff-quizzes.netlify.app/en/ai/quiz/7)

## Formalisering av maskininl√§rning

L√•t oss b√∂rja med att formalisera problemet med maskininl√§rning. Anta att vi har ett tr√§ningsdataset **X** med etiketter **Y**, och vi beh√∂ver bygga en modell *f* som g√∂r de mest exakta f√∂ruts√§gelserna. Kvaliteten p√• f√∂ruts√§gelserna m√§ts med hj√§lp av en **f√∂rlustfunktion** &lagran;. F√∂ljande f√∂rlustfunktioner anv√§nds ofta:

* F√∂r regressionsproblem, n√§r vi beh√∂ver f√∂ruts√§ga ett tal, kan vi anv√§nda **absolut fel** &sum;<sub>i</sub>|f(x<sup>(i)</sup>)-y<sup>(i)</sup>|, eller **kvadratiskt fel** &sum;<sub>i</sub>(f(x<sup>(i)</sup>)-y<sup>(i)</sup>)<sup>2</sup>  
* F√∂r klassificering anv√§nder vi **0-1-f√∂rlust** (vilket i princip √§r detsamma som modellens **noggrannhet**) eller **logistisk f√∂rlust**.

F√∂r en enlagers perceptron definierades funktionen *f* som en linj√§r funktion *f(x)=wx+b* (h√§r √§r *w* viktmatrisen, *x* √§r vektorn av indataegenskaper, och *b* √§r biasvektorn). F√∂r olika arkitekturer av neurala n√§tverk kan denna funktion anta en mer komplex form.

> Vid klassificering √§r det ofta √∂nskv√§rt att f√• sannolikheter f√∂r motsvarande klasser som n√§tverkets utdata. F√∂r att omvandla godtyckliga tal till sannolikheter (t.ex. f√∂r att normalisera utdatan) anv√§nder vi ofta **softmax**-funktionen &sigma;, och funktionen *f* blir *f(x)=&sigma;(wx+b)*.

I definitionen av *f* ovan kallas *w* och *b* f√∂r **parametrar** &theta;=‚ü®*w,b*‚ü©. Givet datasetet ‚ü®**X**,**Y**‚ü© kan vi ber√§kna ett totalt fel f√∂r hela datasetet som en funktion av parametrarna &theta;.

> ‚úÖ **M√•let med tr√§ning av neurala n√§tverk √§r att minimera felet genom att variera parametrarna &theta;**

## Optimering med gradientnedstigning

Det finns en v√§lk√§nd metod f√∂r funktionsoptimering som kallas **gradientnedstigning**. Id√©n √§r att vi kan ber√§kna en derivata (i flerdimensionella fall kallad **gradient**) av f√∂rlustfunktionen med avseende p√• parametrarna och variera parametrarna p√• ett s√§tt som minskar felet. Detta kan formaliseras enligt f√∂ljande:

* Initiera parametrarna med n√•gra slumpm√§ssiga v√§rden w<sup>(0)</sup>, b<sup>(0)</sup>  
* Upprepa f√∂ljande steg m√•nga g√•nger:  
    - w<sup>(i+1)</sup> = w<sup>(i)</sup>-&eta;&part;&lagran;/&part;w  
    - b<sup>(i+1)</sup> = b<sup>(i)</sup>-&eta;&part;&lagran;/&part;b  

Under tr√§ningen ska optimeringsstegen ber√§knas med h√§nsyn till hela datasetet (kom ih√•g att f√∂rlusten ber√§knas som en summa √∂ver alla tr√§ningsprover). Men i praktiken tar vi sm√• delar av datasetet som kallas **minibatcher** och ber√§knar gradienter baserat p√• ett delm√§ngd av data. Eftersom delm√§ngden v√§ljs slumpm√§ssigt varje g√•ng kallas denna metod f√∂r **stokastisk gradientnedstigning** (SGD).

## Flerlagers perceptron och backpropagation

Ett enlagers n√§tverk, som vi har sett ovan, kan klassificera linj√§rt separerbara klasser. F√∂r att bygga en rikare modell kan vi kombinera flera lager i n√§tverket. Matematiskt inneb√§r det att funktionen *f* f√•r en mer komplex form och ber√§knas i flera steg:
* z<sub>1</sub>=w<sub>1</sub>x+b<sub>1</sub>  
* z<sub>2</sub>=w<sub>2</sub>&alpha;(z<sub>1</sub>)+b<sub>2</sub>  
* f = &sigma;(z<sub>2</sub>)  

H√§r √§r &alpha; en **icke-linj√§r aktiveringsfunktion**, &sigma; √§r en softmax-funktion, och parametrarna √§r &theta;=<*w<sub>1</sub>,b<sub>1</sub>,w<sub>2</sub>,b<sub>2</sub>*>.

Gradientnedstigningsalgoritmen f√∂rblir densamma, men det blir sv√•rare att ber√§kna gradienterna. Med hj√§lp av kedjeregeln f√∂r derivator kan vi ber√§kna derivatorna som:

* &part;&lagran;/&part;w<sub>2</sub> = (&part;&lagran;/&part;&sigma;)(&part;&sigma;/&part;z<sub>2</sub>)(&part;z<sub>2</sub>/&part;w<sub>2</sub>)  
* &part;&lagran;/&part;w<sub>1</sub> = (&part;&lagran;/&part;&sigma;)(&part;&sigma;/&part;z<sub>2</sub>)(&part;z<sub>2</sub>/&part;&alpha;)(&part;&alpha;/&part;z<sub>1</sub>)(&part;z<sub>1</sub>/&part;w<sub>1</sub>)  

> ‚úÖ Kedjeregeln anv√§nds f√∂r att ber√§kna derivator av f√∂rlustfunktionen med avseende p√• parametrarna.

Observera att den v√§nstra delen av alla dessa uttryck √§r densamma, och d√§rf√∂r kan vi effektivt ber√§kna derivatorna genom att b√∂rja fr√•n f√∂rlustfunktionen och g√• "bak√•t" genom ber√§kningsgrafen. D√§rf√∂r kallas metoden f√∂r att tr√§na ett flerlagers perceptron f√∂r **backpropagation**, eller 'backprop'.

<img alt="compute graph" src="../../../../../translated_images/sv/ComputeGraphGrad.4626252c0de03507.webp"/>

> TODO: bildcitering

> ‚úÖ Vi kommer att g√• igenom backpropagation mycket mer detaljerat i v√•rt notebook-exempel.  

## Slutsats

I denna lektion har vi byggt v√•rt eget bibliotek f√∂r neurala n√§tverk och anv√§nt det f√∂r en enkel tv√•dimensionell klassificeringsuppgift.

## üöÄ Utmaning

I den medf√∂ljande notebooken kommer du att implementera din egen ram f√∂r att bygga och tr√§na flerlagers perceptron. Du kommer att kunna se i detalj hur moderna neurala n√§tverk fungerar.

G√• vidare till [OwnFramework](OwnFramework.ipynb) och arbeta igenom den.

## [Eftertest](https://ff-quizzes.netlify.app/en/ai/quiz/8)

## Granskning och sj√§lvstudier

Backpropagation √§r en vanlig algoritm som anv√§nds inom AI och ML, och det √§r v√§rt att studera [mer i detalj](https://wikipedia.org/wiki/Backpropagation).

## [Uppgift](lab/README.md)

I detta labb ombeds du anv√§nda den ram du konstruerade i denna lektion f√∂r att l√∂sa klassificering av handskrivna siffror i MNIST-datasetet.

* [Instruktioner](lab/README.md)  
* [Notebook](lab/MyFW_MNIST.ipynb)  

---

