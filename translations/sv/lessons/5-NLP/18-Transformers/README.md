<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "7e617f0b8de85a43957a853aba09bfeb",
  "translation_date": "2025-08-28T15:54:22+00:00",
  "source_file": "lessons/5-NLP/18-Transformers/README.md",
  "language_code": "sv"
}
-->
# Uppm칛rksamhetsmekanismer och Transformatorer

## [Quiz f칬re f칬rel칛sningen](https://ff-quizzes.netlify.app/en/ai/quiz/35)

Ett av de viktigaste problemen inom NLP-omr친det 칛r **maskin칬vers칛ttning**, en grundl칛ggande uppgift som ligger till grund f칬r verktyg som Google Translate. I detta avsnitt kommer vi att fokusera p친 maskin칬vers칛ttning, eller mer generellt, p친 alla *sekvens-till-sekvens*-uppgifter (som ocks친 kallas **meningstransduktion**).

Med RNN:er implementeras sekvens-till-sekvens med tv친 rekurrenta n칛tverk, d칛r ett n칛tverk, **kodaren**, komprimerar en inmatningssekvens till ett dolt tillst친nd, medan ett annat n칛tverk, **avkodaren**, utvecklar detta dolda tillst친nd till ett 칬versatt resultat. Det finns ett par problem med detta tillv칛gag친ngss칛tt:

* Det slutliga tillst친ndet i kodarn칛tverket har sv친rt att komma ih친g b칬rjan av en mening, vilket leder till d친lig modellkvalitet f칬r l친nga meningar.
* Alla ord i en sekvens har samma p친verkan p친 resultatet. I verkligheten har dock specifika ord i inmatningssekvensen ofta st칬rre p친verkan p친 sekventiella utg친ngar 칛n andra.

**Uppm칛rksamhetsmekanismer** ger ett s칛tt att v칛ga den kontextuella p친verkan av varje inmatningsvektor p친 varje utg친ngsprediktion av RNN. Detta implementeras genom att skapa genv칛gar mellan mellanliggande tillst친nd i inmatnings-RNN och utg친ngs-RNN. P친 detta s칛tt, n칛r vi genererar utg친ngssymbolen y<sub>t</sub>, tar vi h칛nsyn till alla dolda inmatningstillst친nd h<sub>i</sub>, med olika viktkoefficienter 풤<sub>t,i</sub>.

![Bild som visar en kodare/avkodare-modell med ett additivt uppm칛rksamhetslager](../../../../../translated_images/encoder-decoder-attention.7a726296894fb567aa2898c94b17b3289087f6705c11907df8301df9e5eeb3de.sv.png)

> Kodare-avkodare-modellen med additiv uppm칛rksamhetsmekanism i [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf), citerad fr친n [denna bloggpost](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)

Uppm칛rksamhetsmatrisen {풤<sub>i,j</sub>} representerar graden av p친verkan som vissa inmatningsord har vid genereringen av ett visst ord i utg친ngssekvensen. Nedan 칛r ett exempel p친 en s친dan matris:

![Bild som visar ett exempel p친 justering funnen av RNNsearch-50, h칛mtad fr친n Bahdanau - arviz.org](../../../../../translated_images/bahdanau-fig3.09ba2d37f202a6af11de6c82d2d197830ba5f4528d9ea430eb65fd3a75065973.sv.png)

> Figur fr친n [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) (Fig.3)

Uppm칛rksamhetsmekanismer 칛r ansvariga f칬r mycket av den nuvarande eller n칛ra nuvarande toppniv친n inom NLP. Att l칛gga till uppm칛rksamhet 칬kar dock kraftigt antalet modellparametrar, vilket ledde till skalningsproblem med RNN:er. En viktig begr칛nsning f칬r att skala RNN:er 칛r att modellernas rekurrenta natur g칬r det utmanande att batcha och parallellisera tr칛ning. I en RNN m친ste varje element i en sekvens bearbetas i sekventiell ordning, vilket inneb칛r att det inte enkelt kan parallelliseras.

![Kodare Avkodare med Uppm칛rksamhet](../../../../../lessons/5-NLP/18-Transformers/images/EncDecAttention.gif)

> Figur fr친n [Googles Blogg](https://research.googleblog.com/2016/09/a-neural-network-for-machine.html)

Adoptionen av uppm칛rksamhetsmekanismer kombinerat med denna begr칛nsning ledde till skapandet av de nuvarande toppmodellerna f칬r transformatorer som vi k칛nner till och anv칛nder idag, s친som BERT och Open-GPT3.

## Transformator-modeller

En av huvudid칠erna bakom transformatorer 칛r att undvika den sekventiella naturen hos RNN:er och skapa en modell som kan parallelliseras under tr칛ning. Detta uppn친s genom att implementera tv친 id칠er:

* positionskodning
* anv칛nda sj칛lvuppm칛rksamhetsmekanism f칬r att f친nga m칬nster ist칛llet f칬r RNN:er (eller CNN:er) (d칛rav namnet p친 artikeln som introducerar transformatorer: *[Attention is all you need](https://arxiv.org/abs/1706.03762)*)

### Positionskodning/Positionsinb칛ddning

Id칠n med positionskodning 칛r f칬ljande.  
1. N칛r man anv칛nder RNN:er representeras den relativa positionen f칬r token av antalet steg och beh칬ver d칛rf칬r inte representeras explicit.  
2. N칛r vi d칛remot byter till uppm칛rksamhet m친ste vi veta de relativa positionerna f칬r token inom en sekvens.  
3. F칬r att f친 positionskodning f칬rst칛rker vi v친r sekvens av token med en sekvens av tokenpositioner i sekvensen (dvs. en sekvens av siffror 0,1, ...).  
4. Vi blandar sedan tokenpositionen med en tokeninb칛ddningsvektor. F칬r att transformera positionen (heltal) till en vektor kan vi anv칛nda olika metoder:

* Tr칛ningsbar inb칛ddning, liknande tokeninb칛ddning. Detta 칛r den metod vi 칬verv칛ger h칛r. Vi applicerar inb칛ddningslager ovanp친 b친de token och deras positioner, vilket resulterar i inb칛ddningsvektorer med samma dimensioner, som vi sedan l칛gger ihop.
* Fast positionskodningsfunktion, som f칬reslogs i den ursprungliga artikeln.

<img src="images/pos-embedding.png" width="50%"/>

> Bild av f칬rfattaren

Resultatet vi f친r med positionsinb칛ddning inb칛ddar b친de den ursprungliga token och dess position inom en sekvens.

### Multi-Head Sj칛lvuppm칛rksamhet

N칛sta steg 칛r att f친nga vissa m칬nster inom v친r sekvens. F칬r att g칬ra detta anv칛nder transformatorer en **sj칛lvuppm칛rksamhetsmekanism**, som i grunden 칛r uppm칛rksamhet applicerad p친 samma sekvens som inmatning och utg친ng. Genom att applicera sj칛lvuppm칛rksamhet kan vi ta h칛nsyn till **kontext** inom meningen och se vilka ord som 칛r relaterade till varandra. Till exempel kan vi se vilka ord som refereras av korreferenser, s친som *det*, och ocks친 ta kontexten i beaktande:

![](../../../../../translated_images/CoreferenceResolution.861924d6d384a7d68d8d0039d06a71a151f18a796b8b1330239d3590bd4947eb.sv.png)

> Bild fr친n [Google Blog](https://research.googleblog.com/2017/08/transformer-novel-neural-network.html)

I transformatorer anv칛nder vi **Multi-Head Attention** f칬r att ge n칛tverket kraften att f친nga flera olika typer av beroenden, t.ex. l친ngsiktiga kontra kortsiktiga relationer mellan ord, korreferenser kontra n친got annat, etc.

[TensorFlow Notebook](TransformersTF.ipynb) inneh친ller fler detaljer om implementeringen av transformatorlager.

### Kodare-Avkodare Uppm칛rksamhet

I transformatorer anv칛nds uppm칛rksamhet p친 tv친 st칛llen:

* F칬r att f친nga m칬nster inom inmatningstexten med hj칛lp av sj칛lvuppm칛rksamhet.
* F칬r att utf칬ra sekvens칬vers칛ttning - det 칛r uppm칛rksamhetslagret mellan kodaren och avkodaren.

Kodare-avkodare-uppm칛rksamhet 칛r mycket lik den uppm칛rksamhetsmekanism som anv칛nds i RNN:er, som beskrivs i b칬rjan av detta avsnitt. Denna animerade diagram f칬rklarar rollen av kodare-avkodare-uppm칛rksamhet.

![Animerad GIF som visar hur utv칛rderingarna utf칬rs i transformator-modeller.](../../../../../lessons/5-NLP/18-Transformers/images/transformer-animated-explanation.gif)

Eftersom varje inmatningsposition mappas oberoende till varje utg친ngsposition kan transformatorer parallellisera b칛ttre 칛n RNN:er, vilket m칬jligg칬r mycket st칬rre och mer uttrycksfulla spr친kmodeller. Varje uppm칛rksamhetshuvud kan anv칛ndas f칬r att l칛ra sig olika relationer mellan ord som f칬rb칛ttrar nedstr칬msuppgifter inom naturlig spr친kbehandling.

## BERT

**BERT** (Bidirectional Encoder Representations from Transformers) 칛r ett mycket stort flerskikts transformatorn칛tverk med 12 lager f칬r *BERT-base* och 24 f칬r *BERT-large*. Modellen f칬rtr칛nas f칬rst p친 en stor textkorpus (Wikipedia + b칬cker) med hj칛lp av osupervised tr칛ning (f칬ruts칛ga maskerade ord i en mening). Under f칬rtr칛ningen absorberar modellen betydande niv친er av spr친kf칬rst친else som sedan kan utnyttjas med andra dataset genom finjustering. Denna process kallas **transfer learning**.

![bild fr친n http://jalammar.github.io/illustrated-bert/](../../../../../translated_images/jalammarBERT-language-modeling-masked-lm.34f113ea5fec4362e39ee4381aab7cad06b5465a0b5f053a0f2aa05fbe14e746.sv.png)

> Bild [k칛lla](http://jalammar.github.io/illustrated-bert/)

## 九꽲잺 칐vningar: Transformatorer

Forts칛tt ditt l칛rande i f칬ljande notebooks:

* [Transformatorer i PyTorch](TransformersPyTorch.ipynb)
* [Transformatorer i TensorFlow](TransformersTF.ipynb)

## Slutsats

I denna lektion l칛rde du dig om Transformatorer och Uppm칛rksamhetsmekanismer, alla viktiga verktyg i NLP-verktygsl친dan. Det finns m친nga variationer av transformatorarkitekturer, inklusive BERT, DistilBERT, BigBird, OpenGPT3 och fler som kan finjusteras. Paketet [HuggingFace](https://github.com/huggingface/) tillhandah친ller ett arkiv f칬r att tr칛na m친nga av dessa arkitekturer med b친de PyTorch och TensorFlow.

## 游 Utmaning

## [Quiz efter f칬rel칛sningen](https://ff-quizzes.netlify.app/en/ai/quiz/36)

## Granskning & Sj칛lvstudier

* [Bloggpost](https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/), som f칬rklarar den klassiska artikeln [Attention is all you need](https://arxiv.org/abs/1706.03762) om transformatorer.
* [En serie bloggposter](https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452) om transformatorer, som f칬rklarar arkitekturen i detalj.

## [Uppgift](assignment.md)

---

**Ansvarsfriskrivning**:  
Detta dokument har 칬versatts med hj칛lp av AI-칬vers칛ttningstj칛nsten [Co-op Translator](https://github.com/Azure/co-op-translator). 츿ven om vi str칛var efter noggrannhet, b칬r du vara medveten om att automatiserade 칬vers칛ttningar kan inneh친lla fel eller felaktigheter. Det ursprungliga dokumentet p친 dess originalspr친k b칬r betraktas som den auktoritativa k칛llan. F칬r kritisk information rekommenderas professionell m칛nsklig 칬vers칛ttning. Vi ansvarar inte f칬r eventuella missf칬rst친nd eller feltolkningar som uppst친r vid anv칛ndning av denna 칬vers칛ttning.