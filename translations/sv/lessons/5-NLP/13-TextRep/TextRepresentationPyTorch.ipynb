{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Textklassificeringsuppgift\n",
    "\n",
    "Som vi nämnt kommer vi att fokusera på en enkel textklassificeringsuppgift baserad på **AG_NEWS**-datasetet, där nyhetsrubriker ska klassificeras i en av fyra kategorier: Världen, Sport, Ekonomi och Vetenskap/Teknik.\n",
    "\n",
    "## Datasetet\n",
    "\n",
    "Detta dataset är inbyggt i [`torchtext`](https://github.com/pytorch/text)-modulen, så vi kan enkelt komma åt det.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "import os\n",
    "import collections\n",
    "os.makedirs('./data',exist_ok=True)\n",
    "train_dataset, test_dataset = torchtext.datasets.AG_NEWS(root='./data')\n",
    "classes = ['World', 'Sports', 'Business', 'Sci/Tech']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Här innehåller `train_dataset` och `test_dataset` samlingar som returnerar par av etikett (klassnummer) och text respektive, till exempel:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,\n",
       " \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(train_dataset)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Så, låt oss skriva ut de första 10 nya rubrikerna från vår datamängd:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Sci/Tech** -> Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\band of ultra-cynics, are seeing green again.\n",
      "**Sci/Tech** -> Carlyle Looks Toward Commercial Aerospace (Reuters) Reuters - Private investment firm Carlyle Group,\\which has a reputation for making well-timed and occasionally\\controversial plays in the defense industry, has quietly placed\\its bets on another part of the market.\n",
      "**Sci/Tech** -> Oil and Economy Cloud Stocks' Outlook (Reuters) Reuters - Soaring crude prices plus worries\\about the economy and the outlook for earnings are expected to\\hang over the stock market next week during the depth of the\\summer doldrums.\n",
      "**Sci/Tech** -> Iraq Halts Oil Exports from Main Southern Pipeline (Reuters) Reuters - Authorities have halted oil export\\flows from the main pipeline in southern Iraq after\\intelligence showed a rebel militia could strike\\infrastructure, an oil official said on Saturday.\n",
      "**Sci/Tech** -> Oil prices soar to all-time record, posing new menace to US economy (AFP) AFP - Tearaway world oil prices, toppling records and straining wallets, present a new economic menace barely three months before the US presidential elections.\n"
     ]
    }
   ],
   "source": [
    "for i,x in zip(range(5),train_dataset):\n",
    "    print(f\"**{classes[x[0]]}** -> {x[1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eftersom dataset är iteratorer, om vi vill använda datan flera gånger måste vi konvertera den till en lista:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = torchtext.datasets.AG_NEWS(root='./data')\n",
    "train_dataset = list(train_dataset)\n",
    "test_dataset = list(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenisering\n",
    "\n",
    "Nu behöver vi konvertera text till **siffror** som kan representeras som tensorer. Om vi vill ha ordnivårepresentation måste vi göra två saker:\n",
    "* använda **tokenizer** för att dela upp texten i **token**\n",
    "* bygga ett **ordförråd** av dessa token.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['he', 'said', 'hello']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = torchtext.data.utils.get_tokenizer('basic_english')\n",
    "tokenizer('He said: hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = collections.Counter()\n",
    "for (label, line) in train_dataset:\n",
    "    counter.update(tokenizer(line))\n",
    "vocab = torchtext.vocab.vocab(counter, min_freq=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Genom att använda ordförråd kan vi enkelt koda vår tokeniserade sträng till en uppsättning siffror:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size if 95810\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[599, 3279, 97, 1220, 329, 225, 7368]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(vocab)\n",
    "print(f\"Vocab size if {vocab_size}\")\n",
    "\n",
    "stoi = vocab.get_stoi() # dict to convert tokens to indices\n",
    "\n",
    "def encode(x):\n",
    "    return [stoi[s] for s in tokenizer(x)]\n",
    "\n",
    "encode('I love to play with my words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Påsmodell för textrepresentation\n",
    "\n",
    "Eftersom ord representerar betydelse kan vi ibland förstå innebörden av en text bara genom att titta på de enskilda orden, oavsett deras ordning i meningen. Till exempel, när vi klassificerar nyheter, är ord som *väder*, *snö* sannolikt att indikera *väderprognos*, medan ord som *aktier*, *dollar* skulle peka mot *finansiella nyheter*.\n",
    "\n",
    "**Påsmodell** (BoW) vektorrepresentation är den mest använda traditionella vektorrepresentationen. Varje ord är kopplat till ett vektorindex, och varje element i vektorn innehåller antalet förekomster av ett ord i ett givet dokument.\n",
    "\n",
    "![Bild som visar hur en påsmodell-vektorrepresentation lagras i minnet.](../../../../../translated_images/sv/bag-of-words-example.606fc1738f1d7ba9.webp) \n",
    "\n",
    "> **Note**: Du kan också tänka på BoW som en summa av alla enskilda one-hot-kodade vektorer för individuella ord i texten.\n",
    "\n",
    "Nedan är ett exempel på hur man genererar en påsmodellrepresentation med hjälp av Scikit Learn python-biblioteket:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 2, 0, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "corpus = [\n",
    "        'I like hot dogs.',\n",
    "        'The dog ran fast.',\n",
    "        'Its hot outside.',\n",
    "    ]\n",
    "vectorizer.fit_transform(corpus)\n",
    "vectorizer.transform(['My dog likes hot dogs on a hot day.']).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "För att beräkna bag-of-words-vektorn från vektorrepresentationen av vår AG_NEWS-dataset kan vi använda följande funktion:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 1., 2.,  ..., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(vocab)\n",
    "\n",
    "def to_bow(text,bow_vocab_size=vocab_size):\n",
    "    res = torch.zeros(bow_vocab_size,dtype=torch.float32)\n",
    "    for i in encode(text):\n",
    "        if i<bow_vocab_size:\n",
    "            res[i] += 1\n",
    "    return res\n",
    "\n",
    "print(to_bow(train_dataset[0][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Observera:** Här använder vi den globala variabeln `vocab_size` för att ange standardstorleken på ordförrådet. Eftersom ordförrådets storlek ofta är ganska stor kan vi begränsa storleken på ordförrådet till de mest frekventa orden. Försök att sänka värdet på `vocab_size` och köra koden nedan, och se hur det påverkar noggrannheten. Du bör förvänta dig en viss minskning i noggrannhet, men inte dramatisk, i utbyte mot högre prestanda.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Träna BoW-klassificerare\n",
    "\n",
    "Nu när vi har lärt oss att bygga en Bag-of-Words-representation av vår text, låt oss träna en klassificerare ovanpå den. Först behöver vi konvertera vår dataset för träning på ett sådant sätt att alla positionsvektorrepresentationer omvandlas till Bag-of-Words-representation. Detta kan uppnås genom att skicka funktionen `bowify` som parametern `collate_fn` till standard torch `DataLoader`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import numpy as np \n",
    "\n",
    "# this collate function gets list of batch_size tuples, and needs to \n",
    "# return a pair of label-feature tensors for the whole minibatch\n",
    "def bowify(b):\n",
    "    return (\n",
    "            torch.LongTensor([t[0]-1 for t in b]),\n",
    "            torch.stack([to_bow(t[1]) for t in b])\n",
    "    )\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, collate_fn=bowify, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, collate_fn=bowify, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nu ska vi definiera ett enkelt klassificeringsneuronätverk som innehåller ett linjärt lager. Storleken på ingångsvektorn är lika med `vocab_size`, och utgångsstorleken motsvarar antalet klasser (4). Eftersom vi löser en klassificeringsuppgift är den slutliga aktiveringsfunktionen `LogSoftmax()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = torch.nn.Sequential(torch.nn.Linear(vocab_size,4),torch.nn.LogSoftmax(dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nu ska vi definiera standardträningsloopen i PyTorch. Eftersom vår dataset är ganska stor, kommer vi för undervisningssyfte att träna endast i en epok, och ibland till och med mindre än en epok (genom att ange parametern `epoch_size` kan vi begränsa träningen). Vi kommer också att rapportera ackumulerad träningsnoggrannhet under träningen; frekvensen för rapportering anges med parametern `report_freq`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(net,dataloader,lr=0.01,optimizer=None,loss_fn = torch.nn.NLLLoss(),epoch_size=None, report_freq=200):\n",
    "    optimizer = optimizer or torch.optim.Adam(net.parameters(),lr=lr)\n",
    "    net.train()\n",
    "    total_loss,acc,count,i = 0,0,0,0\n",
    "    for labels,features in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        out = net(features)\n",
    "        loss = loss_fn(out,labels) #cross_entropy(out,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss+=loss\n",
    "        _,predicted = torch.max(out,1)\n",
    "        acc+=(predicted==labels).sum()\n",
    "        count+=len(labels)\n",
    "        i+=1\n",
    "        if i%report_freq==0:\n",
    "            print(f\"{count}: acc={acc.item()/count}\")\n",
    "        if epoch_size and count>epoch_size:\n",
    "            break\n",
    "    return total_loss.item()/count, acc.item()/count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200: acc=0.8028125\n",
      "6400: acc=0.8371875\n",
      "9600: acc=0.8534375\n",
      "12800: acc=0.85765625\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.026090790722161722, 0.8620069296375267)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_epoch(net,train_loader,epoch_size=15000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BiGrams, TriGrams och N-Grams\n",
    "\n",
    "En begränsning med en bag-of-words-metod är att vissa ord ingår i flerordsuttryck. Till exempel har ordet 'hot dog' en helt annan betydelse än orden 'hot' och 'dog' i andra sammanhang. Om vi alltid representerar orden 'hot' och 'dog' med samma vektorer kan det förvirra vår modell.\n",
    "\n",
    "För att hantera detta används ofta **N-gram-representationer** i metoder för dokumentklassificering, där frekvensen av varje ord, tvåords- eller treordsuttryck är en användbar egenskap för att träna klassificerare. I en bigram-representation, till exempel, lägger vi till alla ordpar i vokabulären, utöver de ursprungliga orden.\n",
    "\n",
    "Nedan är ett exempel på hur man genererar en bigram bag-of-words-representation med hjälp av Scikit Learn:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:\n",
      " {'i': 7, 'like': 11, 'hot': 4, 'dogs': 2, 'i like': 8, 'like hot': 12, 'hot dogs': 5, 'the': 16, 'dog': 0, 'ran': 14, 'fast': 3, 'the dog': 17, 'dog ran': 1, 'ran fast': 15, 'its': 9, 'outside': 13, 'its hot': 10, 'hot outside': 6}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 1, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_vectorizer = CountVectorizer(ngram_range=(1, 2), token_pattern=r'\\b\\w+\\b', min_df=1)\n",
    "corpus = [\n",
    "        'I like hot dogs.',\n",
    "        'The dog ran fast.',\n",
    "        'Its hot outside.',\n",
    "    ]\n",
    "bigram_vectorizer.fit_transform(corpus)\n",
    "print(\"Vocabulary:\\n\",bigram_vectorizer.vocabulary_)\n",
    "bigram_vectorizer.transform(['My dog likes hot dogs on a hot day.']).toarray()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Den största nackdelen med N-gram-metoden är att ordförrådet börjar växa extremt snabbt. I praktiken behöver vi kombinera N-gram-representationen med några tekniker för dimensionsreduktion, såsom *embeddings*, vilket vi kommer att diskutera i nästa avsnitt.\n",
    "\n",
    "För att använda N-gram-representation i vår **AG News**-dataset behöver vi skapa ett speciellt ngram-ordförråd:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram vocabulary length =  1308842\n"
     ]
    }
   ],
   "source": [
    "counter = collections.Counter()\n",
    "for (label, line) in train_dataset:\n",
    "    l = tokenizer(line)\n",
    "    counter.update(torchtext.data.utils.ngrams_iterator(l,ngrams=2))\n",
    "    \n",
    "bi_vocab = torchtext.vocab.vocab(counter, min_freq=1)\n",
    "\n",
    "print(\"Bigram vocabulary length = \",len(bi_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi skulle kunna använda samma kod som ovan för att träna klassificeraren, men det skulle vara väldigt minnesineffektivt. I nästa avsnitt kommer vi att träna en bigram-klassificerare med hjälp av embeddings.\n",
    "\n",
    "> **Note:** Du kan endast behålla de ngram som förekommer i texten fler gånger än det angivna antalet. Detta säkerställer att sällsynta bigram utesluts och minskar dimensionen avsevärt. För att göra detta, ställ in parametern `min_freq` till ett högre värde och observera hur vokabulärens längd förändras.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Termfrekvens-Invers Dokumentfrekvens (TF-IDF)\n",
    "\n",
    "I BoW-representationen vägs ords förekomster lika, oavsett vilket ord det är. Det är dock uppenbart att frekventa ord, som *en*, *i*, etc., är mycket mindre viktiga för klassificering än specialiserade termer. Faktum är att i de flesta NLP-uppgifter är vissa ord mer relevanta än andra.\n",
    "\n",
    "**TF-IDF** står för **termfrekvens–invers dokumentfrekvens**. Det är en variant av bag of words, där man istället för ett binärt 0/1-värde som indikerar förekomsten av ett ord i ett dokument, använder ett flyttalsvärde som är relaterat till frekvensen av ordets förekomst i korpusen.\n",
    "\n",
    "Mer formellt definieras vikten $w_{ij}$ för ett ord $i$ i dokumentet $j$ som:\n",
    "$$\n",
    "w_{ij} = tf_{ij}\\times\\log({N\\over df_i})\n",
    "$$\n",
    "där\n",
    "* $tf_{ij}$ är antalet förekomster av $i$ i $j$, dvs. det BoW-värde vi sett tidigare\n",
    "* $N$ är antalet dokument i samlingen\n",
    "* $df_i$ är antalet dokument som innehåller ordet $i$ i hela samlingen\n",
    "\n",
    "TF-IDF-värdet $w_{ij}$ ökar proportionellt med antalet gånger ett ord förekommer i ett dokument och justeras efter antalet dokument i korpusen som innehåller ordet, vilket hjälper till att kompensera för det faktum att vissa ord förekommer oftare än andra. Till exempel, om ordet förekommer i *varje* dokument i samlingen, $df_i=N$, och $w_{ij}=0$, och dessa termer skulle helt ignoreras.\n",
    "\n",
    "Du kan enkelt skapa TF-IDF-vektorisering av text med hjälp av Scikit Learn:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.43381609, 0.        , 0.43381609, 0.        , 0.65985664,\n",
       "        0.43381609, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,2))\n",
    "vectorizer.fit_transform(corpus)\n",
    "vectorizer.transform(['My dog likes hot dogs on a hot day.']).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slutsats\n",
    "\n",
    "Även om TF-IDF-representationer ger frekvensvikt till olika ord, kan de inte representera betydelse eller ordning. Som den berömda lingvisten J. R. Firth sa år 1935: \"Den fullständiga betydelsen av ett ord är alltid kontextuell, och ingen studie av betydelse utan kontext kan tas på allvar.\" Senare i kursen kommer vi att lära oss hur man fångar kontextuell information från text med hjälp av språkmodellering.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Ansvarsfriskrivning**:  \nDetta dokument har översatts med hjälp av AI-översättningstjänsten [Co-op Translator](https://github.com/Azure/co-op-translator). Även om vi strävar efter noggrannhet, bör du vara medveten om att automatiserade översättningar kan innehålla fel eller felaktigheter. Det ursprungliga dokumentet på dess originalspråk bör betraktas som den auktoritativa källan. För kritisk information rekommenderas professionell mänsklig översättning. Vi ansvarar inte för eventuella missförstånd eller feltolkningar som uppstår vid användning av denna översättning.\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "16af2a8bbb083ea23e5e41c7f5787656b2ce26968575d8763f2c4b17f9cd711f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('py38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "coopTranslator": {
   "original_hash": "7b9040985e748e4e2d4c689892456ad7",
   "translation_date": "2025-08-28T17:52:37+00:00",
   "source_file": "lessons/5-NLP/13-TextRep/TextRepresentationPyTorch.ipynb",
   "language_code": "sv"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}