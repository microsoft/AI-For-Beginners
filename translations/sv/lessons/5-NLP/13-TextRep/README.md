<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "4522e22e150be0845e03aa41209a39d5",
  "translation_date": "2025-08-28T16:02:13+00:00",
  "source_file": "lessons/5-NLP/13-TextRep/README.md",
  "language_code": "sv"
}
-->
# Representera text som tensorer

## [Quiz f칬re f칬rel칛sning](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/113)

## Textklassificering

Under den f칬rsta delen av detta avsnitt kommer vi att fokusera p친 uppgiften **textklassificering**. Vi kommer att anv칛nda [AG News](https://www.kaggle.com/amananandrai/ag-news-classification-dataset)-datasetet, som inneh친ller nyhetsartiklar som f칬ljande:

* Kategori: Vetenskap/teknik
* Titel: Ky. F칬retag vinner bidrag f칬r att studera peptider (AP)
* Text: AP - Ett f칬retag grundat av en kemiforskare vid University of Louisville vann ett bidrag f칬r att utveckla...

V친rt m친l kommer att vara att klassificera nyhetsartikeln i en av kategorierna baserat p친 texten.

## Representera text

Om vi vill l칬sa uppgifter inom Natural Language Processing (NLP) med neurala n칛tverk beh칬ver vi ett s칛tt att representera text som tensorer. Datorer representerar redan texttecken som siffror som mappar till typsnitt p친 din sk칛rm med hj칛lp av kodningar som ASCII eller UTF-8.

<img alt="Bild som visar diagram som mappar ett tecken till en ASCII- och bin칛r representation" src="images/ascii-character-map.png" width="50%"/>

> [Bildk칛lla](https://www.seobility.net/en/wiki/ASCII)

Som m칛nniskor f칬rst친r vi vad varje bokstav **representerar**, och hur alla tecken tillsammans bildar orden i en mening. Datorer har dock inte en s친dan f칬rst친else p친 egen hand, och det neurala n칛tverket m친ste l칛ra sig betydelsen under tr칛ningen.

D칛rf칬r kan vi anv칛nda olika metoder f칬r att representera text:

* **Teckenniv친representation**, d칛r vi representerar text genom att behandla varje tecken som ett nummer. Givet att vi har *C* olika tecken i v친r textkorpus, skulle ordet *Hello* representeras av en 5x*C*-tensor. Varje bokstav skulle motsvara en tensor-kolumn i en one-hot-kodning.
* **Ordniv친representation**, d칛r vi skapar ett **ordf칬rr친d** av alla ord i v친r text och sedan representerar ord med hj칛lp av one-hot-kodning. Denna metod 칛r p친 s칛tt och vis b칛ttre, eftersom varje bokstav i sig inte har mycket betydelse, och genom att anv칛nda h칬gre semantiska koncept - ord - f칬renklar vi uppgiften f칬r det neurala n칛tverket. Dock, med tanke p친 den stora ordbokens storlek, m친ste vi hantera h칬gdimensionella glesa tensorer.

Oavsett representation m친ste vi f칬rst konvertera texten till en sekvens av **tokens**, d칛r en token 칛r antingen ett tecken, ett ord eller ibland till och med en del av ett ord. Sedan konverterar vi token till ett nummer, vanligtvis med hj칛lp av ett **ordf칬rr친d**, och detta nummer kan matas in i ett neuralt n칛tverk med hj칛lp av one-hot-kodning.

## N-Grams

I naturligt spr친k kan den exakta betydelsen av ord endast best칛mmas i kontext. Till exempel 칛r betydelserna av *neural network* och *fishing network* helt olika. Ett s칛tt att ta h칛nsyn till detta 칛r att bygga v친r modell p친 par av ord och betrakta ordpar som separata tokens i ordf칬rr친det. P친 detta s칛tt kommer meningen *I like to go fishing* att representeras av f칬ljande sekvens av tokens: *I like*, *like to*, *to go*, *go fishing*. Problemet med denna metod 칛r att ordf칬rr친dets storlek 칬kar avsev칛rt, och kombinationer som *go fishing* och *go shopping* representeras av olika tokens, som inte delar n친gon semantisk likhet trots samma verb.

I vissa fall kan vi 칬verv칛ga att anv칛nda tri-grams -- kombinationer av tre ord -- ocks친. D칛rf칬r kallas denna metod ofta **n-grams**. Det kan ocks친 vara vettigt att anv칛nda n-grams med teckenniv친representation, d칛r n-grams ungef칛r motsvarar olika stavelser.

## Bag-of-Words och TF/IDF

N칛r vi l칬ser uppgifter som textklassificering beh칬ver vi kunna representera text med en fast storleksvektor, som vi kommer att anv칛nda som indata till den slutliga t칛ta klassificeraren. Ett av de enklaste s칛tten att g칬ra detta 칛r att kombinera alla individuella ordrepresentationer, t.ex. genom att l칛gga till dem. Om vi l칛gger till one-hot-kodningar av varje ord, kommer vi att f친 en frekvensvektor som visar hur m친nga g친nger varje ord f칬rekommer i texten. En s친dan representation av text kallas **bag of words** (BoW).

<img src="images/bow.png" width="90%"/>

> Bild av f칬rfattaren

En BoW representerar i princip vilka ord som f칬rekommer i texten och i vilka m칛ngder, vilket faktiskt kan vara en bra indikation p친 vad texten handlar om. Till exempel 칛r en nyhetsartikel om politik sannolikt att inneh친lla ord som *president* och *land*, medan en vetenskaplig publikation skulle ha n친got som *kolliderare*, *uppt칛ckt*, etc. S친ledes kan ordfrekvenser i m친nga fall vara en bra indikator p친 textens inneh친ll.

Problemet med BoW 칛r att vissa vanliga ord, s친som *och*, *칛r*, etc. f칬rekommer i de flesta texter och har h칬gsta frekvenser, vilket d칬ljer de ord som verkligen 칛r viktiga. Vi kan minska vikten av dessa ord genom att ta h칛nsyn till frekvensen med vilken ord f칬rekommer i hela dokumentkollektionen. Detta 칛r huvudid칠n bakom TF/IDF-metoden, som behandlas mer detaljerat i de anteckningsb칬cker som 칛r kopplade till denna lektion.

Dock kan ingen av dessa metoder fullt ut ta h칛nsyn till textens **semantik**. Vi beh칬ver mer kraftfulla modeller f칬r neurala n칛tverk f칬r att g칬ra detta, vilket vi kommer att diskutera senare i detta avsnitt.

## 九꽲잺 칐vningar: Textrepresentation

Forts칛tt ditt l칛rande i f칬ljande anteckningsb칬cker:

* [Textrepresentation med PyTorch](TextRepresentationPyTorch.ipynb)
* [Textrepresentation med TensorFlow](TextRepresentationTF.ipynb)

## Slutsats

Hittills har vi studerat tekniker som kan l칛gga till frekvensvikt till olika ord. De kan dock inte representera betydelse eller ordning. Som den ber칬mda lingvisten J. R. Firth sa 1935: "Den fullst칛ndiga betydelsen av ett ord 칛r alltid kontextuell, och ingen studie av betydelse bortsett fr친n kontext kan tas p친 allvar." Vi kommer senare i kursen att l칛ra oss hur man f친ngar kontextuell information fr친n text med hj칛lp av spr친kmodellering.

## 游 Utmaning

Prova n친gra andra 칬vningar med bag-of-words och olika datamodeller. Du kan bli inspirerad av denna [t칛vling p친 Kaggle](https://www.kaggle.com/competitions/word2vec-nlp-tutorial/overview/part-1-for-beginners-bag-of-words)

## [Quiz efter f칬rel칛sning](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/213)

## Granskning & Sj칛lvstudier

칐va dina f칛rdigheter med textinb칛ddningar och bag-of-words-tekniker p친 [Microsoft Learn](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-pytorch/?WT.mc_id=academic-77998-cacaste)

## [Uppgift: Anteckningsb칬cker](assignment.md)

---

**Ansvarsfriskrivning**:  
Detta dokument har 칬versatts med hj칛lp av AI-칬vers칛ttningstj칛nsten [Co-op Translator](https://github.com/Azure/co-op-translator). 츿ven om vi str칛var efter noggrannhet, b칬r det noteras att automatiserade 칬vers칛ttningar kan inneh친lla fel eller brister. Det ursprungliga dokumentet p친 dess originalspr친k b칬r betraktas som den auktoritativa k칛llan. F칬r kritisk information rekommenderas professionell m칛nsklig 칬vers칛ttning. Vi ansvarar inte f칬r eventuella missf칬rst친nd eller feltolkningar som kan uppst친 vid anv칛ndning av denna 칬vers칛ttning.