{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inbäddningar\n",
    "\n",
    "I vårt tidigare exempel arbetade vi med högdimensionella bag-of-words-vektorer med längden `vocab_size`, och vi konverterade uttryckligen lågdimensionella positionsrepresentationsvektorer till glesa one-hot-representationer. Denna one-hot-representation är inte minneseffektiv. Dessutom behandlas varje ord oberoende av varandra, så one-hot-kodade vektorer uttrycker inte semantiska likheter mellan ord.\n",
    "\n",
    "I denna enhet kommer vi att fortsätta utforska **News AG**-datasetet. För att börja, låt oss ladda data och hämta några definitioner från föregående enhet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "\n",
    "ds_train, ds_test = tfds.load('ag_news_subset').values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vad är en embedding?\n",
    "\n",
    "Idén med **embedding** är att representera ord med lågdimensionella täta vektorer som reflekterar ordets semantiska betydelse. Vi kommer senare att diskutera hur man bygger meningsfulla ordembeddings, men för tillfället kan vi tänka på embeddings som ett sätt att minska dimensionaliteten hos en ordvektor.\n",
    "\n",
    "En embedding-lager tar alltså ett ord som input och producerar en utgångsvektor med en specificerad `embedding_size`. På ett sätt liknar det ett `Dense`-lager, men istället för att ta en one-hot-kodad vektor som input kan det ta ett ordnummer.\n",
    "\n",
    "Genom att använda ett embedding-lager som det första lagret i vårt nätverk kan vi byta från bag-of-words till en **embedding bag**-modell, där vi först konverterar varje ord i vår text till motsvarande embedding och sedan beräknar någon aggregeringsfunktion över alla dessa embeddings, såsom `sum`, `average` eller `max`.\n",
    "\n",
    "![Bild som visar en embedding-klassificerare för fem sekvensord.](../../../../../translated_images/sv/embedding-classifier-example.b77f021a7ee67eee.webp)\n",
    "\n",
    "Vårt klassificeringsnätverk består av följande lager:\n",
    "\n",
    "* `TextVectorization`-lager, som tar en sträng som input och producerar en tensor av tokennummer. Vi kommer att specificera en rimlig vokabulärstorlek `vocab_size` och ignorera mindre frekvent använda ord. Input-formen kommer att vara 1, och output-formen kommer att vara $n$, eftersom vi får $n$ tokens som resultat, där varje token innehåller nummer från 0 till `vocab_size`.\n",
    "* `Embedding`-lager, som tar $n$ nummer och reducerar varje nummer till en tät vektor med en given längd (100 i vårt exempel). Således kommer input-tensorn med formen $n$ att transformeras till en $n\\times 100$-tensor.\n",
    "* Aggregeringslager, som tar medelvärdet av denna tensor längs den första axeln, dvs. det kommer att beräkna medelvärdet av alla $n$ input-tensorer som motsvarar olika ord. För att implementera detta lager kommer vi att använda ett `Lambda`-lager och skicka in funktionen för att beräkna medelvärdet. Output kommer att ha formen 100 och kommer att vara den numeriska representationen av hela input-sekvensen.\n",
    "* Slutligt `Dense` linjärt klassificeringslager.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization (TextVec  (None, None)             0         \n",
      " torization)                                                     \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, None, 100)         3000000   \n",
      "                                                                 \n",
      " lambda (Lambda)             (None, 100)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4)                 404       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,000,404\n",
      "Trainable params: 3,000,404\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 30000\n",
    "batch_size = 128\n",
    "\n",
    "vectorizer = keras.layers.experimental.preprocessing.TextVectorization(max_tokens=vocab_size,input_shape=(1,))\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    vectorizer,    \n",
    "    keras.layers.Embedding(vocab_size,100),\n",
    "    keras.layers.Lambda(lambda x: tf.reduce_mean(x,axis=1)),\n",
    "    keras.layers.Dense(4, activation='softmax')\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I sammanfattningen, i kolumnen **output shape**, motsvarar den första tensordimensionen `None` minibatch-storleken, och den andra motsvarar längden på token-sekvensen. Alla token-sekvenser i minibatchen har olika längder. Vi kommer att diskutera hur man hanterar detta i nästa avsnitt.\n",
    "\n",
    "Nu tränar vi nätverket:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training vectorizer\n",
      "938/938 [==============================] - 20s 20ms/step - loss: 0.7891 - acc: 0.8155 - val_loss: 0.4470 - val_acc: 0.8642\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22255515100>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_text(x):\n",
    "    return x['title']+' '+x['description']\n",
    "\n",
    "def tupelize(x):\n",
    "    return (extract_text(x),x['label'])\n",
    "\n",
    "print(\"Training vectorizer\")\n",
    "vectorizer.adapt(ds_train.take(500).map(extract_text))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'])\n",
    "model.fit(ds_train.map(tupelize).batch(batch_size),validation_data=ds_test.map(tupelize).batch(batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "> **Observera** att vi bygger en vektorisering baserad på en delmängd av data. Detta görs för att påskynda processen, och det kan resultera i en situation där inte alla token från vår text finns med i vokabulären. I sådana fall kommer dessa token att ignoreras, vilket kan leda till något lägre noggrannhet. Dock ger en delmängd av text ofta en bra uppskattning av vokabulären i verkliga livet.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hantera variabla sekvensstorlekar\n",
    "\n",
    "Låt oss förstå hur träning sker i minibatcher. I exemplet ovan har indatatensorn dimensionen 1, och vi använder minibatcher med längden 128, så den faktiska storleken på tensorn är $128 \\times 1$. Antalet token i varje mening är dock olika. Om vi applicerar `TextVectorization`-lagret på ett enda indata, är antalet returnerade token olika beroende på hur texten tokeniseras:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 1 45], shape=(2,), dtype=int64)\n",
      "tf.Tensor([ 112 1271    1    3 1747  158], shape=(6,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer('Hello, world!'))\n",
    "print(vectorizer('I am glad to meet you!'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Men när vi använder vektoriseringen på flera sekvenser måste den producera en tensor med rektangulär form, så den fyller oanvända element med PAD-token (vilket i vårt fall är noll):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 6), dtype=int64, numpy=\n",
       "array([[   1,   45,    0,    0,    0,    0],\n",
       "       [ 112, 1271,    1,    3, 1747,  158]], dtype=int64)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer(['Hello, world!','I am glad to meet you!'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Här kan vi se inbäddningarna:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 1.53059261e-02,  6.80514947e-02,  3.14026810e-02, ...,\n",
       "         -8.92002955e-02,  1.52911525e-04, -5.65562584e-02],\n",
       "        [ 2.57456154e-01,  2.79364467e-01, -2.03605562e-01, ...,\n",
       "         -2.07474351e-01,  8.31158683e-02, -2.03911960e-01],\n",
       "        [ 3.98201384e-02, -8.03454965e-03,  2.39790026e-02, ...,\n",
       "         -7.18549127e-04,  2.66963355e-02, -4.30646613e-02],\n",
       "        [ 3.98201384e-02, -8.03454965e-03,  2.39790026e-02, ...,\n",
       "         -7.18549127e-04,  2.66963355e-02, -4.30646613e-02],\n",
       "        [ 3.98201384e-02, -8.03454965e-03,  2.39790026e-02, ...,\n",
       "         -7.18549127e-04,  2.66963355e-02, -4.30646613e-02],\n",
       "        [ 3.98201384e-02, -8.03454965e-03,  2.39790026e-02, ...,\n",
       "         -7.18549127e-04,  2.66963355e-02, -4.30646613e-02]],\n",
       "\n",
       "       [[ 1.89674050e-01,  2.61548996e-01, -3.67433839e-02, ...,\n",
       "         -2.07366899e-01, -1.05442435e-01, -2.36952081e-01],\n",
       "        [ 6.16133213e-02,  1.80511594e-01,  9.77298319e-02, ...,\n",
       "         -5.46628237e-02, -1.07340455e-01, -1.06589928e-01],\n",
       "        [ 1.53059261e-02,  6.80514947e-02,  3.14026810e-02, ...,\n",
       "         -8.92002955e-02,  1.52911525e-04, -5.65562584e-02],\n",
       "        [-4.84890305e-02, -8.41715634e-02,  1.51529670e-01, ...,\n",
       "          1.28192469e-01, -7.77286515e-02,  1.26041949e-01],\n",
       "        [-4.17212099e-02, -5.60694858e-02,  4.08860669e-02, ...,\n",
       "          8.70475471e-02,  8.92383084e-02,  1.67974353e-01],\n",
       "        [ 2.85779923e-01,  4.57767487e-01,  4.52292450e-02, ...,\n",
       "         -1.97419018e-01, -2.04659685e-01, -2.79758364e-01]]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[1](vectorizer(['Hello, world!','I am glad to meet you!'])).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Observera**: För att minimera mängden utfyllnad kan det i vissa fall vara vettigt att sortera alla sekvenser i datasetet i stigande ordning efter längd (eller, mer exakt, antalet token). Detta säkerställer att varje minibatch innehåller sekvenser med liknande längd.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantiska inbäddningar: Word2Vec\n",
    "\n",
    "I vårt tidigare exempel lärde sig inbäddningslagret att kartlägga ord till vektorrepresentationer, men dessa representationer hade ingen semantisk betydelse. Det vore bra att lära sig en vektorrepresentation där liknande ord eller synonymer motsvarar vektorer som ligger nära varandra baserat på någon vektordistans (till exempel euklidisk distans).\n",
    "\n",
    "För att göra detta behöver vi förträna vår inbäddningsmodell på en stor samling text med hjälp av en teknik som [Word2Vec](https://en.wikipedia.org/wiki/Word2vec). Den bygger på två huvudsakliga arkitekturer som används för att skapa en distribuerad representation av ord:\n",
    "\n",
    " - **Continuous bag-of-words** (CBoW), där vi tränar modellen att förutsäga ett ord utifrån den omgivande kontexten. Givet ngrammet $(W_{-2},W_{-1},W_0,W_1,W_2)$ är målet för modellen att förutsäga $W_0$ utifrån $(W_{-2},W_{-1},W_1,W_2)$.\n",
    " - **Continuous skip-gram** är motsatsen till CBoW. Modellen använder det omgivande fönstret av kontextord för att förutsäga det aktuella ordet.\n",
    "\n",
    "CBoW är snabbare, medan skip-gram är långsammare men gör ett bättre jobb med att representera sällsynta ord.\n",
    "\n",
    "![Bild som visar både CBoW- och Skip-Gram-algoritmer för att konvertera ord till vektorer.](../../../../../translated_images/sv/example-algorithms-for-converting-words-to-vectors.fbe9207a726922f6.webp)\n",
    "\n",
    "För att experimentera med Word2Vec-inbäddningen förtränad på Google News-datasetet kan vi använda **gensim**-biblioteket. Nedan hittar vi de ord som är mest liknande 'neural'.\n",
    "\n",
    "> **Note:** När du först skapar ordvektorer kan nedladdningen ta lite tid!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "w2v = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neuronal -> 0.7804799675941467\n",
      "neurons -> 0.7326500415802002\n",
      "neural_circuits -> 0.7252851724624634\n",
      "neuron -> 0.7174385190010071\n",
      "cortical -> 0.6941086649894714\n",
      "brain_circuitry -> 0.6923246383666992\n",
      "synaptic -> 0.6699118614196777\n",
      "neural_circuitry -> 0.6638563275337219\n",
      "neurochemical -> 0.6555314064025879\n",
      "neuronal_activity -> 0.6531826257705688\n"
     ]
    }
   ],
   "source": [
    "for w,p in w2v.most_similar('neural'):\n",
    "    print(f\"{w} -> {p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi kan också extrahera vektorinbäddningen från ordet, för att användas vid träning av klassificeringsmodellen. Inbäddningen har 300 komponenter, men här visar vi endast de första 20 komponenterna av vektorn för tydlighet:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01226807,  0.06225586,  0.10693359,  0.05810547,  0.23828125,\n",
       "        0.03686523,  0.05151367, -0.20703125,  0.01989746,  0.10058594,\n",
       "       -0.03759766, -0.1015625 , -0.15820312, -0.08105469, -0.0390625 ,\n",
       "       -0.05053711,  0.16015625,  0.2578125 ,  0.10058594, -0.25976562],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v['play'][:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Det fantastiska med semantiska inbäddningar är att du kan manipulera vektorkodningen baserat på semantik. Till exempel kan vi be om att hitta ett ord vars vektorrepresentation är så nära som möjligt orden *kung* och *kvinna*, och så långt som möjligt från ordet *man*:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('queen', 0.7118192911148071)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar(positive=['king','woman'],negative=['man'])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Ett exempel ovan använder lite intern GenSym-magi, men den underliggande logiken är faktiskt ganska enkel. En intressant sak med inbäddningar är att du kan utföra normala vektoroperationer på inbäddningsvektorer, och det skulle reflektera operationer på ords **betydelser**. Exemplet ovan kan uttryckas i termer av vektoroperationer: vi beräknar vektorn som motsvarar **KING-MAN+WOMAN** (operationerna `+` och `-` utförs på vektorrepresentationer av motsvarande ord), och sedan hittar vi det närmaste ordet i ordboken till den vektorn:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'queen'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the vector corresponding to kind-man+woman\n",
    "qvec = w2v['king']-1.7*w2v['man']+1.7*w2v['woman']\n",
    "# find the index of the closest embedding vector \n",
    "d = np.sum((w2v.vectors-qvec)**2,axis=1)\n",
    "min_idx = np.argmin(d)\n",
    "# find the corresponding word\n",
    "w2v.index_to_key[min_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NOTE**: Vi var tvungna att lägga till små koefficienter till *man*- och *kvinna*-vektorerna - prova att ta bort dem för att se vad som händer.\n",
    "\n",
    "För att hitta den närmaste vektorn använder vi TensorFlow-verktyg för att beräkna en vektor av avstånd mellan vår vektor och alla vektorer i vokabulären, och sedan hittar vi indexet för det minsta ordet med hjälp av `argmin`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Medan Word2Vec verkar vara ett utmärkt sätt att uttrycka ordsemantik, har det många nackdelar, inklusive följande:\n",
    "\n",
    "* Både CBoW- och skip-gram-modeller är **prediktiva inbäddningar**, och de tar endast hänsyn till lokal kontext. Word2Vec utnyttjar inte global kontext.\n",
    "* Word2Vec tar inte hänsyn till ords **morfologi**, det vill säga det faktum att ordets betydelse kan bero på olika delar av ordet, såsom roten.\n",
    "\n",
    "**FastText** försöker övervinna den andra begränsningen och bygger vidare på Word2Vec genom att lära sig vektorrepresentationer för varje ord och de teckenn-gram som finns inom varje ord. Värdena för representationerna genomsnittas sedan till en vektor vid varje träningssteg. Även om detta lägger till mycket extra beräkning under förträningen, möjliggör det att ordinbäddningar kan koda information på subordnivå.\n",
    "\n",
    "En annan metod, **GloVe**, använder en annan strategi för ordinbäddningar, baserad på faktorisering av ord-kontext-matrisen. Först bygger den en stor matris som räknar antalet förekomster av ord i olika kontexter, och sedan försöker den representera denna matris i lägre dimensioner på ett sätt som minimerar rekonstruktionsförlusten.\n",
    "\n",
    "Biblioteket gensim stöder dessa ordinbäddningar, och du kan experimentera med dem genom att ändra modellens laddningskod ovan.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Använda förtränade inbäddningar i Keras\n",
    "\n",
    "Vi kan modifiera exemplet ovan för att förfylla matrisen i vårt inbäddningslager med semantiska inbäddningar, såsom Word2Vec. Ordförråden för den förtränade inbäddningen och textkorpusen kommer sannolikt inte att matcha, så vi måste välja ett. Här utforskar vi de två möjliga alternativen: att använda tokenizer-ordförrådet och att använda ordförrådet från Word2Vec-inbäddningar.\n",
    "\n",
    "### Använda tokenizer-ordförrådet\n",
    "\n",
    "När vi använder tokenizer-ordförrådet kommer vissa av orden i ordförrådet att ha motsvarande Word2Vec-inbäddningar, medan andra kommer att saknas. Givet att vår ordförrådsstorlek är `vocab_size` och att Word2Vec-inbäddningsvektorns längd är `embed_size`, kommer inbäddningslagret att representeras av en viktmatris med formen `vocab_size`$\\times$`embed_size`. Vi kommer att fylla denna matris genom att gå igenom ordförrådet:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding size: 300\n",
      "Populating matrix, this will take some time...Done, found 4551 words, 784 words missing\n"
     ]
    }
   ],
   "source": [
    "embed_size = len(w2v.get_vector('hello'))\n",
    "print(f'Embedding size: {embed_size}')\n",
    "\n",
    "vocab = vectorizer.get_vocabulary()\n",
    "W = np.zeros((vocab_size,embed_size))\n",
    "print('Populating matrix, this will take some time...',end='')\n",
    "found, not_found = 0,0\n",
    "for i,w in enumerate(vocab):\n",
    "    try:\n",
    "        W[i] = w2v.get_vector(w)\n",
    "        found+=1\n",
    "    except:\n",
    "        # W[i] = np.random.normal(0.0,0.3,size=(embed_size,))\n",
    "        not_found+=1\n",
    "\n",
    "print(f\"Done, found {found} words, {not_found} words missing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "För ord som inte finns i Word2Vec-ordförrådet kan vi antingen lämna dem som nollor eller generera en slumpmässig vektor.\n",
    "\n",
    "Nu kan vi definiera ett inbäddningslager med förtränade vikter:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = keras.layers.Embedding(vocab_size,embed_size,weights=[W],trainable=False)\n",
    "model = keras.models.Sequential([\n",
    "    vectorizer, emb,\n",
    "    keras.layers.Lambda(lambda x: tf.reduce_mean(x,axis=1)),\n",
    "    keras.layers.Dense(4, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 10s 10ms/step - loss: 1.1075 - acc: 0.7822 - val_loss: 0.9134 - val_acc: 0.8175\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2220226ef10>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'])\n",
    "model.fit(ds_train.map(tupelize).batch(batch_size),\n",
    "          validation_data=ds_test.map(tupelize).batch(batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Observera**: Notera att vi sätter `trainable=False` när vi skapar `Embedding`, vilket innebär att vi inte tränar om Embedding-lagret. Detta kan leda till något lägre noggrannhet, men det påskyndar träningen.\n",
    "\n",
    "### Använda inbäddningsordförråd\n",
    "\n",
    "Ett problem med det tidigare tillvägagångssättet är att ordförråden som används i TextVectorization och Embedding är olika. För att lösa detta problem kan vi använda en av följande lösningar:\n",
    "* Träna om Word2Vec-modellen på vårt ordförråd.\n",
    "* Ladda vår dataset med ordförrådet från den förtränade Word2Vec-modellen. Ordförråd som används för att ladda datasetet kan specificeras under laddningen.\n",
    "\n",
    "Det senare tillvägagångssättet verkar enklare, så låt oss implementera det. Först och främst kommer vi att skapa ett `TextVectorization`-lager med det specificerade ordförrådet, hämtat från Word2Vec-inbäddningarna:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(w2v.vocab.keys())\n",
    "vectorizer = keras.layers.experimental.preprocessing.TextVectorization(input_shape=(1,))\n",
    "vectorizer.set_vocabulary(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gensim-biblioteket för ordbaser innehåller en praktisk funktion, `get_keras_embeddings`, som automatiskt skapar det motsvarande Keras-embeddingslagret åt dig.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "938/938 [==============================] - 20s 14ms/step - loss: 1.3377 - acc: 0.4978 - val_loss: 1.2995 - val_acc: 0.5647\n",
      "Epoch 2/5\n",
      "938/938 [==============================] - 10s 10ms/step - loss: 1.2587 - acc: 0.5722 - val_loss: 1.2339 - val_acc: 0.5842\n",
      "Epoch 3/5\n",
      "938/938 [==============================] - 10s 10ms/step - loss: 1.1980 - acc: 0.5884 - val_loss: 1.1826 - val_acc: 0.5954\n",
      "Epoch 4/5\n",
      "938/938 [==============================] - 12s 13ms/step - loss: 1.1503 - acc: 0.6002 - val_loss: 1.1417 - val_acc: 0.6018\n",
      "Epoch 5/5\n",
      "938/938 [==============================] - 11s 12ms/step - loss: 1.1120 - acc: 0.6097 - val_loss: 1.1083 - val_acc: 0.6104\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2220ccb81c0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    vectorizer, \n",
    "    w2v.get_keras_embedding(train_embeddings=False),\n",
    "    keras.layers.Lambda(lambda x: tf.reduce_mean(x,axis=1)),\n",
    "    keras.layers.Dense(4, activation='softmax')\n",
    "])\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'])\n",
    "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128),epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En av anledningarna till att vi inte ser högre noggrannhet är att vissa ord från vår dataset saknas i den förtränade GloVe-ordlistan och därför i princip ignoreras. För att lösa detta kan vi träna våra egna inbäddningar baserade på vår dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kontextuella inbäddningar\n",
    "\n",
    "En viktig begränsning med traditionella förtränade inbäddningsrepresentationer som Word2Vec är att även om de kan fånga en viss betydelse av ett ord, kan de inte skilja mellan olika betydelser. Detta kan orsaka problem i nedströmsmodeller.\n",
    "\n",
    "Till exempel har ordet 'play' olika betydelser i dessa två meningar:\n",
    "- Jag gick på en **pjäs** på teatern.\n",
    "- John vill **leka** med sina vänner.\n",
    "\n",
    "De förtränade inbäddningarna vi pratade om representerar båda betydelserna av ordet 'play' i samma inbäddning. För att övervinna denna begränsning behöver vi bygga inbäddningar baserade på **språkmodellen**, som är tränad på en stor textkorpus och *vet* hur ord kan sättas ihop i olika kontexter. Att diskutera kontextuella inbäddningar ligger utanför ramen för denna handledning, men vi kommer tillbaka till dem när vi pratar om språkmodeller i nästa enhet.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Ansvarsfriskrivning**:  \nDetta dokument har översatts med hjälp av AI-översättningstjänsten [Co-op Translator](https://github.com/Azure/co-op-translator). Även om vi strävar efter noggrannhet, bör du vara medveten om att automatiserade översättningar kan innehålla fel eller brister. Det ursprungliga dokumentet på dess originalspråk bör betraktas som den auktoritativa källan. För kritisk information rekommenderas professionell mänsklig översättning. Vi ansvarar inte för eventuella missförstånd eller feltolkningar som uppstår vid användning av denna översättning.\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0cb620c6d4b9f7a635928804c26cf22403d89d98d79684e4529119355ee6d5a5"
  },
  "kernel_info": {
   "name": "conda-env-py37_tensorflow-py"
  },
  "kernelspec": {
   "display_name": "py37_tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "coopTranslator": {
   "original_hash": "b859482be7f61d1eadc2c6a2720a37e4",
   "translation_date": "2025-08-28T17:46:17+00:00",
   "source_file": "lessons/5-NLP/14-Embeddings/EmbeddingsTF.ipynb",
   "language_code": "sv"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}