<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "e40b47ac3fd48f71304ede1474e66293",
  "translation_date": "2025-08-28T16:00:44+00:00",
  "source_file": "lessons/5-NLP/14-Embeddings/README.md",
  "language_code": "sv"
}
-->
# Inb√§ddningar

## [Quiz f√∂re f√∂rel√§sning](https://ff-quizzes.netlify.app/en/ai/quiz/27)

N√§r vi tr√§nade klassificerare baserade p√• BoW eller TF/IDF arbetade vi med h√∂gdimensionella bag-of-words-vektorer med l√§ngden `vocab_size`, och vi konverterade explicit fr√•n l√•gdimensionella positionsrepresentationsvektorer till glesa one-hot-representationer. Denna one-hot-representation √§r dock inte minneseffektiv. Dessutom behandlas varje ord oberoende av varandra, dvs. one-hot-kodade vektorer uttrycker ingen semantisk likhet mellan ord.

Id√©n med **inb√§ddning** √§r att representera ord med l√•gdimensionella t√§ta vektorer som p√• n√•got s√§tt reflekterar den semantiska betydelsen av ett ord. Vi kommer senare att diskutera hur man bygger meningsfulla ordb√§ddningar, men f√∂r tillf√§llet kan vi t√§nka p√• inb√§ddningar som ett s√§tt att minska dimensionen av en ordvektor.

Inb√§ddningslagret tar allts√• ett ord som indata och producerar en utdata-vektor med en specificerad `embedding_size`. P√• ett s√§tt liknar det ett `Linear`-lager, men ist√§llet f√∂r att ta en one-hot-kodad vektor kan det ta ett ordnummer som indata, vilket g√∂r att vi kan undvika att skapa stora one-hot-kodade vektorer.

Genom att anv√§nda ett inb√§ddningslager som det f√∂rsta lagret i v√•rt klassificeringsn√§tverk kan vi byta fr√•n en bag-of-words-modell till en **embedding bag**-modell, d√§r vi f√∂rst konverterar varje ord i v√•r text till motsvarande inb√§ddning och sedan ber√§knar n√•gon aggregeringsfunktion √∂ver alla dessa inb√§ddningar, s√•som `sum`, `average` eller `max`.

![Bild som visar en inb√§ddningsklassificerare f√∂r fem sekvensord.](../../../../../translated_images/embedding-classifier-example.b77f021a7ee67eeec8e68bfe11636c5b97d6eaa067515a129bfb1d0034b1ac5b.sv.png)

> Bild av f√∂rfattaren

## ‚úçÔ∏è √ñvningar: Inb√§ddningar

Forts√§tt ditt l√§rande i f√∂ljande anteckningsb√∂cker:
* [Inb√§ddningar med PyTorch](EmbeddingsPyTorch.ipynb)
* [Inb√§ddningar med TensorFlow](EmbeddingsTF.ipynb)

## Semantiska inb√§ddningar: Word2Vec

√Ñven om inb√§ddningslagret l√§rde sig att mappa ord till vektorrepresentationer, hade denna representation inte n√∂dv√§ndigtvis mycket semantisk betydelse. Det vore bra att l√§ra sig en vektorrepresentation d√§r liknande ord eller synonymer motsvarar vektorer som √§r n√§ra varandra i termer av n√•gon vektordistans (t.ex. euklidisk distans).

F√∂r att g√∂ra detta beh√∂ver vi f√∂rtr√§na v√•r inb√§ddningsmodell p√• en stor samling text p√• ett specifikt s√§tt. Ett s√§tt att tr√§na semantiska inb√§ddningar kallas [Word2Vec](https://en.wikipedia.org/wiki/Word2vec). Det bygger p√• tv√• huvudsakliga arkitekturer som anv√§nds f√∂r att producera en distribuerad representation av ord:

 - **Continuous bag-of-words** (CBoW) ‚Äî i denna arkitektur tr√§nar vi modellen att f√∂ruts√§ga ett ord fr√•n omgivande kontext. Givet n-grammet $(W_{-2},W_{-1},W_0,W_1,W_2)$ √§r m√•let f√∂r modellen att f√∂ruts√§ga $W_0$ fr√•n $(W_{-2},W_{-1},W_1,W_2)$.
 - **Continuous skip-gram** √§r motsatsen till CBoW. Modellen anv√§nder det omgivande f√∂nstret av kontextord f√∂r att f√∂ruts√§ga det aktuella ordet.

CBoW √§r snabbare, medan skip-gram √§r l√•ngsammare men g√∂r ett b√§ttre jobb med att representera s√§llsynta ord.

![Bild som visar b√•de CBoW- och Skip-Gram-algoritmer f√∂r att konvertera ord till vektorer.](../../../../../translated_images/example-algorithms-for-converting-words-to-vectors.fbe9207a726922f6f0f5de66427e8a6eda63809356114e28fb1fa5f4a83ebda7.sv.png)

> Bild fr√•n [denna artikel](https://arxiv.org/pdf/1301.3781.pdf)

F√∂rtr√§nade Word2Vec-inb√§ddningar (liksom andra liknande modeller, s√•som GloVe) kan ocks√• anv√§ndas ist√§llet f√∂r inb√§ddningslager i neurala n√§tverk. Vi m√•ste dock hantera vokabul√§rer, eftersom vokabul√§ren som anv√§ndes f√∂r att f√∂rtr√§na Word2Vec/GloVe sannolikt skiljer sig fr√•n vokabul√§ren i v√•r textkorpus. Titta i de ovanst√•ende anteckningsb√∂ckerna f√∂r att se hur detta problem kan l√∂sas.

## Kontextuella inb√§ddningar

En viktig begr√§nsning med traditionella f√∂rtr√§nade inb√§ddningsrepresentationer som Word2Vec √§r problemet med ords betydelse i olika sammanhang. √Ñven om f√∂rtr√§nade inb√§ddningar kan f√•nga en del av ordens betydelse i kontext, kodas varje m√∂jlig betydelse av ett ord i samma inb√§ddning. Detta kan orsaka problem i nedstr√∂msmodeller, eftersom m√•nga ord, som ordet 'play', har olika betydelser beroende p√• sammanhanget de anv√§nds i.

Till exempel har ordet 'play' i dessa tv√• meningar ganska olika betydelser:

- Jag gick p√• en **pj√§s** p√• teatern.
- John vill **leka** med sina v√§nner.

De f√∂rtr√§nade inb√§ddningarna ovan representerar b√•da dessa betydelser av ordet 'play' i samma inb√§ddning. F√∂r att √∂vervinna denna begr√§nsning beh√∂ver vi bygga inb√§ddningar baserade p√• **spr√•kmodeller**, som tr√§nas p√• en stor textkorpus och *vet* hur ord kan s√§ttas ihop i olika sammanhang. Att diskutera kontextuella inb√§ddningar ligger utanf√∂r ramen f√∂r denna handledning, men vi kommer tillbaka till dem n√§r vi pratar om spr√•kmodeller senare i kursen.

## Slutsats

I denna lektion uppt√§ckte du hur man bygger och anv√§nder inb√§ddningslager i TensorFlow och PyTorch f√∂r att b√§ttre reflektera ordens semantiska betydelser.

## üöÄ Utmaning

Word2Vec har anv√§nts f√∂r n√•gra intressanta till√§mpningar, inklusive att generera s√•ngtexter och poesi. Ta en titt p√• [denna artikel](https://www.politetype.com/blog/word2vec-color-poems) som g√•r igenom hur f√∂rfattaren anv√§nde Word2Vec f√∂r att generera poesi. Titta ocks√• p√• [denna video av Dan Shiffmann](https://www.youtube.com/watch?v=LSS_bos_TPI&ab_channel=TheCodingTrain) f√∂r att uppt√§cka en annan f√∂rklaring av denna teknik. F√∂rs√∂k sedan att till√§mpa dessa tekniker p√• din egen textkorpus, kanske h√§mtad fr√•n Kaggle.

## [Quiz efter f√∂rel√§sning](https://ff-quizzes.netlify.app/en/ai/quiz/28)

## Granskning & Sj√§lvstudier

L√§s igenom denna artikel om Word2Vec: [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf)

## [Uppgift: Anteckningsb√∂cker](assignment.md)

---

**Ansvarsfriskrivning**:  
Detta dokument har √∂versatts med hj√§lp av AI-√∂vers√§ttningstj√§nsten [Co-op Translator](https://github.com/Azure/co-op-translator). √Ñven om vi str√§var efter noggrannhet, b√∂r du vara medveten om att automatiserade √∂vers√§ttningar kan inneh√•lla fel eller felaktigheter. Det ursprungliga dokumentet p√• dess ursprungliga spr√•k b√∂r betraktas som den auktoritativa k√§llan. F√∂r kritisk information rekommenderas professionell m√§nsklig √∂vers√§ttning. Vi ansvarar inte f√∂r eventuella missf√∂rst√•nd eller feltolkningar som uppst√•r vid anv√§ndning av denna √∂vers√§ttning.