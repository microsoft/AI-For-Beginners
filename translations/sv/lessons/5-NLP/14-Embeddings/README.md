# Inb√§ddningar

## [F√∂rf√∂rel√§sningsquiz](https://ff-quizzes.netlify.app/en/ai/quiz/27)

N√§r vi tr√§nade klassificerare baserade p√• BoW eller TF/IDF arbetade vi med h√∂gdimensionella bag-of-words-vektorer med l√§ngden `vocab_size`, och vi konverterade uttryckligen fr√•n l√•gdimensionella positionsrepresentationsvektorer till glesa one-hot-representationer. Denna one-hot-representation √§r dock inte minneseffektiv. Dessutom behandlas varje ord oberoende av varandra, dvs. one-hot-kodade vektorer uttrycker ingen semantisk likhet mellan ord.

Id√©n med **inb√§ddning** √§r att representera ord med l√•gdimensionella t√§ta vektorer som p√• n√•got s√§tt reflekterar den semantiska betydelsen av ett ord. Vi kommer senare att diskutera hur man bygger meningsfulla ordinb√§ddningar, men f√∂r tillf√§llet kan vi bara t√§nka p√• inb√§ddningar som ett s√§tt att minska dimensionaliteten hos en ordvektor.

S√•, inb√§ddningslagret skulle ta ett ord som indata och producera en utdata-vektor med en specificerad `embedding_size`. P√• s√§tt och vis √§r det mycket likt ett `Linear`-lager, men ist√§llet f√∂r att ta en one-hot-kodad vektor kan det ta ett ordnummer som indata, vilket g√∂r att vi kan undvika att skapa stora one-hot-kodade vektorer.

Genom att anv√§nda ett inb√§ddningslager som det f√∂rsta lagret i v√•rt klassificeringsn√§tverk kan vi byta fr√•n en bag-of-words till en **embedding bag**-modell, d√§r vi f√∂rst konverterar varje ord i v√•r text till motsvarande inb√§ddning och sedan ber√§knar n√•gon aggregeringsfunktion √∂ver alla dessa inb√§ddningar, s√•som `sum`, `average` eller `max`.  

![Bild som visar en inb√§ddningsklassificerare f√∂r fem sekvensord.](../../../../../translated_images/sv/embedding-classifier-example.b77f021a7ee67eee.webp)

> Bild av f√∂rfattaren

## ‚úçÔ∏è √ñvningar: Inb√§ddningar

Forts√§tt ditt l√§rande i f√∂ljande anteckningsb√∂cker:
* [Inb√§ddningar med PyTorch](EmbeddingsPyTorch.ipynb)
* [Inb√§ddningar med TensorFlow](EmbeddingsTF.ipynb)

## Semantiska inb√§ddningar: Word2Vec

√Ñven om inb√§ddningslagret l√§rde sig att kartl√§gga ord till vektorrepresentation, hade denna representation inte n√∂dv√§ndigtvis mycket semantisk betydelse. Det skulle vara bra att l√§ra sig en vektorrepresentation d√§r liknande ord eller synonymer motsvarar vektorer som √§r n√§ra varandra i termer av n√•gon vektordistans (t.ex. euklidisk distans).

F√∂r att g√∂ra detta beh√∂ver vi f√∂rtr√§na v√•r inb√§ddningsmodell p√• en stor samling text p√• ett specifikt s√§tt. Ett s√§tt att tr√§na semantiska inb√§ddningar kallas [Word2Vec](https://en.wikipedia.org/wiki/Word2vec). Det baseras p√• tv√• huvudsakliga arkitekturer som anv√§nds f√∂r att producera en distribuerad representation av ord:

 - **Continuous bag-of-words** (CBoW) ‚Äî i denna arkitektur tr√§nar vi modellen att f√∂ruts√§ga ett ord fr√•n den omgivande kontexten. Givet ngrammet $(W_{-2},W_{-1},W_0,W_1,W_2)$ √§r m√•let f√∂r modellen att f√∂ruts√§ga $W_0$ fr√•n $(W_{-2},W_{-1},W_1,W_2)$.
 - **Continuous skip-gram** √§r motsatsen till CBoW. Modellen anv√§nder det omgivande f√∂nstret av kontextord f√∂r att f√∂ruts√§ga det aktuella ordet.

CBoW √§r snabbare, medan skip-gram √§r l√•ngsammare men g√∂r ett b√§ttre jobb med att representera s√§llsynta ord.

![Bild som visar b√•de CBoW- och Skip-Gram-algoritmer f√∂r att konvertera ord till vektorer.](../../../../../translated_images/sv/example-algorithms-for-converting-words-to-vectors.fbe9207a726922f6.webp)

> Bild fr√•n [denna artikel](https://arxiv.org/pdf/1301.3781.pdf)

F√∂rtr√§nade Word2Vec-inb√§ddningar (liksom andra liknande modeller, s√•som GloVe) kan ocks√• anv√§ndas ist√§llet f√∂r inb√§ddningslager i neurala n√§tverk. Dock m√•ste vi hantera vokabul√§rer, eftersom vokabul√§ren som anv√§ndes f√∂r att f√∂rtr√§na Word2Vec/GloVe sannolikt skiljer sig fr√•n vokabul√§ren i v√•r textkorpus. Titta i ovanst√•ende anteckningsb√∂cker f√∂r att se hur detta problem kan l√∂sas.

## Kontextuella inb√§ddningar

En viktig begr√§nsning med traditionella f√∂rtr√§nade inb√§ddningsrepresentationer som Word2Vec √§r problemet med ords betydelseutredning. √Ñven om f√∂rtr√§nade inb√§ddningar kan f√•nga en del av ordens betydelse i kontext, kodas varje m√∂jlig betydelse av ett ord i samma inb√§ddning. Detta kan orsaka problem i nedstr√∂msmodeller, eftersom m√•nga ord, s√•som ordet 'play', har olika betydelser beroende p√• kontexten de anv√§nds i.

Till exempel har ordet 'play' i dessa tv√• olika meningar ganska olika betydelser:

- Jag gick p√• en **pj√§s** p√• teatern.
- John vill **leka** med sina v√§nner.

De f√∂rtr√§nade inb√§ddningarna ovan representerar b√•da dessa betydelser av ordet 'play' i samma inb√§ddning. F√∂r att √∂vervinna denna begr√§nsning beh√∂ver vi bygga inb√§ddningar baserade p√• **spr√•kmodellen**, som tr√§nas p√• en stor textkorpus och *vet* hur ord kan s√§ttas ihop i olika kontexter. Att diskutera kontextuella inb√§ddningar ligger utanf√∂r denna tutorials omfattning, men vi kommer tillbaka till dem n√§r vi pratar om spr√•kmodeller senare i kursen.

## Slutsats

I denna lektion uppt√§ckte du hur man bygger och anv√§nder inb√§ddningslager i TensorFlow och PyTorch f√∂r att b√§ttre reflektera ordens semantiska betydelser.

## üöÄ Utmaning

Word2Vec har anv√§nts f√∂r n√•gra intressanta till√§mpningar, inklusive att generera s√•ngtexter och poesi. Ta en titt p√• [denna artikel](https://www.politetype.com/blog/word2vec-color-poems) som g√•r igenom hur f√∂rfattaren anv√§nde Word2Vec f√∂r att generera poesi. Titta ocks√• p√• [denna video av Dan Shiffmann](https://www.youtube.com/watch?v=LSS_bos_TPI&ab_channel=TheCodingTrain) f√∂r att uppt√§cka en annan f√∂rklaring av denna teknik. F√∂rs√∂k sedan att till√§mpa dessa tekniker p√• din egen textkorpus, kanske h√§mtad fr√•n Kaggle.

## [Efterf√∂rel√§sningsquiz](https://ff-quizzes.netlify.app/en/ai/quiz/28)

## Granskning & Sj√§lvstudier

L√§s igenom denna artikel om Word2Vec: [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf)

## [Uppgift: Anteckningsb√∂cker](assignment.md)

---

