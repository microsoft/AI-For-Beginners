# Djup F칬rst칛rkningsinl칛rning

F칬rst칛rkningsinl칛rning (RL) ses som en av de grundl칛ggande paradigmerna inom maskininl칛rning, vid sidan av 칬vervakad inl칛rning och o칬vervakad inl칛rning. Medan vi i 칬vervakad inl칛rning f칬rlitar oss p친 dataset med k칛nda resultat, bygger RL p친 **att l칛ra genom att g칬ra**. Till exempel, n칛r vi f칬rst ser ett datorspel b칬rjar vi spela, 칛ven utan att k칛nna till reglerna, och snart kan vi f칬rb칛ttra v친ra f칛rdigheter bara genom att spela och justera v친rt beteende.

## [Quiz f칬re f칬rel칛sningen](https://ff-quizzes.netlify.app/en/ai/quiz/43)

F칬r att utf칬ra RL beh칬ver vi:

* En **milj칬** eller **simulator** som s칛tter reglerna f칬r spelet. Vi b칬r kunna k칬ra experiment i simulatorn och observera resultaten.
* N친gon form av **bel칬ningsfunktion**, som indikerar hur framg친ngsrikt v친rt experiment var. N칛r det g칛ller att l칛ra sig spela ett datorspel skulle bel칬ningen vara v친r slutpo칛ng.

Baserat p친 bel칬ningsfunktionen b칬r vi kunna justera v친rt beteende och f칬rb칛ttra v친ra f칛rdigheter, s친 att vi spelar b칛ttre n칛sta g친ng. Den st칬rsta skillnaden mellan andra typer av maskininl칛rning och RL 칛r att vi i RL vanligtvis inte vet om vi vinner eller f칬rlorar f칬rr칛n vi har avslutat spelet. D칛rf칬r kan vi inte s칛ga om ett visst drag i sig 칛r bra eller inte - vi f친r bara en bel칬ning i slutet av spelet.

Under RL utf칬r vi vanligtvis m친nga experiment. Under varje experiment beh칬ver vi balansera mellan att f칬lja den optimala strategi vi har l칛rt oss hittills (**exploatering**) och att utforska nya m칬jliga tillst친nd (**utforskning**).

## OpenAI Gym

Ett utm칛rkt verktyg f칬r RL 칛r [OpenAI Gym](https://gym.openai.com/) - en **simuleringsmilj칬** som kan simulera m친nga olika milj칬er, fr친n Atari-spel till fysiken bakom balans med en st친ng. Det 칛r en av de mest popul칛ra simuleringsmilj칬erna f칬r att tr칛na f칬rst칛rkningsinl칛rningsalgoritmer och underh친lls av [OpenAI](https://openai.com/).

> **Note**: Du kan se alla milj칬er som finns tillg칛ngliga fr친n OpenAI Gym [h칛r](https://gym.openai.com/envs/#classic_control).

## Balans med CartPole

Ni har f칬rmodligen alla sett moderna balansapparater som *Segway* eller *Gyroscooters*. De kan automatiskt balansera genom att justera sina hjul baserat p친 signaler fr친n en accelerometer eller gyroskop. I detta avsnitt kommer vi att l칛ra oss att l칬sa ett liknande problem - att balansera en st친ng. Det liknar en situation d칛r en cirkusartist beh칬ver balansera en st친ng p친 sin hand - men denna balans sker endast i 1D.

En f칬renklad version av balansproblemet 칛r k칛nt som **CartPole**-problemet. I CartPole-v칛rlden har vi en horisontell slider som kan r칬ra sig 친t v칛nster eller h칬ger, och m친let 칛r att balansera en vertikal st친ng ovanp친 slidern medan den r칬r sig.

<img alt="en cartpole" src="../../../../../translated_images/sv/cartpole.f52a67f27e058170.webp" width="200"/>

F칬r att skapa och anv칛nda denna milj칬 beh칬ver vi n친gra rader Python-kod:

```python
import gym
env = gym.make("CartPole-v1")

env.reset()
done = False
total_reward = 0
while not done:
   env.render()
   action = env.action_space.sample()
   observaton, reward, done, info = env.step(action)
   total_reward += reward

print(f"Total reward: {total_reward}")
```

Varje milj칬 kan n친s p친 exakt samma s칛tt:
* `env.reset` startar ett nytt experiment
* `env.step` utf칬r ett simuleringssteg. Det tar emot en **action** fr친n **action space**, och returnerar en **observation** (fr친n observation space), samt en bel칬ning och en flagga f칬r avslutning.

I exemplet ovan utf칬r vi en slumpm칛ssig handling vid varje steg, vilket g칬r att experimentets livsl칛ngd blir mycket kort:

![icke-balanserande cartpole](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-nobalance.gif)

M친let med en RL-algoritm 칛r att tr칛na en modell - den s친 kallade **policyn** &pi; - som kommer att returnera handlingen som svar p친 ett givet tillst친nd. Vi kan ocks친 betrakta policyn som probabilistisk, t.ex. f칬r varje tillst친nd *s* och handling *a* kommer den att returnera sannolikheten &pi;(*a*|*s*) att vi b칬r ta *a* i tillst친ndet *s*.

## Policy Gradients-algoritm

Det mest uppenbara s칛ttet att modellera en policy 칛r att skapa ett neuralt n칛tverk som tar tillst친nd som input och returnerar motsvarande handlingar (eller snarare sannolikheterna f칬r alla handlingar). P친 ett s칛tt skulle det likna en vanlig klassificeringsuppgift, med en stor skillnad - vi vet inte i f칬rv칛g vilka handlingar vi b칬r ta vid varje steg.

Id칠n h칛r 칛r att uppskatta dessa sannolikheter. Vi bygger en vektor av **kumulativa bel칬ningar** som visar v친r totala bel칬ning vid varje steg av experimentet. Vi till칛mpar ocks친 **bel칬ningsdiskontering** genom att multiplicera tidigare bel칬ningar med en koefficient &gamma;=0.99, f칬r att minska betydelsen av tidigare bel칬ningar. Sedan f칬rst칛rker vi de steg l칛ngs experimentv칛gen som ger st칬rre bel칬ningar.

> L칛s mer om Policy Gradients-algoritmen och se den i aktion i [exempelfilen](CartPole-RL-TF.ipynb).

## Actor-Critic-algoritm

En f칬rb칛ttrad version av Policy Gradients-metoden kallas **Actor-Critic**. Huvudid칠n bakom den 칛r att det neurala n칛tverket ska tr칛nas f칬r att returnera tv친 saker:

* Policyn, som avg칬r vilken handling som ska utf칬ras. Denna del kallas **actor**.
* En uppskattning av den totala bel칬ningen vi kan f칬rv칛nta oss att f친 i detta tillst친nd - denna del kallas **critic**.

P친 ett s칛tt liknar denna arkitektur en [GAN](../../4-ComputerVision/10-GANs/README.md), d칛r vi har tv친 n칛tverk som tr칛nas mot varandra. I Actor-Critic-modellen f칬resl친r akt칬ren den handling vi beh칬ver utf칬ra, och kritikern f칬rs칬ker vara kritisk och uppskatta resultatet. Men v친rt m친l 칛r att tr칛na dessa n칛tverk i samspel.

Eftersom vi k칛nner till b친de de verkliga kumulativa bel칬ningarna och resultaten som kritikern returnerar under experimentet, 칛r det relativt enkelt att bygga en f칬rlustfunktion som minimerar skillnaden mellan dem. Det skulle ge oss **critic loss**. Vi kan ber칛kna **actor loss** genom att anv칛nda samma metod som i Policy Gradients-algoritmen.

Efter att ha k칬rt en av dessa algoritmer kan vi f칬rv칛nta oss att v친r CartPole beter sig s친 h칛r:

![en balanserande cartpole](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-balance.gif)

## 九꽲잺 칐vningar: Policy Gradients och Actor-Critic RL

Forts칛tt ditt l칛rande i f칬ljande notebook-filer:

* [RL i TensorFlow](CartPole-RL-TF.ipynb)
* [RL i PyTorch](CartPole-RL-PyTorch.ipynb)

## Andra RL-uppgifter

F칬rst칛rkningsinl칛rning 칛r idag ett snabbt v칛xande forskningsomr친de. N친gra intressanta exempel p친 f칬rst칛rkningsinl칛rning 칛r:

* Att l칛ra en dator att spela **Atari-spel**. Den utmanande delen i detta problem 칛r att vi inte har ett enkelt tillst친nd representerat som en vektor, utan snarare en sk칛rmdump - och vi beh칬ver anv칛nda CNN f칬r att konvertera denna sk칛rmbild till en funktionsvektor eller f칬r att extrahera bel칬ningsinformation. Atari-spel finns tillg칛ngliga i Gym.
* Att l칛ra en dator att spela br칛dspel, s친som schack och Go. Nyligen har toppmoderna program som **Alpha Zero** tr칛nats fr친n grunden av tv친 agenter som spelar mot varandra och f칬rb칛ttras vid varje steg.
* Inom industrin anv칛nds RL f칬r att skapa styrsystem fr친n simulering. En tj칛nst som [Bonsai](https://azure.microsoft.com/services/project-bonsai/?WT.mc_id=academic-77998-cacaste) 칛r specifikt utformad f칬r detta.

## Slutsats

Vi har nu l칛rt oss hur man tr칛nar agenter att uppn친 goda resultat bara genom att ge dem en bel칬ningsfunktion som definierar det 칬nskade tillst친ndet f칬r spelet och genom att ge dem m칬jlighet att intelligent utforska s칬krymden. Vi har framg친ngsrikt testat tv친 algoritmer och uppn친tt ett bra resultat p친 relativt kort tid. Men detta 칛r bara b칬rjan p친 din resa inom RL, och du b칬r definitivt 칬verv칛ga att ta en separat kurs om du vill f칬rdjupa dig.

## 游 Utmaning

Utforska de applikationer som n칛mns i avsnittet 'Andra RL-uppgifter' och f칬rs칬k implementera en!

## [Quiz efter f칬rel칛sningen](https://ff-quizzes.netlify.app/en/ai/quiz/44)

## Granskning & Sj칛lvstudier

L칛r dig mer om klassisk f칬rst칛rkningsinl칛rning i v친r [Maskininl칛rning f칬r nyb칬rjare-kurs](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/README.md).

Titta p친 [denna fantastiska video](https://www.youtube.com/watch?v=qv6UVOQ0F44) som handlar om hur en dator kan l칛ra sig spela Super Mario.

## Uppgift: [Tr칛na en Mountain Car](lab/README.md)

Ditt m친l under denna uppgift 칛r att tr칛na en annan Gym-milj칬 - [Mountain Car](https://www.gymlibrary.ml/environments/classic_control/mountain_car/).

---

