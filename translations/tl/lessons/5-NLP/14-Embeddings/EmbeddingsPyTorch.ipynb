{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "\n",
    "Sa ating nakaraang halimbawa, gumamit tayo ng high-dimensional na bag-of-words vectors na may habang `vocab_size`, at tahasan nating kino-convert ang low-dimensional positional representation vectors sa sparse one-hot representation. Ang one-hot representation na ito ay hindi memory-efficient. Bukod dito, ang bawat salita ay itinuturing na hiwalay sa isa't isa, ibig sabihin, ang one-hot encoded vectors ay hindi nagpapakita ng anumang semantikong pagkakatulad sa pagitan ng mga salita.\n",
    "\n",
    "Sa yunit na ito, ipagpapatuloy natin ang pag-explore sa **News AG** dataset. Upang magsimula, i-load natin ang data at kunin ang ilang mga depinisyon mula sa nakaraang notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\WORK\\ai-for-beginners\\5-NLP\\14-Embeddings\\data\\train.csv: 29.5MB [00:01, 18.8MB/s]                            \n",
      "d:\\WORK\\ai-for-beginners\\5-NLP\\14-Embeddings\\data\\test.csv: 1.86MB [00:00, 11.2MB/s]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building vocab...\n",
      "Vocab size =  95812\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "import numpy as np\n",
    "from torchnlp import *\n",
    "train_dataset, test_dataset, classes, vocab = load_dataset()\n",
    "vocab_size = len(vocab)\n",
    "print(\"Vocab size = \",vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ano ang embedding?\n",
    "\n",
    "Ang ideya ng **embedding** ay ang pagrepresenta ng mga salita gamit ang mas mababang-dimensional na dense vectors, na sa isang paraan ay nagpapakita ng semantikong kahulugan ng isang salita. Tatalakayin natin sa susunod kung paano bumuo ng makabuluhang word embeddings, ngunit sa ngayon, isipin muna natin ang embeddings bilang isang paraan upang bawasan ang dimensionality ng isang word vector.\n",
    "\n",
    "Kaya, ang embedding layer ay tatanggap ng isang salita bilang input, at maglalabas ng output vector na may tinukoy na `embedding_size`. Sa isang paraan, ito ay halos katulad ng `Linear` layer, ngunit sa halip na tumanggap ng one-hot encoded vector, ito ay makakatanggap ng word number bilang input.\n",
    "\n",
    "Sa paggamit ng embedding layer bilang unang layer sa ating network, maaari tayong lumipat mula sa bag-of-words patungo sa **embedding bag** model, kung saan una nating kino-convert ang bawat salita sa ating teksto sa kaukulang embedding, at pagkatapos ay kinakalkula ang isang aggregate function sa lahat ng mga embeddings na iyon, tulad ng `sum`, `average`, o `max`.\n",
    "\n",
    "![Larawan na nagpapakita ng isang embedding classifier para sa limang sequence na salita.](../../../../../translated_images/tl/embedding-classifier-example.b77f021a7ee67eee.webp)\n",
    "\n",
    "Ang ating classifier neural network ay magsisimula sa embedding layer, pagkatapos ay aggregation layer, at linear classifier sa ibabaw nito:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbedClassifier(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super().__init__()\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, embed_dim)\n",
    "        self.fc = torch.nn.Linear(embed_dim, num_class)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = torch.mean(x,dim=1)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pagtugon sa Iba't Ibang Laki ng Sequence ng Variable\n",
    "\n",
    "Dahil sa arkitekturang ito, kailangang likhain ang mga minibatch para sa ating network sa isang tiyak na paraan. Sa nakaraang unit, noong ginagamit ang bag-of-words, lahat ng BoW tensors sa isang minibatch ay may pantay na laki na `vocab_size`, anuman ang aktwal na haba ng ating text sequence. Kapag lumipat na tayo sa word embeddings, magkakaroon tayo ng iba't ibang bilang ng mga salita sa bawat text sample, at kapag pinagsama-sama ang mga sample na ito sa minibatches, kailangan nating maglagay ng padding.\n",
    "\n",
    "Magagawa ito gamit ang parehong teknik ng pagbibigay ng `collate_fn` function sa datasource:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padify(b):\n",
    "    # b is the list of tuples of length batch_size\n",
    "    #   - first element of a tuple = label, \n",
    "    #   - second = feature (text sequence)\n",
    "    # build vectorized sequence\n",
    "    v = [encode(x[1]) for x in b]\n",
    "    # first, compute max length of a sequence in this minibatch\n",
    "    l = max(map(len,v))\n",
    "    return ( # tuple of two tensors - labels and features\n",
    "        torch.LongTensor([t[0]-1 for t in b]),\n",
    "        torch.stack([torch.nn.functional.pad(torch.tensor(t),(0,l-len(t)),mode='constant',value=0) for t in v])\n",
    "    )\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, collate_fn=padify, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pagsasanay ng embedding classifier\n",
    "\n",
    "Ngayon na naitakda na natin ang tamang dataloader, maaari na nating sanayin ang modelo gamit ang training function na itinakda natin sa nakaraang unit:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200: acc=0.6415625\n",
      "6400: acc=0.6865625\n",
      "9600: acc=0.7103125\n",
      "12800: acc=0.726953125\n",
      "16000: acc=0.739375\n",
      "19200: acc=0.75046875\n",
      "22400: acc=0.7572321428571429\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.889799795315499, 0.7623160588611644)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = EmbedClassifier(vocab_size,32,len(classes)).to(device)\n",
    "train_epoch(net,train_loader, lr=1, epoch_size=25000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Tandaan**: Nagsasanay lamang tayo para sa 25k na tala dito (mas mababa sa isang buong epoch) para makatipid sa oras, ngunit maaari mong ipagpatuloy ang pagsasanay, magsulat ng isang function para magsanay sa ilang mga epoch, at mag-eksperimento sa learning rate parameter upang makamit ang mas mataas na katumpakan. Dapat mong maabot ang katumpakan na humigit-kumulang 90%.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EmbeddingBag Layer at Representasyon ng Variable-Length Sequence\n",
    "\n",
    "Sa nakaraang arkitektura, kinakailangan nating i-pad ang lahat ng sequence upang magkapareho ang haba para magkasya ang mga ito sa isang minibatch. Hindi ito ang pinakaepektibong paraan upang i-representa ang mga sequence na may iba't ibang haba - isang alternatibong paraan ay ang paggamit ng **offset** vector, na maglalaman ng mga offset ng lahat ng sequence na nakaimbak sa isang malaking vector.\n",
    "\n",
    "![Larawan na nagpapakita ng offset sequence representation](../../../../../translated_images/tl/offset-sequence-representation.eb73fcefb29b46ee.webp)\n",
    "\n",
    "> **Note**: Sa larawan sa itaas, ipinapakita namin ang isang sequence ng mga karakter, ngunit sa ating halimbawa, nagtatrabaho tayo sa mga sequence ng salita. Gayunpaman, ang pangkalahatang prinsipyo ng pagre-representa ng mga sequence gamit ang offset vector ay nananatiling pareho.\n",
    "\n",
    "Upang magtrabaho gamit ang offset representation, ginagamit natin ang [`EmbeddingBag`](https://pytorch.org/docs/stable/generated/torch.nn.EmbeddingBag.html) layer. Katulad ito ng `Embedding`, ngunit tumatanggap ito ng content vector at offset vector bilang input, at kasama rin nito ang averaging layer, na maaaring `mean`, `sum`, o `max`.\n",
    "\n",
    "Narito ang binagong network na gumagamit ng `EmbeddingBag`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbedClassifier(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super().__init__()\n",
    "        self.embedding = torch.nn.EmbeddingBag(vocab_size, embed_dim)\n",
    "        self.fc = torch.nn.Linear(embed_dim, num_class)\n",
    "\n",
    "    def forward(self, text, off):\n",
    "        x = self.embedding(text, off)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upang ihanda ang dataset para sa pagsasanay, kailangan nating magbigay ng conversion function na maghahanda sa offset vector:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def offsetify(b):\n",
    "    # first, compute data tensor from all sequences\n",
    "    x = [torch.tensor(encode(t[1])) for t in b]\n",
    "    # now, compute the offsets by accumulating the tensor of sequence lengths\n",
    "    o = [0] + [len(t) for t in x]\n",
    "    o = torch.tensor(o[:-1]).cumsum(dim=0)\n",
    "    return ( \n",
    "        torch.LongTensor([t[0]-1 for t in b]), # labels\n",
    "        torch.cat(x), # text \n",
    "        o\n",
    "    )\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, collate_fn=offsetify, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tandaan, na hindi tulad ng lahat ng naunang halimbawa, ang ating network ngayon ay tumatanggap ng dalawang parameter: data vector at offset vector, na may magkaibang sukat. Gayundin, ang ating data loader ay nagbibigay sa atin ng 3 halaga sa halip na 2: parehong text at offset vectors ay ibinibigay bilang mga tampok. Samakatuwid, kailangan nating bahagyang ayusin ang ating training function upang maasikaso ito:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200: acc=0.6153125\n",
      "6400: acc=0.6615625\n",
      "9600: acc=0.6932291666666667\n",
      "12800: acc=0.715078125\n",
      "16000: acc=0.7270625\n",
      "19200: acc=0.7382291666666667\n",
      "22400: acc=0.7486160714285715\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(22.771553103007037, 0.7551983365323096)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = EmbedClassifier(vocab_size,32,len(classes)).to(device)\n",
    "\n",
    "def train_epoch_emb(net,dataloader,lr=0.01,optimizer=None,loss_fn = torch.nn.CrossEntropyLoss(),epoch_size=None, report_freq=200):\n",
    "    optimizer = optimizer or torch.optim.Adam(net.parameters(),lr=lr)\n",
    "    loss_fn = loss_fn.to(device)\n",
    "    net.train()\n",
    "    total_loss,acc,count,i = 0,0,0,0\n",
    "    for labels,text,off in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        labels,text,off = labels.to(device), text.to(device), off.to(device)\n",
    "        out = net(text, off)\n",
    "        loss = loss_fn(out,labels) #cross_entropy(out,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss+=loss\n",
    "        _,predicted = torch.max(out,1)\n",
    "        acc+=(predicted==labels).sum()\n",
    "        count+=len(labels)\n",
    "        i+=1\n",
    "        if i%report_freq==0:\n",
    "            print(f\"{count}: acc={acc.item()/count}\")\n",
    "        if epoch_size and count>epoch_size:\n",
    "            break\n",
    "    return total_loss.item()/count, acc.item()/count\n",
    "\n",
    "\n",
    "train_epoch_emb(net,train_loader, lr=4, epoch_size=25000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Embeddings: Word2Vec\n",
    "\n",
    "Sa ating nakaraang halimbawa, natutunan ng model embedding layer na i-map ang mga salita sa vector representation, ngunit ang representasyong ito ay walang gaanong semantikong kahulugan. Maganda sana kung makakagawa tayo ng ganitong uri ng vector representation, kung saan ang mga magkatulad na salita o mga kasingkahulugan ay magkakaroon ng mga vector na malapit sa isa't isa batay sa ilang uri ng distansya ng vector (hal. euclidian distance).\n",
    "\n",
    "Upang magawa ito, kailangan nating i-pre-train ang ating embedding model sa isang malaking koleksyon ng teksto sa isang partikular na paraan. Isa sa mga unang paraan upang sanayin ang semantic embeddings ay tinatawag na [Word2Vec](https://en.wikipedia.org/wiki/Word2vec). Ito ay nakabatay sa dalawang pangunahing arkitektura na ginagamit upang makabuo ng distributed representation ng mga salita:\n",
    "\n",
    " - **Continuous bag-of-words** (CBoW) — sa arkitekturang ito, sinasanay natin ang modelo upang hulaan ang isang salita mula sa nakapaligid na konteksto. Ibinigay ang ngram $(W_{-2},W_{-1},W_0,W_1,W_2)$, ang layunin ng modelo ay hulaan ang $W_0$ mula sa $(W_{-2},W_{-1},W_1,W_2)$.\n",
    " - **Continuous skip-gram** — kabaligtaran ng CBoW. Ginagamit ng modelo ang nakapaligid na window ng mga salita sa konteksto upang hulaan ang kasalukuyang salita.\n",
    "\n",
    "Mas mabilis ang CBoW, habang ang skip-gram ay mas mabagal, ngunit mas mahusay sa pag-representa ng mga bihirang salita.\n",
    "\n",
    "![Larawan na nagpapakita ng parehong CBoW at Skip-Gram na mga algorithm para i-convert ang mga salita sa mga vector.](../../../../../translated_images/tl/example-algorithms-for-converting-words-to-vectors.fbe9207a726922f6.webp)\n",
    "\n",
    "Upang mag-eksperimento gamit ang word2vec embedding na pre-trained sa Google News dataset, maaari nating gamitin ang **gensim** library. Sa ibaba, makikita natin ang mga salitang pinakamalapit sa 'neural'.\n",
    "\n",
    "> **Note:** Kapag unang gumawa ng word vectors, maaaring tumagal ng kaunting oras ang pag-download ng mga ito!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "w2v = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neuronal -> 0.7804799675941467\n",
      "neurons -> 0.7326500415802002\n",
      "neural_circuits -> 0.7252851724624634\n",
      "neuron -> 0.7174385190010071\n",
      "cortical -> 0.6941086649894714\n",
      "brain_circuitry -> 0.6923246383666992\n",
      "synaptic -> 0.6699118614196777\n",
      "neural_circuitry -> 0.6638563275337219\n",
      "neurochemical -> 0.6555314064025879\n",
      "neuronal_activity -> 0.6531826257705688\n"
     ]
    }
   ],
   "source": [
    "for w,p in w2v.most_similar('neural'):\n",
    "    print(f\"{w} -> {p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maaari rin nating kalkulahin ang vector embeddings mula sa salita, na gagamitin sa pagsasanay ng classification model (ipinapakita lamang namin ang unang 20 na bahagi ng vector para sa kalinawan):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01226807,  0.06225586,  0.10693359,  0.05810547,  0.23828125,\n",
       "        0.03686523,  0.05151367, -0.20703125,  0.01989746,  0.10058594,\n",
       "       -0.03759766, -0.1015625 , -0.15820312, -0.08105469, -0.0390625 ,\n",
       "       -0.05053711,  0.16015625,  0.2578125 ,  0.10058594, -0.25976562],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.word_vec('play')[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ang magandang bagay tungkol sa semantical embeddings ay maaari mong manipulahin ang vector encoding upang baguhin ang semantika. Halimbawa, maaari tayong magtanong upang makahanap ng isang salita, na ang vector representation ay magiging kasing lapit hangga't maaari sa mga salitang *hari* at *babae*, at kasing layo hangga't maaari mula sa salitang *lalaki*:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('queen', 0.7118192911148071)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar(positive=['king','woman'],negative=['man'])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parehong CBoW at Skip-Grams ay mga “predictive” embeddings, dahil tanging mga lokal na konteksto lamang ang isinasaalang-alang nila. Ang Word2Vec ay hindi gumagamit ng global na konteksto.\n",
    "\n",
    "**FastText**, ay nakabatay sa Word2Vec sa pamamagitan ng pag-aaral ng mga vector na representasyon para sa bawat salita at ang mga character n-grams na matatagpuan sa loob ng bawat salita. Ang mga halaga ng mga representasyon ay pagkatapos ay ina-average sa isang vector sa bawat hakbang ng pagsasanay. Bagama't nagdadagdag ito ng maraming karagdagang pagkalkula sa pre-training, pinapahintulutan nito ang word embeddings na mag-encode ng impormasyon sa sub-word.\n",
    "\n",
    "Isa pang pamamaraan, ang **GloVe**, ay gumagamit ng ideya ng co-occurrence matrix, at gumagamit ng mga neural na pamamaraan upang i-decompose ang co-occurrence matrix sa mas ekspresibo at hindi linear na mga word vector.\n",
    "\n",
    "Maaari mong subukan ang halimbawa sa pamamagitan ng pagpapalit ng embeddings sa FastText at GloVe, dahil sinusuportahan ng gensim ang iba't ibang mga modelo ng word embedding.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paggamit ng Pre-Trained Embeddings sa PyTorch\n",
    "\n",
    "Maaari nating baguhin ang halimbawa sa itaas upang punan ang matrix sa ating embedding layer gamit ang semantikal na embeddings, tulad ng Word2Vec. Kailangan nating isaalang-alang na ang mga bokabularyo ng pre-trained embedding at ng ating text corpus ay malamang na hindi magtutugma, kaya i-initialize natin ang mga weights para sa mga nawawalang salita gamit ang random na mga halaga:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding size: 300\n",
      "Populating matrix, this will take some time...Done, found 41080 words, 54732 words missing\n"
     ]
    }
   ],
   "source": [
    "embed_size = len(w2v.get_vector('hello'))\n",
    "print(f'Embedding size: {embed_size}')\n",
    "\n",
    "net = EmbedClassifier(vocab_size,embed_size,len(classes))\n",
    "\n",
    "print('Populating matrix, this will take some time...',end='')\n",
    "found, not_found = 0,0\n",
    "for i,w in enumerate(vocab.get_itos()):\n",
    "    try:\n",
    "        net.embedding.weight[i].data = torch.tensor(w2v.get_vector(w))\n",
    "        found+=1\n",
    "    except:\n",
    "        net.embedding.weight[i].data = torch.normal(0.0,1.0,(embed_size,))\n",
    "        not_found+=1\n",
    "\n",
    "print(f\"Done, found {found} words, {not_found} words missing\")\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ngayon, mag-ensayo tayo ng ating modelo. Tandaan na ang oras na kinakailangan upang mag-ensayo ng modelo ay mas mahaba kumpara sa nakaraang halimbawa, dahil sa mas malaking sukat ng embedding layer, at sa gayon mas mataas na bilang ng mga parameter. Gayundin, dahil dito, maaaring kailanganin nating mag-ensayo ng ating modelo sa mas maraming halimbawa kung nais nating maiwasan ang overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200: acc=0.6359375\n",
      "6400: acc=0.68109375\n",
      "9600: acc=0.7067708333333333\n",
      "12800: acc=0.723671875\n",
      "16000: acc=0.73625\n",
      "19200: acc=0.7463541666666667\n",
      "22400: acc=0.7560714285714286\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(214.1013875559821, 0.7626759436980166)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_epoch_emb(net,train_loader, lr=4, epoch_size=25000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sa ating kaso, hindi natin nakikita ang malaking pagtaas sa accuracy, na malamang dahil sa magkaibang bokabularyo.  \n",
    "Upang malampasan ang problema ng magkaibang bokabularyo, maaari nating gamitin ang isa sa mga sumusunod na solusyon:  \n",
    "* Muling i-train ang word2vec model gamit ang ating bokabularyo  \n",
    "* I-load ang ating dataset gamit ang bokabularyo mula sa pre-trained word2vec model. Ang bokabularyo na gagamitin para i-load ang dataset ay maaaring tukuyin habang naglo-load.  \n",
    "\n",
    "Ang huling paraan ay mukhang mas madali, lalo na dahil ang PyTorch `torchtext` framework ay may built-in na suporta para sa embeddings. Halimbawa, maaari nating i-instantiate ang GloVe-based na bokabularyo sa ganitong paraan:  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 399999/400000 [00:15<00:00, 25411.14it/s]\n"
     ]
    }
   ],
   "source": [
    "vocab = torchtext.vocab.GloVe(name='6B', dim=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ang na-load na bokabularyo ay may mga sumusunod na pangunahing operasyon:\n",
    "* Ang `vocab.stoi` na diksyunaryo ay nagbibigay-daan sa atin na i-convert ang salita sa index nito sa diksyunaryo\n",
    "* Ang `vocab.itos` ay gumagawa ng kabaligtaran - kino-convert ang numero sa salita\n",
    "* Ang `vocab.vectors` ay ang array ng embedding vectors, kaya upang makuha ang embedding ng isang salita na `s`, kailangan nating gamitin ang `vocab.vectors[vocab.stoi[s]]`\n",
    "\n",
    "Narito ang halimbawa ng pag-manipula ng embeddings upang ipakita ang equation **kind-man+woman = queen** (Kinailangan kong ayusin nang kaunti ang coefficient para gumana ito):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'queen'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the vector corresponding to kind-man+woman\n",
    "qvec = vocab.vectors[vocab.stoi['king']]-vocab.vectors[vocab.stoi['man']]+1.3*vocab.vectors[vocab.stoi['woman']]\n",
    "# find the index of the closest embedding vector \n",
    "d = torch.sum((vocab.vectors-qvec)**2,dim=1)\n",
    "min_idx = torch.argmin(d)\n",
    "# find the corresponding word\n",
    "vocab.itos[min_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upang sanayin ang classifier gamit ang mga embeddings na iyon, kailangan muna nating i-encode ang ating dataset gamit ang GloVe vocabulary:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def offsetify(b):\n",
    "    # first, compute data tensor from all sequences\n",
    "    x = [torch.tensor(encode(t[1],voc=vocab)) for t in b] # pass the instance of vocab to encode function!\n",
    "    # now, compute the offsets by accumulating the tensor of sequence lengths\n",
    "    o = [0] + [len(t) for t in x]\n",
    "    o = torch.tensor(o[:-1]).cumsum(dim=0)\n",
    "    return ( \n",
    "        torch.LongTensor([t[0]-1 for t in b]), # labels\n",
    "        torch.cat(x), # text \n",
    "        o\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tulad ng nakita natin sa itaas, lahat ng vector embeddings ay nakaimbak sa `vocab.vectors` matrix. Ginagawa nitong napakadaling i-load ang mga weights na iyon sa weights ng embedding layer gamit ang simpleng pagkopya:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = EmbedClassifier(len(vocab),len(vocab.vectors[0]),len(classes))\n",
    "net.embedding.weight.data = vocab.vectors\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200: acc=0.6271875\n",
      "6400: acc=0.68078125\n",
      "9600: acc=0.7030208333333333\n",
      "12800: acc=0.71984375\n",
      "16000: acc=0.7346875\n",
      "19200: acc=0.7455729166666667\n",
      "22400: acc=0.7529464285714286\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(35.53972978646833, 0.7575175943698017)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, collate_fn=offsetify, shuffle=True)\n",
    "train_epoch_emb(net,train_loader, lr=4, epoch_size=25000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isa sa mga dahilan kung bakit hindi natin nakikita ang makabuluhang pagtaas sa katumpakan ay dahil sa ang ilang mga salita mula sa ating dataset ay nawawala sa pre-trained na GloVe vocabulary, at dahil dito, sila ay karaniwang hindi pinapansin. Upang malampasan ang katotohanang ito, maaari tayong mag-train ng sarili nating embeddings sa ating dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mga Kontekstwal na Embeddings\n",
    "\n",
    "Isang pangunahing limitasyon ng tradisyunal na pretrained embedding representations tulad ng Word2Vec ay ang problema ng word sense disambiguation. Bagama't ang pretrained embeddings ay maaaring makuha ang ilang kahulugan ng mga salita sa konteksto, ang bawat posibleng kahulugan ng isang salita ay naka-encode sa parehong embedding. Ito ay maaaring magdulot ng problema sa mga downstream models, dahil maraming salita, tulad ng salitang 'play', ay may iba't ibang kahulugan depende sa konteksto kung saan ito ginagamit.\n",
    "\n",
    "Halimbawa, ang salitang 'play' sa dalawang magkaibang pangungusap ay may magkaibang kahulugan:\n",
    "- Pumunta ako sa isang **play** sa teatro.\n",
    "- Gusto ni John na **maglaro** kasama ang kanyang mga kaibigan.\n",
    "\n",
    "Ang pretrained embeddings sa itaas ay kumakatawan sa parehong mga kahulugan ng salitang 'play' sa iisang embedding. Upang malampasan ang limitasyong ito, kailangan nating bumuo ng embeddings batay sa **language model**, na sinanay sa isang malaking corpus ng teksto, at *alam* kung paano maaaring pagsamahin ang mga salita sa iba't ibang konteksto. Ang talakayan tungkol sa kontekstwal na embeddings ay labas sa saklaw ng tutorial na ito, ngunit babalikan natin ito kapag pinag-uusapan ang mga language models sa susunod na unit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Paunawa**:  \nAng dokumentong ito ay isinalin gamit ang AI translation service na [Co-op Translator](https://github.com/Azure/co-op-translator). Bagama't sinisikap naming maging tumpak, tandaan na ang mga awtomatikong pagsasalin ay maaaring maglaman ng mga pagkakamali o hindi pagkakatugma. Ang orihinal na dokumento sa kanyang katutubong wika ang dapat ituring na opisyal na sanggunian. Para sa mahalagang impormasyon, inirerekomenda ang propesyonal na pagsasalin ng tao. Hindi kami mananagot sa anumang hindi pagkakaunawaan o maling interpretasyon na maaaring magmula sa paggamit ng pagsasaling ito.\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0cb620c6d4b9f7a635928804c26cf22403d89d98d79684e4529119355ee6d5a5"
  },
  "kernelspec": {
   "display_name": "py37_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "coopTranslator": {
   "original_hash": "f50b026abce5cf36783a560ea72cb9b1",
   "translation_date": "2025-08-28T04:34:53+00:00",
   "source_file": "lessons/5-NLP/14-Embeddings/EmbeddingsPyTorch.ipynb",
   "language_code": "tl"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}