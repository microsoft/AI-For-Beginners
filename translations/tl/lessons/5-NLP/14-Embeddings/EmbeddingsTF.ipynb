{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "\n",
    "Sa ating nakaraang halimbawa, gumamit tayo ng high-dimensional na bag-of-words vectors na may haba na `vocab_size`, at tahasang kinonvert ang low-dimensional positional representation vectors sa sparse one-hot representation. Ang one-hot representation na ito ay hindi memory-efficient. Bukod dito, ang bawat salita ay itinuturing na hiwalay sa isa't isa, kaya't ang one-hot encoded vectors ay hindi nagpapakita ng semantikong pagkakatulad sa pagitan ng mga salita.\n",
    "\n",
    "Sa unit na ito, ipagpapatuloy natin ang pag-explore sa **News AG** dataset. Upang magsimula, i-load natin ang data at kunin ang ilang mga depinisyon mula sa nakaraang unit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "\n",
    "ds_train, ds_test = tfds.load('ag_news_subset').values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ano ang embedding?\n",
    "\n",
    "Ang ideya ng **embedding** ay ang pagrepresenta ng mga salita gamit ang mas mababang-dimensional na dense vectors na sumasalamin sa semantikong kahulugan ng salita. Tatalakayin natin mamaya kung paano bumuo ng makabuluhang word embeddings, pero sa ngayon, isipin muna natin ang embeddings bilang isang paraan upang bawasan ang dimensionality ng isang word vector.\n",
    "\n",
    "Kaya, ang isang embedding layer ay tumatanggap ng isang salita bilang input, at naglalabas ng output vector na may tinukoy na `embedding_size`. Sa isang banda, ito ay halos katulad ng isang `Dense` layer, ngunit sa halip na tumanggap ng one-hot encoded vector bilang input, kaya nitong tumanggap ng word number.\n",
    "\n",
    "Sa pamamagitan ng paggamit ng embedding layer bilang unang layer sa ating network, maaari tayong lumipat mula sa bag-of-words patungo sa isang **embedding bag** model, kung saan una nating kino-convert ang bawat salita sa ating teksto sa kaukulang embedding, at pagkatapos ay kinakalkula ang isang aggregate function sa lahat ng mga embeddings na iyon, tulad ng `sum`, `average`, o `max`.\n",
    "\n",
    "![Larawan na nagpapakita ng isang embedding classifier para sa limang sequence na salita.](../../../../../translated_images/tl/embedding-classifier-example.b77f021a7ee67eee.webp)\n",
    "\n",
    "Ang ating classifier neural network ay binubuo ng mga sumusunod na layer:\n",
    "\n",
    "* `TextVectorization` layer, na tumatanggap ng string bilang input, at naglalabas ng tensor ng mga token numbers. Magtatakda tayo ng isang makatwirang laki ng bokabularyo na `vocab_size`, at babalewalain ang mga salitang hindi madalas gamitin. Ang input shape ay magiging 1, at ang output shape ay magiging $n$, dahil makakakuha tayo ng $n$ tokens bilang resulta, bawat isa ay naglalaman ng mga numero mula 0 hanggang `vocab_size`.\n",
    "* `Embedding` layer, na tumatanggap ng $n$ na numero, at binabawasan ang bawat numero sa isang dense vector na may tinukoy na haba (100 sa ating halimbawa). Kaya, ang input tensor na may shape na $n$ ay mababago sa isang $n\\times 100$ tensor.\n",
    "* Aggregation layer, na kumukuha ng average ng tensor na ito sa kahabaan ng unang axis, i.e., kakalkulahin nito ang average ng lahat ng $n$ input tensors na tumutugma sa iba't ibang salita. Upang ipatupad ang layer na ito, gagamit tayo ng isang `Lambda` layer, at ipapasa dito ang function upang kalkulahin ang average. Ang output ay magkakaroon ng shape na 100, at ito ang magiging numerikal na representasyon ng buong input sequence.\n",
    "* Panghuling `Dense` linear classifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization (TextVec  (None, None)             0         \n",
      " torization)                                                     \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, None, 100)         3000000   \n",
      "                                                                 \n",
      " lambda (Lambda)             (None, 100)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4)                 404       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,000,404\n",
      "Trainable params: 3,000,404\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 30000\n",
    "batch_size = 128\n",
    "\n",
    "vectorizer = keras.layers.experimental.preprocessing.TextVectorization(max_tokens=vocab_size,input_shape=(1,))\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    vectorizer,    \n",
    "    keras.layers.Embedding(vocab_size,100),\n",
    "    keras.layers.Lambda(lambda x: tf.reduce_mean(x,axis=1)),\n",
    "    keras.layers.Dense(4, activation='softmax')\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sa `summary` na printout, sa **output shape** na column, ang unang dimensyon ng tensor na `None` ay tumutukoy sa laki ng minibatch, at ang pangalawa ay tumutukoy sa haba ng token sequence. Ang lahat ng token sequences sa minibatch ay may iba't ibang haba. Tatalakayin natin kung paano ito haharapin sa susunod na seksyon.\n",
    "\n",
    "Ngayon, simulan na natin ang pag-train ng network:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training vectorizer\n",
      "938/938 [==============================] - 20s 20ms/step - loss: 0.7891 - acc: 0.8155 - val_loss: 0.4470 - val_acc: 0.8642\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22255515100>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_text(x):\n",
    "    return x['title']+' '+x['description']\n",
    "\n",
    "def tupelize(x):\n",
    "    return (extract_text(x),x['label'])\n",
    "\n",
    "print(\"Training vectorizer\")\n",
    "vectorizer.adapt(ds_train.take(500).map(extract_text))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'])\n",
    "model.fit(ds_train.map(tupelize).batch(batch_size),validation_data=ds_test.map(tupelize).batch(batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "> **Tandaan** na gumagawa tayo ng vectorizer batay sa isang subset ng data. Ginagawa ito upang mapabilis ang proseso, at maaaring magresulta ito sa sitwasyon kung saan hindi lahat ng token mula sa ating teksto ay nasa bokabularyo. Sa ganitong kaso, ang mga token na iyon ay hindi isasama, na maaaring magresulta sa bahagyang mas mababang katumpakan. Gayunpaman, sa totoong buhay, ang isang subset ng teksto ay madalas na nagbibigay ng magandang pagtataya ng bokabularyo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pagharap sa iba't ibang laki ng sequence ng variable\n",
    "\n",
    "Unawain natin kung paano nangyayari ang training sa minibatches. Sa halimbawa sa itaas, ang input tensor ay may dimensyong 1, at gumagamit tayo ng 128-long minibatches, kaya ang aktwal na laki ng tensor ay $128 \\times 1$. Gayunpaman, ang bilang ng mga token sa bawat pangungusap ay magkakaiba. Kung gagamitin natin ang `TextVectorization` layer sa isang input, ang bilang ng mga token na ibinabalik ay iba-iba, depende sa kung paano tinokenize ang teksto:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 1 45], shape=(2,), dtype=int64)\n",
      "tf.Tensor([ 112 1271    1    3 1747  158], shape=(6,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer('Hello, world!'))\n",
    "print(vectorizer('I am glad to meet you!'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gayunpaman, kapag inilapat natin ang vectorizer sa ilang mga sequence, kailangan nitong gumawa ng tensor na may hugis na parihaba, kaya pinupunan nito ang mga hindi nagamit na elemento gamit ang PAD token (na sa ating kaso ay zero):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 6), dtype=int64, numpy=\n",
       "array([[   1,   45,    0,    0,    0,    0],\n",
       "       [ 112, 1271,    1,    3, 1747,  158]], dtype=int64)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer(['Hello, world!','I am glad to meet you!'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Narito makikita natin ang mga embeddings:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 1.53059261e-02,  6.80514947e-02,  3.14026810e-02, ...,\n",
       "         -8.92002955e-02,  1.52911525e-04, -5.65562584e-02],\n",
       "        [ 2.57456154e-01,  2.79364467e-01, -2.03605562e-01, ...,\n",
       "         -2.07474351e-01,  8.31158683e-02, -2.03911960e-01],\n",
       "        [ 3.98201384e-02, -8.03454965e-03,  2.39790026e-02, ...,\n",
       "         -7.18549127e-04,  2.66963355e-02, -4.30646613e-02],\n",
       "        [ 3.98201384e-02, -8.03454965e-03,  2.39790026e-02, ...,\n",
       "         -7.18549127e-04,  2.66963355e-02, -4.30646613e-02],\n",
       "        [ 3.98201384e-02, -8.03454965e-03,  2.39790026e-02, ...,\n",
       "         -7.18549127e-04,  2.66963355e-02, -4.30646613e-02],\n",
       "        [ 3.98201384e-02, -8.03454965e-03,  2.39790026e-02, ...,\n",
       "         -7.18549127e-04,  2.66963355e-02, -4.30646613e-02]],\n",
       "\n",
       "       [[ 1.89674050e-01,  2.61548996e-01, -3.67433839e-02, ...,\n",
       "         -2.07366899e-01, -1.05442435e-01, -2.36952081e-01],\n",
       "        [ 6.16133213e-02,  1.80511594e-01,  9.77298319e-02, ...,\n",
       "         -5.46628237e-02, -1.07340455e-01, -1.06589928e-01],\n",
       "        [ 1.53059261e-02,  6.80514947e-02,  3.14026810e-02, ...,\n",
       "         -8.92002955e-02,  1.52911525e-04, -5.65562584e-02],\n",
       "        [-4.84890305e-02, -8.41715634e-02,  1.51529670e-01, ...,\n",
       "          1.28192469e-01, -7.77286515e-02,  1.26041949e-01],\n",
       "        [-4.17212099e-02, -5.60694858e-02,  4.08860669e-02, ...,\n",
       "          8.70475471e-02,  8.92383084e-02,  1.67974353e-01],\n",
       "        [ 2.85779923e-01,  4.57767487e-01,  4.52292450e-02, ...,\n",
       "         -1.97419018e-01, -2.04659685e-01, -2.79758364e-01]]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[1](vectorizer(['Hello, world!','I am glad to meet you!'])).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Tandaan**: Upang mabawasan ang dami ng padding, sa ilang mga kaso makatuwiran na ayusin ang lahat ng mga sequence sa dataset ayon sa pagtaas ng haba (o, mas tiyak, bilang ng mga token). Ito ay magtitiyak na ang bawat minibatch ay naglalaman ng mga sequence na may magkakatulad na haba.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic embeddings: Word2Vec\n",
    "\n",
    "Sa ating nakaraang halimbawa, ang embedding layer ay natutong i-map ang mga salita sa vector representations, ngunit ang mga representasyong ito ay walang semantikong kahulugan. Maganda sana kung makakagawa tayo ng vector representation kung saan ang magkatulad na salita o mga kasingkahulugan ay tumutugma sa mga vectors na malapit sa isa't isa batay sa ilang uri ng vector distance (halimbawa, euclidian distance).\n",
    "\n",
    "Upang magawa ito, kailangan nating i-pretrain ang ating embedding model sa isang malaking koleksyon ng teksto gamit ang isang teknik tulad ng [Word2Vec](https://en.wikipedia.org/wiki/Word2vec). Ito ay nakabatay sa dalawang pangunahing arkitektura na ginagamit upang makabuo ng distributed representation ng mga salita:\n",
    "\n",
    " - **Continuous bag-of-words** (CBoW), kung saan tinetrain natin ang modelo upang hulaan ang isang salita mula sa nakapaligid na konteksto. Ibinigay ang ngram $(W_{-2},W_{-1},W_0,W_1,W_2)$, ang layunin ng modelo ay hulaan ang $W_0$ mula sa $(W_{-2},W_{-1},W_1,W_2)$.\n",
    " - **Continuous skip-gram** ay kabaligtaran ng CBoW. Ginagamit ng modelo ang nakapaligid na window ng mga salita sa konteksto upang hulaan ang kasalukuyang salita.\n",
    "\n",
    "Mas mabilis ang CBoW, ngunit habang mas mabagal ang skip-gram, mas mahusay ito sa pagrepresenta ng mga bihirang salita.\n",
    "\n",
    "![Larawan na nagpapakita ng parehong CBoW at Skip-Gram na mga algorithm para i-convert ang mga salita sa vectors.](../../../../../translated_images/tl/example-algorithms-for-converting-words-to-vectors.fbe9207a726922f6.webp)\n",
    "\n",
    "Upang mag-eksperimento gamit ang Word2Vec embedding na pre-trained sa Google News dataset, maaari nating gamitin ang **gensim** library. Sa ibaba, makikita natin ang mga salitang pinakamalapit sa 'neural'.\n",
    "\n",
    "> **Note:** Kapag unang gumawa ng word vectors, maaaring tumagal ang pag-download ng mga ito!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "w2v = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neuronal -> 0.7804799675941467\n",
      "neurons -> 0.7326500415802002\n",
      "neural_circuits -> 0.7252851724624634\n",
      "neuron -> 0.7174385190010071\n",
      "cortical -> 0.6941086649894714\n",
      "brain_circuitry -> 0.6923246383666992\n",
      "synaptic -> 0.6699118614196777\n",
      "neural_circuitry -> 0.6638563275337219\n",
      "neurochemical -> 0.6555314064025879\n",
      "neuronal_activity -> 0.6531826257705688\n"
     ]
    }
   ],
   "source": [
    "for w,p in w2v.most_similar('neural'):\n",
    "    print(f\"{w} -> {p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maaari rin nating kunin ang vector embedding mula sa salita, upang magamit sa pagsasanay ng classification model. Ang embedding ay may 300 na bahagi, ngunit dito ipinapakita lamang natin ang unang 20 bahagi ng vector para sa kalinawan:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01226807,  0.06225586,  0.10693359,  0.05810547,  0.23828125,\n",
       "        0.03686523,  0.05151367, -0.20703125,  0.01989746,  0.10058594,\n",
       "       -0.03759766, -0.1015625 , -0.15820312, -0.08105469, -0.0390625 ,\n",
       "       -0.05053711,  0.16015625,  0.2578125 ,  0.10058594, -0.25976562],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v['play'][:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ang mahusay na bagay tungkol sa semantic embeddings ay maaari mong manipulahin ang vector encoding batay sa semantika. Halimbawa, maaari nating hanapin ang isang salita na ang vector representation ay pinakamalapit sa mga salitang *hari* at *babae*, at pinakamalayo sa salitang *lalaki*:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('queen', 0.7118192911148071)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar(positive=['king','woman'],negative=['man'])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Ang halimbawa sa itaas ay gumagamit ng ilang panloob na GenSym magic, ngunit ang pangunahing lohika ay talagang medyo simple. Isang kawili-wiling bagay tungkol sa embeddings ay maaari kang magsagawa ng normal na operasyon ng vector sa embedding vectors, at iyon ay magpapakita ng mga operasyon sa mga **kahulugan** ng salita. Ang halimbawa sa itaas ay maaaring ipahayag sa mga termino ng operasyon ng vector: kinakalkula natin ang vector na tumutugma sa **HARI-LALAKI+BABAE** (ang mga operasyon na `+` at `-` ay isinasagawa sa mga representasyon ng vector ng mga kaukulang salita), at pagkatapos ay hinahanap ang pinakamalapit na salita sa diksyunaryo sa vector na iyon:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'queen'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the vector corresponding to kind-man+woman\n",
    "qvec = w2v['king']-1.7*w2v['man']+1.7*w2v['woman']\n",
    "# find the index of the closest embedding vector \n",
    "d = np.sum((w2v.vectors-qvec)**2,axis=1)\n",
    "min_idx = np.argmin(d)\n",
    "# find the corresponding word\n",
    "w2v.index_to_key[min_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NOTE**: Kinailangan naming magdagdag ng maliit na coefficients sa *man* at *woman* na mga vector - subukang alisin ito upang makita ang mangyayari.\n",
    "\n",
    "Upang mahanap ang pinakamalapit na vector, ginagamit namin ang TensorFlow machinery upang kalkulahin ang isang vector ng mga distansya sa pagitan ng aming vector at lahat ng mga vector sa bokabularyo, at pagkatapos ay hanapin ang index ng pinakamaliit na salita gamit ang `argmin`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Habang ang Word2Vec ay mukhang isang mahusay na paraan upang ipahayag ang semantika ng mga salita, mayroon itong maraming mga kahinaan, kabilang ang mga sumusunod:\n",
    "\n",
    "* Parehong CBoW at skip-gram na mga modelo ay **predictive embeddings**, at isinasaalang-alang lamang nila ang lokal na konteksto. Hindi ginagamit ng Word2Vec ang global na konteksto.\n",
    "* Hindi isinasaalang-alang ng Word2Vec ang **morpolohiya** ng salita, ibig sabihin, ang katotohanan na ang kahulugan ng salita ay maaaring nakadepende sa iba't ibang bahagi nito, tulad ng ugat.\n",
    "\n",
    "Sinusubukan ng **FastText** na malampasan ang ikalawang limitasyon, at pinapahusay ang Word2Vec sa pamamagitan ng pag-aaral ng mga vector na representasyon para sa bawat salita at ang mga character n-grams na matatagpuan sa loob ng bawat salita. Ang mga halaga ng mga representasyon ay pagkatapos ay ina-average sa isang vector sa bawat hakbang ng pagsasanay. Bagama't nagdadagdag ito ng maraming karagdagang pagkalkula sa pretraining, pinapahintulutan nito ang word embeddings na mag-encode ng impormasyon sa sub-word.\n",
    "\n",
    "Ang isa pang pamamaraan, **GloVe**, ay gumagamit ng ibang diskarte sa word embeddings, na nakabatay sa factorization ng word-context matrix. Una, bumubuo ito ng isang malaking matrix na nagbibilang ng dami ng paglitaw ng mga salita sa iba't ibang konteksto, at pagkatapos ay sinusubukan nitong i-representa ang matrix na ito sa mas mababang dimensyon sa paraang nagpapaliit ng reconstruction loss.\n",
    "\n",
    "Sinusuportahan ng gensim library ang mga word embeddings na ito, at maaari kang mag-eksperimento sa mga ito sa pamamagitan ng pagbabago ng model loading code sa itaas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paggamit ng pretrained embeddings sa Keras\n",
    "\n",
    "Maaari nating baguhin ang halimbawa sa itaas upang punan ang matrix sa ating embedding layer gamit ang semantic embeddings, tulad ng Word2Vec. Malamang na hindi magtutugma ang mga bokabularyo ng pretrained embedding at ng text corpus, kaya kailangan nating pumili ng isa. Dito natin susuriin ang dalawang posibleng opsyon: paggamit ng bokabularyo ng tokenizer, at paggamit ng bokabularyo mula sa Word2Vec embeddings.\n",
    "\n",
    "### Paggamit ng bokabularyo ng tokenizer\n",
    "\n",
    "Kapag ginamit ang bokabularyo ng tokenizer, ang ilan sa mga salita mula sa bokabularyo ay magkakaroon ng kaukulang Word2Vec embeddings, at ang ilan ay mawawala. Dahil ang laki ng ating bokabularyo ay `vocab_size`, at ang haba ng Word2Vec embedding vector ay `embed_size`, ang embedding layer ay kakatawanin ng isang weight matrix na may hugis na `vocab_size`$\\times$`embed_size`. Pupunan natin ang matrix na ito sa pamamagitan ng pagdaan sa bokabularyo:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding size: 300\n",
      "Populating matrix, this will take some time...Done, found 4551 words, 784 words missing\n"
     ]
    }
   ],
   "source": [
    "embed_size = len(w2v.get_vector('hello'))\n",
    "print(f'Embedding size: {embed_size}')\n",
    "\n",
    "vocab = vectorizer.get_vocabulary()\n",
    "W = np.zeros((vocab_size,embed_size))\n",
    "print('Populating matrix, this will take some time...',end='')\n",
    "found, not_found = 0,0\n",
    "for i,w in enumerate(vocab):\n",
    "    try:\n",
    "        W[i] = w2v.get_vector(w)\n",
    "        found+=1\n",
    "    except:\n",
    "        # W[i] = np.random.normal(0.0,0.3,size=(embed_size,))\n",
    "        not_found+=1\n",
    "\n",
    "print(f\"Done, found {found} words, {not_found} words missing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para sa mga salitang wala sa Word2Vec vocabulary, maaari nating iwanan ang mga ito bilang mga zero, o bumuo ng random na vector.\n",
    "\n",
    "Ngayon, maaari na tayong magtakda ng embedding layer na may pretrained na mga timbang:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = keras.layers.Embedding(vocab_size,embed_size,weights=[W],trainable=False)\n",
    "model = keras.models.Sequential([\n",
    "    vectorizer, emb,\n",
    "    keras.layers.Lambda(lambda x: tf.reduce_mean(x,axis=1)),\n",
    "    keras.layers.Dense(4, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 10s 10ms/step - loss: 1.1075 - acc: 0.7822 - val_loss: 0.9134 - val_acc: 0.8175\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2220226ef10>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'])\n",
    "model.fit(ds_train.map(tupelize).batch(batch_size),\n",
    "          validation_data=ds_test.map(tupelize).batch(batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Tandaan**: Pansinin na itinakda natin ang `trainable=False` kapag nilikha ang `Embedding`, na nangangahulugang hindi natin muling sinasanay ang Embedding layer. Maaaring bahagyang bumaba ang katumpakan nito, ngunit mas pinapabilis nito ang pagsasanay.\n",
    "\n",
    "### Paggamit ng bokabularyo ng embedding\n",
    "\n",
    "Isang isyu sa naunang paraan ay ang pagkakaiba ng mga bokabularyo na ginamit sa TextVectorization at Embedding. Upang malampasan ang problemang ito, maaari nating gamitin ang isa sa mga sumusunod na solusyon:\n",
    "* Muling sanayin ang Word2Vec model gamit ang ating bokabularyo.\n",
    "* I-load ang ating dataset gamit ang bokabularyo mula sa pretrained na Word2Vec model. Ang mga bokabularyong gagamitin para i-load ang dataset ay maaaring tukuyin habang naglo-load.\n",
    "\n",
    "Ang huling paraan ay tila mas madali, kaya't ipapatupad natin ito. Una sa lahat, lilikha tayo ng isang `TextVectorization` layer gamit ang tinukoy na bokabularyo, na kinuha mula sa Word2Vec embeddings:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(w2v.vocab.keys())\n",
    "vectorizer = keras.layers.experimental.preprocessing.TextVectorization(input_shape=(1,))\n",
    "vectorizer.set_vocabulary(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ang gensim word embeddings library ay naglalaman ng isang maginhawang function, `get_keras_embeddings`, na awtomatikong lilikha ng kaukulang Keras embeddings layer para sa iyo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "938/938 [==============================] - 20s 14ms/step - loss: 1.3377 - acc: 0.4978 - val_loss: 1.2995 - val_acc: 0.5647\n",
      "Epoch 2/5\n",
      "938/938 [==============================] - 10s 10ms/step - loss: 1.2587 - acc: 0.5722 - val_loss: 1.2339 - val_acc: 0.5842\n",
      "Epoch 3/5\n",
      "938/938 [==============================] - 10s 10ms/step - loss: 1.1980 - acc: 0.5884 - val_loss: 1.1826 - val_acc: 0.5954\n",
      "Epoch 4/5\n",
      "938/938 [==============================] - 12s 13ms/step - loss: 1.1503 - acc: 0.6002 - val_loss: 1.1417 - val_acc: 0.6018\n",
      "Epoch 5/5\n",
      "938/938 [==============================] - 11s 12ms/step - loss: 1.1120 - acc: 0.6097 - val_loss: 1.1083 - val_acc: 0.6104\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2220ccb81c0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    vectorizer, \n",
    "    w2v.get_keras_embedding(train_embeddings=False),\n",
    "    keras.layers.Lambda(lambda x: tf.reduce_mean(x,axis=1)),\n",
    "    keras.layers.Dense(4, activation='softmax')\n",
    "])\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'])\n",
    "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128),epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isa sa mga dahilan kung bakit hindi natin nakikita ang mas mataas na katumpakan ay dahil ang ilang mga salita mula sa aming dataset ay nawawala sa pretrained na bokabularyo ng GloVe, kaya't sila ay karaniwang hindi pinapansin. Upang malampasan ito, maaari tayong mag-train ng sarili nating mga embeddings batay sa aming dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mga Kontekstwal na Embedding\n",
    "\n",
    "Isa sa mga pangunahing limitasyon ng tradisyunal na pretrained embedding representations tulad ng Word2Vec ay ang katotohanang, kahit na kaya nilang makuha ang ilang kahulugan ng isang salita, hindi nila kayang pag-iba-ibahin ang magkakaibang kahulugan nito. Maaari itong magdulot ng mga problema sa mga downstream na modelo.\n",
    "\n",
    "Halimbawa, ang salitang 'play' ay may iba't ibang kahulugan sa dalawang magkaibang pangungusap na ito:\n",
    "- Pumunta ako sa isang **play** sa teatro.\n",
    "- Gusto ni John na **maglaro** kasama ang kanyang mga kaibigan.\n",
    "\n",
    "Ang mga pretrained embeddings na nabanggit natin ay kumakatawan sa parehong kahulugan ng salitang 'play' sa iisang embedding. Upang malampasan ang limitasyong ito, kailangan nating bumuo ng mga embedding batay sa **language model**, na sinanay sa isang malaking corpus ng teksto, at *alam* kung paano maaaring pagsama-samahin ang mga salita sa iba't ibang konteksto. Ang pagtalakay sa mga kontekstwal na embedding ay lampas sa saklaw ng tutorial na ito, ngunit babalikan natin ito kapag pinag-usapan na ang mga language model sa susunod na yunit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Paunawa**:  \nAng dokumentong ito ay isinalin gamit ang AI translation service na [Co-op Translator](https://github.com/Azure/co-op-translator). Bagama't sinisikap naming maging tumpak, tandaan na ang mga awtomatikong pagsasalin ay maaaring maglaman ng mga pagkakamali o hindi eksaktong salin. Ang orihinal na dokumento sa kanyang katutubong wika ang dapat ituring na opisyal na pinagmulan. Para sa mahalagang impormasyon, inirerekomenda ang propesyonal na pagsasalin ng tao. Hindi kami mananagot sa anumang hindi pagkakaunawaan o maling interpretasyon na dulot ng paggamit ng pagsasaling ito.\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0cb620c6d4b9f7a635928804c26cf22403d89d98d79684e4529119355ee6d5a5"
  },
  "kernel_info": {
   "name": "conda-env-py37_tensorflow-py"
  },
  "kernelspec": {
   "display_name": "py37_tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "coopTranslator": {
   "original_hash": "b859482be7f61d1eadc2c6a2720a37e4",
   "translation_date": "2025-08-28T04:31:32+00:00",
   "source_file": "lessons/5-NLP/14-Embeddings/EmbeddingsTF.ipynb",
   "language_code": "tl"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}