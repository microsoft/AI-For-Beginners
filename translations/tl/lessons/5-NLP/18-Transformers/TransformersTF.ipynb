{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mga Mekanismo ng Atensyon at mga Transformer\n",
    "\n",
    "Isang pangunahing kahinaan ng mga recurrent network ay ang lahat ng salita sa isang sequence ay may parehong epekto sa resulta. Nagdudulot ito ng hindi optimal na performance sa mga karaniwang LSTM encoder-decoder na modelo para sa mga sequence-to-sequence na gawain, tulad ng Named Entity Recognition at Machine Translation. Sa totoong buhay, may mga partikular na salita sa input sequence na mas may epekto sa mga output kaysa sa iba.\n",
    "\n",
    "Isaalang-alang ang isang sequence-to-sequence na modelo, tulad ng machine translation. Ito ay ipinatutupad gamit ang dalawang recurrent network, kung saan ang isang network (**encoder**) ay nagko-collapse ng input sequence sa isang hidden state, at ang isa pa, ang **decoder**, ay nag-u-unroll ng hidden state na ito sa isinaling resulta. Ang problema sa ganitong paraan ay ang huling estado ng network ay nahihirapang maalala ang simula ng pangungusap, na nagdudulot ng mababang kalidad ng modelo sa mahahabang pangungusap.\n",
    "\n",
    "Ang **Mekanismo ng Atensyon** ay nagbibigay ng paraan upang timbangin ang kontekstwal na epekto ng bawat input vector sa bawat output prediction ng RNN. Ang paraan ng pagpapatupad nito ay sa pamamagitan ng paglikha ng mga shortcut sa pagitan ng mga intermediate state ng input RNN at output RNN. Sa ganitong paraan, kapag gumagawa ng output symbol $y_t$, isasaalang-alang natin ang lahat ng input hidden states $h_i$, na may iba't ibang weight coefficients $\\alpha_{t,i}$. \n",
    "\n",
    "![Larawan na nagpapakita ng encoder/decoder model na may additive attention layer](../../../../../translated_images/tl/encoder-decoder-attention.7a726296894fb567.webp)\n",
    "*Ang encoder-decoder model na may additive attention mechanism mula sa [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf), na binanggit mula sa [blog post na ito](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)*\n",
    "\n",
    "Ang attention matrix $\\{\\alpha_{i,j}\\}$ ay kumakatawan sa antas kung saan ang ilang input na salita ay may papel sa pagbuo ng isang partikular na salita sa output sequence. Narito ang halimbawa ng ganitong matrix:\n",
    "\n",
    "![Larawan na nagpapakita ng sample alignment na natagpuan ng RNNsearch-50, kinuha mula sa Bahdanau - arviz.org](../../../../../translated_images/tl/bahdanau-fig3.09ba2d37f202a6af.webp)\n",
    "\n",
    "*Larawan mula sa [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) (Fig.3)*\n",
    "\n",
    "Ang mga mekanismo ng atensyon ay responsable sa karamihan ng kasalukuyan o malapit sa kasalukuyang state-of-the-art sa Natural Language Processing. Gayunpaman, ang pagdaragdag ng atensyon ay lubos na nagpapataas ng bilang ng mga parameter ng modelo na nagdulot ng mga isyu sa scaling sa mga RNN. Isang pangunahing limitasyon ng pag-scale ng RNN ay ang likas na katangian ng mga modelong ito na mahirap i-batch at i-parallelize ang training. Sa isang RNN, ang bawat elemento ng sequence ay kailangang iproseso nang sunud-sunod, na nangangahulugang hindi ito madaling ma-parallelize.\n",
    "\n",
    "Ang pagsasama ng mga mekanismo ng atensyon sa limitasyong ito ay nagresulta sa paglikha ng mga Transformer Models na ngayon ay itinuturing na State of the Art, mula sa BERT hanggang sa OpenGPT3.\n",
    "\n",
    "## Mga Transformer Model\n",
    "\n",
    "Sa halip na ipasa ang konteksto ng bawat nakaraang prediksyon sa susunod na hakbang ng pagsusuri, ang **transformer models** ay gumagamit ng **positional encodings** at **attention** upang makuha ang konteksto ng isang input sa loob ng isang ibinigay na window ng teksto. Ang larawan sa ibaba ay nagpapakita kung paano nakukuha ng positional encodings na may atensyon ang konteksto sa loob ng isang window.\n",
    "\n",
    "![Animated GIF na nagpapakita kung paano isinasagawa ang mga pagsusuri sa transformer models.](../../../../../lessons/5-NLP/18-Transformers/images/transformer-animated-explanation.gif) \n",
    "\n",
    "Dahil ang bawat input na posisyon ay independiyenteng naiuugnay sa bawat output na posisyon, ang mga transformer ay mas mahusay sa pag-parallelize kumpara sa mga RNN, na nagbibigay-daan sa mas malalaking at mas expressive na mga modelo ng wika. Ang bawat attention head ay maaaring gamitin upang matutunan ang iba't ibang relasyon sa pagitan ng mga salita na nagpapabuti sa mga gawain sa Natural Language Processing.\n",
    "\n",
    "## Pagbuo ng Simpleng Transformer Model\n",
    "\n",
    "Ang Keras ay walang built-in na Transformer layer, ngunit maaari tayong gumawa ng sarili natin. Tulad ng dati, magpo-focus tayo sa text classification ng AG News dataset, ngunit mahalagang banggitin na ang mga Transformer models ay nagpapakita ng pinakamahusay na resulta sa mas mahihirap na gawain sa NLP.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "\n",
    "ds_train, ds_test = tfds.load('ag_news_subset').values()\n",
    "\n",
    "def extract_text(x):\n",
    "    return x['title']+' '+x['description']\n",
    "\n",
    "def tupelize(x):\n",
    "    return (extract_text(x),x['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ang mga bagong layer sa Keras ay dapat mag-subclass sa klase na `Layer`, at mag-implement ng `call` na method. Magsimula tayo sa **Positional Embedding** na layer. Gagamit tayo ng [ilang code mula sa opisyal na dokumentasyon ng Keras](https://keras.io/examples/nlp/text_classification_with_transformer/). Ipagpapalagay natin na ang lahat ng input sequences ay pinapad sa haba na `maxlen`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(keras.layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = keras.layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "        self.maxlen = maxlen\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = self.maxlen\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x+positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ngayon, ipapatupad natin ang transformer block. Tatanggapin nito ang output ng naunang tinukoy na embedding layer:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim, name='attn')\n",
    "        self.ffn = keras.Sequential(\n",
    "            [keras.layers.Dense(ff_dim, activation=\"relu\"), keras.layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = keras.layers.Dropout(rate)\n",
    "        self.dropout2 = keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ngayon, handa na tayong i-define ang kumpletong modelo ng transformer:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "text_vectorization (TextVect (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "token_and_position_embedding (None, 256, 32)           648192    \n",
      "_________________________________________________________________\n",
      "transformer_block (Transform (None, 256, 32)           10656     \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 20)                660       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 84        \n",
      "=================================================================\n",
      "Total params: 659,592\n",
      "Trainable params: 659,592\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 32  # Embedding size for each token\n",
    "num_heads = 2  # Number of attention heads\n",
    "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
    "maxlen = 256\n",
    "vocab_size = 20000\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.experimental.preprocessing.TextVectorization(max_tokens=vocab_size,output_sequence_length=maxlen, input_shape=(1,)),\n",
    "    TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim),\n",
    "    TransformerBlock(embed_dim, num_heads, ff_dim),\n",
    "    keras.layers.GlobalAveragePooling1D(),\n",
    "    keras.layers.Dropout(0.1),\n",
    "    keras.layers.Dense(20, activation=\"relu\"),\n",
    "    keras.layers.Dropout(0.1),\n",
    "    keras.layers.Dense(4, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokenizer\n",
      "938/938 [==============================] - 45s 39ms/step - loss: 0.4978 - acc: 0.8068 - val_loss: 0.2808 - val_acc: 0.9124\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9c2427a0d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Training tokenizer')\n",
    "model.layers[0].adapt(ds_train.map(extract_text))\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'], optimizer='adam')\n",
    "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mga Modelong Transformer ng BERT\n",
    "\n",
    "**BERT** (Bidirectional Encoder Representations from Transformers) ay isang napakalaking multi-layer transformer network na may 12 layers para sa *BERT-base*, at 24 para sa *BERT-large*. Ang modelo ay unang sinanay gamit ang malaking corpus ng text data (WikiPedia + mga libro) gamit ang unsupervised training (pagpapredikta ng mga nakatagong salita sa isang pangungusap). Sa panahon ng pre-training, ang modelo ay nakakapulot ng mataas na antas ng pag-unawa sa wika na maaaring magamit sa iba pang mga dataset sa pamamagitan ng fine tuning. Ang prosesong ito ay tinatawag na **transfer learning**.\n",
    "\n",
    "![larawan mula sa http://jalammar.github.io/illustrated-bert/](../../../../../translated_images/tl/jalammarBERT-language-modeling-masked-lm.34f113ea5fec4362.webp)\n",
    "\n",
    "Maraming mga bersyon ng Transformer architectures kabilang ang BERT, DistilBERT, BigBird, OpenGPT3, at iba pa na maaaring i-fine tune.\n",
    "\n",
    "Tingnan natin kung paano natin magagamit ang pre-trained na modelo ng BERT para sa paglutas ng ating tradisyunal na problema sa sequence classification. Hihiramin natin ang ideya at ilang code mula sa [opisyal na dokumentasyon](https://www.tensorflow.org/text/tutorials/classify_text_with_bert).\n",
    "\n",
    "Upang mag-load ng pre-trained na mga modelo, gagamit tayo ng **Tensorflow hub**. Una, i-load natin ang BERT-specific vectorizer:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_41180/4216669875.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow_text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow_hub\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mhub\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhub\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mKerasLayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_text'"
     ]
    }
   ],
   "source": [
    "import tensorflow_text \n",
    "import tensorflow_hub as hub\n",
    "vectorizer = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_type_ids': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       " array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "       dtype=int32)>,\n",
       " 'input_word_ids': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       " array([[  101,  1045,  2293, 19081,   102,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]], dtype=int32)>,\n",
       " 'input_mask': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       " array([[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "       dtype=int32)>}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer(['I love transformers'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mahalagang gamitin mo ang parehong vectorizer na ginamit sa orihinal na network na sinanay. Bukod dito, ang BERT vectorizer ay nagbabalik ng tatlong bahagi:\n",
    "* `input_word_ids`, na isang sequence ng mga numero ng token para sa input na pangungusap\n",
    "* `input_mask`, na nagpapakita kung aling bahagi ng sequence ang naglalaman ng aktwal na input, at kung alin ang padding. Katulad ito ng mask na ginawa ng `Masking` layer\n",
    "* `input_type_ids` ay ginagamit para sa mga gawain sa language modeling, at nagbibigay-daan upang tukuyin ang dalawang input na pangungusap sa isang sequence.\n",
    "\n",
    "Pagkatapos, maaari nating i-instantiate ang BERT feature extractor:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = hub.KerasLayer('https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pooled_output -> (1, 128)\n",
      "encoder_outputs -> 4\n",
      "sequence_output -> (1, 128, 128)\n",
      "default -> (1, 128)\n"
     ]
    }
   ],
   "source": [
    "z = bert(vectorizer(['I love transformers']))\n",
    "for i,x in z.items():\n",
    "    print(f\"{i} -> { len(x) if isinstance(x, list) else x.shape }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kaya, ang BERT layer ay nagbibigay ng ilang kapaki-pakinabang na resulta:\n",
    "* Ang `pooled_output` ay resulta ng pag-average ng lahat ng tokens sa sequence. Maaari mo itong ituring bilang isang matalinong semantic embedding ng buong network. Katumbas ito ng output ng `GlobalAveragePooling1D` layer sa ating naunang modelo.\n",
    "* Ang `sequence_output` ay ang output ng huling transformer layer (katumbas ng output ng `TransformerBlock` sa ating modelo sa itaas).\n",
    "* Ang `encoder_outputs` ay ang mga output ng lahat ng transformer layers. Dahil nag-load tayo ng 4-layer BERT model (tulad ng maaari mong mahulaan mula sa pangalan, na naglalaman ng `4_H`), mayroon itong 4 na tensors. Ang huli ay pareho sa `sequence_output`.\n",
    "\n",
    "Ngayon, magde-define tayo ng end-to-end classification model. Gagamit tayo ng *functional model definition*, kung saan ide-define natin ang model input, at pagkatapos ay magbibigay ng serye ng mga expression upang kalkulahin ang output nito. Gagawin din nating hindi-trainable ang mga BERT model weights, at ang final classifier lamang ang ating ita-train:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer (KerasLayer)        {'input_type_ids': ( 0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer_1 (KerasLayer)      {'pooled_output': (N 4782465     keras_layer[0][0]                \n",
      "                                                                 keras_layer[0][1]                \n",
      "                                                                 keras_layer[0][2]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 128)          0           keras_layer_1[0][5]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 4)            516         dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 4,782,981\n",
      "Trainable params: 516\n",
      "Non-trainable params: 4,782,465\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inp = keras.Input(shape=(),dtype=tf.string)\n",
    "x = vectorizer(inp)\n",
    "x = bert(x)\n",
    "x = keras.layers.Dropout(0.1)(x['pooled_output'])\n",
    "out = keras.layers.Dense(4,activation='softmax')(x)\n",
    "model = keras.models.Model(inp,out)\n",
    "bert.trainable = False\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 528s 559ms/step - loss: 0.8056 - acc: 0.6983 - val_loss: 0.5953 - val_acc: 0.7888\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9bb1e36d00>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'], optimizer='adam')\n",
    "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kahit kaunti lang ang mga trainable na parameter, medyo mabagal ang proseso dahil mabigat ang computational load ng BERT feature extractor. Mukhang hindi natin naabot ang makatwirang accuracy, maaaring dahil sa kakulangan ng training o kakulangan ng mga parameter ng modelo.\n",
    "\n",
    "Subukan nating i-unfreeze ang mga BERT weights at i-train ito rin. Kakailanganin nito ng napakaliit na learning rate, pati na rin mas maingat na training strategy gamit ang **warmup**, at ang **AdamW** optimizer. Gagamitin natin ang `tf-models-official` package para gumawa ng optimizer:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer (KerasLayer)        {'input_type_ids': ( 0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer_1 (KerasLayer)      {'pooled_output': (N 4782465     keras_layer[0][0]                \n",
      "                                                                 keras_layer[0][1]                \n",
      "                                                                 keras_layer[0][2]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 128)          0           keras_layer_1[0][5]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 4)            516         dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 4,782,981\n",
      "Trainable params: 4,782,980\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n",
      "938/938 [==============================] - 629s 664ms/step - loss: 0.6344 - acc: 0.7658 - val_loss: 0.4876 - val_acc: 0.8247\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9bb0bd0070>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from official.nlp import optimization \n",
    "bert.trainable=True\n",
    "model.summary()\n",
    "epochs = 3\n",
    "opt = optimization.create_optimizer(\n",
    "    init_lr=3e-5,\n",
    "    num_train_steps=epochs*len(ds_train),\n",
    "    num_warmup_steps=0.1*epochs*len(ds_train),\n",
    "    optimizer_type='adamw')\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'], optimizer=opt)\n",
    "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tulad ng nakikita mo, medyo mabagal ang pagsasanay - pero maaari mong subukang mag-eksperimento at sanayin ang modelo nang ilang epochs (5-10) at tingnan kung makakakuha ka ng mas magandang resulta kumpara sa mga pamamaraang ginamit natin dati.\n",
    "\n",
    "## Huggingface Transformers Library\n",
    "\n",
    "Isa pang karaniwang (at medyo mas simple) paraan ng paggamit ng Transformer models ay ang [HuggingFace package](https://github.com/huggingface/), na nagbibigay ng mga simpleng building blocks para sa iba't ibang NLP tasks. Available ito para sa parehong Tensorflow at PyTorch, isa pang napakapopular na neural network framework.\n",
    "\n",
    "> **Note**: Kung hindi ka interesado na makita kung paano gumagana ang Transformers library - maaari mong laktawan ang dulo ng notebook na ito, dahil wala kang makikitang malaki ang pagkakaiba mula sa ginawa natin sa itaas. Uulitin lang natin ang parehong mga hakbang ng pagsasanay sa BERT model gamit ang ibang library at mas malaking modelo. Dahil dito, ang proseso ay nangangailangan ng medyo mahabang pagsasanay, kaya maaaring gusto mo na lang tingnan ang code. \n",
    "\n",
    "Tingnan natin kung paano masosolusyunan ang ating problema gamit ang [Huggingface Transformers](http://huggingface.co).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ang unang bagay na kailangan nating gawin ay pumili ng modelong gagamitin natin. Bukod sa ilang built-in na mga modelo, ang Huggingface ay mayroong [online na repositoryo ng mga modelo](https://huggingface.co/models), kung saan makakahanap ka ng mas maraming pre-trained na mga modelo mula sa komunidad. Ang lahat ng mga modelong ito ay maaaring i-load at gamitin sa pamamagitan lamang ng pagbibigay ng pangalan ng modelo. Ang lahat ng kinakailangang binary files para sa modelo ay awtomatikong mada-download.\n",
    "\n",
    "Sa ilang pagkakataon, maaaring kailanganin mong i-load ang sarili mong mga modelo. Sa ganitong kaso, maaari mong tukuyin ang direktoryo na naglalaman ng lahat ng kaugnay na mga file, kabilang ang mga parameter para sa tokenizer, ang `config.json` na file na may mga parameter ng modelo, mga binary weights, at iba pa.\n",
    "\n",
    "Mula sa pangalan ng modelo, maaari nating i-instantiate ang parehong modelo at ang tokenizer. Magsimula tayo sa tokenizer:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "# To load the model from Internet repository using model name. \n",
    "# Use this if you are running from your own copy of the notebooks\n",
    "bert_model = 'bert-base-uncased' \n",
    "\n",
    "# To load the model from the directory on disk. Use this for Microsoft Learn module, because we have\n",
    "# prepared all required files for you.\n",
    "#bert_model = './bert'\n",
    "\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(bert_model)\n",
    "\n",
    "MAX_SEQ_LEN = 128\n",
    "PAD_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "UNK_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ang `tokenizer` na object ay naglalaman ng `encode` na function na maaaring direktang gamitin upang i-encode ang teksto:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 23435, 12314, 2003, 1037, 2307, 7705, 2005, 17953, 2361, 102]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('Tensorflow is a great framework for NLP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maaari rin nating gamitin ang tokenizer upang i-encode ang isang sequence sa paraang angkop para maipasa sa modelo, halimbawa, kabilang ang `token_ids`, `input_mask` na mga field, at iba pa. Maaari rin nating tukuyin na gusto natin ng Tensorflow tensors sa pamamagitan ng pagbibigay ng argumentong `return_tensors='tf'`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[ 101, 7592, 1010, 2045,  102]], dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[0, 0, 0, 0, 0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[1, 1, 1, 1, 1]], dtype=int32)>}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(['Hello, there'],return_tensors='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sa ating kaso, gagamit tayo ng pre-trained na BERT model na tinatawag na `bert-base-uncased`. Ang *Uncased* ay nangangahulugang hindi sensitibo sa malalaking at maliliit na titik ang model.\n",
    "\n",
    "Kapag tine-train ang model, kailangan nating magbigay ng tokenized na sequence bilang input, kaya magdidisenyo tayo ng data processing pipeline. Dahil ang `tokenizer.encode` ay isang Python function, gagamitin natin ang parehong paraan tulad ng sa nakaraang unit sa pamamagitan ng pagtawag dito gamit ang `py_function`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(x):\n",
    "    return tokenizer.encode(x.numpy().decode('utf-8'),return_tensors='tf',padding='max_length',max_length=MAX_SEQ_LEN,truncation=True)[0]\n",
    "\n",
    "def process_fn(x):\n",
    "    s = x['title']+' '+x['description']\n",
    "    e = tf.py_function(process,inp=[s],Tout=(tf.int32))\n",
    "    e.set_shape(MAX_SEQ_LEN)\n",
    "    return e,x['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ngayon maaari na nating i-load ang aktwal na modelo gamit ang `BertForSequenceClassification` na package. Tinitiyak nito na ang ating modelo ay mayroon nang kinakailangang arkitektura para sa klasipikasyon, kabilang ang panghuling classifier. Makakakita ka ng babala na nagsasaad na ang mga timbang ng panghuling classifier ay hindi na-initialize, at ang modelo ay mangangailangan ng pre-training - ayos lang iyon, dahil eksakto iyon ang gagawin natin!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = transformers.TFBertForSequenceClassification.from_pretrained(bert_model,num_labels=4,output_attentions=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  109482240 \n",
      "_________________________________________________________________\n",
      "dropout_75 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  3076      \n",
      "=================================================================\n",
      "Total params: 109,485,316\n",
      "Trainable params: 109,485,316\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tulad ng makikita mo mula sa `summary()`, ang modelo ay naglalaman ng halos 110 milyong mga parameter! Marahil, kung nais natin ng simpleng gawain ng klasipikasyon sa medyo maliit na dataset, hindi natin nais na i-train ang BERT base layer:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  109482240 \n",
      "_________________________________________________________________\n",
      "dropout_75 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  3076      \n",
      "=================================================================\n",
      "Total params: 109,485,316\n",
      "Trainable params: 3,076\n",
      "Non-trainable params: 109,482,240\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.layers[0].trainable = False\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ngayon, handa na tayong magsimula sa pagsasanay!\n",
    "\n",
    "> **Tandaan**: Ang pagsasanay ng full-scale na BERT model ay maaaring ubos ng maraming oras! Kaya't sasanayin lang natin ito para sa unang 32 batches. Ito ay para lamang ipakita kung paano isinasagawa ang pagsasanay ng modelo. Kung interesado kang subukan ang full-scale na pagsasanay - alisin lamang ang mga parameter na `steps_per_epoch` at `validation_steps`, at maghanda kang maghintay!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 142s 4s/step - loss: 1.3896 - acc: 0.2500 - val_loss: 1.3863 - val_acc: 0.2480\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f1d40a4b6a0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile('adam','sparse_categorical_crossentropy',['acc'])\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "model.fit(ds_train.map(process_fn).batch(32),validation_data=ds_test.map(process_fn).batch(32),steps_per_epoch=32,validation_steps=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kung tataasan mo ang bilang ng mga iterations at maghihintay nang sapat na tagal, at magsasanay para sa ilang epochs, maaasahan mong ang BERT classification ang magbibigay ng pinakamataas na accuracy! Ito ay dahil ang BERT ay may mahusay nang pagkaunawa sa istruktura ng wika, at kailangan lang nating i-fine-tune ang panghuling classifier. Gayunpaman, dahil ang BERT ay isang malaking modelo, ang buong proseso ng pagsasanay ay tumatagal ng mahabang oras at nangangailangan ng malakas na computational power! (GPU, at mas mainam kung higit sa isa).\n",
    "\n",
    "> **Note:** Sa ating halimbawa, gumagamit tayo ng isa sa pinakamaliit na pre-trained na BERT models. Mayroon ding mas malalaking modelo na malamang na magbigay ng mas magagandang resulta.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mga Mahalagang Punto\n",
    "\n",
    "Sa yunit na ito, tinalakay natin ang mga pinakabagong arkitektura ng modelo na batay sa **transformers**. Ginamit natin ang mga ito para sa ating gawain sa text classification, ngunit katulad nito, maaaring gamitin ang mga BERT models para sa entity extraction, question answering, at iba pang mga gawain sa NLP.\n",
    "\n",
    "Ang mga transformer models ang kasalukuyang nangunguna sa larangan ng NLP, at sa karamihan ng mga kaso, ito ang dapat na unang solusyon na subukan mo kapag gumagawa ng mga custom na solusyon sa NLP. Gayunpaman, ang pag-unawa sa mga pangunahing prinsipyo ng recurrent neural networks na tinalakay sa module na ito ay napakahalaga kung nais mong bumuo ng mga advanced na neural models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Paunawa**:  \nAng dokumentong ito ay isinalin gamit ang AI translation service na [Co-op Translator](https://github.com/Azure/co-op-translator). Bagama't sinisikap naming maging tumpak, pakitandaan na ang mga awtomatikong pagsasalin ay maaaring maglaman ng mga pagkakamali o hindi pagkakatugma. Ang orihinal na dokumento sa orihinal nitong wika ang dapat ituring na opisyal na sanggunian. Para sa mahalagang impormasyon, inirerekomenda ang propesyonal na pagsasalin ng tao. Hindi kami mananagot sa anumang hindi pagkakaunawaan o maling interpretasyon na maaaring magmula sa paggamit ng pagsasaling ito.\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0cb620c6d4b9f7a635928804c26cf22403d89d98d79684e4529119355ee6d5a5"
  },
  "kernelspec": {
   "display_name": "py38_tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "coopTranslator": {
   "original_hash": "ab59c532409774988ab875f2260e8e53",
   "translation_date": "2025-08-28T04:19:53+00:00",
   "source_file": "lessons/5-NLP/18-Transformers/TransformersTF.ipynb",
   "language_code": "tl"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}