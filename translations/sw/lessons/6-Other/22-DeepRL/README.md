# Djup F칬rst칛rkningsinl칛rning

F칬rst칛rkningsinl칛rning (RL) ses som en av de grundl칛ggande paradigmerna inom maskininl칛rning, bredvid 칬vervakad inl칛rning och o칬vervakad inl칛rning. Medan vi i 칬vervakad inl칛rning f칬rlitar oss p친 dataset med k칛nda utfall, bygger RL p친 **att l칛ra sig genom att g칬ra**. Till exempel, n칛r vi f칬rst ser ett datorspel b칬rjar vi spela, 칛ven utan att k칛nna till reglerna, och snart kan vi f칬rb칛ttra v친ra f칛rdigheter bara genom att spela och justera v친rt beteende.

## [F칬r- f칬rel칛sningsquiz](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/122)

F칬r att utf칬ra RL beh칬ver vi:

* En **milj칬** eller **simulator** som s칛tter reglerna f칬r spelet. Vi ska kunna k칬ra experimenten i simulatorn och observera resultaten.
* En **Bel칬ningsfunktion**, som indikerar hur framg친ngsrikt v친rt experiment var. I fallet med att l칛ra sig spela ett datorspel skulle bel칬ningen vara v친r slutpo칛ng.

Baserat p친 bel칬ningsfunktionen ska vi kunna justera v친rt beteende och f칬rb칛ttra v친ra f칛rdigheter, s친 att vi n칛sta g친ng spelar b칛ttre. Den st칬rsta skillnaden mellan andra typer av maskininl칛rning och RL 칛r att vi i RL vanligtvis inte vet om vi vinner eller f칬rlorar f칬rr칛n vi har avslutat spelet. D칛rf칬r kan vi inte s칛ga om ett visst drag ensam 칛r bra eller inte - vi f친r bara en bel칬ning i slutet av spelet.

Under RL utf칬r vi vanligtvis m친nga experiment. Under varje experiment beh칬ver vi balansera mellan att f칬lja den optimala strategin som vi har l칛rt oss hittills (**utnyttjande**) och att utforska nya m칬jliga tillst친nd (**utforskning**).

## OpenAI Gym

Ett utm칛rkt verktyg f칬r RL 칛r [OpenAI Gym](https://gym.openai.com/) - en **simuleringsmilj칬**, som kan simulera m친nga olika milj칬er, fr친n Atari-spel till fysiken bakom polbalansering. Det 칛r en av de mest popul칛ra simuleringsmilj칬erna f칬r tr칛ning av f칬rst칛rkningsinl칛rningsalgoritmer och underh친lls av [OpenAI](https://openai.com/).

> **Notera**: Du kan se alla milj칬er som finns tillg칛ngliga fr친n OpenAI Gym [h칛r](https://gym.openai.com/envs/#classic_control).

## CartPole Balansering

Du har f칬rmodligen sett moderna balanseringsanordningar som *Segway* eller *Gyroscooters*. De kan automatiskt balansera genom att justera sina hjul som svar p친 en signal fr친n en accelerometer eller gyroskop. I den h칛r sektionen kommer vi att l칛ra oss hur man l칬ser ett liknande problem - att balansera en st친ng. Det liknar en situation n칛r en cirkusartist beh칬ver balansera en st친ng p친 sin hand - men denna st친ngbalansering sker endast i 1D.

En f칬renklad version av balansering 칛r k칛nd som **CartPole**-problemet. I cartpole-v칛rlden har vi en horisontell glidare som kan r칬ra sig 친t v칛nster eller h칬ger, och m친let 칛r att balansera en vertikal st친ng ovanp친 glidaren medan den r칬r sig.

<img alt="en cartpole" src="images/cartpole.png" width="200"/>

F칬r att skapa och anv칛nda denna milj칬 beh칬ver vi ett par rader Python-kod:

```python
import gym
env = gym.make("CartPole-v1")

env.reset()
done = False
total_reward = 0
while not done:
   env.render()
   action = env.action_space.sample()
   observaton, reward, done, info = env.step(action)
   total_reward += reward

print(f"Total reward: {total_reward}")
```

Varje milj칬 kan n친s p친 exakt samma s칛tt:
* `env.reset` starts a new experiment
* `env.step` utf칬r ett simuleringssteg. Den tar emot en **친tg칛rd** fr친n **친tg칛rdsutrymmet** och returnerar en **observation** (fr친n observationsutrymmet), samt en bel칬ning och en avslutningsflagga.

I exemplet ovan utf칬r vi en slumpm칛ssig 친tg칛rd vid varje steg, vilket 칛r anledningen till att experimentets livsl칛ngd 칛r mycket kort:

![icke-balanserande cartpole](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-nobalance.gif)

M친let med en RL-algoritm 칛r att tr칛na en modell - den s친 kallade **policyn**  - som kommer att returnera 친tg칛rden som svar p친 ett givet tillst친nd. Vi kan ocks친 betrakta policyn som probabilistisk, t.ex. f칬r n친got tillst친nd *s* och 친tg칛rd *a* kommer den att returnera sannolikheten (*a*|*s*) att vi b칬r ta *a* i tillst친nd *s*.

## Policy Gradient Algoritm

Det mest uppenbara s칛ttet att modellera en policy 칛r att skapa ett neuralt n칛tverk som tar tillst친nd som indata och returnerar motsvarande 친tg칛rder (eller snarare sannolikheterna f칬r alla 친tg칛rder). P친 ett s칛tt skulle det likna en normal klassificeringsuppgift, med en stor skillnad - vi vet inte i f칬rv칛g vilka 친tg칛rder vi ska ta vid varje steg.

Id칠n h칛r 칛r att uppskatta dessa sannolikheter. Vi bygger en vektor av **kumulativa bel칬ningar** som visar v친r totala bel칬ning vid varje steg av experimentet. Vi till칛mpar ocks친 **bel칬ningsdiskontering** genom att multiplicera tidigare bel칬ningar med en koefficient 풥=0.99, f칬r att minska betydelsen av tidigare bel칬ningar. Sedan f칬rst칛rker vi de steg l칛ngs experimentets v칛g som ger st칬rre bel칬ningar.

> L칛r dig mer om Policy Gradient-algoritmen och se den i aktion i [exempelnotebooken](../../../../../lessons/6-Other/22-DeepRL/CartPole-RL-TF.ipynb).

## Actor-Critic Algoritm

En f칬rb칛ttrad version av Policy Gradients-metoden kallas **Actor-Critic**. Huvudid칠n bakom den 칛r att det neurala n칛tverket skulle tr칛nas f칬r att returnera tv친 saker:

* Policyn, som avg칬r vilken 친tg칛rd som ska vidtas. Denna del kallas **akt칬r**
* Uppskattningen av den totala bel칬ningen vi kan f칬rv칛nta oss att f친 i detta tillst친nd - denna del kallas **kritiker**.

P친 ett s칛tt liknar denna arkitektur en [GAN](../../4-ComputerVision/10-GANs/README.md), d칛r vi har tv친 n칛tverk som tr칛nas mot varandra. I actor-critic-modellen f칬resl친r akt칬ren den 친tg칛rd vi beh칬ver vidta, och kritikern f칬rs칬ker vara kritisk och uppskatta resultatet. Men v친rt m친l 칛r att tr칛na dessa n칛tverk i enhet.

Eftersom vi k칛nner till b친de de verkliga kumulativa bel칬ningarna och de resultat som returnerats av kritikern under experimentet, 칛r det relativt enkelt att bygga en f칬rlustfunktion som minimerar skillnaden mellan dem. Det skulle ge oss **kritikerf칬rlust**. Vi kan ber칛kna **akt칬rf칬rlust** genom att anv칛nda samma metod som i policy gradient-algoritmen.

Efter att ha k칬rt en av dessa algoritmer kan vi f칬rv칛nta oss att v친r CartPole beter sig s친 h칛r:

![en balanserande cartpole](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-balance.gif)

## 九꽲잺 칐vningar: Policy Gradients och Actor-Critic RL

Forts칛tt din inl칛rning i f칬ljande notebooks:

* [RL i TensorFlow](../../../../../lessons/6-Other/22-DeepRL/CartPole-RL-TF.ipynb)
* [RL i PyTorch](../../../../../lessons/6-Other/22-DeepRL/CartPole-RL-PyTorch.ipynb)

## Andra RL-uppgifter

F칬rst칛rkningsinl칛rning 칛r idag ett snabbt v칛xande forskningsomr친de. N친gra intressanta exempel p친 f칬rst칛rkningsinl칛rning 칛r:

* Att l칛ra en dator att spela **Atari-spel**. Den utmanande delen av detta problem 칛r att vi inte har ett enkelt tillst친nd representerat som en vektor, utan snarare en sk칛rmdump - och vi beh칬ver anv칛nda CNN f칬r att konvertera denna sk칛rmbild till en funktionsvektor, eller f칬r att extrahera bel칬ningsinformation. Atari-spel finns tillg칛ngliga i Gym.
* Att l칛ra en dator att spela br칛dspel, s친som Schack och Go. Nyligen har toppmoderna program som **Alpha Zero** tr칛nats fr친n grunden av tv친 agenter som spelar mot varandra och f칬rb칛ttras vid varje steg.
* Inom industrin anv칛nds RL f칬r att skapa kontrollsystem fr친n simulering. En tj칛nst som heter [Bonsai](https://azure.microsoft.com/services/project-bonsai/?WT.mc_id=academic-77998-cacaste) 칛r speciellt utformad f칬r det.

## Slutsats

Vi har nu l칛rt oss hur man tr칛nar agenter f칬r att uppn친 bra resultat genom att bara ge dem en bel칬ningsfunktion som definierar det 칬nskade tillst친ndet f칬r spelet, och genom att ge dem m칬jlighet att intelligent utforska s칬komr친det. Vi har framg친ngsrikt pr칬vat tv친 algoritmer och uppn친tt ett bra resultat p친 en relativt kort tid. Men detta 칛r bara b칬rjan p친 din resa in i RL, och du b칬r definitivt 칬verv칛ga att ta en separat kurs om du vill gr칛va djupare.

## 游 Utmaning

Utforska de till칛mpningar som listas i avsnittet "Andra RL-uppgifter" och f칬rs칬k att implementera en!

## [Efter-f칬rel칛sningsquiz](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/222)

## Granskning & Sj칛lvstudie

L칛r dig mer om klassisk f칬rst칛rkningsinl칛rning i v친r [Maskininl칛rning f칬r Nyb칬rjare L칛roplan](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/README.md).

Titta p친 [denna fantastiska video](https://www.youtube.com/watch?v=qv6UVOQ0F44) som handlar om hur en dator kan l칛ra sig att spela Super Mario.

## Uppgift: [Tr칛na en Mountain Car](lab/README.md)

Ditt m친l under denna uppgift skulle vara att tr칛na en annan Gym-milj칬 - [Mountain Car](https://www.gymlibrary.ml/environments/classic_control/mountain_car/).

**Ansvarsfriskrivning**:  
Detta dokument har 칬versatts med hj칛lp av maskinbaserade AI-칬vers칛ttningstj칛nster. 츿ven om vi str칛var efter noggrannhet, v칛nligen var medveten om att automatiska 칬vers칛ttningar kan inneh친lla fel eller brister. Det ursprungliga dokumentet p친 sitt modersm친l b칬r betraktas som den auktoritativa k칛llan. F칬r kritisk information rekommenderas professionell m칛nsklig 칬vers칛ttning. Vi ansvarar inte f칬r n친gra missf칬rst친nd eller feltolkningar som uppst친r fr친n anv칛ndningen av denna 칬vers칛ttning.