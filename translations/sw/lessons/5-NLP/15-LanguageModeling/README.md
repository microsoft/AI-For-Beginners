# Spr친kmodellering

Semantiska inb칛ddningar, s친som Word2Vec och GloVe, 칛r faktiskt ett f칬rsta steg mot **spr친kmodellering** - att skapa modeller som p친 n친got s칛tt *f칬rst친r* (eller *representerar*) spr친kets natur.

## [F칬r-l칛rare quiz](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/115)

Den huvudsakliga id칠n bakom spr친kmodellering 칛r att tr칛na dem p친 oetiketterade dataset p친 ett icke 칬vervakat s칛tt. Detta 칛r viktigt eftersom vi har stora m칛ngder oetiketterad text tillg칛nglig, medan m칛ngden etiketterad text alltid skulle vara begr칛nsad av den insats vi kan l칛gga p친 att m칛rka. Oftast kan vi bygga spr친kmodeller som kan **f칬ruts칛ga saknade ord** i texten, eftersom det 칛r enkelt att maskera ett slumpm칛ssigt ord i texten och anv칛nda det som ett tr칛ningsprov.

## Tr칛ning av inb칛ddningar

I v친ra tidigare exempel anv칛nde vi f칬rtr칛nade semantiska inb칛ddningar, men det 칛r intressant att se hur dessa inb칛ddningar kan tr칛nas. Det finns flera m칬jliga id칠er som kan anv칛ndas:

* **N-Gram** spr친kmodellering, n칛r vi f칬ruts칛ger en token genom att titta p친 N f칬reg친ende tokens (N-gram)
* **Kontinuerlig Bag-of-Words** (CBoW), n칛r vi f칬ruts칛ger den mittersta token $W_0$ i en tokensekvens $W_{-N}$, ..., $W_N$.
* **Skip-gram**, d칛r vi f칬ruts칛ger en upps칛ttning granntokens {$W_{-N},\dots, W_{-1}, W_1,\dots, W_N$} fr친n den mittersta token $W_0$.

![bild fr친n artikel om att omvandla ord till vektorer](../../../../../translated_images/example-algorithms-for-converting-words-to-vectors.fbe9207a726922f6f0f5de66427e8a6eda63809356114e28fb1fa5f4a83ebda7.sw.png)

> Bild fr친n [denna artikel](https://arxiv.org/pdf/1301.3781.pdf)

## 九꽲잺 Exempel Notebooks: Tr칛ning av CBoW-modell

Forts칛tt din inl칛rning i f칬ljande notebooks:

* [Tr칛ning av CBoW Word2Vec med TensorFlow](../../../../../lessons/5-NLP/15-LanguageModeling/CBoW-TF.ipynb)
* [Tr칛ning av CBoW Word2Vec med PyTorch](../../../../../lessons/5-NLP/15-LanguageModeling/CBoW-PyTorch.ipynb)

## Slutsats

I den f칬reg친ende lektionen har vi sett att ordinb칛ddningar fungerar som magi! Nu vet vi att tr칛ning av ordinb칛ddningar inte 칛r en s칛rskilt komplex uppgift, och vi borde kunna tr칛na v친ra egna ordinb칛ddningar f칬r dom칛nspecifik text om det beh칬vs.

## [Efter-l칛rare quiz](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/215)

## 칐versyn & Sj칛lvstudie

* [Officiell PyTorch-handledning om spr친kmodellering](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html).
* [Officiell TensorFlow-handledning om tr칛ning av Word2Vec-modell](https://www.TensorFlow.org/tutorials/text/word2vec).
* Att anv칛nda **gensim**-ramverket f칬r att tr칛na de mest anv칛nda inb칛ddningarna p친 n친gra f친 rader kod beskrivs [i denna dokumentation](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html).

## 游 [Uppgift: Tr칛na Skip-Gram-modell](lab/README.md)

I labbet utmanar vi dig att modifiera koden fr친n denna lektion f칬r att tr칛na skip-gram-modellen ist칛llet f칬r CBoW. [L칛s detaljerna](lab/README.md)

**Ansvarsfriskrivning**:  
Detta dokument har 칬versatts med hj칛lp av maskinbaserade AI-칬vers칛ttningstj칛nster. 츿ven om vi str칛var efter noggrannhet, v칛nligen var medveten om att automatiska 칬vers칛ttningar kan inneh친lla fel eller oegentligheter. Det ursprungliga dokumentet p친 sitt modersm친l b칬r betraktas som den auktoritativa k칛llan. F칬r kritisk information rekommenderas professionell m칛nsklig 칬vers칛ttning. Vi ansvarar inte f칬r n친gra missf칬rst친nd eller felaktiga tolkningar som uppst친r till f칬ljd av anv칛ndningen av denna 칬vers칛ttning.