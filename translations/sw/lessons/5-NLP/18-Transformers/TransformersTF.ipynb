{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mbinu za Uangalizi na Transformers\n",
    "\n",
    "Changamoto kubwa ya mitandao ya kurudiarudia (recurrent networks) ni kwamba maneno yote katika mfuatano yana athari sawa kwenye matokeo. Hii husababisha utendaji usiofaa katika mifano ya kawaida ya LSTM encoder-decoder kwa kazi za mfuatano hadi mfuatano, kama Utambuzi wa Vitu Vilivyotajwa (Named Entity Recognition) na Tafsiri ya Mashine. Kwa kweli, maneno maalum katika mfuatano wa pembejeo mara nyingi yana athari kubwa zaidi kwenye matokeo ya mfuatano kuliko mengine.\n",
    "\n",
    "Fikiria mfano wa mfuatano hadi mfuatano, kama tafsiri ya mashine. Inatekelezwa na mitandao miwili ya kurudiarudia, ambapo mtandao mmoja (**encoder**) unakunjwa mfuatano wa pembejeo kuwa hali iliyofichwa, na mwingine, **decoder**, unafungua hali hiyo iliyofichwa kuwa matokeo yaliyotafsiriwa. Tatizo la mbinu hii ni kwamba hali ya mwisho ya mtandao itakuwa na ugumu wa kukumbuka mwanzo wa sentensi, hivyo kusababisha ubora duni wa mfano kwa sentensi ndefu.\n",
    "\n",
    "**Mbinu za Uangalizi** hutoa njia ya kupima athari ya muktadha wa kila vector ya pembejeo kwenye kila utabiri wa matokeo wa RNN. Njia inavyotekelezwa ni kwa kuunda njia za mkato kati ya hali za kati za RNN ya pembejeo na RNN ya matokeo. Kwa njia hii, tunapozalisha alama ya matokeo $y_t$, tutazingatia hali zote zilizofichwa za pembejeo $h_i$, kwa kutumia viwango tofauti vya uzito $\\alpha_{t,i}$.\n",
    "\n",
    "![Picha inayoonyesha mfano wa encoder/decoder na safu ya uangalizi wa kuongeza](../../../../../translated_images/sw/encoder-decoder-attention.7a726296894fb567.webp)\n",
    "*Mfano wa encoder-decoder na mbinu ya uangalizi wa kuongeza katika [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf), iliyotajwa kutoka [blogu hii](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)*\n",
    "\n",
    "Matriki ya uangalizi $\\{\\alpha_{i,j}\\}$ itaonyesha kiwango ambacho maneno fulani ya pembejeo yanachangia katika uzalishaji wa neno fulani katika mfuatano wa matokeo. Hapo chini kuna mfano wa matriki kama hiyo:\n",
    "\n",
    "![Picha inayoonyesha mpangilio wa mfano uliopatikana na RNNsearch-50, kutoka Bahdanau - arviz.org](../../../../../translated_images/sw/bahdanau-fig3.09ba2d37f202a6af.webp)\n",
    "\n",
    "*Picha iliyotolewa kutoka [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) (Fig.3)*\n",
    "\n",
    "Mbinu za uangalizi zinahusika na sehemu kubwa ya hali ya juu ya sasa au karibu na hali ya juu katika Usindikaji wa Lugha Asilia. Hata hivyo, kuongeza uangalizi huongeza sana idadi ya vigezo vya mfano, jambo ambalo limesababisha changamoto za kupanua RNNs. Kizuizi kikuu cha kupanua RNNs ni kwamba asili ya kurudiarudia ya mifano inafanya iwe changamoto kuunda vikundi na kuendesha mafunzo kwa sambamba. Katika RNN, kila kipengele cha mfuatano kinahitaji kusindika kwa mpangilio wa mfuatano, jambo ambalo lina maana kuwa haiwezi kufanywa kwa urahisi kwa sambamba.\n",
    "\n",
    "Kupitishwa kwa mbinu za uangalizi pamoja na kizuizi hiki kumesababisha kuundwa kwa mifano ya Transformer, ambayo sasa ni hali ya juu ya sanaa (State of the Art) tunayoijua na kuitumia leo, kutoka BERT hadi OpenGPT3.\n",
    "\n",
    "## Mifano ya Transformer\n",
    "\n",
    "Badala ya kupeleka muktadha wa kila utabiri wa awali kwenye hatua inayofuata ya tathmini, **mifano ya transformer** hutumia **usimbaji wa nafasi** na **uangalizi** ili kunasa muktadha wa pembejeo fulani ndani ya dirisha lililotolewa la maandishi. Picha hapa chini inaonyesha jinsi usimbaji wa nafasi na uangalizi vinavyoweza kunasa muktadha ndani ya dirisha lililotolewa.\n",
    "\n",
    "![GIF ya michoro inayoonyesha jinsi tathmini zinavyofanywa katika mifano ya transformer.](../../../../../lessons/5-NLP/18-Transformers/images/transformer-animated-explanation.gif) \n",
    "\n",
    "Kwa kuwa kila nafasi ya pembejeo inahusishwa kwa uhuru na kila nafasi ya matokeo, transformers zinaweza kufanya kazi kwa sambamba vizuri zaidi kuliko RNNs, jambo ambalo linawezesha mifano mikubwa zaidi na yenye lugha ya kuelezea zaidi. Kila kichwa cha uangalizi kinaweza kutumika kujifunza mahusiano tofauti kati ya maneno, jambo ambalo linaboresha kazi za Usindikaji wa Lugha Asilia.\n",
    "\n",
    "## Kujenga Mfano Rahisi wa Transformer\n",
    "\n",
    "Keras haina safu ya Transformer iliyojengwa ndani, lakini tunaweza kujenga yetu wenyewe. Kama ilivyokuwa awali, tutazingatia uainishaji wa maandishi wa seti ya data ya AG News, lakini ni muhimu kutaja kwamba mifano ya Transformer inaonyesha matokeo bora zaidi katika kazi ngumu zaidi za NLP.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "\n",
    "ds_train, ds_test = tfds.load('ag_news_subset').values()\n",
    "\n",
    "def extract_text(x):\n",
    "    return x['title']+' '+x['description']\n",
    "\n",
    "def tupelize(x):\n",
    "    return (extract_text(x),x['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tabaka mpya katika Keras yanapaswa kurithi darasa la `Layer`, na kutekeleza njia ya `call`. Hebu tuanze na tabaka la **Positional Embedding**. Tutatumia [baadhi ya msimbo kutoka kwa nyaraka rasmi za Keras](https://keras.io/examples/nlp/text_classification_with_transformer/). Tutadhania kwamba tunajaza mfuatano wote wa ingizo hadi urefu wa `maxlen`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(keras.layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = keras.layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "        self.maxlen = maxlen\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = self.maxlen\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x+positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tabaka hili lina tabaka mbili za `Embedding`: moja kwa kuweka tokeni (kwa njia tuliyojadili awali) na nyingine kwa nafasi za tokeni. Nafasi za tokeni zinaundwa kama mlolongo wa nambari za asili kutoka 0 hadi `maxlen` kwa kutumia `tf.range`, kisha hupitishwa kupitia tabaka ya embedding. Vekta mbili za embedding zinazotokana zinaongezwa, na kutoa uwakilishi wa pembejeo ulio na nafasi ya umbo `maxlen`$\\times$`embed_dim`.\n",
    "\n",
    "Sasa, hebu tekeleze kizuizi cha transformer. Kitachukua matokeo ya tabaka ya embedding iliyofafanuliwa hapo awali:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim, name='attn')\n",
    "        self.ffn = keras.Sequential(\n",
    "            [keras.layers.Dense(ff_dim, activation=\"relu\"), keras.layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = keras.layers.Dropout(rate)\n",
    "        self.dropout2 = keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sasa, tuko tayari kufafanua mfano kamili wa transformer: \n",
    "\n",
    "Transformer hutumia `MultiHeadAttention` kwa pembejeo iliyowekwa nafasi ili kuzalisha vector ya umakini yenye dimension `maxlen`$\\times$`embed_dim`, ambayo kisha huchanganywa na pembejeo na kusawazishwa kwa kutumia `LayerNormalization`.\n",
    "\n",
    "> **Note**: `LayerNormalization` ni sawa na `BatchNormalization` iliyojadiliwa katika sehemu ya *Computer Vision* ya njia hii ya kujifunza, lakini husawazisha matokeo ya safu ya awali kwa kila sampuli ya mafunzo kwa kujitegemea, ili kuyafikisha katika kiwango [-1..1].\n",
    "\n",
    "Matokeo ya safu hii kisha hupitishwa kupitia mtandao wa `Dense` (katika kesi yetu - perceptron ya safu mbili), na matokeo huongezwa kwenye matokeo ya mwisho (ambayo husawazishwa tena).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "text_vectorization (TextVect (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "token_and_position_embedding (None, 256, 32)           648192    \n",
      "_________________________________________________________________\n",
      "transformer_block (Transform (None, 256, 32)           10656     \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 20)                660       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 84        \n",
      "=================================================================\n",
      "Total params: 659,592\n",
      "Trainable params: 659,592\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 32  # Embedding size for each token\n",
    "num_heads = 2  # Number of attention heads\n",
    "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
    "maxlen = 256\n",
    "vocab_size = 20000\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.experimental.preprocessing.TextVectorization(max_tokens=vocab_size,output_sequence_length=maxlen, input_shape=(1,)),\n",
    "    TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim),\n",
    "    TransformerBlock(embed_dim, num_heads, ff_dim),\n",
    "    keras.layers.GlobalAveragePooling1D(),\n",
    "    keras.layers.Dropout(0.1),\n",
    "    keras.layers.Dense(20, activation=\"relu\"),\n",
    "    keras.layers.Dropout(0.1),\n",
    "    keras.layers.Dense(4, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokenizer\n",
      "938/938 [==============================] - 45s 39ms/step - loss: 0.4978 - acc: 0.8068 - val_loss: 0.2808 - val_acc: 0.9124\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9c2427a0d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Training tokenizer')\n",
    "model.layers[0].adapt(ds_train.map(extract_text))\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'], optimizer='adam')\n",
    "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT Transformer Models\n",
    "\n",
    "**BERT** (Bidirectional Encoder Representations from Transformers) ni mtandao mkubwa sana wa tabaka nyingi wa transformer wenye tabaka 12 kwa *BERT-base*, na 24 kwa *BERT-large*. Modeli hii huanza kwa kufundishwa awali kwenye mkusanyiko mkubwa wa data ya maandishi (WikiPedia + vitabu) kwa kutumia mafunzo yasiyo ya usimamizi (kutabiri maneno yaliyofichwa katika sentensi). Wakati wa mafunzo ya awali, modeli hujifunza kiwango kikubwa cha uelewa wa lugha ambacho kinaweza kutumika na seti nyingine za data kwa kutumia kurekebisha mafunzo. Mchakato huu unaitwa **transfer learning**.\n",
    "\n",
    "![picha kutoka http://jalammar.github.io/illustrated-bert/](../../../../../translated_images/sw/jalammarBERT-language-modeling-masked-lm.34f113ea5fec4362.webp)\n",
    "\n",
    "Kuna aina nyingi za usanifu wa Transformer ikiwa ni pamoja na BERT, DistilBERT, BigBird, OpenGPT3 na nyinginezo ambazo zinaweza kurekebishwa. \n",
    "\n",
    "Hebu tuone jinsi tunavyoweza kutumia modeli ya BERT iliyofundishwa awali kutatua tatizo letu la jadi la uainishaji wa mfululizo. Tutakopa wazo na baadhi ya msimbo kutoka [nyaraka rasmi](https://www.tensorflow.org/text/tutorials/classify_text_with_bert).\n",
    "\n",
    "Ili kupakia modeli zilizofundishwa awali, tutatumia **Tensorflow hub**. Kwanza, hebu tupakie vectorizer maalum ya BERT:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_41180/4216669875.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow_text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow_hub\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mhub\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhub\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mKerasLayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_text'"
     ]
    }
   ],
   "source": [
    "import tensorflow_text \n",
    "import tensorflow_hub as hub\n",
    "vectorizer = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_type_ids': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       " array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "       dtype=int32)>,\n",
       " 'input_word_ids': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       " array([[  101,  1045,  2293, 19081,   102,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]], dtype=int32)>,\n",
       " 'input_mask': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       " array([[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "       dtype=int32)>}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer(['I love transformers'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ni muhimu kutumia vectorizer sawa na ile ambayo mtandao wa awali ulifundishwa nayo. Pia, BERT vectorizer inarudisha vipengele vitatu:\n",
    "* `input_word_ids`, ambayo ni mlolongo wa namba za tokeni kwa sentensi ya ingizo\n",
    "* `input_mask`, inayoonyesha ni sehemu gani ya mlolongo ina ingizo halisi, na ni ipi ni padding. Hii ni sawa na mask inayozalishwa na safu ya `Masking`\n",
    "* `input_type_ids` hutumika kwa kazi za uundaji wa lugha, na inaruhusu kubainisha sentensi mbili za ingizo katika mlolongo mmoja.\n",
    "\n",
    "Kisha, tunaweza kuanzisha BERT feature extractor:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = hub.KerasLayer('https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pooled_output -> (1, 128)\n",
      "encoder_outputs -> 4\n",
      "sequence_output -> (1, 128, 128)\n",
      "default -> (1, 128)\n"
     ]
    }
   ],
   "source": [
    "z = bert(vectorizer(['I love transformers']))\n",
    "for i,x in z.items():\n",
    "    print(f\"{i} -> { len(x) if isinstance(x, list) else x.shape }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kwa hivyo, safu ya BERT inarudisha matokeo kadhaa muhimu:\n",
    "* `pooled_output` ni matokeo ya wastani wa alama zote katika mlolongo. Unaweza kuiona kama uwakilishi wa kijasusi wa maana ya mtandao mzima. Hii ni sawa na matokeo ya safu ya `GlobalAveragePooling1D` katika mfano wetu wa awali.\n",
    "* `sequence_output` ni matokeo ya safu ya mwisho ya transformer (inayolingana na matokeo ya `TransformerBlock` katika mfano wetu hapo juu).\n",
    "* `encoder_outputs` ni matokeo ya safu zote za transformer. Kwa kuwa tumepakia mfano wa BERT wenye safu 4 (kama unavyoweza kudhani kutoka kwa jina, ambalo lina `4_H`), ina tensors 4. Tensor ya mwisho ni sawa na `sequence_output`.\n",
    "\n",
    "Sasa tutaelezea mfano wa uainishaji wa mwisho hadi mwisho. Tutatumia *ufafanuzi wa mfano wa kiutendaji*, ambapo tunaelezea ingizo la mfano, kisha tunatoa mfululizo wa maelezo ili kuhesabu matokeo yake. Pia tutafanya uzito wa mfano wa BERT usiweze kufundishwa, na kufundisha tu uainishaji wa mwisho:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer (KerasLayer)        {'input_type_ids': ( 0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer_1 (KerasLayer)      {'pooled_output': (N 4782465     keras_layer[0][0]                \n",
      "                                                                 keras_layer[0][1]                \n",
      "                                                                 keras_layer[0][2]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 128)          0           keras_layer_1[0][5]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 4)            516         dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 4,782,981\n",
      "Trainable params: 516\n",
      "Non-trainable params: 4,782,465\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inp = keras.Input(shape=(),dtype=tf.string)\n",
    "x = vectorizer(inp)\n",
    "x = bert(x)\n",
    "x = keras.layers.Dropout(0.1)(x['pooled_output'])\n",
    "out = keras.layers.Dense(4,activation='softmax')(x)\n",
    "model = keras.models.Model(inp,out)\n",
    "bert.trainable = False\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 528s 559ms/step - loss: 0.8056 - acc: 0.6983 - val_loss: 0.5953 - val_acc: 0.7888\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9bb1e36d00>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'], optimizer='adam')\n",
    "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ingawa kuna vigezo vichache vinavyoweza kufundishwa, mchakato ni wa polepole sana kwa sababu kipengele cha uchimbaji cha BERT kinahitaji hesabu nyingi. Inaonekana hatukuweza kufikia usahihi wa kuridhisha, labda kutokana na ukosefu wa mafunzo au ukosefu wa vigezo vya modeli.\n",
    "\n",
    "Hebu jaribu kufungua uzito wa BERT na kuifundisha pia. Hii inahitaji kiwango kidogo sana cha kujifunza, na pia mkakati wa mafunzo wa uangalifu zaidi na **warmup**, kwa kutumia **AdamW** optimizer. Tutatumia kifurushi cha `tf-models-official` kuunda optimizer:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer (KerasLayer)        {'input_type_ids': ( 0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer_1 (KerasLayer)      {'pooled_output': (N 4782465     keras_layer[0][0]                \n",
      "                                                                 keras_layer[0][1]                \n",
      "                                                                 keras_layer[0][2]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 128)          0           keras_layer_1[0][5]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 4)            516         dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 4,782,981\n",
      "Trainable params: 4,782,980\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n",
      "938/938 [==============================] - 629s 664ms/step - loss: 0.6344 - acc: 0.7658 - val_loss: 0.4876 - val_acc: 0.8247\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9bb0bd0070>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from official.nlp import optimization \n",
    "bert.trainable=True\n",
    "model.summary()\n",
    "epochs = 3\n",
    "opt = optimization.create_optimizer(\n",
    "    init_lr=3e-5,\n",
    "    num_train_steps=epochs*len(ds_train),\n",
    "    num_warmup_steps=0.1*epochs*len(ds_train),\n",
    "    optimizer_type='adamw')\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'], optimizer=opt)\n",
    "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kama unavyoona, mafunzo yanaenda polepole sana - lakini unaweza kutaka kujaribu na kufundisha modeli kwa vipindi vichache (5-10) na uone kama unaweza kupata matokeo bora ukilinganisha na mbinu tulizotumia hapo awali.\n",
    "\n",
    "## Maktaba ya Huggingface Transformers\n",
    "\n",
    "Njia nyingine maarufu (na rahisi kidogo) ya kutumia modeli za Transformer ni [HuggingFace package](https://github.com/huggingface/), ambayo hutoa vipengele rahisi kwa kazi mbalimbali za NLP. Inapatikana kwa Tensorflow na PyTorch, mfumo mwingine maarufu wa mitandao ya neva.\n",
    "\n",
    "> **Note**: Ikiwa huna nia ya kuona jinsi maktaba ya Transformers inavyofanya kazi - unaweza kuruka hadi mwisho wa daftari hili, kwa sababu hautaona kitu tofauti sana na kile tulichofanya hapo juu. Tutakuwa tukirudia hatua zile zile za kufundisha modeli ya BERT kwa kutumia maktaba tofauti na modeli kubwa zaidi. Kwa hivyo, mchakato unahusisha mafunzo ya muda mrefu, hivyo unaweza kutaka tu kupitia msimbo.\n",
    "\n",
    "Hebu tuone jinsi tatizo letu linaweza kutatuliwa kwa kutumia [Huggingface Transformers](http://huggingface.co).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jambo la kwanza tunalohitaji kufanya ni kuchagua mfano ambao tutatumia. Mbali na baadhi ya mifano iliyojengwa ndani, Huggingface ina [hifadhi ya mifano mtandaoni](https://huggingface.co/models), ambapo unaweza kupata mifano mingi zaidi iliyofunzwa awali na jamii. Mifano yote hiyo inaweza kupakiwa na kutumika kwa kutoa tu jina la mfano. Faili zote za binary zinazohitajika kwa mfano huo zitapakuliwa moja kwa moja.\n",
    "\n",
    "Wakati fulani utahitaji kupakia mifano yako mwenyewe, ambapo unaweza kubainisha saraka inayojumuisha faili zote husika, ikiwa ni pamoja na vigezo vya tokenizer, faili `config.json` yenye vigezo vya mfano, uzito wa binary, n.k.\n",
    "\n",
    "Kutoka kwa jina la mfano, tunaweza kuanzisha mfano na tokenizer. Hebu tuanze na tokenizer:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "# To load the model from Internet repository using model name. \n",
    "# Use this if you are running from your own copy of the notebooks\n",
    "bert_model = 'bert-base-uncased' \n",
    "\n",
    "# To load the model from the directory on disk. Use this for Microsoft Learn module, because we have\n",
    "# prepared all required files for you.\n",
    "#bert_model = './bert'\n",
    "\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(bert_model)\n",
    "\n",
    "MAX_SEQ_LEN = 128\n",
    "PAD_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "UNK_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kipengele cha `tokenizer` kina kazi ya `encode` ambayo inaweza kutumika moja kwa moja kusimba maandishi:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 23435, 12314, 2003, 1037, 2307, 7705, 2005, 17953, 2361, 102]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('Tensorflow is a great framework for NLP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tunaweza pia kutumia tokenizer kusimba mlolongo kwa njia inayofaa kwa kupitisha kwa modeli, yaani, ikijumuisha `token_ids`, `input_mask` na mengineyo. Tunaweza pia kubainisha kwamba tunataka Tensorflow tensors kwa kutoa hoja ya `return_tensors='tf'`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[ 101, 7592, 1010, 2045,  102]], dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[0, 0, 0, 0, 0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[1, 1, 1, 1, 1]], dtype=int32)>}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(['Hello, there'],return_tensors='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Katika hali yetu, tutatumia mfano wa awali wa BERT unaoitwa `bert-base-uncased`. *Uncased* inaonyesha kuwa mfano huu haujali herufi kubwa au ndogo.\n",
    "\n",
    "Wakati wa kufundisha mfano, tunahitaji kutoa mlolongo uliogawanywa katika tokeni kama pembejeo, na kwa hivyo tutatengeneza mchakato wa usindikaji wa data. Kwa kuwa `tokenizer.encode` ni kazi ya Python, tutatumia mbinu ile ile kama katika kitengo cha mwisho kwa kuiita kwa kutumia `py_function`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(x):\n",
    "    return tokenizer.encode(x.numpy().decode('utf-8'),return_tensors='tf',padding='max_length',max_length=MAX_SEQ_LEN,truncation=True)[0]\n",
    "\n",
    "def process_fn(x):\n",
    "    s = x['title']+' '+x['description']\n",
    "    e = tf.py_function(process,inp=[s],Tout=(tf.int32))\n",
    "    e.set_shape(MAX_SEQ_LEN)\n",
    "    return e,x['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sasa tunaweza kupakia mfano halisi kwa kutumia kifurushi cha `BertForSequenceClassification`. Hii inahakikisha kwamba mfano wetu tayari una usanifu unaohitajika kwa ajili ya uainishaji, ikijumuisha uainishaji wa mwisho. Utaona ujumbe wa onyo ukisema kwamba uzito wa uainishaji wa mwisho haujaanzishwa, na mfano utahitaji mafunzo ya awali - hilo ni sawa kabisa, kwa sababu ndivyo hasa tunavyotarajia kufanya!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = transformers.TFBertForSequenceClassification.from_pretrained(bert_model,num_labels=4,output_attentions=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  109482240 \n",
      "_________________________________________________________________\n",
      "dropout_75 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  3076      \n",
      "=================================================================\n",
      "Total params: 109,485,316\n",
      "Trainable params: 109,485,316\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kama unavyoona kutoka kwa `summary()`, modeli ina karibu vigezo milioni 110! Inawezekana, ikiwa tunataka kazi rahisi ya uainishaji kwenye seti ndogo ya data, hatutaki kufundisha safu ya msingi ya BERT:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  109482240 \n",
      "_________________________________________________________________\n",
      "dropout_75 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  3076      \n",
      "=================================================================\n",
      "Total params: 109,485,316\n",
      "Trainable params: 3,076\n",
      "Non-trainable params: 109,482,240\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.layers[0].trainable = False\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sasa tuko tayari kuanza mafunzo!\n",
    "\n",
    "> **Note**: Kufundisha mfano wa BERT kwa kiwango kikubwa kunaweza kuchukua muda mwingi! Kwa hivyo, tutaufundisha tu kwa kundi la kwanza la 32. Hii ni kuonyesha tu jinsi mafunzo ya mfano yanavyowekwa. Ikiwa unavutiwa kujaribu mafunzo ya kiwango kikubwa - ondoa tu vigezo vya `steps_per_epoch` na `validation_steps`, na jiandae kusubiri!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 142s 4s/step - loss: 1.3896 - acc: 0.2500 - val_loss: 1.3863 - val_acc: 0.2480\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f1d40a4b6a0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile('adam','sparse_categorical_crossentropy',['acc'])\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "model.fit(ds_train.map(process_fn).batch(32),validation_data=ds_test.map(process_fn).batch(32),steps_per_epoch=32,validation_steps=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ikiwa utaongeza idadi ya marudio na kusubiri kwa muda wa kutosha, na kufundisha kwa vipindi kadhaa, unaweza kutarajia kwamba uainishaji wa BERT utatupa usahihi bora! Hii ni kwa sababu BERT tayari inaelewa vizuri muundo wa lugha, na tunachohitaji ni kurekebisha kidogo uainishaji wa mwisho. Hata hivyo, kwa sababu BERT ni mfano mkubwa, mchakato mzima wa mafunzo huchukua muda mrefu, na unahitaji nguvu kubwa ya kompyuta! (GPU, na ikiwezekana zaidi ya moja).\n",
    "\n",
    "> **Note:** Katika mfano wetu, tumekuwa tukitumia moja ya mifano midogo zaidi ya BERT iliyofundishwa awali. Kuna mifano mikubwa zaidi ambayo huenda ikatoa matokeo bora.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Muhimu\n",
    "\n",
    "Katika kipengele hiki, tumeangalia miundo ya hivi karibuni ya mifano inayotegemea **transformers**. Tumeitumia kwa kazi yetu ya uainishaji wa maandishi, lakini vivyo hivyo, mifano ya BERT inaweza kutumika kwa uchimbaji wa entiti, kujibu maswali, na kazi nyingine za NLP.\n",
    "\n",
    "Mifano ya transformer inawakilisha hali ya juu zaidi ya teknolojia ya sasa katika NLP, na mara nyingi inapaswa kuwa suluhisho la kwanza unaloanza kujaribu unapotekeleza suluhisho maalum za NLP. Hata hivyo, kuelewa kanuni za msingi za mitandao ya neva inayorudiwa zilizojadiliwa katika moduli hii ni muhimu sana ikiwa unataka kujenga mifano ya neva ya hali ya juu.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Kanusho**:  \nHati hii imetafsiriwa kwa kutumia huduma ya kutafsiri ya AI [Co-op Translator](https://github.com/Azure/co-op-translator). Ingawa tunajitahidi kuhakikisha usahihi, tafadhali fahamu kuwa tafsiri za kiotomatiki zinaweza kuwa na makosa au kutokuwa sahihi. Hati ya asili katika lugha yake ya awali inapaswa kuzingatiwa kama chanzo cha mamlaka. Kwa taarifa muhimu, tafsiri ya kitaalamu ya binadamu inapendekezwa. Hatutawajibika kwa kutoelewana au tafsiri zisizo sahihi zinazotokana na matumizi ya tafsiri hii.\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0cb620c6d4b9f7a635928804c26cf22403d89d98d79684e4529119355ee6d5a5"
  },
  "kernelspec": {
   "display_name": "py38_tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "coopTranslator": {
   "original_hash": "ab59c532409774988ab875f2260e8e53",
   "translation_date": "2025-08-29T16:03:25+00:00",
   "source_file": "lessons/5-NLP/18-Transformers/TransformersTF.ipynb",
   "language_code": "sw"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}