# Mekanismer f칬r uppm칛rksamhet och Transformatorer

## [F칬r-l칛rare quiz](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/118)

Ett av de mest betydelsefulla problemen inom NLP-omr친det 칛r **maskin칬vers칛ttning**, en grundl칛ggande uppgift som ligger till grund f칬r verktyg som Google Translate. I denna sektion kommer vi att fokusera p친 maskin칬vers칛ttning, eller mer generellt, p친 vilken *sekvens-till-sekvens* uppgift som helst (vilket ocks친 kallas **meningstransduktion**).

Med RNN:er implementeras sekvens-till-sekvens av tv친 친terkommande n칛tverk, d칛r ett n칛tverk, **kodaren**, komprimerar en ing친ngssekvens till ett dolt tillst친nd, medan ett annat n칛tverk, **avkodaren**, utvecklar detta dolda tillst친nd till ett 칬versatt resultat. Det finns ett par problem med denna metod:

* Det slutliga tillst친ndet f칬r kodarn칛tverket har sv친rt att komma ih친g b칬rjan av en mening, vilket leder till d친lig kvalitet p친 modellen f칬r l친nga meningar.
* Alla ord i en sekvens har samma inverkan p친 resultatet. I verkligheten har dock specifika ord i ing친ngssekvensen ofta mer inverkan p친 sekventiella utdata 칛n andra.

**Uppm칛rksamhetsmekanismer** ger ett s칛tt att vikta den kontextuella p친verkan av varje ing친ngsvektor p친 varje utdataf칬ruts칛gelse av RNN. S칛ttet det implementeras p친 칛r genom att skapa genv칛gar mellan mellanliggande tillst친nd av ing친ngs-RNN och utg친ngs-RNN. P친 detta s칛tt, n칛r vi genererar utdata symbol y<sub>t</sub>, kommer vi att ta h칛nsyn till alla ing친ngs dolda tillst친nd h<sub>i</sub>, med olika viktkoefficienter 풤<sub>t,i</sub>.

![Bild som visar en kodare/avkodare-modell med ett additivt uppm칛rksamhetslager](../../../../../translated_images/encoder-decoder-attention.7a726296894fb567aa2898c94b17b3289087f6705c11907df8301df9e5eeb3de.sw.png)

> Kodare-avkodare-modell med additiv uppm칛rksamhetsmekanism i [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf), citerad fr친n [denna bloggpost](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)

Uppm칛rksamhetsmatrisen {풤<sub>i,j</sub>} skulle representera graden av att vissa ing친ngsord spelar en roll i generationen av ett givet ord i utg친ngssekvensen. Nedan 칛r ett exempel p친 en s친dan matris:

![Bild som visar en exempeljustering som hittats av RNNsearch-50, tagen fr친n Bahdanau - arviz.org](../../../../../translated_images/bahdanau-fig3.09ba2d37f202a6af11de6c82d2d197830ba5f4528d9ea430eb65fd3a75065973.sw.png)

> Figur fr친n [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) (Fig.3)

Uppm칛rksamhetsmekanismer 칛r ansvariga f칬r mycket av den nuvarande eller n칛ra nuvarande toppmoderna inom NLP. Att l칛gga till uppm칛rksamhet 칬kar dock kraftigt antalet modellparametrar vilket ledde till skalningsproblem med RNN:er. En viktig begr칛nsning av att skala RNN:er 칛r att den 친terkommande naturen av modellerna g칬r det utmanande att batcha och parallellisera tr칛ning. I en RNN m친ste varje element i en sekvens bearbetas i sekventiell ordning, vilket inneb칛r att det inte kan parallelliseras enkelt.

![Kodare Avkodare med Uppm칛rksamhet](../../../../../lessons/5-NLP/18-Transformers/images/EncDecAttention.gif)

> Figur fr친n [Google's Blog](https://research.googleblog.com/2016/09/a-neural-network-for-machine.html)

Antagandet av uppm칛rksamhetsmekanismer kombinerat med denna begr칛nsning ledde till skapandet av de nuvarande toppmoderna transformatormodellerna som vi k칛nner och anv칛nder idag, s친som BERT till Open-GPT3.

## Transformatormodeller

En av de huvudsakliga id칠erna bakom transformatorer 칛r att undvika den sekventiella naturen av RNN:er och att skapa en modell som 칛r parallelliserbar under tr칛ning. Detta uppn친s genom att implementera tv친 id칠er:

* positionskodning
* anv칛nda sj칛lvuppm칛rksamhetsmekanism f칬r att f친nga m칬nster ist칛llet f칬r RNN:er (eller CNN:er) (det 칛r d칛rf칬r artikeln som introducerar transformatorer kallas *[Attention is all you need](https://arxiv.org/abs/1706.03762)*)

### Positionskodning/Embedding

Id칠n med positionskodning 칛r f칬ljande. 
1. N칛r man anv칛nder RNN:er representeras den relativa positionen av token av antalet steg, och beh칬ver d칛rf칬r inte uttryckligen representeras. 
2. Men n칛r vi v칛xlar till uppm칛rksamhet, beh칬ver vi veta de relativa positionerna f칬r token inom en sekvens. 
3. F칬r att f친 positionskodning, kompletterar vi v친r sekvens av token med en sekvens av tokenpositioner i sekvensen (dvs. en sekvens av siffror 0,1, ...).
4. Vi blandar sedan tokenpositionen med en tokeninb칛ddningsvektor. F칬r att omvandla positionen (heltal) till en vektor kan vi anv칛nda olika tillv칛gag친ngss칛tt:

* Tr칛ningsbar inb칛ddning, liknande tokeninb칛ddning. Detta 칛r den metod vi 칬verv칛ger h칛r. Vi till칛mpar inb칛ddningslager ovanp친 b친de token och deras positioner, vilket resulterar i inb칛ddningsvektorer av samma dimensioner, som vi sedan l칛gger ihop.
* Fast positionskodningsfunktion, som f칬reslagits i den ursprungliga artikeln.

<img src="images/pos-embedding.png" width="50%"/>

> Bild av f칬rfattaren

Resultatet vi f친r med positionsinb칛ddning inb칛ddas b친de den ursprungliga token och dess position inom en sekvens.

### Multi-Head Sj칛lv-Uppm칛rksamhet

N칛sta steg 칛r att f친nga vissa m칬nster inom v친r sekvens. F칬r att g칬ra detta anv칛nder transformatorer en **sj칛lvuppm칛rksamhets**mekanism, som i grunden 칛r uppm칛rksamhet till칛mpad p친 samma sekvens som ing친ng och utg친ng. Till칛mpning av sj칛lvuppm칛rksamhet g칬r att vi kan ta h칛nsyn till **kontext** inom meningen och se vilka ord som 칛r relaterade. Till exempel g칬r det att vi kan se vilka ord som h칛nvisas till av referenser, s친som *det*, och 칛ven ta kontexten i beaktande:

![](../../../../../translated_images/CoreferenceResolution.861924d6d384a7d68d8d0039d06a71a151f18a796b8b1330239d3590bd4947eb.sw.png)

> Bild fr친n [Google Blog](https://research.googleblog.com/2017/08/transformer-novel-neural-network.html)

I transformatorer anv칛nder vi **Multi-Head Attention** f칬r att ge n칛tverket kraften att f친nga flera olika typer av beroenden, t.ex. l친ngsiktiga vs. kortsiktiga ordf칬rh친llanden, medreferens vs. n친got annat, osv.

[TensorFlow Notebook](../../../../../lessons/5-NLP/18-Transformers/TransformersTF.ipynb) inneh친ller mer information om implementeringen av transformatorlager.

### Kodare-Avkodare Uppm칛rksamhet

I transformatorer anv칛nds uppm칛rksamhet p친 tv친 st칛llen:

* F칬r att f친nga m칬nster inom ing친ngstexten med hj칛lp av sj칛lvuppm칛rksamhet
* F칬r att utf칬ra sekvens칬vers칛ttning - det 칛r uppm칛rksamhetslagret mellan kodaren och avkodaren.

Kodare-avkodare uppm칛rksamhet 칛r mycket lik den uppm칛rksamhetsmekanism som anv칛nds i RNN:er, som beskrivits i b칬rjan av denna sektion. Detta animerade diagram f칬rklarar rollen av kodare-avkodare uppm칛rksamhet.

![Animerad GIF som visar hur utv칛rderingarna utf칬rs i transformatormodeller.](../../../../../lessons/5-NLP/18-Transformers/images/transformer-animated-explanation.gif)

Eftersom varje ing친ngsposition mappas oberoende till varje utg친ngsposition kan transformatorer parallellisera b칛ttre 칛n RNN:er, vilket m칬jligg칬r mycket st칬rre och mer uttrycksfulla spr친kmodeller. Varje uppm칛rksamhetshuvud kan anv칛ndas f칬r att l칛ra sig olika relationer mellan ord som f칬rb칛ttrar efterf칬ljande NLP-uppgifter.

## BERT

**BERT** (Bidirectional Encoder Representations from Transformers) 칛r ett mycket stort flerlagers transformatorn칛tverk med 12 lager f칬r *BERT-base*, och 24 f칬r *BERT-large*. Modellen f칬rtr칛nas f칬rst p친 en stor korpus av textdata (WikiPedia + b칬cker) med hj칛lp av osupervised tr칛ning (f칬ruts칛ga maskerade ord i en mening). Under f칬rtr칛ningen absorberar modellen betydande niv친er av spr친kf칬rst친else som sedan kan utnyttjas med andra dataset genom finjustering. Denna process kallas **칬verf칬ringsinl칛rning**.

![Bild fr친n http://jalammar.github.io/illustrated-bert/](../../../../../translated_images/jalammarBERT-language-modeling-masked-lm.34f113ea5fec4362e39ee4381aab7cad06b5465a0b5f053a0f2aa05fbe14e746.sw.png)

> Bild [k칛lla](http://jalammar.github.io/illustrated-bert/)

## 九꽲잺 칐vningar: Transformatorer

Forts칛tt din inl칛rning i f칬ljande anteckningsb칬cker:

* [Transformatorer i PyTorch](../../../../../lessons/5-NLP/18-Transformers/TransformersPyTorch.ipynb)
* [Transformatorer i TensorFlow](../../../../../lessons/5-NLP/18-Transformers/TransformersTF.ipynb)

## Slutsats

I denna lektion l칛rde du dig om transformatorer och uppm칛rksamhetsmekanismer, alla viktiga verktyg i NLP-verktygsl친dan. Det finns m친nga varianter av transformatorarkitekturer inklusive BERT, DistilBERT, BigBird, OpenGPT3 och mer som kan finjusteras. [HuggingFace-paketet](https://github.com/huggingface/) tillhandah친ller ett f칬rr친d f칬r tr칛ning av m친nga av dessa arkitekturer med b친de PyTorch och TensorFlow.

## 游 Utmaning

## [Efter-l칛rare quiz](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/218)

## Granskning & Sj칛lvstudie

* [Bloggpost](https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/), som f칬rklarar den klassiska [Attention is all you need](https://arxiv.org/abs/1706.03762) artikeln om transformatorer.
* [En serie bloggposter](https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452) om transformatorer, som f칬rklarar arkitekturen i detalj.

## [Uppgift](assignment.md)

**Ansvarsfriskrivning**:  
Detta dokument har 칬versatts med hj칛lp av maskinbaserade AI-칬vers칛ttningstj칛nster. 츿ven om vi str칛var efter noggrannhet, v칛nligen var medveten om att automatiska 칬vers칛ttningar kan inneh친lla fel eller inkonsekvenser. Det ursprungliga dokumentet p친 sitt modersm친l b칬r betraktas som den auktoritativa k칛llan. F칬r kritisk information rekommenderas professionell m칛nsklig 칬vers칛ttning. Vi ansvarar inte f칬r n친gra missf칬rst친nd eller feltolkningar som uppst친r till f칬ljd av anv칛ndningen av denna 칬vers칛ttning.