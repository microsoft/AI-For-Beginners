# Att Representera Text som Tensors

## [F칬r-lektionens quiz](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/113)

## Textklassificering

Under den f칬rsta delen av denna sektion kommer vi att fokusera p친 uppgiften **textklassificering**. Vi kommer att anv칛nda [AG News](https://www.kaggle.com/amananandrai/ag-news-classification-dataset) dataset, som inneh친ller nyhetsartiklar som f칬ljande:

* Kategori: Sci/Tech
* Titel: Ky. F칬retag Vinner Bidrag f칬r att Studera Peptider (AP)
* Inneh친ll: AP - Ett f칬retag grundat av en kemiforskare vid University of Louisville vann ett bidrag f칬r att utveckla...

V친rt m친l kommer att vara att klassificera nyhetsartikeln i en av kategorierna baserat p친 texten.

## Representera text

Om vi vill l칬sa uppgifter inom Natural Language Processing (NLP) med neurala n칛tverk, beh칬ver vi ett s칛tt att representera text som tensors. Datorer representerar redan texttecken som siffror som mappas till typsnitt p친 din sk칛rm med hj칛lp av kodningar som ASCII eller UTF-8.

<img alt="Bild som visar diagram som mappar ett tecken till en ASCII- och bin칛r representation" src="images/ascii-character-map.png" width="50%"/>

> [Bildk칛lla](https://www.seobility.net/en/wiki/ASCII)

Som m칛nniskor f칬rst친r vi vad varje bokstav **representerar**, och hur alla tecken samverkar f칬r att bilda orden i en mening. Datorer har dock inte en s친dan f칬rst친else av sig sj칛lva, och det neurala n칛tverket m친ste l칛ra sig betydelsen under tr칛ningen.

D칛rf칬r kan vi anv칛nda olika metoder n칛r vi representerar text:

* **Tecken-niv친 representation**, n칛r vi representerar text genom att behandla varje tecken som en siffra. Givet att vi har *C* olika tecken i v친r textkorpus, skulle ordet *Hello* representeras av en 5x*C* tensor. Varje bokstav skulle motsvara en tensor kolumn i one-hot encoding.
* **Ord-niv친 representation**, d칛r vi skapar ett **ordf칬rr친d** av alla ord i v친r text, och sedan representerar orden med one-hot encoding. Denna metod 칛r p친 n친got s칛tt b칛ttre, eftersom varje bokstav i sig sj칛lv inte har mycket mening, och genom att anv칛nda h칬gre semantiska begrepp - ord - f칬renklar vi uppgiften f칬r det neurala n칛tverket. Men givet den stora storleken p친 ordf칬rr친det m친ste vi hantera h칬gdimensionella glesa tensors.

Oavsett representationen m친ste vi f칬rst konvertera texten till en sekvens av **tokens**, d칛r en token kan vara antingen ett tecken, ett ord eller ibland till och med en del av ett ord. Sedan konverterar vi token till en siffra, vanligtvis med hj칛lp av **ordf칬rr친d**, och denna siffra kan matas in i ett neuralt n칛tverk med one-hot encoding.

## N-Grams

I naturligt spr친k kan den exakta betydelsen av ord endast best칛mmas i kontext. Till exempel 칛r betydelserna av *neural network* och *fishing network* helt olika. Ett av s칛tten att ta h칛nsyn till detta 칛r att bygga v친r modell p친 ordpar och betrakta ordpar som separata ordf칬rr친dstokens. P친 s친 s칛tt kommer meningen *I like to go fishing* att representeras av f칬ljande sekvens av tokens: *I like*, *like to*, *to go*, *go fishing*. Problemet med detta tillv칛gag친ngss칛tt 칛r att ordf칬rr친dets storlek v칛xer betydligt, och kombinationer som *go fishing* och *go shopping* representeras av olika tokens, som inte delar n친gon semantisk likhet trots att de har samma verb.

I vissa fall kan vi 칬verv칛ga att anv칛nda tri-grams -- kombinationer av tre ord -- ocks친. S친ledes kallas denna metod ofta f칬r **n-grams**. Det 칛r ocks친 meningsfullt att anv칛nda n-grams med tecken-niv친 representation, i vilket fall n-grams ungef칛r motsvarar olika stavelser.

## Bag-of-Words och TF/IDF

N칛r vi l칬ser uppgifter som textklassificering, beh칬ver vi kunna representera text med en fast storlek vektor, som vi kommer att anv칛nda som ing친ng till den slutliga t칛ta klassificeraren. Ett av de enklaste s칛tten att g칬ra detta 칛r att kombinera alla individuella ordrepresentationer, t.ex. genom att addera dem. Om vi l칛gger till one-hot encodings av varje ord, kommer vi att f친 en frekvensvektor som visar hur m친nga g친nger varje ord f칬rekommer i texten. En s친dan representation av text kallas **bag of words** (BoW).

<img src="images/bow.png" width="90%"/>

> Bild av f칬rfattaren

En BoW representerar i grunden vilka ord som f칬rekommer i texten och i vilka m칛ngder, vilket faktiskt kan vara en bra indikation p친 vad texten handlar om. Till exempel 칛r en nyhetsartikel om politik sannolikt att inneh친lla ord som *president* och *land*, medan en vetenskaplig publikation skulle ha n친got som *collider*, *discovered*, etc. S친ledes kan ordens frekvenser i m친nga fall vara en bra indikator p친 textens inneh친ll.

Problemet med BoW 칛r att vissa vanliga ord, som *and*, *is*, etc. f칬rekommer i de flesta texter och har de h칬gsta frekvenserna, vilket d칬ljer de ord som verkligen 칛r viktiga. Vi kan s칛nka vikten av dessa ord genom att ta h칛nsyn till frekvensen med vilken ord f칬rekommer i hela dokumentkollektionen. Detta 칛r den huvudsakliga id칠n bakom TF/IDF-metoden, som t칛cks mer detaljerat i anteckningarna kopplade till denna lektion.

Men ingen av dessa metoder kan helt ta h칛nsyn till **semantiken** i texten. Vi beh칬ver mer kraftfulla modeller av neurala n칛tverk f칬r att g칬ra detta, vilket vi kommer att diskutera senare i denna sektion.

## 九꽲잺 칐vningar: Textrepresentation

Forts칛tt din inl칛rning i f칬ljande anteckningar:

* [Textrepresentation med PyTorch](../../../../../lessons/5-NLP/13-TextRep/TextRepresentationPyTorch.ipynb)
* [Textrepresentation med TensorFlow](../../../../../lessons/5-NLP/13-TextRep/TextRepresentationTF.ipynb)

## Slutsats

Hittills har vi studerat tekniker som kan tilldela frekvensvikt till olika ord. De kan dock inte representera mening eller ordning. Som den ber칬mda lingvisten J. R. Firth sa 1935, "Den fullst칛ndiga betydelsen av ett ord 칛r alltid kontextuell, och ingen studie av betydelse bortom kontext kan tas p친 allvar." Vi kommer senare i kursen att l칛ra oss hur man f친ngar kontextuell information fr친n text genom spr친kmodellering.

## 游 Utmaning

F칬rs칬k med n친gra andra 칬vningar som anv칛nder bag-of-words och olika datamodeller. Du kanske inspireras av denna [t칛vling p친 Kaggle](https://www.kaggle.com/competitions/word2vec-nlp-tutorial/overview/part-1-for-beginners-bag-of-words)

## [Efter-lektionens quiz](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/213)

## Granskning & Sj칛lvstudie

칐va dina f칛rdigheter med textembedding och bag-of-words tekniker p친 [Microsoft Learn](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-pytorch/?WT.mc_id=academic-77998-cacaste)

## [Uppgift: Anteckningar](assignment.md)

**Ansvarsfriskrivning**:  
Detta dokument har 칬versatts med hj칛lp av maskinbaserade AI-칬vers칛ttningstj칛nster. 츿ven om vi str칛var efter noggrannhet, var medveten om att automatiska 칬vers칛ttningar kan inneh친lla fel eller brister. Det ursprungliga dokumentet p친 sitt modersm친l b칬r betraktas som den auktoritativa k칛llan. F칬r kritisk information rekommenderas professionell m칛nsklig 칬vers칛ttning. Vi ansvarar inte f칬r n친gra missf칬rst친nd eller feltolkningar som uppst친r till f칬ljd av anv칛ndningen av denna 칬vers칛ttning.