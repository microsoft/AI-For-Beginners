# تدريب نموذج Skip-Gram

مهمة عملية من [منهج الذكاء الاصطناعي للمبتدئين](https://github.com/microsoft/ai-for-beginners).

## المهمة

في هذه المهمة، نتحداك لتدريب نموذج Word2Vec باستخدام تقنية Skip-Gram. قم بتدريب شبكة تحتوي على تضمين للتنبؤ بالكلمات المجاورة في نافذة Skip-Gram بعرض $N$-رموز. يمكنك استخدام [الكود من هذا الدرس](../../../../../../lessons/5-NLP/15-LanguageModeling/CBoW-TF.ipynb)، مع إجراء تعديلات بسيطة عليه.

## مجموعة البيانات

يمكنك استخدام أي كتاب ترغب فيه. يمكنك العثور على العديد من النصوص المجانية في [مشروع غوتنبرغ](https://www.gutenberg.org/)، على سبيل المثال، هنا رابط مباشر إلى [مغامرات أليس في بلاد العجائب](https://www.gutenberg.org/files/11/11-0.txt) للكاتب لويس كارول. أو يمكنك استخدام مسرحيات شكسبير، والتي يمكنك الحصول عليها باستخدام الكود التالي:

```python
path_to_file = tf.keras.utils.get_file(
   'shakespeare.txt', 
   'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')
text = open(path_to_file, 'rb').read().decode(encoding='utf-8')
```

## استكشاف!

إذا كان لديك وقت وترغب في التعمق أكثر في الموضوع، حاول استكشاف عدة أمور:

* كيف يؤثر حجم التضمين على النتائج؟
* كيف تؤثر أنماط النصوص المختلفة على النتائج؟
* اختر عدة أنواع مختلفة جدًا من الكلمات ومرادفاتها، واحصل على تمثيلها في شكل متجهات، ثم قم بتطبيق PCA لتقليل الأبعاد إلى 2، وقم برسمها في مساحة ثنائية الأبعاد. هل ترى أي أنماط؟

**إخلاء المسؤولية**:  
تم ترجمة هذا المستند باستخدام خدمة الترجمة بالذكاء الاصطناعي [Co-op Translator](https://github.com/Azure/co-op-translator). بينما نسعى لتحقيق الدقة، يرجى العلم أن الترجمات الآلية قد تحتوي على أخطاء أو معلومات غير دقيقة. يجب اعتبار المستند الأصلي بلغته الأصلية هو المصدر الموثوق. للحصول على معلومات حاسمة، يُوصى بالاستعانة بترجمة بشرية احترافية. نحن غير مسؤولين عن أي سوء فهم أو تفسيرات خاطئة ناتجة عن استخدام هذه الترجمة.