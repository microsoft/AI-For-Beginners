{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# آليات الانتباه والنماذج المحولة\n",
    "\n",
    "أحد العيوب الرئيسية للشبكات العودية هو أن جميع الكلمات في تسلسل معين لها نفس التأثير على النتيجة. يؤدي هذا إلى أداء غير مثالي مع نماذج LSTM القياسية للترميز وفك الترميز في مهام تحويل التسلسل إلى تسلسل، مثل التعرف على الكيانات المسماة والترجمة الآلية. في الواقع، غالبًا ما يكون لبعض الكلمات في تسلسل الإدخال تأثير أكبر على المخرجات المتسلسلة مقارنة بغيرها.\n",
    "\n",
    "لنأخذ نموذج تحويل التسلسل إلى تسلسل، مثل الترجمة الآلية. يتم تنفيذه باستخدام شبكتين عوديتين، حيث تقوم شبكة واحدة (**المشفّر**) بضغط تسلسل الإدخال إلى حالة مخفية، وتقوم الأخرى، **المفكّك**، بفك هذه الحالة المخفية إلى النتيجة المترجمة. المشكلة في هذا النهج هي أن الحالة النهائية للشبكة ستواجه صعوبة في تذكر بداية الجملة، مما يؤدي إلى ضعف جودة النموذج في الجمل الطويلة.\n",
    "\n",
    "**آليات الانتباه** توفر وسيلة لوزن التأثير السياقي لكل متجه إدخال على كل توقع إخراج للشبكة العودية. يتم تنفيذ ذلك من خلال إنشاء اختصارات بين الحالات الوسيطة لشبكة الإدخال العودية وشبكة الإخراج العودية. بهذه الطريقة، عند توليد رمز الإخراج $y_t$، سنأخذ في الاعتبار جميع الحالات المخفية للإدخال $h_i$، مع معاملات وزن مختلفة $\\alpha_{t,i}$.\n",
    "\n",
    "![صورة توضح نموذج الترميز/فك الترميز مع طبقة انتباه إضافية](../../../../../translated_images/ar/encoder-decoder-attention.7a726296894fb567.webp)\n",
    "*نموذج الترميز-فك الترميز مع آلية الانتباه الإضافية في [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf)، مقتبس من [هذا المقال](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)*\n",
    "\n",
    "مصفوفة الانتباه $\\{\\alpha_{i,j}\\}$ تمثل الدرجة التي تلعب بها كلمات الإدخال دورًا في توليد كلمة معينة في تسلسل الإخراج. أدناه مثال على مثل هذه المصفوفة:\n",
    "\n",
    "![صورة توضح محاذاة نموذج RNNsearch-50، مأخوذة من Bahdanau - arviz.org](../../../../../translated_images/ar/bahdanau-fig3.09ba2d37f202a6af.webp)\n",
    "\n",
    "*الصورة مأخوذة من [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) (الشكل 3)*\n",
    "\n",
    "آليات الانتباه مسؤولة عن الكثير من التقدم الحالي أو القريب من الحالي في معالجة اللغة الطبيعية. ومع ذلك، فإن إضافة الانتباه تزيد بشكل كبير من عدد معلمات النموذج، مما أدى إلى مشكلات في التوسع مع الشبكات العودية. أحد القيود الرئيسية لتوسيع الشبكات العودية هو أن الطبيعة العودية للنماذج تجعل من الصعب تجميع وتوازي التدريب. في الشبكات العودية، يجب معالجة كل عنصر من عناصر التسلسل بترتيب متسلسل، مما يعني أنه لا يمكن توازيته بسهولة.\n",
    "\n",
    "اعتماد آليات الانتباه مع هذا القيد أدى إلى إنشاء نماذج المحولات التي تمثل الآن أحدث ما توصلت إليه التكنولوجيا، والتي نعرفها ونستخدمها اليوم مثل BERT وOpenGPT3.\n",
    "\n",
    "## نماذج المحولات\n",
    "\n",
    "بدلاً من تمرير سياق كل توقع سابق إلى خطوة التقييم التالية، تستخدم **نماذج المحولات** **الترميزات الموضعية** والانتباه لالتقاط سياق الإدخال ضمن نافذة نصية محددة. الصورة أدناه توضح كيف يمكن للترميزات الموضعية مع الانتباه التقاط السياق ضمن نافذة معينة.\n",
    "\n",
    "![صورة متحركة توضح كيفية إجراء التقييمات في نماذج المحولات.](../../../../../lessons/5-NLP/18-Transformers/images/transformer-animated-explanation.gif)\n",
    "\n",
    "نظرًا لأن كل موضع إدخال يتم تعيينه بشكل مستقل إلى كل موضع إخراج، يمكن للمحولات التوازي بشكل أفضل من الشبكات العودية، مما يتيح نماذج لغوية أكبر وأكثر تعبيرًا. يمكن لكل رأس انتباه أن يُستخدم لتعلم علاقات مختلفة بين الكلمات، مما يحسن مهام معالجة اللغة الطبيعية.\n",
    "\n",
    "**BERT** (تمثيلات الترميز ثنائية الاتجاه من المحولات) هو شبكة محولات متعددة الطبقات كبيرة جدًا تحتوي على 12 طبقة في *BERT-base*، و24 طبقة في *BERT-large*. يتم تدريب النموذج أولاً على مجموعة نصوص كبيرة (ويكيبيديا + كتب) باستخدام تدريب غير خاضع للإشراف (توقع الكلمات المحجوبة في الجملة). أثناء التدريب الأولي، يكتسب النموذج مستوى كبيرًا من فهم اللغة يمكن الاستفادة منه لاحقًا مع مجموعات بيانات أخرى باستخدام التخصيص الدقيق. تُعرف هذه العملية باسم **التعلم بالنقل**.\n",
    "\n",
    "![صورة من http://jalammar.github.io/illustrated-bert/](../../../../../translated_images/ar/jalammarBERT-language-modeling-masked-lm.34f113ea5fec4362.webp)\n",
    "\n",
    "هناك العديد من التعديلات على بنية المحولات، بما في ذلك BERT، وDistilBERT، وBigBird، وOpenGPT3، والمزيد، التي يمكن تخصيصها. حزمة [HuggingFace](https://github.com/huggingface/) توفر مستودعًا لتدريب العديد من هذه البنى باستخدام PyTorch.\n",
    "\n",
    "## استخدام BERT لتصنيف النصوص\n",
    "\n",
    "دعونا نرى كيف يمكننا استخدام نموذج BERT المدرب مسبقًا لحل مهمتنا التقليدية: تصنيف التسلسل. سنقوم بتصنيف مجموعة بيانات AG News الأصلية.\n",
    "\n",
    "أولاً، دعونا نحمل مكتبة HuggingFace ومجموعة البيانات الخاصة بنا:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Building vocab...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "from torchnlp import *\n",
    "import transformers\n",
    "train_dataset, test_dataset, classes, vocab = load_dataset()\n",
    "vocab_len = len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "نظرًا لأننا سنستخدم نموذج BERT المدرب مسبقًا، سنحتاج إلى استخدام مُجزئ محدد. أولاً، سنقوم بتحميل مُجزئ مرتبط بنموذج BERT المدرب مسبقًا.\n",
    "\n",
    "تحتوي مكتبة HuggingFace على مستودع للنماذج المدربة مسبقًا، والتي يمكنك استخدامها فقط عن طريق تحديد أسمائها كوسائط في دوال `from_pretrained`. سيتم تنزيل جميع الملفات الثنائية المطلوبة للنموذج تلقائيًا.\n",
    "\n",
    "ومع ذلك، في بعض الأحيان قد تحتاج إلى تحميل نماذجك الخاصة، وفي هذه الحالة يمكنك تحديد الدليل الذي يحتوي على جميع الملفات ذات الصلة، بما في ذلك معلمات المُجزئ، ملف `config.json` الذي يحتوي على معلمات النموذج، الأوزان الثنائية، وما إلى ذلك.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load the model from Internet repository using model name. \n",
    "# Use this if you are running from your own copy of the notebooks\n",
    "bert_model = 'bert-base-uncased' \n",
    "\n",
    "# To load the model from the directory on disk. Use this for Microsoft Learn module, because we have\n",
    "# prepared all required files for you.\n",
    "bert_model = './bert'\n",
    "\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(bert_model)\n",
    "\n",
    "MAX_SEQ_LEN = 128\n",
    "PAD_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "UNK_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "يحتوي كائن `tokenizer` على وظيفة `encode` التي يمكن استخدامها مباشرةً لترميز النص:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 1052, 22123, 2953, 2818, 2003, 1037, 2307, 7705, 2005, 17953, 2361, 102]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('PyTorch is a great framework for NLP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ثم، دعونا ننشئ المكررات التي سنستخدمها أثناء التدريب للوصول إلى البيانات. نظرًا لأن BERT يستخدم وظيفة الترميز الخاصة به، سنحتاج إلى تعريف وظيفة تعبئة مشابهة لـ `padify` التي قمنا بتعريفها سابقًا:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_bert(b):\n",
    "    # b is the list of tuples of length batch_size\n",
    "    #   - first element of a tuple = label, \n",
    "    #   - second = feature (text sequence)\n",
    "    # build vectorized sequence\n",
    "    v = [tokenizer.encode(x[1]) for x in b]\n",
    "    # compute max length of a sequence in this minibatch\n",
    "    l = max(map(len,v))\n",
    "    return ( # tuple of two tensors - labels and features\n",
    "        torch.LongTensor([t[0] for t in b]),\n",
    "        torch.stack([torch.nn.functional.pad(torch.tensor(t),(0,l-len(t)),mode='constant',value=0) for t in v])\n",
    "    )\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=8, collate_fn=pad_bert, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=8, collate_fn=pad_bert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "في حالتنا، سنستخدم نموذج BERT المدرب مسبقًا يسمى `bert-base-uncased`. دعنا نقوم بتحميل النموذج باستخدام حزمة `BertForSequenceClassfication`. هذا يضمن أن النموذج لدينا يحتوي بالفعل على البنية المطلوبة للتصنيف، بما في ذلك المصنف النهائي. سترى رسالة تحذير تشير إلى أن أوزان المصنف النهائي غير مهيأة، وأن النموذج سيحتاج إلى تدريب مسبق - وهذا أمر طبيعي تمامًا، لأنه بالضبط ما نحن على وشك القيام به!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./bert were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./bert and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = transformers.BertForSequenceClassification.from_pretrained(bert_model,num_labels=4).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "الآن نحن مستعدون لبدء التدريب! نظرًا لأن BERT مُدرّب مسبقًا، نرغب في البدء بمعدل تعلم صغير جدًا حتى لا نتسبب في تدمير الأوزان الأولية.\n",
    "\n",
    "كل العمل الشاق يتم بواسطة نموذج `BertForSequenceClassification`. عندما نستدعي النموذج على بيانات التدريب، فإنه يعيد كلًا من الخسارة ومخرجات الشبكة للدفعة المصغرة المدخلة. نستخدم الخسارة لتحسين المعاملات (`loss.backward()` يقوم بالتمرير العكسي)، و`out` لحساب دقة التدريب من خلال مقارنة التصنيفات المحصلة `labs` (المحسوبة باستخدام `argmax`) مع التصنيفات المتوقعة `labels`.\n",
    "\n",
    "للتحكم في العملية، نقوم بتجميع الخسارة والدقة على مدى عدة تكرارات، ونعرضها كل `report_freq` دورة تدريبية.\n",
    "\n",
    "من المحتمل أن يستغرق هذا التدريب وقتًا طويلًا، لذا نقوم بتحديد عدد التكرارات.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 1.1254194641113282, Accuracy = 0.585\n",
      "Loss = 0.6194715118408203, Accuracy = 0.83\n",
      "Loss = 0.46665248870849607, Accuracy = 0.8475\n",
      "Loss = 0.4309701919555664, Accuracy = 0.8575\n",
      "Loss = 0.35427074432373046, Accuracy = 0.8825\n",
      "Loss = 0.3306886291503906, Accuracy = 0.8975\n",
      "Loss = 0.30340143203735354, Accuracy = 0.8975\n",
      "Loss = 0.26139299392700194, Accuracy = 0.915\n",
      "Loss = 0.26708646774291994, Accuracy = 0.9225\n",
      "Loss = 0.3667240524291992, Accuracy = 0.8675\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
    "\n",
    "report_freq = 50\n",
    "iterations = 500 # make this larger to train for longer time!\n",
    "\n",
    "model.train()\n",
    "\n",
    "i,c = 0,0\n",
    "acc_loss = 0\n",
    "acc_acc = 0\n",
    "\n",
    "for labels,texts in train_loader:\n",
    "    labels = labels.to(device)-1 # get labels in the range 0-3         \n",
    "    texts = texts.to(device)\n",
    "    loss, out = model(texts, labels=labels)[:2]\n",
    "    labs = out.argmax(dim=1)\n",
    "    acc = torch.mean((labs==labels).type(torch.float32))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    acc_loss += loss\n",
    "    acc_acc += acc\n",
    "    i+=1\n",
    "    c+=1\n",
    "    if i%report_freq==0:\n",
    "        print(f\"Loss = {acc_loss.item()/c}, Accuracy = {acc_acc.item()/c}\")\n",
    "        c = 0\n",
    "        acc_loss = 0\n",
    "        acc_acc = 0\n",
    "    iterations-=1\n",
    "    if not iterations:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "يمكنك أن تلاحظ (خصوصًا إذا زدت عدد التكرارات وانتظرت لفترة كافية) أن تصنيف BERT يمنحنا دقة جيدة جدًا! يعود ذلك إلى أن BERT يفهم بالفعل بنية اللغة بشكل جيد، وكل ما نحتاجه هو ضبط المصنف النهائي. ومع ذلك، نظرًا لأن BERT نموذج كبير، فإن عملية التدريب بأكملها تستغرق وقتًا طويلًا وتتطلب قوة حوسبة كبيرة! (وحدة معالجة رسومات GPU، ويفضل أن تكون أكثر من واحدة).\n",
    "\n",
    "> **ملاحظة:** في مثالنا، كنا نستخدم أحد أصغر نماذج BERT المدربة مسبقًا. هناك نماذج أكبر من المحتمل أن تحقق نتائج أفضل.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## تقييم أداء النموذج\n",
    "\n",
    "الآن يمكننا تقييم أداء النموذج على مجموعة البيانات الخاصة بالاختبار. حلقة التقييم تشبه إلى حد كبير حلقة التدريب، ولكن يجب ألا ننسى تبديل النموذج إلى وضع التقييم عن طريق استدعاء `model.eval()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy: 0.9047029702970297\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "iterations = 100\n",
    "acc = 0\n",
    "i = 0\n",
    "for labels,texts in test_loader:\n",
    "    labels = labels.to(device)-1      \n",
    "    texts = texts.to(device)\n",
    "    _, out = model(texts, labels=labels)[:2]\n",
    "    labs = out.argmax(dim=1)\n",
    "    acc += torch.mean((labs==labels).type(torch.float32))\n",
    "    i+=1\n",
    "    if i>iterations: break\n",
    "        \n",
    "print(f\"Final accuracy: {acc.item()/i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## النقاط الرئيسية\n",
    "\n",
    "في هذه الوحدة، رأينا مدى سهولة استخدام نموذج لغة مدرب مسبقًا من مكتبة **transformers** وتكييفه مع مهمة تصنيف النصوص الخاصة بنا. وبالمثل، يمكن استخدام نماذج BERT لاستخراج الكيانات، والإجابة على الأسئلة، والمهام الأخرى في معالجة اللغة الطبيعية.\n",
    "\n",
    "تمثل نماذج المحولات (Transformers) أحدث ما توصلت إليه التكنولوجيا في مجال معالجة اللغة الطبيعية، وفي معظم الحالات، يجب أن تكون الحل الأول الذي تبدأ بتجربته عند تنفيذ حلول مخصصة لمعالجة اللغة الطبيعية. ومع ذلك، فإن فهم المبادئ الأساسية للشبكات العصبية المتكررة التي تمت مناقشتها في هذه الوحدة أمر بالغ الأهمية إذا كنت ترغب في بناء نماذج عصبية متقدمة.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**إخلاء المسؤولية**:  \nتم ترجمة هذا المستند باستخدام خدمة الترجمة بالذكاء الاصطناعي [Co-op Translator](https://github.com/Azure/co-op-translator). بينما نسعى لتحقيق الدقة، يرجى العلم أن الترجمات الآلية قد تحتوي على أخطاء أو معلومات غير دقيقة. يجب اعتبار المستند الأصلي بلغته الأصلية المصدر الموثوق. للحصول على معلومات حاسمة، يُوصى بالاستعانة بترجمة بشرية احترافية. نحن غير مسؤولين عن أي سوء فهم أو تفسيرات خاطئة تنشأ عن استخدام هذه الترجمة.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37_pytorch",
   "language": "python",
   "name": "conda-env-py37_pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "coopTranslator": {
   "original_hash": "753865967678a92dbce7d7efbd36d980",
   "translation_date": "2025-08-28T04:14:34+00:00",
   "source_file": "lessons/5-NLP/18-Transformers/TransformersPyTorch.ipynb",
   "language_code": "ar"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}