{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# آليات الانتباه والنماذج المحولة\n",
    "\n",
    "أحد العيوب الرئيسية للشبكات العصبية المتكررة هو أن جميع الكلمات في التسلسل لها نفس التأثير على النتيجة. يؤدي هذا إلى أداء غير مثالي مع نماذج LSTM القياسية للترميز وفك الترميز في مهام التسلسل إلى التسلسل، مثل التعرف على الكيانات المسماة والترجمة الآلية. في الواقع، غالبًا ما يكون لبعض الكلمات في تسلسل الإدخال تأثير أكبر على المخرجات المتسلسلة مقارنة بغيرها.\n",
    "\n",
    "لنأخذ نموذج التسلسل إلى التسلسل كمثال، مثل الترجمة الآلية. يتم تنفيذه باستخدام شبكتين متكررتين، حيث تقوم شبكة واحدة (**المشفّر**) بضغط تسلسل الإدخال إلى حالة مخفية، وتقوم الأخرى، **المفكّك**، بفك هذه الحالة المخفية إلى النتيجة المترجمة. المشكلة في هذا النهج هي أن الحالة النهائية للشبكة ستواجه صعوبة في تذكر بداية الجملة، مما يؤدي إلى ضعف جودة النموذج في الجمل الطويلة.\n",
    "\n",
    "**آليات الانتباه** توفر وسيلة لوزن التأثير السياقي لكل متجه إدخال على كل توقع إخراج للشبكة العصبية المتكررة. يتم تنفيذ ذلك من خلال إنشاء اختصارات بين الحالات الوسيطة لشبكة الإدخال العصبية المتكررة وشبكة الإخراج العصبية المتكررة. بهذه الطريقة، عند توليد رمز الإخراج $y_t$، سنأخذ في الاعتبار جميع الحالات المخفية للإدخال $h_i$، مع معاملات وزن مختلفة $\\alpha_{t,i}$.\n",
    "\n",
    "![صورة توضح نموذج الترميز/فك الترميز مع طبقة انتباه إضافية](../../../../../translated_images/ar/encoder-decoder-attention.7a726296894fb567.webp)\n",
    "*نموذج الترميز-فك الترميز مع آلية الانتباه الإضافية في [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf)، مقتبس من [هذا المنشور](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)*\n",
    "\n",
    "مصفوفة الانتباه $\\{\\alpha_{i,j}\\}$ تمثل الدرجة التي تلعب بها كلمات الإدخال دورًا في توليد كلمة معينة في تسلسل الإخراج. أدناه مثال على مثل هذه المصفوفة:\n",
    "\n",
    "![صورة توضح محاذاة نموذج RNNsearch-50، مأخوذة من Bahdanau - arviz.org](../../../../../translated_images/ar/bahdanau-fig3.09ba2d37f202a6af.webp)\n",
    "\n",
    "*الصورة مأخوذة من [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) (الشكل 3)*\n",
    "\n",
    "آليات الانتباه مسؤولة عن الكثير من التقدم الحالي أو القريب من الحالي في معالجة اللغة الطبيعية. ومع ذلك، فإن إضافة الانتباه تزيد بشكل كبير من عدد معلمات النموذج، مما أدى إلى مشكلات في التوسع مع الشبكات العصبية المتكررة. أحد القيود الرئيسية لتوسيع الشبكات العصبية المتكررة هو أن الطبيعة المتكررة للنماذج تجعل من الصعب تجميع وتوازي التدريب. في الشبكات العصبية المتكررة، يجب معالجة كل عنصر من عناصر التسلسل بترتيب متسلسل، مما يعني أنه لا يمكن توازيه بسهولة.\n",
    "\n",
    "أدى تبني آليات الانتباه مع هذا القيد إلى إنشاء نماذج المحولات التي تمثل الآن أحدث ما توصلت إليه التكنولوجيا، والتي نعرفها ونستخدمها اليوم مثل BERT وOpenGPT3.\n",
    "\n",
    "## نماذج المحولات\n",
    "\n",
    "بدلاً من تمرير سياق كل توقع سابق إلى خطوة التقييم التالية، تستخدم **نماذج المحولات** **الترميزات الموضعية** و**الانتباه** لالتقاط سياق الإدخال المعطى ضمن نافذة نصية محددة. الصورة أدناه توضح كيف يمكن للترميزات الموضعية مع الانتباه التقاط السياق ضمن نافذة معينة.\n",
    "\n",
    "![صورة متحركة توضح كيفية إجراء التقييمات في نماذج المحولات.](../../../../../lessons/5-NLP/18-Transformers/images/transformer-animated-explanation.gif)\n",
    "\n",
    "نظرًا لأن كل موضع إدخال يتم تعيينه بشكل مستقل إلى كل موضع إخراج، يمكن للمحولات التوازي بشكل أفضل من الشبكات العصبية المتكررة، مما يتيح نماذج لغوية أكبر وأكثر تعبيرًا. يمكن استخدام كل رأس انتباه لتعلم علاقات مختلفة بين الكلمات، مما يحسن مهام معالجة اللغة الطبيعية.\n",
    "\n",
    "## بناء نموذج محول بسيط\n",
    "\n",
    "لا تحتوي مكتبة Keras على طبقة محول مدمجة، ولكن يمكننا بناء واحدة بأنفسنا. كما في السابق، سنركز على تصنيف النصوص باستخدام مجموعة بيانات AG News، ولكن من الجدير بالذكر أن نماذج المحولات تظهر أفضل النتائج في المهام الأكثر صعوبة في معالجة اللغة الطبيعية.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "\n",
    "ds_train, ds_test = tfds.load('ag_news_subset').values()\n",
    "\n",
    "def extract_text(x):\n",
    "    return x['title']+' '+x['description']\n",
    "\n",
    "def tupelize(x):\n",
    "    return (extract_text(x),x['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "يجب أن تكون الطبقات الجديدة في Keras من الفئة الفرعية `Layer`، وتنفذ طريقة `call`. لنبدأ بطبقة **التضمين الموضعي**. سنستخدم [بعض التعليمات البرمجية من وثائق Keras الرسمية](https://keras.io/examples/nlp/text_classification_with_transformer/). سنفترض أننا نقوم بملء جميع تسلسلات الإدخال إلى الطول `maxlen`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(keras.layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = keras.layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "        self.maxlen = maxlen\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = self.maxlen\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x+positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "تتكون هذه الطبقة من طبقتين `Embedding`: واحدة لتضمين الرموز (بالطريقة التي ناقشناها سابقًا) والأخرى لتضمين مواضع الرموز. يتم إنشاء مواضع الرموز كسلسلة من الأعداد الطبيعية من 0 إلى `maxlen` باستخدام `tf.range`، ثم يتم تمريرها عبر طبقة التضمين. يتم بعد ذلك جمع متجهي التضمين الناتجين، مما ينتج تمثيلًا مضمنًا موضعيًا للإدخال بالشكل `maxlen`$\\times$`embed_dim`.\n",
    "\n",
    "الآن، دعونا ننفذ كتلة المحول. ستأخذ هذه الكتلة مخرجات طبقة التضمين التي تم تعريفها سابقًا:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim, name='attn')\n",
    "        self.ffn = keras.Sequential(\n",
    "            [keras.layers.Dense(ff_dim, activation=\"relu\"), keras.layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = keras.layers.Dropout(rate)\n",
    "        self.dropout2 = keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "الآن، نحن جاهزون لتعريف نموذج المحول الكامل:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "text_vectorization (TextVect (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "token_and_position_embedding (None, 256, 32)           648192    \n",
      "_________________________________________________________________\n",
      "transformer_block (Transform (None, 256, 32)           10656     \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 20)                660       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 84        \n",
      "=================================================================\n",
      "Total params: 659,592\n",
      "Trainable params: 659,592\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 32  # Embedding size for each token\n",
    "num_heads = 2  # Number of attention heads\n",
    "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
    "maxlen = 256\n",
    "vocab_size = 20000\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.experimental.preprocessing.TextVectorization(max_tokens=vocab_size,output_sequence_length=maxlen, input_shape=(1,)),\n",
    "    TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim),\n",
    "    TransformerBlock(embed_dim, num_heads, ff_dim),\n",
    "    keras.layers.GlobalAveragePooling1D(),\n",
    "    keras.layers.Dropout(0.1),\n",
    "    keras.layers.Dense(20, activation=\"relu\"),\n",
    "    keras.layers.Dropout(0.1),\n",
    "    keras.layers.Dense(4, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokenizer\n",
      "938/938 [==============================] - 45s 39ms/step - loss: 0.4978 - acc: 0.8068 - val_loss: 0.2808 - val_acc: 0.9124\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9c2427a0d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Training tokenizer')\n",
    "model.layers[0].adapt(ds_train.map(extract_text))\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'], optimizer='adam')\n",
    "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## نماذج BERT Transformer\n",
    "\n",
    "**BERT** (تمثيلات التشفير ثنائية الاتجاه من المحولات) هو شبكة محولات متعددة الطبقات كبيرة جدًا تحتوي على 12 طبقة في *BERT-base*، و24 طبقة في *BERT-large*. يتم تدريب النموذج مبدئيًا على مجموعة بيانات نصية ضخمة (ويكيبيديا + كتب) باستخدام تدريب غير خاضع للإشراف (التنبؤ بالكلمات المحجوبة في الجملة). خلال مرحلة التدريب المبدئي، يكتسب النموذج مستوى كبيرًا من فهم اللغة، والذي يمكن استغلاله لاحقًا مع مجموعات بيانات أخرى باستخدام التخصيص الدقيق. تُعرف هذه العملية باسم **التعلم بالنقل**.\n",
    "\n",
    "![صورة من http://jalammar.github.io/illustrated-bert/](../../../../../translated_images/ar/jalammarBERT-language-modeling-masked-lm.34f113ea5fec4362.webp)\n",
    "\n",
    "هناك العديد من التعديلات على بنية المحولات، بما في ذلك BERT، وDistilBERT، وBigBird، وOpenGPT3، والمزيد، والتي يمكن تخصيصها بدقة.\n",
    "\n",
    "دعونا نرى كيف يمكننا استخدام نموذج BERT المدرب مسبقًا لحل مشكلة تصنيف التسلسل التقليدية لدينا. سنستعير الفكرة وبعض الشيفرات من [التوثيق الرسمي](https://www.tensorflow.org/text/tutorials/classify_text_with_bert).\n",
    "\n",
    "لتحميل النماذج المدربة مسبقًا، سنستخدم **Tensorflow hub**. أولاً، دعونا نحمل الموجه الخاص بـ BERT:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_41180/4216669875.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow_text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow_hub\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mhub\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhub\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mKerasLayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_text'"
     ]
    }
   ],
   "source": [
    "import tensorflow_text \n",
    "import tensorflow_hub as hub\n",
    "vectorizer = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_type_ids': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       " array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "       dtype=int32)>,\n",
       " 'input_word_ids': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       " array([[  101,  1045,  2293, 19081,   102,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]], dtype=int32)>,\n",
       " 'input_mask': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       " array([[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "       dtype=int32)>}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer(['I love transformers'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "من المهم أن تستخدم نفس الـ vectorizer الذي تم تدريب الشبكة الأصلية عليه. بالإضافة إلى ذلك، يقوم BERT vectorizer بإرجاع ثلاثة مكونات:\n",
    "* `input_word_ids`، وهو تسلسل أرقام الرموز لجملة الإدخال\n",
    "* `input_mask`، الذي يوضح أي جزء من التسلسل يحتوي على الإدخال الفعلي، وأي جزء هو padding. وهو مشابه للقناع الذي يتم إنتاجه بواسطة طبقة `Masking`\n",
    "* `input_type_ids` يُستخدم لمهام نمذجة اللغة، ويسمح بتحديد جملتين إدخاليتين في تسلسل واحد.\n",
    "\n",
    "بعد ذلك، يمكننا إنشاء مستخرج ميزات BERT:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = hub.KerasLayer('https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pooled_output -> (1, 128)\n",
      "encoder_outputs -> 4\n",
      "sequence_output -> (1, 128, 128)\n",
      "default -> (1, 128)\n"
     ]
    }
   ],
   "source": [
    "z = bert(vectorizer(['I love transformers']))\n",
    "for i,x in z.items():\n",
    "    print(f\"{i} -> { len(x) if isinstance(x, list) else x.shape }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "لذا، تقوم طبقة BERT بإرجاع عدد من النتائج المفيدة:\n",
    "* `pooled_output` هو نتيجة متوسط جميع الرموز في التسلسل. يمكنك اعتباره تمثيلاً ذكياً للمعنى العام للشبكة بأكملها. وهو يعادل ناتج الطبقة `GlobalAveragePooling1D` في نموذجنا السابق.\n",
    "* `sequence_output` هو ناتج الطبقة الأخيرة من المحول (يتوافق مع ناتج `TransformerBlock` في نموذجنا أعلاه).\n",
    "* `encoder_outputs` هي نواتج جميع طبقات المحول. بما أننا قمنا بتحميل نموذج BERT مكون من 4 طبقات (كما يمكنك أن تخمن من الاسم الذي يحتوي على `4_H`)، فإنه يحتوي على 4 موترات. الأخير منها هو نفسه `sequence_output`.\n",
    "\n",
    "الآن سنقوم بتعريف نموذج التصنيف الشامل. سنستخدم *تعريف النموذج الوظيفي*، حيث نقوم بتحديد مدخلات النموذج، ثم نقدم سلسلة من التعبيرات لحساب مخرجاته. سنجعل أيضاً أوزان نموذج BERT غير قابلة للتدريب، وسنقوم بتدريب المصنف النهائي فقط:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer (KerasLayer)        {'input_type_ids': ( 0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer_1 (KerasLayer)      {'pooled_output': (N 4782465     keras_layer[0][0]                \n",
      "                                                                 keras_layer[0][1]                \n",
      "                                                                 keras_layer[0][2]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 128)          0           keras_layer_1[0][5]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 4)            516         dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 4,782,981\n",
      "Trainable params: 516\n",
      "Non-trainable params: 4,782,465\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inp = keras.Input(shape=(),dtype=tf.string)\n",
    "x = vectorizer(inp)\n",
    "x = bert(x)\n",
    "x = keras.layers.Dropout(0.1)(x['pooled_output'])\n",
    "out = keras.layers.Dense(4,activation='softmax')(x)\n",
    "model = keras.models.Model(inp,out)\n",
    "bert.trainable = False\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 528s 559ms/step - loss: 0.8056 - acc: 0.6983 - val_loss: 0.5953 - val_acc: 0.7888\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9bb1e36d00>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'], optimizer='adam')\n",
    "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "على الرغم من أن هناك عددًا قليلاً من المعاملات القابلة للتدريب، إلا أن العملية بطيئة جدًا، لأن مستخرج ميزات BERT يتطلب حسابات مكثفة. يبدو أننا لم نتمكن من تحقيق دقة معقولة، إما بسبب نقص التدريب أو نقص في معاملات النموذج.\n",
    "\n",
    "دعونا نحاول فك تجميد أوزان BERT وتدريبه أيضًا. يتطلب هذا معدل تعلم صغير جدًا، بالإضافة إلى استراتيجية تدريب أكثر حذرًا مع **الإحماء**، باستخدام مُحسّن **AdamW**. سنستخدم حزمة `tf-models-official` لإنشاء المُحسّن:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer (KerasLayer)        {'input_type_ids': ( 0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer_1 (KerasLayer)      {'pooled_output': (N 4782465     keras_layer[0][0]                \n",
      "                                                                 keras_layer[0][1]                \n",
      "                                                                 keras_layer[0][2]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 128)          0           keras_layer_1[0][5]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 4)            516         dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 4,782,981\n",
      "Trainable params: 4,782,980\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n",
      "938/938 [==============================] - 629s 664ms/step - loss: 0.6344 - acc: 0.7658 - val_loss: 0.4876 - val_acc: 0.8247\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9bb0bd0070>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from official.nlp import optimization \n",
    "bert.trainable=True\n",
    "model.summary()\n",
    "epochs = 3\n",
    "opt = optimization.create_optimizer(\n",
    "    init_lr=3e-5,\n",
    "    num_train_steps=epochs*len(ds_train),\n",
    "    num_warmup_steps=0.1*epochs*len(ds_train),\n",
    "    optimizer_type='adamw')\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'], optimizer=opt)\n",
    "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "كما ترى، التدريب يسير ببطء شديد - ولكن قد ترغب في التجربة وتدريب النموذج لعدة دورات (5-10) لترى ما إذا كان بإمكانك الحصول على أفضل نتيجة مقارنة بالنهج التي استخدمناها سابقًا.\n",
    "\n",
    "## مكتبة Huggingface Transformers\n",
    "\n",
    "طريقة أخرى شائعة جدًا (وأبسط قليلاً) لاستخدام نماذج Transformer هي [حزمة HuggingFace](https://github.com/huggingface/)، التي توفر مكونات بسيطة لمهام معالجة اللغة الطبيعية المختلفة. وهي متاحة لكل من Tensorflow وPyTorch، وهو إطار عمل آخر شائع للشبكات العصبية.\n",
    "\n",
    "> **ملاحظة**: إذا لم تكن مهتمًا برؤية كيفية عمل مكتبة Transformers - يمكنك تخطي نهاية هذا الدفتر، لأنك لن ترى أي شيء مختلف بشكل جوهري عما قمنا به أعلاه. سنقوم بتكرار نفس خطوات تدريب نموذج BERT باستخدام مكتبة مختلفة ونموذج أكبر بشكل كبير. وبالتالي، تتضمن العملية بعض التدريب الطويل جدًا، لذا قد ترغب فقط في استعراض الكود.\n",
    "\n",
    "دعونا نرى كيف يمكن حل مشكلتنا باستخدام [Huggingface Transformers](http://huggingface.co).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "أول شيء نحتاج إلى القيام به هو اختيار النموذج الذي سنستخدمه. بالإضافة إلى بعض النماذج المدمجة، يحتوي Huggingface على [مستودع نماذج عبر الإنترنت](https://huggingface.co/models)، حيث يمكنك العثور على العديد من النماذج المدربة مسبقًا من قبل المجتمع. يمكن تحميل واستخدام جميع هذه النماذج فقط من خلال توفير اسم النموذج. سيتم تنزيل جميع الملفات الثنائية المطلوبة للنموذج تلقائيًا.\n",
    "\n",
    "في بعض الأحيان قد تحتاج إلى تحميل نماذجك الخاصة، وفي هذه الحالة يمكنك تحديد الدليل الذي يحتوي على جميع الملفات ذات الصلة، بما في ذلك معلمات الـ tokenizer، ملف `config.json` الذي يحتوي على معلمات النموذج، الأوزان الثنائية، وما إلى ذلك.\n",
    "\n",
    "من اسم النموذج، يمكننا إنشاء كل من النموذج والـ tokenizer. لنبدأ مع الـ tokenizer:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "# To load the model from Internet repository using model name. \n",
    "# Use this if you are running from your own copy of the notebooks\n",
    "bert_model = 'bert-base-uncased' \n",
    "\n",
    "# To load the model from the directory on disk. Use this for Microsoft Learn module, because we have\n",
    "# prepared all required files for you.\n",
    "#bert_model = './bert'\n",
    "\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(bert_model)\n",
    "\n",
    "MAX_SEQ_LEN = 128\n",
    "PAD_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "UNK_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "يحتوي كائن `tokenizer` على وظيفة `encode` التي يمكن استخدامها مباشرة لترميز النص:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 23435, 12314, 2003, 1037, 2307, 7705, 2005, 17953, 2361, 102]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('Tensorflow is a great framework for NLP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "يمكننا أيضًا استخدام أداة تقسيم النص لترميز تسلسل بطريقة مناسبة لتمريره إلى النموذج، أي تضمين الحقول مثل `token_ids` و `input_mask`، وما إلى ذلك. يمكننا أيضًا تحديد أننا نريد متغيرات Tensorflow عن طريق تقديم الوسيطة `return_tensors='tf'`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[ 101, 7592, 1010, 2045,  102]], dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[0, 0, 0, 0, 0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[1, 1, 1, 1, 1]], dtype=int32)>}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(['Hello, there'],return_tensors='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "في حالتنا، سنستخدم نموذج BERT المدرب مسبقًا والذي يُسمى `bert-base-uncased`. يشير *Uncased* إلى أن النموذج غير حساس لحالة الأحرف.\n",
    "\n",
    "عند تدريب النموذج، نحتاج إلى تقديم تسلسل مرمز كمدخل، وبالتالي سنقوم بتصميم خط معالجة البيانات. وبما أن `tokenizer.encode` هي وظيفة في بايثون، سنستخدم نفس النهج كما في الوحدة السابقة من خلال استدعائها باستخدام `py_function`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(x):\n",
    "    return tokenizer.encode(x.numpy().decode('utf-8'),return_tensors='tf',padding='max_length',max_length=MAX_SEQ_LEN,truncation=True)[0]\n",
    "\n",
    "def process_fn(x):\n",
    "    s = x['title']+' '+x['description']\n",
    "    e = tf.py_function(process,inp=[s],Tout=(tf.int32))\n",
    "    e.set_shape(MAX_SEQ_LEN)\n",
    "    return e,x['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "الآن يمكننا تحميل النموذج الفعلي باستخدام حزمة `BertForSequenceClassification`. يضمن ذلك أن النموذج الخاص بنا يحتوي بالفعل على البنية المطلوبة للتصنيف، بما في ذلك المصنف النهائي. ستظهر رسالة تحذير تشير إلى أن أوزان المصنف النهائي غير مهيأة، وأن النموذج سيحتاج إلى تدريب مسبق - وهذا أمر طبيعي تمامًا، لأنه بالضبط ما نحن على وشك القيام به!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = transformers.TFBertForSequenceClassification.from_pretrained(bert_model,num_labels=4,output_attentions=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  109482240 \n",
      "_________________________________________________________________\n",
      "dropout_75 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  3076      \n",
      "=================================================================\n",
      "Total params: 109,485,316\n",
      "Trainable params: 109,485,316\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "كما ترى من `summary()`، يحتوي النموذج على ما يقرب من 110 مليون معلمة! من المفترض، إذا أردنا مهمة تصنيف بسيطة على مجموعة بيانات صغيرة نسبيًا، فلا نريد تدريب طبقة BERT الأساسية:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  109482240 \n",
      "_________________________________________________________________\n",
      "dropout_75 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  3076      \n",
      "=================================================================\n",
      "Total params: 109,485,316\n",
      "Trainable params: 3,076\n",
      "Non-trainable params: 109,482,240\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.layers[0].trainable = False\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "الآن نحن جاهزون لبدء التدريب!\n",
    "\n",
    "> **ملاحظة**: تدريب نموذج BERT كامل النطاق يمكن أن يكون مستهلكًا جدًا للوقت! لذلك سنقوم فقط بتدريبه لأول 32 دفعة. هذا فقط لإظهار كيفية إعداد تدريب النموذج. إذا كنت مهتمًا بتجربة التدريب كامل النطاق - فقط قم بإزالة معلمات `steps_per_epoch` و `validation_steps`، واستعد للانتظار!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 142s 4s/step - loss: 1.3896 - acc: 0.2500 - val_loss: 1.3863 - val_acc: 0.2480\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f1d40a4b6a0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile('adam','sparse_categorical_crossentropy',['acc'])\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "model.fit(ds_train.map(process_fn).batch(32),validation_data=ds_test.map(process_fn).batch(32),steps_per_epoch=32,validation_steps=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "إذا قمت بزيادة عدد التكرارات وانتظرت لفترة كافية، وقمت بالتدريب لعدة دورات، يمكنك أن تتوقع أن تصنيف BERT سيعطينا أفضل دقة! ذلك لأن BERT يفهم بالفعل هيكل اللغة بشكل جيد، وكل ما نحتاجه هو تحسين المصنف النهائي. ومع ذلك، نظرًا لأن BERT نموذج كبير، فإن عملية التدريب بأكملها تستغرق وقتًا طويلًا، وتتطلب قوة حسابية كبيرة! (وحدة معالجة الرسومات GPU، ويفضل أن تكون أكثر من واحدة).\n",
    "\n",
    "> **ملاحظة:** في مثالنا، كنا نستخدم أحد أصغر نماذج BERT المدربة مسبقًا. هناك نماذج أكبر من المحتمل أن تقدم نتائج أفضل.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## النقاط الرئيسية\n",
    "\n",
    "في هذه الوحدة، استعرضنا أحدث بنى النماذج المعتمدة على **المحولات**. قمنا بتطبيقها على مهمة تصنيف النصوص، وبالمثل، يمكن استخدام نماذج BERT لاستخراج الكيانات، والإجابة على الأسئلة، وغيرها من مهام معالجة اللغة الطبيعية.\n",
    "\n",
    "تمثل نماذج المحولات أحدث ما توصلت إليه التكنولوجيا في مجال معالجة اللغة الطبيعية، وفي معظم الحالات، ينبغي أن تكون الحل الأول الذي تبدأ بتجربته عند تنفيذ حلول مخصصة لمعالجة اللغة الطبيعية. ومع ذلك، فإن فهم المبادئ الأساسية للشبكات العصبية التكرارية التي تمت مناقشتها في هذه الوحدة يُعد أمرًا بالغ الأهمية إذا كنت ترغب في بناء نماذج عصبية متقدمة.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**إخلاء المسؤولية**:  \nتم ترجمة هذا المستند باستخدام خدمة الترجمة الآلية [Co-op Translator](https://github.com/Azure/co-op-translator). بينما نسعى لتحقيق الدقة، يرجى العلم أن الترجمات الآلية قد تحتوي على أخطاء أو معلومات غير دقيقة. يجب اعتبار المستند الأصلي بلغته الأصلية هو المصدر الموثوق. للحصول على معلومات حساسة أو هامة، يُوصى بالاستعانة بترجمة بشرية احترافية. نحن غير مسؤولين عن أي سوء فهم أو تفسيرات خاطئة ناتجة عن استخدام هذه الترجمة.\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0cb620c6d4b9f7a635928804c26cf22403d89d98d79684e4529119355ee6d5a5"
  },
  "kernelspec": {
   "display_name": "py38_tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "coopTranslator": {
   "original_hash": "ab59c532409774988ab875f2260e8e53",
   "translation_date": "2025-08-28T04:17:21+00:00",
   "source_file": "lessons/5-NLP/18-Transformers/TransformersTF.ipynb",
   "language_code": "ar"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}