# حيل تدريب التعلم العميق

مع ازدياد عمق الشبكات العصبية، يصبح تدريبها أكثر تحديًا. إحدى المشكلات الرئيسية هي ما يُعرف بـ [تلاشي التدرجات](https://en.wikipedia.org/wiki/Vanishing_gradient_problem) أو [انفجار التدرجات](https://deepai.org/machine-learning-glossary-and-terms/exploding-gradient-problem#:~:text=Exploding%20gradients%20are%20a%20problem,updates%20are%20small%20and%20controlled.). [هذا المقال](https://towardsdatascience.com/the-vanishing-exploding-gradient-problem-in-deep-neural-networks-191358470c11) يقدم مقدمة جيدة حول هذه المشكلات.

لجعل تدريب الشبكات العميقة أكثر كفاءة، هناك بعض التقنيات التي يمكن استخدامها.

## الحفاظ على القيم ضمن نطاق معقول

لجعل العمليات الحسابية العددية أكثر استقرارًا، نريد التأكد من أن جميع القيم داخل شبكتنا العصبية تقع ضمن نطاق معقول، عادةً [-1..1] أو [0..1]. هذا ليس شرطًا صارمًا، ولكن طبيعة العمليات الحسابية بالنقاط العائمة تجعل القيم ذات الأحجام المختلفة غير دقيقة عند معالجتها معًا. على سبيل المثال، إذا أضفنا 10<sup>-10</sup> و10<sup>10</sup>، فمن المحتمل أن نحصل على 10<sup>10</sup>، لأن القيمة الأصغر سيتم "تحويلها" إلى نفس مرتبة القيمة الأكبر، وبالتالي ستُفقد المانتيستا.

معظم دوال التفعيل تحتوي على لاخطيات حول [-1..1]، وبالتالي من المنطقي أن نقوم بتوسيع نطاق جميع بيانات الإدخال إلى [-1..1] أو [0..1].

## تهيئة الأوزان الأولية

من الناحية المثالية، نريد أن تكون القيم ضمن نفس النطاق بعد المرور عبر طبقات الشبكة. لذلك من المهم تهيئة الأوزان بطريقة تحافظ على توزيع القيم.

التوزيع الطبيعي **N(0,1)** ليس فكرة جيدة، لأنه إذا كان لدينا *n* مدخلات، فإن الانحراف المعياري للإخراج سيكون *n*، ومن المحتمل أن تخرج القيم عن نطاق [0..1].

التهيئات التالية تُستخدم غالبًا:

- التوزيع المنتظم -- `uniform`
- **N(0,1/n)** -- `gaussian`
- **N(0,1/√n_in)** يضمن أنه بالنسبة للمدخلات ذات المتوسط الصفري والانحراف المعياري 1، سيبقى نفس المتوسط والانحراف المعياري
- **N(0,√2/(n_in+n_out))** -- ما يُعرف بـ **تهيئة Xavier** (`glorot`)، يساعد في الحفاظ على الإشارات ضمن النطاق أثناء الانتشار الأمامي والخلفي

## التطبيع بالدفعات

حتى مع التهيئة الصحيحة للأوزان، يمكن أن تصبح الأوزان كبيرة جدًا أو صغيرة جدًا أثناء التدريب، مما يؤدي إلى إخراج الإشارات عن النطاق المناسب. يمكننا إعادة الإشارات إلى النطاق باستخدام إحدى تقنيات **التطبيع**. بينما توجد عدة تقنيات (تطبيع الأوزان، تطبيع الطبقات)، فإن الأكثر استخدامًا هو التطبيع بالدفعات.

فكرة **التطبيع بالدفعات** هي أخذ جميع القيم عبر الدفعة المصغرة في الاعتبار، وإجراء التطبيع (أي طرح المتوسط والقسمة على الانحراف المعياري) بناءً على تلك القيم. يتم تنفيذ ذلك كطبقة في الشبكة تقوم بهذا التطبيع بعد تطبيق الأوزان، ولكن قبل دالة التفعيل. ونتيجة لذلك، من المحتمل أن نرى دقة نهائية أعلى وتدريبًا أسرع.

إليك [البحث الأصلي](https://arxiv.org/pdf/1502.03167.pdf) حول التطبيع بالدفعات، و[الشرح على ويكيبيديا](https://en.wikipedia.org/wiki/Batch_normalization)، و[مقال تمهيدي جيد](https://towardsdatascience.com/batch-normalization-in-3-levels-of-understanding-14c2da90a338) (وآخر [بالروسية](https://habrahabr.ru/post/309302/)).

## الإسقاط

**الإسقاط** هو تقنية مثيرة للاهتمام تقوم بإزالة نسبة معينة من الخلايا العصبية العشوائية أثناء التدريب. يتم تنفيذها أيضًا كطبقة تحتوي على معلمة واحدة (نسبة الخلايا العصبية التي سيتم إزالتها، عادةً 10%-50%)، وأثناء التدريب يتم تصفير عناصر عشوائية من متجه الإدخال قبل تمريره إلى الطبقة التالية.

على الرغم من أن هذا قد يبدو فكرة غريبة، يمكنك رؤية تأثير الإسقاط على تدريب مصنف أرقام MNIST في دفتر الملاحظات [`Dropout.ipynb`](../../../../../lessons/4-ComputerVision/08-TransferLearning/Dropout.ipynb). فهو يسرّع التدريب ويسمح لنا بتحقيق دقة أعلى في عدد أقل من الحلقات التدريبية.

يمكن تفسير هذا التأثير بعدة طرق:

- يمكن اعتباره عامل صدمة عشوائي للنموذج، مما يخرجه من الحد الأدنى المحلي
- يمكن اعتباره *متوسطًا ضمنيًا للنموذج*، لأنه يمكننا القول إنه أثناء الإسقاط نقوم بتدريب نموذج مختلف قليلاً

> *يقول بعض الناس إنه عندما يحاول شخص مخمور تعلم شيء ما، فإنه سيتذكره بشكل أفضل في صباح اليوم التالي مقارنة بشخص غير مخمور، لأن الدماغ مع بعض الخلايا العصبية المعطلة يحاول التكيف بشكل أفضل لفهم المعنى. لم نختبر بأنفسنا ما إذا كان هذا صحيحًا أم لا.*

## منع الإفراط في التكيّف

أحد الجوانب المهمة جدًا في التعلم العميق هو القدرة على منع [الإفراط في التكيّف](../../3-NeuralNetworks/05-Frameworks/Overfitting.md). على الرغم من أنه قد يكون مغريًا استخدام نموذج شبكة عصبية قوي جدًا، يجب علينا دائمًا موازنة عدد معلمات النموذج مع عدد عينات التدريب.

> تأكد من فهمك لمفهوم [الإفراط في التكيّف](../../3-NeuralNetworks/05-Frameworks/Overfitting.md) الذي قدمناه سابقًا!

هناك عدة طرق لمنع الإفراط في التكيّف:

- التوقف المبكر -- مراقبة الخطأ باستمرار على مجموعة التحقق وإيقاف التدريب عندما يبدأ خطأ التحقق في الزيادة.
- الانحلال الصريح للأوزان / التنظيم -- إضافة عقوبة إضافية إلى دالة الخسارة للقيم المطلقة العالية للأوزان، مما يمنع النموذج من الحصول على نتائج غير مستقرة للغاية
- متوسط النموذج -- تدريب عدة نماذج ثم أخذ متوسط النتائج. يساعد ذلك في تقليل التباين.
- الإسقاط (متوسط النموذج الضمني)

## المحسنات / خوارزميات التدريب

جانب آخر مهم في التدريب هو اختيار خوارزمية تدريب جيدة. على الرغم من أن **الانحدار التدرجي** الكلاسيكي هو خيار معقول، إلا أنه قد يكون بطيئًا جدًا في بعض الأحيان، أو يؤدي إلى مشكلات أخرى.

في التعلم العميق، نستخدم **الانحدار التدرجي العشوائي** (SGD)، وهو انحدار تدرجي يُطبق على دفعات مصغرة يتم اختيارها عشوائيًا من مجموعة التدريب. يتم تعديل الأوزان باستخدام هذه الصيغة:

w<sup>t+1</sup> = w<sup>t</sup> - η∇ℒ

### الزخم

في **SGD مع الزخم**، نحتفظ بجزء من التدرج من الخطوات السابقة. يشبه ذلك عندما نتحرك بشيء من القصور الذاتي، ونتلقى دفعة في اتجاه مختلف، فإن مسارنا لا يتغير فورًا، بل يحتفظ بجزء من الحركة الأصلية. هنا نقدم متجهًا آخر v لتمثيل *السرعة*:

- v<sup>t+1</sup> = γ v<sup>t</sup> - η∇ℒ
- w<sup>t+1</sup> = w<sup>t</sup>+v<sup>t+1</sup>

هنا تشير المعلمة γ إلى مدى أخذ القصور الذاتي في الاعتبار: γ=0 يعادل SGD الكلاسيكي؛ γ=1 هو معادلة حركة بحتة.

### آدم، أداغراد، إلخ.

نظرًا لأننا في كل طبقة نضرب الإشارات بمصفوفة W<sub>i</sub>، اعتمادًا على ||W<sub>i</sub>||، يمكن أن يتضاءل التدرج ليقترب من 0، أو يرتفع بلا حدود. هذه هي جوهر مشكلة انفجار/تلاشي التدرجات.

إحدى الحلول لهذه المشكلة هي استخدام اتجاه التدرج فقط في المعادلة، وتجاهل القيمة المطلقة، أي:

w<sup>t+1</sup> = w<sup>t</sup> - η(∇ℒ/||∇ℒ||)، حيث ||∇ℒ|| = √∑(∇ℒ)<sup>2</sup>

تُعرف هذه الخوارزمية بـ **أداغراد**. خوارزميات أخرى تستخدم نفس الفكرة: **RMSProp**، **آدم**

> **آدم** يُعتبر خوارزمية فعالة جدًا للعديد من التطبيقات، لذا إذا لم تكن متأكدًا أي واحدة تستخدم - استخدم آدم.

### قص التدرج

قص التدرج هو امتداد للفكرة أعلاه. عندما يكون ||∇ℒ|| ≤ θ، نأخذ التدرج الأصلي في تحسين الأوزان، وعندما يكون ||∇ℒ|| > θ - نقسم التدرج على معياره. هنا θ هو معلمة، وفي معظم الحالات يمكننا أخذ θ=1 أو θ=10.

### تقليل معدل التعلم

غالبًا ما يعتمد نجاح التدريب على معلمة معدل التعلم η. من المنطقي افتراض أن القيم الأكبر لـ η تؤدي إلى تدريب أسرع، وهو ما نريده عادةً في بداية التدريب، ثم القيم الأصغر لـ η تسمح لنا بضبط الشبكة بدقة. لذلك، في معظم الحالات نريد تقليل η أثناء عملية التدريب.

يمكن القيام بذلك بضرب η في رقم معين (مثل 0.98) بعد كل حقبة تدريبية، أو باستخدام **جدول زمني لمعدل التعلم** أكثر تعقيدًا.

## هياكل الشبكات المختلفة

اختيار الهيكل الصحيح للشبكة لمشكلتك يمكن أن يكون صعبًا. عادةً، نختار هيكلًا أثبت فعاليته لمهمتنا المحددة (أو لمهمة مشابهة). إليك [نظرة عامة جيدة](https://www.topbots.com/a-brief-history-of-neural-network-architectures/) على هياكل الشبكات العصبية للرؤية الحاسوبية.

> من المهم اختيار هيكل يكون قويًا بما يكفي لعدد عينات التدريب التي لدينا. اختيار نموذج قوي جدًا يمكن أن يؤدي إلى [الإفراط في التكيّف](../../3-NeuralNetworks/05-Frameworks/Overfitting.md).

طريقة جيدة أخرى هي استخدام هيكل يمكنه التكيف تلقائيًا مع التعقيد المطلوب. إلى حد ما، هيكل **ResNet** و**Inception** ذاتي التكيف. [المزيد عن هياكل الرؤية الحاسوبية](../07-ConvNets/CNN_Architectures.md)

**إخلاء المسؤولية**:  
تمت ترجمة هذا المستند باستخدام خدمة الترجمة الآلية [Co-op Translator](https://github.com/Azure/co-op-translator). بينما نسعى لتحقيق الدقة، يرجى العلم أن الترجمات الآلية قد تحتوي على أخطاء أو عدم دقة. يجب اعتبار المستند الأصلي بلغته الأصلية هو المصدر الموثوق. للحصول على معلومات حساسة أو هامة، يُوصى بالاستعانة بترجمة بشرية احترافية. نحن غير مسؤولين عن أي سوء فهم أو تفسيرات خاطئة تنشأ عن استخدام هذه الترجمة.