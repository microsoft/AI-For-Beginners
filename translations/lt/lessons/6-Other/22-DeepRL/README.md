<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "dbacf9b1915612981d76059678e563e5",
  "translation_date": "2025-08-31T17:32:45+00:00",
  "source_file": "lessons/6-Other/22-DeepRL/README.md",
  "language_code": "lt"
}
-->
# Giliojo stiprinamojo mokymosi pagrindai

Stiprinamasis mokymasis (angl. Reinforcement Learning, RL) laikomas vienu iÅ¡ pagrindiniÅ³ maÅ¡ininio mokymosi paradigmÅ³, greta priÅ¾iÅ«rimojo mokymosi (supervised learning) ir nepriÅ¾iÅ«rimojo mokymosi (unsupervised learning). PriÅ¾iÅ«rimajame mokymesi remiamÄ—s duomenÅ³ rinkiniu su Å¾inomais rezultatais, o RL pagrÄ¯stas **mokymusi per veiksmÄ…**. PavyzdÅ¾iui, kai pirmÄ… kartÄ… matome kompiuterinÄ¯ Å¾aidimÄ…, pradedame jÄ¯ Å¾aisti net neÅ¾inodami taisykliÅ³, ir netrukus galime pagerinti savo Ä¯gÅ«dÅ¾ius tiesiog Å¾aisdami ir koreguodami savo elgesÄ¯.

## [PrieÅ¡ paskaitÄ…: testas](https://ff-quizzes.netlify.app/en/ai/quiz/43)

Norint atlikti RL, mums reikia:

* **Aplinkos** arba **simuliatoriaus**, kuris nustato Å¾aidimo taisykles. TurÄ—tume turÄ—ti galimybÄ™ atlikti eksperimentus simuliatoriuje ir stebÄ—ti rezultatus.
* **Atlygio funkcijos**, kuri nurodo, kaip sÄ—kmingai atliktas mÅ«sÅ³ eksperimentas. PavyzdÅ¾iui, mokantis Å¾aisti kompiuterinÄ¯ Å¾aidimÄ…, atlygis bÅ«tÅ³ mÅ«sÅ³ galutinis rezultatas.

Remdamiesi atlygio funkcija, turÄ—tume sugebÄ—ti koreguoti savo elgesÄ¯ ir tobulinti Ä¯gÅ«dÅ¾ius, kad kitÄ… kartÄ… pasirodytume geriau. Pagrindinis skirtumas tarp kitÅ³ maÅ¡ininio mokymosi tipÅ³ ir RL yra tas, kad RL atveju daÅ¾niausiai neÅ¾inome, ar laimÄ—jome, ar pralaimÄ—jome, kol nebaigiame Å¾aidimo. TodÄ—l negalime pasakyti, ar tam tikras veiksmas yra geras ar blogas - atlygÄ¯ gauname tik Å¾aidimo pabaigoje.

RL metu paprastai atliekame daug eksperimentÅ³. Kiekvieno eksperimento metu turime rasti pusiausvyrÄ… tarp jau iÅ¡moktos optimalios strategijos taikymo (**eksploatacija**) ir naujÅ³ galimÅ³ bÅ«senÅ³ tyrinÄ—jimo (**eksploracija**).

## OpenAI Gym

Puikus Ä¯rankis RL yra [OpenAI Gym](https://gym.openai.com/) - **simuliacijos aplinka**, galinti simuliuoti daugybÄ™ skirtingÅ³ aplinkÅ³, pradedant Atari Å¾aidimais ir baigiant fizikos uÅ¾daviniais, tokiais kaip stulpo balansavimas. Tai viena populiariausiÅ³ simuliacijos aplinkÅ³ stiprinamojo mokymosi algoritmams treniruoti, kuriÄ… priÅ¾iÅ«ri [OpenAI](https://openai.com/).

> **Note**: Visas OpenAI Gym aplinkas galite rasti [Äia](https://gym.openai.com/envs/#classic_control).

## CartPole Balansavimas

Tikriausiai visi esate matÄ™ Å¡iuolaikinius balansavimo Ä¯renginius, tokius kaip *Segway* ar *giroskuteriai*. Jie automatiÅ¡kai balansuoja, reguliuodami savo ratus pagal signalus iÅ¡ akselerometro ar giroskopo. Å ioje dalyje iÅ¡moksime sprÄ™sti panaÅ¡iÄ… problemÄ… - stulpo balansavimÄ…. Tai primena situacijÄ…, kai cirko artistas bando iÅ¡laikyti stulpÄ… ant rankos, taÄiau Å¡is balansavimas vyksta tik vienoje dimensijoje.

Supaprastinta balansavimo versija vadinama **CartPole** problema. CartPole pasaulyje turime horizontalÅ³ slankiklÄ¯, kuris gali judÄ—ti Ä¯ kairÄ™ arba Ä¯ deÅ¡inÄ™, o tikslas yra iÅ¡laikyti vertikalÅ³ stulpÄ… ant slankiklio, kai jis juda.

<img alt="a cartpole" src="images/cartpole.png" width="200"/>

Norint sukurti ir naudoti Å¡iÄ… aplinkÄ…, reikia keliÅ³ Python kodo eiluÄiÅ³:

```python
import gym
env = gym.make("CartPole-v1")

env.reset()
done = False
total_reward = 0
while not done:
   env.render()
   action = env.action_space.sample()
   observaton, reward, done, info = env.step(action)
   total_reward += reward

print(f"Total reward: {total_reward}")
```

Kiekviena aplinka pasiekiama vienodai:
* `env.reset` pradeda naujÄ… eksperimentÄ…
* `env.step` atlieka simuliacijos Å¾ingsnÄ¯. Jis gauna **veiksmÄ…** iÅ¡ **veiksmÅ³ erdvÄ—s** ir grÄ…Å¾ina **stebÄ—jimÄ…** (iÅ¡ stebÄ—jimÅ³ erdvÄ—s), taip pat atlygÄ¯ ir pabaigos vÄ—liavÄ—lÄ™.

AukÅ¡Äiau pateiktame pavyzdyje kiekviename Å¾ingsnyje atliekame atsitiktinÄ¯ veiksmÄ…, todÄ—l eksperimento trukmÄ— yra labai trumpa:

![non-balancing cartpole](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-nobalance.gif)

RL algoritmo tikslas yra iÅ¡mokyti modelÄ¯ - vadinamÄ…jÄ… **politikÄ…** Ï€ - kuri grÄ…Å¾ins veiksmÄ…, atsiÅ¾velgiant Ä¯ tam tikrÄ… bÅ«senÄ…. Taip pat galime laikyti politikÄ… probabilistine, t. y. bet kuriai bÅ«senai *s* ir veiksmui *a* ji grÄ…Å¾ins tikimybÄ™ Ï€(*a*|*s*), kad turÄ—tume atlikti *a* bÅ«senos *s* metu.

## Politikos gradientÅ³ algoritmas

AkivaizdÅ¾iausias bÅ«das modeliuoti politikÄ… yra sukurti neuroninÄ¯ tinklÄ…, kuris priims bÅ«senas kaip Ä¯vestÄ¯ ir grÄ…Å¾ins atitinkamus veiksmus (arba veiksmÅ³ tikimybes). Tam tikra prasme tai bÅ«tÅ³ panaÅ¡u Ä¯ Ä¯prastÄ… klasifikavimo uÅ¾duotÄ¯, taÄiau su esminiu skirtumu - iÅ¡ anksto neÅ¾inome, kokius veiksmus turÄ—tume atlikti kiekviename Å¾ingsnyje.

IdÄ—ja yra Ä¯vertinti Å¡ias tikimybes. Sukuriame **kaupiamÅ³jÅ³ atlygiÅ³** vektoriÅ³, kuris rodo mÅ«sÅ³ bendrÄ… atlygÄ¯ kiekviename eksperimento Å¾ingsnyje. Taip pat taikome **atlygio diskontavimÄ…**, dauginant ankstesnius atlygius iÅ¡ koeficiento Î³=0.99, kad sumaÅ¾intume ankstesniÅ³ atlygiÅ³ svarbÄ…. Tada sustipriname tuos Å¾ingsnius eksperimento kelyje, kurie duoda didesnius atlygius.

> SuÅ¾inokite daugiau apie politikos gradientÅ³ algoritmÄ… ir pamatykite jÄ¯ veiksmingÄ… [pavyzdiniame uÅ¾raÅ¡Å³ knygelÄ—je](CartPole-RL-TF.ipynb).

## Aktoriaus-kritiko algoritmas

Patobulinta politikos gradientÅ³ metodo versija vadinama **Aktoriaus-kritiko** metodu. PagrindinÄ— idÄ—ja yra ta, kad neuroninis tinklas bÅ«tÅ³ apmokytas grÄ…Å¾inti du dalykus:

* PolitikÄ…, kuri nustato, kokÄ¯ veiksmÄ… atlikti. Å i dalis vadinama **aktorius**.
* Bendro atlygio, kurÄ¯ galime tikÄ—tis gauti Å¡ioje bÅ«senoje, Ä¯vertinimÄ… - Å¡i dalis vadinama **kritikas**.

Tam tikra prasme Å¡i architektÅ«ra primena [GAN](../../4-ComputerVision/10-GANs/README.md), kur turime du tinklus, kurie mokomi vienas prieÅ¡ kitÄ…. Aktoriaus-kritiko modelyje aktorius siÅ«lo veiksmÄ…, kurÄ¯ reikia atlikti, o kritikas bando bÅ«ti kritiÅ¡kas ir Ä¯vertinti rezultatÄ…. TaÄiau mÅ«sÅ³ tikslas yra mokyti Å¡iuos tinklus vieningai.

Kadangi eksperimento metu Å¾inome tiek tikruosius kaupiamuosius atlygius, tiek kritiko grÄ…Å¾intus rezultatus, gana lengva sukurti nuostoliÅ³ funkcijÄ…, kuri sumaÅ¾intÅ³ skirtumÄ… tarp jÅ³. Tai suteiktÅ³ mums **kritiko nuostolÄ¯**. **Aktoriaus nuostolÄ¯** galime apskaiÄiuoti naudodami tÄ… patÄ¯ metodÄ… kaip ir politikos gradientÅ³ algoritme.

PaleidÄ™ vienÄ… iÅ¡ Å¡iÅ³ algoritmÅ³, galime tikÄ—tis, kad mÅ«sÅ³ CartPole elgsis taip:

![a balancing cartpole](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-balance.gif)

## âœï¸ Pratimai: Politikos gradientai ir aktoriaus-kritiko RL

TÄ™skite mokymÄ…si Å¡iuose uÅ¾raÅ¡Å³ knygelÄ—se:

* [RL TensorFlow aplinkoje](CartPole-RL-TF.ipynb)
* [RL PyTorch aplinkoje](CartPole-RL-PyTorch.ipynb)

## Kiti RL uÅ¾daviniai

Å iais laikais stiprinamasis mokymasis yra sparÄiai auganti tyrimÅ³ sritis. Keletas Ä¯domiÅ³ stiprinamojo mokymosi pavyzdÅ¾iÅ³:

* Kompiuterio mokymas Å¾aisti **Atari Å¾aidimus**. Å io uÅ¾davinio iÅ¡Å¡Å«kis yra tas, kad neturime paprastos bÅ«senos, pateiktos kaip vektorius, o tik ekrano nuotraukÄ… - todÄ—l turime naudoti CNN, kad konvertuotume Å¡Ä¯ ekrano vaizdÄ… Ä¯ poÅ¾ymiÅ³ vektoriÅ³ arba iÅ¡gautume atlygio informacijÄ…. Atari Å¾aidimai yra prieinami Gym aplinkoje.
* Kompiuterio mokymas Å¾aisti stalo Å¾aidimus, tokius kaip Å¡achmatai ir Go. Pastaruoju metu paÅ¾angiausios programos, tokios kaip **Alpha Zero**, buvo apmokytos nuo nulio, kai du agentai Å¾aidÄ— vienas prieÅ¡ kitÄ… ir tobulÄ—jo kiekviename Å¾ingsnyje.
* PramonÄ—je RL naudojamas kuriant valdymo sistemas iÅ¡ simuliacijÅ³. Paslauga, vadinama [Bonsai](https://azure.microsoft.com/services/project-bonsai/?WT.mc_id=academic-77998-cacaste), yra specialiai tam skirta.

## IÅ¡vada

Dabar iÅ¡mokome treniruoti agentus, kad jie pasiektÅ³ gerÅ³ rezultatÅ³, tiesiog pateikdami jiems atlygio funkcijÄ…, apibrÄ—Å¾ianÄiÄ… norimÄ… Å¾aidimo bÅ«senÄ…, ir suteikdami galimybÄ™ protingai tyrinÄ—ti paieÅ¡kos erdvÄ™. SÄ—kmingai iÅ¡bandÄ—me du algoritmus ir per gana trumpÄ… laikÄ… pasiekÄ—me gerÅ³ rezultatÅ³. TaÄiau tai tik jÅ«sÅ³ kelionÄ—s Ä¯ RL pradÅ¾ia, ir tikrai turÄ—tumÄ—te apsvarstyti galimybÄ™ lankyti atskirÄ… kursÄ…, jei norite gilintis.

## ğŸš€ IÅ¡Å¡Å«kis

IÅ¡nagrinÄ—kite taikymus, iÅ¡vardytus skyriuje â€Kiti RL uÅ¾daviniaiâ€œ, ir pabandykite Ä¯gyvendinti vienÄ… iÅ¡ jÅ³!

## [Po paskaitos: testas](https://ff-quizzes.netlify.app/en/ai/quiz/44)

## PerÅ¾iÅ«ra ir savarankiÅ¡kas mokymasis

SuÅ¾inokite daugiau apie klasikinÄ¯ stiprinamÄ…jÄ¯ mokymÄ…si mÅ«sÅ³ [MaÅ¡ininio mokymosi pradedantiesiems programoje](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/README.md).

Å½iÅ«rÄ—kite [Å¡Ä¯ puikÅ³ vaizdo Ä¯raÅ¡Ä…](https://www.youtube.com/watch?v=qv6UVOQ0F44), kuriame pasakojama, kaip kompiuteris gali iÅ¡mokti Å¾aisti Super Mario.

## UÅ¾duotis: [Treniruokite kalnÅ³ automobilÄ¯](lab/README.md)

JÅ«sÅ³ tikslas Å¡ios uÅ¾duoties metu bus treniruoti kitÄ… Gym aplinkÄ… - [Mountain Car](https://www.gymlibrary.ml/environments/classic_control/mountain_car/).

---

**AtsakomybÄ—s apribojimas**:  
Å is dokumentas buvo iÅ¡verstas naudojant AI vertimo paslaugÄ… [Co-op Translator](https://github.com/Azure/co-op-translator). Nors siekiame tikslumo, praÅ¡ome atkreipti dÄ—mesÄ¯, kad automatiniai vertimai gali turÄ—ti klaidÅ³ ar netikslumÅ³. Originalus dokumentas jo gimtÄ…ja kalba turÄ—tÅ³ bÅ«ti laikomas autoritetingu Å¡altiniu. Kritinei informacijai rekomenduojama profesionali Å¾mogaus vertimo paslauga. Mes neprisiimame atsakomybÄ—s uÅ¾ nesusipratimus ar klaidingus interpretavimus, atsiradusius naudojant Å¡Ä¯ vertimÄ….