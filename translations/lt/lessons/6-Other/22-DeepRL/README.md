# Giliojo stiprinamojo mokymosi pagrindai

Stiprinamasis mokymasis (RL) laikomas vienu iÅ¡ pagrindiniÅ³ maÅ¡ininio mokymosi paradigmÅ³, greta mokymosi su mokytoju ir mokymosi be mokytojo. Mokymosi su mokytoju metu remiamÄ—s duomenÅ³ rinkiniu su Å¾inomais rezultatais, o RL pagrÄ¯stas **mokymusi per veiksmÄ…**. PavyzdÅ¾iui, kai pirmÄ… kartÄ… matome kompiuterinÄ¯ Å¾aidimÄ…, pradedame jÄ¯ Å¾aisti, net neÅ¾inodami taisykliÅ³, ir greitai galime tobulinti savo Ä¯gÅ«dÅ¾ius tiesiog Å¾aisdami ir koreguodami savo elgesÄ¯.

## [PrieÅ¡ paskaitos testas](https://ff-quizzes.netlify.app/en/ai/quiz/43)

Norint atlikti RL, mums reikia:

* **Aplinkos** arba **simuliatoriaus**, kuris nustato Å¾aidimo taisykles. TurÄ—tume turÄ—ti galimybÄ™ atlikti eksperimentus simuliatoriuje ir stebÄ—ti rezultatus.
* **Atlygio funkcijos**, kuri nurodo, kaip sÄ—kmingas buvo mÅ«sÅ³ eksperimentas. Mokantis Å¾aisti kompiuterinÄ¯ Å¾aidimÄ…, atlygis bÅ«tÅ³ mÅ«sÅ³ galutinis rezultatas.

Remdamiesi atlygio funkcija, turÄ—tume sugebÄ—ti koreguoti savo elgesÄ¯ ir tobulinti Ä¯gÅ«dÅ¾ius, kad kitÄ… kartÄ… Å¾aistume geriau. Pagrindinis skirtumas tarp kitÅ³ maÅ¡ininio mokymosi tipÅ³ ir RL yra tas, kad RL metu paprastai neÅ¾inome, ar laimime, ar pralaimime, kol nebaigiame Å¾aidimo. Taigi negalime pasakyti, ar tam tikras veiksmas vienas pats yra geras ar ne - atlygÄ¯ gauname tik Å¾aidimo pabaigoje.

RL metu paprastai atliekame daug eksperimentÅ³. Kiekvieno eksperimento metu turime rasti pusiausvyrÄ… tarp optimalaus strategijos laikymosi, kuriÄ… iki Å¡iol iÅ¡mokome (**eksploatacija**), ir naujÅ³ galimÅ³ bÅ«senÅ³ tyrinÄ—jimo (**tyrinÄ—jimas**).

## OpenAI Gym

Puikus Ä¯rankis RL yra [OpenAI Gym](https://gym.openai.com/) - **simuliacijos aplinka**, galinti simuliuoti daugybÄ™ skirtingÅ³ aplinkÅ³, pradedant Atari Å¾aidimais ir baigiant fizikos uÅ¾daviniais, tokiais kaip stulpelio balansavimas. Tai viena populiariausiÅ³ simuliacijos aplinkÅ³ stiprinamojo mokymosi algoritmams treniruoti, kuriÄ… priÅ¾iÅ«ri [OpenAI](https://openai.com/).

> **Note**: Visas OpenAI Gym aplinkas galite perÅ¾iÅ«rÄ—ti [Äia](https://gym.openai.com/envs/#classic_control).

## CartPole Balansavimas

Tikriausiai visi esate matÄ™ modernius balansavimo Ä¯renginius, tokius kaip *Segway* ar *Giroskuteriai*. Jie sugeba automatiÅ¡kai balansuoti, reguliuodami savo ratus pagal signalÄ… iÅ¡ akselerometro ar giroskopo. Å ioje dalyje iÅ¡moksime iÅ¡sprÄ™sti panaÅ¡iÄ… problemÄ… - stulpelio balansavimÄ…. Tai panaÅ¡u Ä¯ situacijÄ…, kai cirko atlikÄ—jas turi balansuoti stulpelÄ¯ ant savo rankos - taÄiau Å¡is balansavimas vyksta tik 1D.

Supaprastinta balansavimo versija vadinama **CartPole** problema. CartPole pasaulyje turime horizontalÅ³ slankiklÄ¯, kuris gali judÄ—ti Ä¯ kairÄ™ arba Ä¯ deÅ¡inÄ™, o tikslas yra balansuoti vertikalÅ³ stulpelÄ¯ ant slankiklio, kai jis juda.

<img alt="cartpole" src="../../../../../translated_images/lt/cartpole.f52a67f27e058170.webp" width="200"/>

Norint sukurti ir naudoti Å¡iÄ… aplinkÄ…, reikia keliÅ³ Python kodo eiluÄiÅ³:

```python
import gym
env = gym.make("CartPole-v1")

env.reset()
done = False
total_reward = 0
while not done:
   env.render()
   action = env.action_space.sample()
   observaton, reward, done, info = env.step(action)
   total_reward += reward

print(f"Total reward: {total_reward}")
```

Kiekviena aplinka pasiekiama vienodai:
* `env.reset` pradeda naujÄ… eksperimentÄ…
* `env.step` atlieka simuliacijos Å¾ingsnÄ¯. Jis gauna **veiksmÄ…** iÅ¡ **veiksmÅ³ erdvÄ—s** ir grÄ…Å¾ina **stebÄ—jimÄ…** (iÅ¡ stebÄ—jimÅ³ erdvÄ—s), taip pat atlygÄ¯ ir nutraukimo vÄ—liavÄ….

Pavyzdyje aukÅ¡Äiau kiekviename Å¾ingsnyje atliekame atsitiktinÄ¯ veiksmÄ…, todÄ—l eksperimento trukmÄ— yra labai trumpa:

![nebalansuojantis cartpole](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-nobalance.gif)

RL algoritmo tikslas yra iÅ¡mokyti modelÄ¯ - vadinamÄ…jÄ… **politikÄ…** &pi; - kuri grÄ…Å¾ins veiksmÄ…, atsiÅ¾velgiant Ä¯ tam tikrÄ… bÅ«senÄ…. Taip pat galime laikyti politikÄ… probabilistine, t.y., bet kuriai bÅ«senai *s* ir veiksmui *a* ji grÄ…Å¾ins tikimybÄ™ &pi;(*a*|*s*), kad turÄ—tume atlikti *a* bÅ«senos *s* metu.

## Politikos gradientÅ³ algoritmas

AkivaizdÅ¾iausias bÅ«das modeliuoti politikÄ… yra sukurti neuroninÄ¯ tinklÄ…, kuris priims bÅ«senas kaip Ä¯vestÄ¯ ir grÄ…Å¾ins atitinkamus veiksmus (arba veiksmÅ³ tikimybes). Tam tikra prasme tai bÅ«tÅ³ panaÅ¡u Ä¯ Ä¯prastÄ… klasifikavimo uÅ¾duotÄ¯, su pagrindiniu skirtumu - iÅ¡ anksto neÅ¾inome, kokius veiksmus turÄ—tume atlikti kiekviename Å¾ingsnyje.

IdÄ—ja Äia yra Ä¯vertinti tas tikimybes. Sukuriame **kaupiamÅ³jÅ³ atlygiÅ³** vektoriÅ³, kuris rodo mÅ«sÅ³ bendrÄ… atlygÄ¯ kiekviename eksperimento Å¾ingsnyje. Taip pat taikome **atlygio diskontavimÄ…**, dauginant ankstesnius atlygius iÅ¡ tam tikro koeficiento &gamma;=0.99, kad sumaÅ¾intume ankstesniÅ³ atlygiÅ³ svarbÄ…. Tada sustipriname tuos Å¾ingsnius eksperimento kelyje, kurie duoda didesnius atlygius.

> SuÅ¾inokite daugiau apie politikos gradientÅ³ algoritmÄ… ir pamatykite jÄ¯ veiksmuose [pavyzdiniame uÅ¾raÅ¡Å³ knygelÄ—je](CartPole-RL-TF.ipynb).

## Aktoriaus-kritiko algoritmas

Patobulinta politikos gradientÅ³ metodo versija vadinama **Aktoriaus-kritiko** metodu. PagrindinÄ— idÄ—ja yra ta, kad neuroninis tinklas bÅ«tÅ³ apmokytas grÄ…Å¾inti du dalykus:

* PolitikÄ…, kuri nustato, kokÄ¯ veiksmÄ… atlikti. Å i dalis vadinama **aktorius**.
* Bendro atlygio, kurÄ¯ galime tikÄ—tis gauti Å¡ioje bÅ«senoje, Ä¯vertinimÄ… - Å¡i dalis vadinama **kritikas**.

Tam tikra prasme Å¡i architektÅ«ra primena [GAN](../../4-ComputerVision/10-GANs/README.md), kur turime du tinklus, kurie treniruojami vienas prieÅ¡ kitÄ…. Aktoriaus-kritiko modelyje aktorius siÅ«lo veiksmÄ…, kurÄ¯ turime atlikti, o kritikas bando bÅ«ti kritiÅ¡kas ir Ä¯vertinti rezultatÄ…. TaÄiau mÅ«sÅ³ tikslas yra treniruoti Å¡iuos tinklus kartu.

Kadangi eksperimento metu Å¾inome tiek tikrus kaupiamuosius atlygius, tiek kritiko grÄ…Å¾intus rezultatus, palyginti lengva sukurti nuostoliÅ³ funkcijÄ…, kuri sumaÅ¾intÅ³ skirtumÄ… tarp jÅ³. Tai suteiktÅ³ mums **kritiko nuostolÄ¯**. **Aktoriaus nuostolÄ¯** galime apskaiÄiuoti naudodami tÄ… patÄ¯ metodÄ… kaip ir politikos gradientÅ³ algoritme.

Po vieno iÅ¡ Å¡iÅ³ algoritmÅ³ paleidimo galime tikÄ—tis, kad mÅ«sÅ³ CartPole elgsis taip:

![balansuojantis cartpole](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-balance.gif)

## âœï¸ Pratimai: Politikos gradientai ir aktoriaus-kritiko RL

TÄ™skite mokymÄ…si Å¡iuose uÅ¾raÅ¡Å³ knygelÄ—se:

* [RL su TensorFlow](CartPole-RL-TF.ipynb)
* [RL su PyTorch](CartPole-RL-PyTorch.ipynb)

## Kiti RL uÅ¾daviniai

Stiprinamasis mokymasis Å¡iandien yra sparÄiai auganti tyrimÅ³ sritis. Keletas Ä¯domiÅ³ stiprinamojo mokymosi pavyzdÅ¾iÅ³:

* Kompiuterio mokymas Å¾aisti **Atari Å¾aidimus**. Å io uÅ¾davinio sudÄ—tingumas yra tas, kad neturime paprastos bÅ«senos, pateiktos kaip vektorius, o turime ekrano nuotraukÄ… - ir turime naudoti CNN, kad konvertuotume Å¡Ä¯ ekrano vaizdÄ… Ä¯ poÅ¾ymiÅ³ vektoriÅ³ arba iÅ¡gautume atlygio informacijÄ…. Atari Å¾aidimai yra prieinami Gym.
* Kompiuterio mokymas Å¾aisti stalo Å¾aidimus, tokius kaip Å¡achmatai ir Go. Neseniai paÅ¾angiausios programos, tokios kaip **Alpha Zero**, buvo apmokytos nuo nulio, kai du agentai Å¾aidÄ— vienas prieÅ¡ kitÄ… ir tobulÄ—jo kiekviename Å¾ingsnyje.
* PramonÄ—je RL naudojamas kurti valdymo sistemas iÅ¡ simuliacijos. Paslauga, vadinama [Bonsai](https://azure.microsoft.com/services/project-bonsai/?WT.mc_id=academic-77998-cacaste), yra specialiai tam sukurta.

## IÅ¡vada

Dabar iÅ¡mokome treniruoti agentus, kad jie pasiektÅ³ gerÅ³ rezultatÅ³, tiesiog suteikdami jiems atlygio funkcijÄ…, apibrÄ—Å¾ianÄiÄ… norimÄ… Å¾aidimo bÅ«senÄ…, ir suteikdami jiems galimybÄ™ protingai tyrinÄ—ti paieÅ¡kos erdvÄ™. SÄ—kmingai iÅ¡bandÄ—me du algoritmus ir pasiekÄ—me gerÄ… rezultatÄ… per palyginti trumpÄ… laikÄ…. TaÄiau tai tik jÅ«sÅ³ kelionÄ—s Ä¯ RL pradÅ¾ia, ir tikrai turÄ—tumÄ—te apsvarstyti galimybÄ™ lankyti atskirÄ… kursÄ…, jei norite gilintis.

## ğŸš€ IÅ¡Å¡Å«kis

IÅ¡tyrinÄ—kite taikymus, iÅ¡vardytus skiltyje â€Kiti RL uÅ¾daviniaiâ€œ, ir pabandykite Ä¯gyvendinti vienÄ…!

## [Po paskaitos testas](https://ff-quizzes.netlify.app/en/ai/quiz/44)

## ApÅ¾valga ir savarankiÅ¡kas mokymasis

SuÅ¾inokite daugiau apie klasikinÄ¯ stiprinamÄ…jÄ¯ mokymÄ…si mÅ«sÅ³ [MaÅ¡ininio mokymosi pradedantiesiems programoje](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/README.md).

Å½iÅ«rÄ—kite [puikÅ³ vaizdo Ä¯raÅ¡Ä…](https://www.youtube.com/watch?v=qv6UVOQ0F44), kuriame pasakojama, kaip kompiuteris gali iÅ¡mokti Å¾aisti Super Mario.

## UÅ¾duotis: [Treniruokite kalnÅ³ automobilÄ¯](lab/README.md)

JÅ«sÅ³ tikslas Å¡ios uÅ¾duoties metu bÅ«tÅ³ treniruoti kitÄ… Gym aplinkÄ… - [Mountain Car](https://www.gymlibrary.ml/environments/classic_control/mountain_car/).

---

