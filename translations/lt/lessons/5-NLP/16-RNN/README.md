<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "58bf4adb210aab53e8f78c8082040e7c",
  "translation_date": "2025-08-31T18:00:07+00:00",
  "source_file": "lessons/5-NLP/16-RNN/README.md",
  "language_code": "lt"
}
-->
# Rekurentiniai neuroniniai tinklai

## [PrieÅ¡ paskaitÄ…: testas](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/116)

AnkstesnÄ—se dalyse naudojome turtingas semantines tekstÅ³ reprezentacijas ir paprastÄ… linijinÄ¯ klasifikatoriÅ³ virÅ¡ Ä¯terpimÅ³. Å i architektÅ«ra padeda uÅ¾fiksuoti bendrÄ… Å¾odÅ¾iÅ³ prasmÄ™ sakinyje, taÄiau ji neatsiÅ¾velgia Ä¯ Å¾odÅ¾iÅ³ **tvarkÄ…**, nes Ä¯terpimÅ³ agregavimo operacija paÅ¡alina Å¡iÄ… informacijÄ… iÅ¡ originalaus teksto. Kadangi Å¡ie modeliai negali modeliuoti Å¾odÅ¾iÅ³ tvarkos, jie negali sprÄ™sti sudÄ—tingesniÅ³ ar dviprasmiÅ¡kÅ³ uÅ¾duoÄiÅ³, tokiÅ³ kaip teksto generavimas ar klausimÅ³ atsakymas.

NorÄ—dami uÅ¾fiksuoti teksto sekos prasmÄ™, turime naudoti kitÄ… neuroniniÅ³ tinklÅ³ architektÅ«rÄ…, vadinamÄ… **rekurentiniais neuroniniais tinklais** arba RNN. RNN tinklu sakinÄ¯ perduodame po vienÄ… simbolÄ¯, o tinklas generuoja tam tikrÄ… **bÅ«senÄ…**, kuriÄ… vÄ—liau perduodame tinklui kartu su kitu simboliu.

![RNN](../../../../../translated_images/rnn.27f5c29c53d727b546ad3961637a267f0fe9ec5ab01f2a26a853c92fcefbb574.lt.png)

> Vaizdas sukurtas autoriaus

Turint Ä¯vesties sekÄ… X<sub>0</sub>,...,X<sub>n</sub>, RNN sukuria neuroniniÅ³ tinklÅ³ blokÅ³ sekÄ… ir treniruoja Å¡iÄ… sekÄ… nuo pradÅ¾ios iki pabaigos naudojant atgalinÄ™ sklaidÄ…. Kiekvienas tinklo blokas kaip Ä¯vestÄ¯ gauna porÄ… (X<sub>i</sub>,S<sub>i</sub>) ir generuoja S<sub>i+1</sub> kaip rezultatÄ…. GalutinÄ— bÅ«sena S<sub>n</sub> arba (iÅ¡vestis Y<sub>n</sub>) perduodama linijiniam klasifikatoriui, kad bÅ«tÅ³ gautas rezultatas. Visi tinklo blokai dalijasi tais paÄiais svoriais ir yra treniruojami nuo pradÅ¾ios iki pabaigos per vienÄ… atgalinÄ—s sklaidos ciklÄ….

Kadangi bÅ«senos vektoriai S<sub>0</sub>,...,S<sub>n</sub> perduodami per tinklÄ…, jis gali iÅ¡mokti sekos priklausomybes tarp Å¾odÅ¾iÅ³. PavyzdÅ¾iui, kai Å¾odis *ne* pasirodo kaÅ¾kur sekoje, tinklas gali iÅ¡mokti paneigti tam tikrus bÅ«senos vektoriaus elementus, sukeldamas neigimÄ….

> âœ… Kadangi visÅ³ RNN blokÅ³ svoriai aukÅ¡Äiau pateiktame paveikslÄ—lyje yra bendri, tas pats paveikslÄ—lis gali bÅ«ti pavaizduotas kaip vienas blokas (deÅ¡inÄ—je) su rekurentine grÄ¯Å¾tamojo ryÅ¡io kilpa, kuri perduoda tinklo iÅ¡vesties bÅ«senÄ… atgal Ä¯ Ä¯vestÄ¯.

## RNN lÄ…stelÄ—s anatomija

PaÅ¾velkime, kaip organizuota paprasta RNN lÄ…stelÄ—. Ji priima ankstesnÄ™ bÅ«senÄ… S<sub>i-1</sub> ir dabartinÄ¯ simbolÄ¯ X<sub>i</sub> kaip Ä¯vestis ir turi generuoti iÅ¡vesties bÅ«senÄ… S<sub>i</sub> (kartais mus taip pat domina kita iÅ¡vestis Y<sub>i</sub>, kaip generatyviniuose tinkluose).

Paprasta RNN lÄ…stelÄ— turi du svorio matricas viduje: viena transformuoja Ä¯vesties simbolÄ¯ (vadinkime jÄ… W), o kita transformuoja Ä¯vesties bÅ«senÄ… (H). Tokiu atveju tinklo iÅ¡vestis apskaiÄiuojama kaip Ïƒ(WÃ—X<sub>i</sub>+HÃ—S<sub>i-1</sub>+b), kur Ïƒ yra aktyvacijos funkcija, o b yra papildomas poslinkis.

<img alt="RNN lÄ…stelÄ—s anatomija" src="images/rnn-anatomy.png" width="50%"/>

> Vaizdas sukurtas autoriaus

Daugeliu atvejÅ³ Ä¯vesties simboliai prieÅ¡ patekdami Ä¯ RNN perduodami per Ä¯terpimo sluoksnÄ¯, kad sumaÅ¾Ä—tÅ³ dimensija. Tokiu atveju, jei Ä¯vesties vektoriÅ³ dimensija yra *emb_size*, o bÅ«senos vektorius yra *hid_size*, W dydis yra *emb_size*Ã—*hid_size*, o H dydis yra *hid_size*Ã—*hid_size*.

## IlgalaikÄ— trumpalaikÄ— atmintis (LSTM)

Viena pagrindiniÅ³ klasikiniÅ³ RNN problemÅ³ yra vadinamoji **nykstanÄiÅ³ gradientÅ³** problema. Kadangi RNN treniruojami nuo pradÅ¾ios iki pabaigos per vienÄ… atgalinÄ—s sklaidos ciklÄ…, jiems sunku perduoti klaidÄ… Ä¯ pirmuosius tinklo sluoksnius, todÄ—l tinklas negali iÅ¡mokti ryÅ¡iÅ³ tarp tolimÅ³ simboliÅ³. Vienas iÅ¡ bÅ«dÅ³ iÅ¡vengti Å¡ios problemos yra Ä¯vesti **aiÅ¡kÅ³ bÅ«senos valdymÄ…** naudojant vadinamuosius **vartus**. Yra dvi gerai Å¾inomos tokios architektÅ«ros: **ilgalaikÄ— trumpalaikÄ— atmintis** (LSTM) ir **vartÅ³ relÄ—s vienetas** (GRU).

![Vaizdas, rodantis ilgalaikÄ—s trumpalaikÄ—s atminties lÄ…stelÄ—s pavyzdÄ¯](../../../../../lessons/5-NLP/16-RNN/images/long-short-term-memory-cell.svg)

> Vaizdo Å¡altinis TBD

LSTM tinklas organizuotas panaÅ¡iai kaip RNN, taÄiau yra dvi bÅ«senos, kurios perduodamos iÅ¡ sluoksnio Ä¯ sluoksnÄ¯: faktinÄ— bÅ«sena C ir paslÄ—ptas vektorius H. Kiekviename vienete paslÄ—ptas vektorius H<sub>i</sub> sujungiamas su Ä¯vestimi X<sub>i</sub>, ir jie kontroliuoja, kas vyksta su bÅ«sena C per **vartus**. Kiekvienas vartas yra neuroninis tinklas su sigmoidine aktyvacija (iÅ¡vestis diapazone [0,1]), kuris gali bÅ«ti laikomas bitÅ³ kauke, kai dauginamas iÅ¡ bÅ«senos vektoriaus. Yra Å¡ie vartai (iÅ¡ kairÄ—s Ä¯ deÅ¡inÄ™ paveikslÄ—lyje aukÅ¡Äiau):

* **UÅ¾marÅ¡umo vartai** priima paslÄ—ptÄ… vektoriÅ³ ir nustato, kuriuos vektoriaus C komponentus reikia pamirÅ¡ti, o kuriuos perduoti.
* **Ä®vesties vartai** paima tam tikrÄ… informacijÄ… iÅ¡ Ä¯vesties ir paslÄ—ptÅ³ vektoriÅ³ ir Ä¯terpia jÄ… Ä¯ bÅ«senÄ….
* **IÅ¡vesties vartai** transformuoja bÅ«senÄ… per linijinÄ¯ sluoksnÄ¯ su *tanh* aktyvacija, tada pasirenka kai kuriuos jos komponentus naudodami paslÄ—ptÄ… vektoriÅ³ H<sub>i</sub>, kad generuotÅ³ naujÄ… bÅ«senÄ… C<sub>i+1</sub>.

BÅ«senos C komponentai gali bÅ«ti laikomi tam tikrais Å¾enklais, kuriuos galima Ä¯jungti ir iÅ¡jungti. PavyzdÅ¾iui, kai sekoje sutinkame vardÄ… *Alisa*, galime manyti, kad jis nurodo moteriÅ¡kÄ… veikÄ—jÄ…, ir pakelti Å¾enklÄ… bÅ«senoje, kad sakinyje turime moteriÅ¡kÄ… daiktavardÄ¯. Kai toliau sutinkame frazÄ™ *ir Tomas*, pakelsime Å¾enklÄ…, kad turime daugiskaitinÄ¯ daiktavardÄ¯. Taigi manipuliuodami bÅ«sena galime sekti sakinio daliÅ³ gramatines savybes.

> âœ… Puikus Å¡altinis, padedantis suprasti LSTM vidinÄ™ struktÅ«rÄ…, yra Å¡is puikus straipsnis [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) Christopher Olah.

## DvikrypÄiai ir daugiasluoksniai RNN

AptarÄ—me rekurentinius tinklus, kurie veikia viena kryptimi, nuo sekos pradÅ¾ios iki pabaigos. Tai atrodo natÅ«ralu, nes primena, kaip skaitome ir klausome kalbos. TaÄiau, kadangi daugeliu praktiniÅ³ atvejÅ³ turime atsitiktinÄ™ prieigÄ… prie Ä¯vesties sekos, gali bÅ«ti prasminga vykdyti rekursinÄ¯ skaiÄiavimÄ… abiem kryptimis. Tokie tinklai vadinami **dvikrypÄiais** RNN. Dirbant su dvikrypÄiu tinklu, mums reikÄ—s dviejÅ³ paslÄ—ptÅ³ bÅ«senos vektoriÅ³, po vienÄ… kiekvienai krypÄiai.

Rekurentinis tinklas, nesvarbu, ar vienkryptis, ar dvikryptis, uÅ¾fiksuoja tam tikrus sekos modelius ir gali juos saugoti bÅ«senos vektoriuje arba perduoti Ä¯ iÅ¡vestÄ¯. Kaip ir konvoliuciniuose tinkluose, galime sukurti kitÄ… rekursinÄ¯ sluoksnÄ¯ virÅ¡ pirmojo, kad uÅ¾fiksuotume aukÅ¡tesnio lygio modelius ir sukurtume iÅ¡ Å¾emo lygio modeliÅ³, kuriuos iÅ¡traukÄ— pirmasis sluoksnis. Tai veda mus prie **daugiasluoksnio RNN** sÄ…vokos, kuriÄ… sudaro du ar daugiau rekursiniÅ³ tinklÅ³, kur ankstesnio sluoksnio iÅ¡vestis perduodama kitam sluoksniui kaip Ä¯vestis.

![Vaizdas, rodantis daugiasluoksnÄ¯ ilgalaikÄ—s trumpalaikÄ—s atminties RNN](../../../../../translated_images/multi-layer-lstm.dd975e29bb2a59fe58b429db833932d734c81f211cad2783797a9608984acb8c.lt.jpg)

*Paveikslas iÅ¡ [Å¡io nuostabaus Ä¯raÅ¡o](https://towardsdatascience.com/from-a-lstm-cell-to-a-multilayer-lstm-network-with-pytorch-2899eb5696f3) Fernando LÃ³pez*

## âœï¸ Pratimai: Ä®terpimai

TÄ™skite mokymÄ…si Å¡iuose uÅ¾raÅ¡Å³ knygelÄ—se:

* [RNN su PyTorch](RNNPyTorch.ipynb)
* [RNN su TensorFlow](RNNTF.ipynb)

## IÅ¡vada

Å iame skyriuje matÄ—me, kad RNN gali bÅ«ti naudojami sekÅ³ klasifikavimui, taÄiau iÅ¡ tiesÅ³ jie gali atlikti daug daugiau uÅ¾duoÄiÅ³, tokiÅ³ kaip teksto generavimas, maÅ¡ininis vertimas ir kt. Å ias uÅ¾duotis aptarsime kitame skyriuje.

## ğŸš€ IÅ¡Å¡Å«kis

Perskaitykite literatÅ«rÄ… apie LSTM ir apsvarstykite jÅ³ taikymÄ…:

- [Grid Long Short-Term Memory](https://arxiv.org/pdf/1507.01526v1.pdf)
- [Show, Attend and Tell: Neural Image Caption
Generation with Visual Attention](https://arxiv.org/pdf/1502.03044v2.pdf)

## [Po paskaitos: testas](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/216)

## ApÅ¾valga ir savarankiÅ¡kas mokymasis

- [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) Christopher Olah.

## [UÅ¾duotis: UÅ¾raÅ¡Å³ knygelÄ—s](assignment.md)

---

**AtsakomybÄ—s apribojimas**:  
Å is dokumentas buvo iÅ¡verstas naudojant AI vertimo paslaugÄ… [Co-op Translator](https://github.com/Azure/co-op-translator). Nors siekiame tikslumo, praÅ¡ome atkreipti dÄ—mesÄ¯, kad automatiniai vertimai gali turÄ—ti klaidÅ³ ar netikslumÅ³. Originalus dokumentas jo gimtÄ…ja kalba turÄ—tÅ³ bÅ«ti laikomas autoritetingu Å¡altiniu. Kritinei informacijai rekomenduojama naudoti profesionalÅ³ Å¾mogaus vertimÄ…. Mes neprisiimame atsakomybÄ—s uÅ¾ nesusipratimus ar klaidingus interpretavimus, atsiradusius dÄ—l Å¡io vertimo naudojimo.