# Rekurentiniai neuroniniai tinklai

## [PrieÅ¡ paskaitÄ…: testas](https://ff-quizzes.netlify.app/en/ai/quiz/31)

AnkstesnÄ—se dalyse naudojome turtingas semantines teksto reprezentacijas ir paprastÄ… linijinÄ¯ klasifikatoriÅ³ virÅ¡ Ä¯terpimÅ³. Å i architektÅ«ra padeda uÅ¾fiksuoti bendrÄ… Å¾odÅ¾iÅ³ prasmÄ™ sakinyje, taÄiau ji neatsiÅ¾velgia Ä¯ **Å¾odÅ¾iÅ³ tvarkÄ…**, nes agregavimo operacija virÅ¡ Ä¯terpimÅ³ paÅ¡alina Å¡iÄ… informacijÄ… iÅ¡ originalaus teksto. Kadangi Å¡ie modeliai negali modeliuoti Å¾odÅ¾iÅ³ tvarkos, jie negali sprÄ™sti sudÄ—tingesniÅ³ ar dviprasmiÅ¡kÅ³ uÅ¾duoÄiÅ³, tokiÅ³ kaip teksto generavimas ar klausimÅ³ atsakymas.

Norint uÅ¾fiksuoti teksto sekos prasmÄ™, reikia naudoti kitÄ… neuroninio tinklo architektÅ«rÄ…, vadinamÄ… **rekurentiniais neuroniniais tinklais** (RNN). RNN tinkluose sakinÄ¯ perduodame per tinklÄ… po vienÄ… simbolÄ¯, o tinklas generuoja tam tikrÄ… **bÅ«senÄ…**, kuriÄ… vÄ—liau perduodame tinklui kartu su kitu simboliu.

![RNN](../../../../../translated_images/lt/rnn.27f5c29c53d727b5.webp)

> Vaizdas sukurtas autoriaus

Turint Ä¯vesties sekÄ… X<sub>0</sub>,...,X<sub>n</sub>, RNN sukuria neuroniniÅ³ tinklÅ³ blokÅ³ sekÄ… ir treniruoja Å¡iÄ… sekÄ… nuo pradÅ¾ios iki pabaigos naudojant atgalinÄ™ sklaidÄ…. Kiekvienas tinklo blokas priima porÄ… (X<sub>i</sub>,S<sub>i</sub>) kaip Ä¯vestÄ¯ ir generuoja S<sub>i+1</sub> kaip rezultatÄ…. GalutinÄ— bÅ«sena S<sub>n</sub> arba (iÅ¡vestis Y<sub>n</sub>) perduodama linijiniam klasifikatoriui, kad bÅ«tÅ³ gautas rezultatas. Visi tinklo blokai dalijasi tais paÄiais svoriais ir yra treniruojami nuo pradÅ¾ios iki pabaigos vienu atgalinÄ—s sklaidos etapu.

Kadangi bÅ«senos vektoriai S<sub>0</sub>,...,S<sub>n</sub> perduodami per tinklÄ…, jis gali iÅ¡mokti sekos priklausomybes tarp Å¾odÅ¾iÅ³. PavyzdÅ¾iui, kai Å¾odis *ne* pasirodo kaÅ¾kur sekoje, tinklas gali iÅ¡mokti paneigti tam tikrus elementus bÅ«senos vektoriuje, sukeldamas neigimÄ….

> âœ… Kadangi visÅ³ RNN blokÅ³ svoriai aukÅ¡Äiau pateiktame paveikslÄ—lyje yra bendri, tas pats paveikslÄ—lis gali bÅ«ti pateiktas kaip vienas blokas (deÅ¡inÄ—je) su rekurentine grÄ¯Å¾tamojo ryÅ¡io kilpa, kuri perduoda tinklo iÅ¡vesties bÅ«senÄ… atgal Ä¯ Ä¯vestÄ¯.

## RNN lÄ…stelÄ—s anatomija

PaÅ¾velkime, kaip organizuota paprasta RNN lÄ…stelÄ—. Ji priima ankstesnÄ™ bÅ«senÄ… S<sub>i-1</sub> ir dabartinÄ¯ simbolÄ¯ X<sub>i</sub> kaip Ä¯vestis ir turi generuoti iÅ¡vesties bÅ«senÄ… S<sub>i</sub> (kartais mus taip pat domina kita iÅ¡vestis Y<sub>i</sub>, kaip generatyviniuose tinkluose).

Paprasta RNN lÄ…stelÄ— turi du svorio matricas viduje: viena transformuoja Ä¯vesties simbolÄ¯ (vadinkime jÄ… W), o kita transformuoja Ä¯vesties bÅ«senÄ… (H). Tokiu atveju tinklo iÅ¡vestis apskaiÄiuojama kaip &sigma;(W&times;X<sub>i</sub>+H&times;S<sub>i-1</sub>+b), kur &sigma; yra aktyvavimo funkcija, o b yra papildomas poslinkis.

<img alt="RNN lÄ…stelÄ—s anatomija" src="../../../../../translated_images/lt/rnn-anatomy.79ee3f3920b3294b.webp" width="50%"/>

> Vaizdas sukurtas autoriaus

Daugeliu atvejÅ³ Ä¯vesties simboliai perduodami per Ä¯terpimo sluoksnÄ¯ prieÅ¡ patenkant Ä¯ RNN, kad sumaÅ¾Ä—tÅ³ dimensija. Tokiu atveju, jei Ä¯vesties vektoriÅ³ dimensija yra *emb_size*, o bÅ«senos vektorius yra *hid_size*, W dydis yra *emb_size*&times;*hid_size*, o H dydis yra *hid_size*&times;*hid_size*.

## IlgalaikÄ— trumpalaikÄ— atmintis (LSTM)

Viena pagrindiniÅ³ klasikiniÅ³ RNN problemÅ³ yra vadinamoji **nykstanÄiÅ³ gradientÅ³** problema. Kadangi RNN treniruojami nuo pradÅ¾ios iki pabaigos vienu atgalinÄ—s sklaidos etapu, jiems sunku perduoti klaidÄ… Ä¯ pirmuosius tinklo sluoksnius, todÄ—l tinklas negali iÅ¡mokti ryÅ¡iÅ³ tarp tolimÅ³ simboliÅ³. Vienas iÅ¡ bÅ«dÅ³ iÅ¡vengti Å¡ios problemos yra Ä¯vesti **aiÅ¡kÅ³ bÅ«senos valdymÄ…** naudojant vadinamuosius **vartus**. Yra dvi gerai Å¾inomos tokios architektÅ«ros: **IlgalaikÄ— trumpalaikÄ— atmintis** (LSTM) ir **VartÅ³ relÄ—s vienetas** (GRU).

![Vaizdas, rodantis ilgalaikÄ—s trumpalaikÄ—s atminties lÄ…stelÄ—s pavyzdÄ¯](../../../../../lessons/5-NLP/16-RNN/images/long-short-term-memory-cell.svg)

> Vaizdo Å¡altinis TBD

LSTM tinklas organizuotas panaÅ¡iai kaip RNN, taÄiau yra dvi bÅ«senos, kurios perduodamos iÅ¡ sluoksnio Ä¯ sluoksnÄ¯: faktinÄ— bÅ«sena C ir paslÄ—ptas vektorius H. Kiekviename vienete paslÄ—ptas vektorius H<sub>i</sub> sujungiamas su Ä¯vestimi X<sub>i</sub>, ir jie kontroliuoja, kas vyksta su bÅ«sena C per **vartus**. Kiekvienas vartas yra neuroninis tinklas su sigmoidine aktyvacija (iÅ¡vestis diapazone [0,1]), kuris gali bÅ«ti laikomas bitÅ³ kauke, kai dauginamas iÅ¡ bÅ«senos vektoriaus. Yra Å¡ie vartai (iÅ¡ kairÄ—s Ä¯ deÅ¡inÄ™ paveikslÄ—lyje aukÅ¡Äiau):

* **PamirÅ¡imo vartai** priima paslÄ—ptÄ… vektoriÅ³ ir nustato, kuriuos vektoriaus C komponentus reikia pamirÅ¡ti, o kuriuos perduoti.
* **Ä®vesties vartai** paima tam tikrÄ… informacijÄ… iÅ¡ Ä¯vesties ir paslÄ—ptÅ³ vektoriÅ³ ir Ä¯terpia jÄ… Ä¯ bÅ«senÄ….
* **IÅ¡vesties vartai** transformuoja bÅ«senÄ… per linijinÄ¯ sluoksnÄ¯ su *tanh* aktyvacija, tada pasirenka kai kuriuos jo komponentus naudodami paslÄ—ptÄ… vektoriÅ³ H<sub>i</sub>, kad generuotÅ³ naujÄ… bÅ«senÄ… C<sub>i+1</sub>.

BÅ«senos C komponentai gali bÅ«ti laikomi tam tikrais Å¾enklais, kuriuos galima Ä¯jungti ir iÅ¡jungti. PavyzdÅ¾iui, kai sekoje sutinkame vardÄ… *Alisa*, galime manyti, kad tai moteriÅ¡kas personaÅ¾as, ir Ä¯jungti Å¾enklÄ… bÅ«senoje, kad sakinyje yra moteriÅ¡kas daiktavardis. Kai toliau sutinkame frazÄ™ *ir Tomas*, Ä¯jungsime Å¾enklÄ…, kad turime daugiskaitinÄ¯ daiktavardÄ¯. Taigi manipuliuodami bÅ«sena galime sekti sakinio daliÅ³ gramatines savybes.

> âœ… Puikus Å¡altinis, padedantis suprasti LSTM vidinÄ™ struktÅ«rÄ…, yra Å¡is puikus straipsnis [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) Christopher Olah.

## DvikrypÄiai ir daugiasluoksniai RNN

AptarÄ—me rekurentinius tinklus, kurie veikia viena kryptimi, nuo sekos pradÅ¾ios iki pabaigos. Tai atrodo natÅ«ralu, nes primena, kaip skaitome ir klausome kalbos. TaÄiau, kadangi daugeliu praktiniÅ³ atvejÅ³ turime atsitiktinÄ™ prieigÄ… prie Ä¯vesties sekos, gali bÅ«ti prasminga vykdyti rekurentinÄ¯ skaiÄiavimÄ… abiem kryptimis. Tokie tinklai vadinami **dvikrypÄiais** RNN. Dirbant su dvikrypÄiu tinklu, mums reikÄ—s dviejÅ³ paslÄ—ptÅ³ bÅ«senos vektoriÅ³, po vienÄ… kiekvienai krypÄiai.

Rekurentinis tinklas, nesvarbu, ar vienkryptis, ar dvikryptis, uÅ¾fiksuoja tam tikrus sekos modelius ir gali juos saugoti bÅ«senos vektoriuje arba perduoti Ä¯ iÅ¡vestÄ¯. Kaip ir konvoliuciniuose tinkluose, galime sukurti kitÄ… rekurentinÄ¯ sluoksnÄ¯ virÅ¡ pirmojo, kad uÅ¾fiksuotume aukÅ¡tesnio lygio modelius ir sukurtume iÅ¡ Å¾emo lygio modeliÅ³, kuriuos iÅ¡traukÄ— pirmasis sluoksnis. Tai veda mus prie **daugiasluoksnio RNN** sÄ…vokos, kuriÄ… sudaro du ar daugiau rekurentiniÅ³ tinklÅ³, kur ankstesnio sluoksnio iÅ¡vestis perduodama kitam sluoksniui kaip Ä¯vestis.

![Vaizdas, rodantis daugiasluoksnÄ¯ ilgalaikÄ—s trumpalaikÄ—s atminties RNN](../../../../../translated_images/lt/multi-layer-lstm.dd975e29bb2a59fe.webp)

*Paveikslas iÅ¡ [Å¡io puikaus Ä¯raÅ¡o](https://towardsdatascience.com/from-a-lstm-cell-to-a-multilayer-lstm-network-with-pytorch-2899eb5696f3) Fernando LÃ³pez*

## âœï¸ Pratimai: Ä®terpimai

TÄ™skite mokymÄ…si Å¡iuose uÅ¾raÅ¡Å³ knygelÄ—se:

* [RNN su PyTorch](RNNPyTorch.ipynb)
* [RNN su TensorFlow](RNNTF.ipynb)

## IÅ¡vada

Å iame skyriuje matÄ—me, kad RNN gali bÅ«ti naudojami sekÅ³ klasifikavimui, taÄiau iÅ¡ tiesÅ³ jie gali atlikti daug daugiau uÅ¾duoÄiÅ³, tokiÅ³ kaip teksto generavimas, maÅ¡ininis vertimas ir kt. Å ias uÅ¾duotis aptarsime kitame skyriuje.

## ğŸš€ IÅ¡Å¡Å«kis

Perskaitykite literatÅ«rÄ… apie LSTM ir apsvarstykite jÅ³ taikymÄ…:

- [Grid Long Short-Term Memory](https://arxiv.org/pdf/1507.01526v1.pdf)
- [Show, Attend and Tell: Neural Image Caption
Generation with Visual Attention](https://arxiv.org/pdf/1502.03044v2.pdf)

## [Po paskaitos: testas](https://ff-quizzes.netlify.app/en/ai/quiz/32)

## ApÅ¾valga ir savarankiÅ¡kas mokymasis

- [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) Christopher Olah.

## [UÅ¾duotis: UÅ¾raÅ¡Å³ knygelÄ—s](assignment.md)

---

