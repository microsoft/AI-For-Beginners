<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "31b46ba1f3aa78578134d4829f88be53",
  "translation_date": "2025-08-31T17:56:25+00:00",
  "source_file": "lessons/5-NLP/15-LanguageModeling/README.md",
  "language_code": "lt"
}
-->
# Kalbos modeliavimas

Semantiniai Ä¯terpiniai, tokie kaip Word2Vec ir GloVe, iÅ¡ tiesÅ³ yra pirmasis Å¾ingsnis link **kalbos modeliavimo** â€“ modeliÅ³ kÅ«rimo, kurie kaÅ¾kaip *supranta* (arba *atspindi*) kalbos prigimtÄ¯.

## [PrieÅ¡ paskaitÄ… vykdomas testas](https://ff-quizzes.netlify.app/en/ai/quiz/29)

PagrindinÄ— kalbos modeliavimo idÄ—ja yra jÅ³ mokymas su nepaÅ¾ymÄ—tais duomenÅ³ rinkiniais nesupervizuotu bÅ«du. Tai svarbu, nes turime didÅ¾iulius kiekius nepaÅ¾ymÄ—to teksto, tuo tarpu paÅ¾ymÄ—to teksto kiekis visada bus ribotas dÄ—l pastangÅ³, reikalingÅ³ jÄ¯ paÅ¾ymÄ—ti. DaÅ¾niausiai galime kurti kalbos modelius, kurie gali **prognozuoti trÅ«kstamus Å¾odÅ¾ius** tekste, nes lengva atsitiktinai uÅ¾maskuoti Å¾odÄ¯ tekste ir naudoti jÄ¯ kaip mokymo pavyzdÄ¯.

## Ä®terpiniÅ³ mokymas

Ankstesniuose pavyzdÅ¾iuose naudojome iÅ¡ anksto apmokytus semantinius Ä¯terpinius, taÄiau Ä¯domu pamatyti, kaip Å¡iuos Ä¯terpinius galima apmokyti. Yra keletas galimÅ³ idÄ—jÅ³, kurias galima naudoti:

* **N-Gram** kalbos modeliavimas, kai prognozuojame Å¾odÄ¯, remdamiesi N ankstesniais Å¾odÅ¾iais (N-grama).
* **Nuolatinis Å¾odÅ¾iÅ³ maiÅ¡as** (CBoW), kai prognozuojame vidurinÄ¯ Å¾odÄ¯ $W_0$ Å¾odÅ¾iÅ³ sekoje $W_{-N}$, ..., $W_N$.
* **Skip-gram**, kur prognozuojame kaimyniniÅ³ Å¾odÅ¾iÅ³ rinkinÄ¯ {$W_{-N},\dots, W_{-1}, W_1,\dots, W_N$} iÅ¡ vidurinio Å¾odÅ¾io $W_0$.

![vaizdas iÅ¡ straipsnio apie Å¾odÅ¾iÅ³ konvertavimÄ… Ä¯ vektorius](../../../../../translated_images/example-algorithms-for-converting-words-to-vectors.fbe9207a726922f6f0f5de66427e8a6eda63809356114e28fb1fa5f4a83ebda7.lt.png)

> Vaizdas iÅ¡ [Å¡io straipsnio](https://arxiv.org/pdf/1301.3781.pdf)

## âœï¸ Pavyzdiniai uÅ¾raÅ¡ai: CBoW modelio mokymas

TÄ™skite mokymÄ…si Å¡iuose uÅ¾raÅ¡uose:

* [CBoW Word2Vec mokymas naudojant TensorFlow](CBoW-TF.ipynb)
* [CBoW Word2Vec mokymas naudojant PyTorch](CBoW-PyTorch.ipynb)

## IÅ¡vada

AnkstesnÄ—je pamokoje matÄ—me, kad Å¾odÅ¾iÅ³ Ä¯terpiniai veikia kaip magija! Dabar Å¾inome, kad Å¾odÅ¾iÅ³ Ä¯terpiniÅ³ mokymas nÄ—ra labai sudÄ—tinga uÅ¾duotis, ir prireikus turÄ—tume sugebÄ—ti apmokyti savo Å¾odÅ¾iÅ³ Ä¯terpinius specifiniam tekstui.

## [Po paskaitos vykdomas testas](https://ff-quizzes.netlify.app/en/ai/quiz/30)

## ApÅ¾valga ir savarankiÅ¡kas mokymasis

* [Oficialus PyTorch kalbos modeliavimo mokymo vadovas](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html).
* [Oficialus TensorFlow vadovas apie Word2Vec modelio mokymÄ…](https://www.TensorFlow.org/tutorials/text/word2vec).
* Naudojant **gensim** sistemÄ…, daÅ¾niausiai naudojamÅ³ Ä¯terpiniÅ³ mokymas keliose kodo eilutÄ—se apraÅ¡ytas [Å¡ioje dokumentacijoje](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html).

## ğŸš€ [UÅ¾duotis: Skip-Gram modelio mokymas](lab/README.md)

Laboratorijoje kvieÄiame jus modifikuoti Å¡ios pamokos kodÄ…, kad apmokytumÄ—te Skip-Gram modelÄ¯ vietoj CBoW. [Skaitykite detales](lab/README.md)

---

**AtsakomybÄ—s apribojimas**:  
Å is dokumentas buvo iÅ¡verstas naudojant AI vertimo paslaugÄ… [Co-op Translator](https://github.com/Azure/co-op-translator). Nors siekiame tikslumo, praÅ¡ome atkreipti dÄ—mesÄ¯, kad automatiniai vertimai gali turÄ—ti klaidÅ³ ar netikslumÅ³. Originalus dokumentas jo gimtÄ…ja kalba turÄ—tÅ³ bÅ«ti laikomas autoritetingu Å¡altiniu. Kritinei informacijai rekomenduojama profesionali Å¾mogaus vertimo paslauga. Mes neprisiimame atsakomybÄ—s uÅ¾ nesusipratimus ar klaidingus interpretavimus, atsiradusius naudojant Å¡Ä¯ vertimÄ….