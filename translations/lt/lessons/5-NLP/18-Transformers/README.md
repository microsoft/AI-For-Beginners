# DÄ—mesio mechanizmai ir transformatoriai

## [PrieÅ¡ paskaitÄ… testas](https://ff-quizzes.netlify.app/en/ai/quiz/35)

Viena svarbiausiÅ³ problemÅ³ NLP srityje yra **maÅ¡ininis vertimas**, esminÄ— uÅ¾duotis, kuri yra tokiÅ³ Ä¯rankiÅ³ kaip Google Translate pagrindas. Å ioje dalyje mes sutelksime dÄ—mesÄ¯ Ä¯ maÅ¡ininÄ¯ vertimÄ… arba, plaÄiau kalbant, Ä¯ bet kokiÄ… *sekos Ä¯ sekÄ…* uÅ¾duotÄ¯ (dar vadinamÄ… **sakiniÅ³ transformacija**).

Naudojant RNN, sekos Ä¯ sekÄ… uÅ¾duotis Ä¯gyvendinama naudojant du rekursinius tinklus, kur vienas tinklas, **koduotojas**, suspaudÅ¾ia Ä¯vesties sekÄ… Ä¯ paslÄ—ptÄ… bÅ«senÄ…, o kitas tinklas, **dekoduotojas**, iÅ¡skleidÅ¾ia Å¡iÄ… paslÄ—ptÄ… bÅ«senÄ… Ä¯ iÅ¡verstÄ… rezultatÄ…. Å is metodas turi keletÄ… problemÅ³:

* GalutinÄ— koduotojo tinklo bÅ«sena sunkiai prisimena sakinio pradÅ¾iÄ…, todÄ—l modelio kokybÄ— ilgiems sakiniams yra prasta.
* Visi Å¾odÅ¾iai sekoje turi vienodÄ… poveikÄ¯ rezultatui. TaÄiau realybÄ—je tam tikri Å¾odÅ¾iai Ä¯vesties sekoje daÅ¾nai turi didesnÄ¯ poveikÄ¯ iÅ¡vesties rezultatams nei kiti.

**DÄ—mesio mechanizmai** suteikia galimybÄ™ Ä¯vertinti kiekvieno Ä¯vesties vektoriaus kontekstinÄ¯ poveikÄ¯ kiekvienai RNN iÅ¡vesties prognozei. Tai Ä¯gyvendinama sukuriant trumpesnius ryÅ¡ius tarp Ä¯vesties RNN tarpiniÅ³ bÅ«senÅ³ ir iÅ¡vesties RNN. Tokiu bÅ«du, generuojant iÅ¡vesties simbolÄ¯ y<sub>t</sub>, atsiÅ¾velgiama Ä¯ visas Ä¯vesties paslÄ—ptas bÅ«senas h<sub>i</sub>, su skirtingais svorio koeficientais &alpha;<sub>t,i</sub>.

![Vaizdas, rodantis koduotojo/dekoduotojo modelÄ¯ su papildomu dÄ—mesio sluoksniu](../../../../../translated_images/lt/encoder-decoder-attention.7a726296894fb567.webp)

> Koduotojo-dekoduotojo modelis su papildomu dÄ—mesio mechanizmu [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf), cituota iÅ¡ [Å¡io tinklaraÅ¡Äio Ä¯raÅ¡o](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)

DÄ—mesio matrica {&alpha;<sub>i,j</sub>} atspindÄ—tÅ³, kokiu mastu tam tikri Ä¯vesties Å¾odÅ¾iai dalyvauja generuojant tam tikrÄ… Å¾odÄ¯ iÅ¡vesties sekoje. Å½emiau pateiktas tokios matricos pavyzdys:

![Vaizdas, rodantis pavyzdinÄ¯ suderinimÄ…, rastÄ… RNNsearch-50, paimta iÅ¡ Bahdanau - arviz.org](../../../../../translated_images/lt/bahdanau-fig3.09ba2d37f202a6af.webp)

> Paveikslas iÅ¡ [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) (Fig.3)

DÄ—mesio mechanizmai yra atsakingi uÅ¾ daugelÄ¯ dabartiniÅ³ ar beveik dabartiniÅ³ NLP pasiekimÅ³. TaÄiau dÄ—mesio pridÄ—jimas Å¾ymiai padidina modelio parametrÅ³ skaiÄiÅ³, o tai sukÄ—lÄ— mastelio problemas su RNN. Vienas pagrindiniÅ³ RNN mastelio apribojimÅ³ yra tai, kad modeliÅ³ rekursinis pobÅ«dis apsunkina mokymo partijÅ³ sudarymÄ… ir paralelizavimÄ…. RNN kiekvienas sekos elementas turi bÅ«ti apdorojamas nuosekliai, todÄ—l jo negalima lengvai paralelizuoti.

![Koduotojas Dekoduotojas su DÄ—mesiu](../../../../../lessons/5-NLP/18-Transformers/images/EncDecAttention.gif)

> Paveikslas iÅ¡ [Google tinklaraÅ¡Äio](https://research.googleblog.com/2016/09/a-neural-network-for-machine.html)

DÄ—mesio mechanizmÅ³ pritaikymas kartu su Å¡iuo apribojimu paskatino sukurti dabartinius paÅ¾angiausius transformatoriÅ³ modelius, tokius kaip BERT ir Open-GPT3.

## TransformatoriÅ³ modeliai

Viena pagrindiniÅ³ transformatoriÅ³ idÄ—jÅ³ yra iÅ¡vengti RNN nuoseklumo ir sukurti modelÄ¯, kuris bÅ«tÅ³ paralelizuojamas mokymo metu. Tai pasiekiama Ä¯gyvendinant dvi idÄ—jas:

* pozicinÄ¯ kodavimÄ…
* savidÄ—mesio mechanizmo naudojimÄ… modeliams kurti vietoj RNN (arba CNN) (todÄ—l straipsnis, pristatantis transformatorius, vadinasi *[Attention is all you need](https://arxiv.org/abs/1706.03762)*)

### Pozicinis kodavimas/Ä¯terpimas

Pozicinio kodavimo idÄ—ja yra tokia:  
1. Naudojant RNN, santykinÄ— Å¾etonÅ³ pozicija yra atspindÄ—ta Å¾ingsniÅ³ skaiÄiumi, todÄ—l jos nereikia aiÅ¡kiai reprezentuoti.  
2. TaÄiau perÄ—jus prie dÄ—mesio, reikia Å¾inoti santykines Å¾etonÅ³ pozicijas sekoje.  
3. Norint gauti pozicinÄ¯ kodavimÄ…, mÅ«sÅ³ Å¾etonÅ³ sekÄ… papildome Å¾etonÅ³ pozicijÅ³ seka (pvz., skaiÄiÅ³ seka 0,1, ...).  
4. Tada sumaiÅ¡ome Å¾etono pozicijÄ… su Å¾etono Ä¯terpimo vektoriumi. NorÄ—dami transformuoti pozicijÄ… (sveikÄ…jÄ¯ skaiÄiÅ³) Ä¯ vektoriÅ³, galime naudoti skirtingus metodus:

* Mokomasis Ä¯terpimas, panaÅ¡us Ä¯ Å¾etono Ä¯terpimÄ…. Tai metodas, kurÄ¯ Äia apsvarstysime. Mes taikome Ä¯terpimo sluoksnius tiek Å¾etonams, tiek jÅ³ pozicijoms, gaudami vienodo dydÅ¾io Ä¯terpimo vektorius, kuriuos vÄ—liau sudedame.
* Fiksuota pozicijos kodavimo funkcija, kaip siÅ«loma originaliame straipsnyje.

<img src="../../../../../translated_images/lt/pos-embedding.e41ce9b6cf6078af.webp" width="50%"/>

> Vaizdas autoriaus

Rezultatas, kurÄ¯ gauname su poziciniu Ä¯terpimu, Ä¯terpia tiek originalÅ³ Å¾etonÄ…, tiek jo pozicijÄ… sekoje.

### Daugiafunkcinis savidÄ—mesys

Toliau mums reikia uÅ¾fiksuoti tam tikrus modelius mÅ«sÅ³ sekoje. Tam transformatoriai naudoja **savidÄ—mesio** mechanizmÄ…, kuris iÅ¡ esmÄ—s yra dÄ—mesys, taikomas tai paÄiai sekai kaip Ä¯vestis ir iÅ¡vestis. Taikant savidÄ—mesÄ¯, galime atsiÅ¾velgti Ä¯ **kontekstÄ…** sakinyje ir pamatyti, kurie Å¾odÅ¾iai yra tarpusavyje susijÄ™. PavyzdÅ¾iui, tai leidÅ¾ia pamatyti, Ä¯ kÄ… nurodo koreferencijos, tokios kaip *tai*, ir taip pat atsiÅ¾velgti Ä¯ kontekstÄ…:

![](../../../../../translated_images/lt/CoreferenceResolution.861924d6d384a7d6.webp)

> Vaizdas iÅ¡ [Google tinklaraÅ¡Äio](https://research.googleblog.com/2017/08/transformer-novel-neural-network.html)

Transformatoriuose naudojame **daugiafunkcinÄ¯ dÄ—mesÄ¯**, kad tinklas galÄ—tÅ³ uÅ¾fiksuoti kelis skirtingus priklausomybiÅ³ tipus, pvz., ilgalaikius ir trumpalaikius Å¾odÅ¾iÅ³ ryÅ¡ius, koreferencijas ir kitus.

[TensorFlow uÅ¾raÅ¡Å³ knygelÄ—](TransformersTF.ipynb) pateikia daugiau detaliÅ³ apie transformatoriÅ³ sluoksniÅ³ Ä¯gyvendinimÄ….

### Koduotojo-dekoduotojo dÄ—mesys

Transformatoriuose dÄ—mesys naudojamas dviejose vietose:

* Norint uÅ¾fiksuoti modelius Ä¯vesties tekste naudojant savidÄ—mesÄ¯
* Norint atlikti sekos vertimÄ… - tai yra dÄ—mesio sluoksnis tarp koduotojo ir dekoduotojo.

Koduotojo-dekoduotojo dÄ—mesys yra labai panaÅ¡us Ä¯ dÄ—mesio mechanizmÄ…, naudojamÄ… RNN, kaip apraÅ¡yta Å¡ios dalies pradÅ¾ioje. Å i animuota diagrama paaiÅ¡kina koduotojo-dekoduotojo dÄ—mesio vaidmenÄ¯.

![Animuotas GIF, rodantis, kaip atliekami vertinimai transformatoriÅ³ modeliuose.](../../../../../lessons/5-NLP/18-Transformers/images/transformer-animated-explanation.gif)

Kadangi kiekviena Ä¯vesties pozicija yra nepriklausomai susieta su kiekviena iÅ¡vesties pozicija, transformatoriai gali geriau paralelizuoti nei RNN, o tai leidÅ¾ia kurti daug didesnius ir iÅ¡raiÅ¡kingesnius kalbos modelius. Kiekvienas dÄ—mesio galvutÄ— gali bÅ«ti naudojama mokytis skirtingÅ³ Å¾odÅ¾iÅ³ santykiÅ³, kurie pagerina NLP uÅ¾duotis.

## BERT

**BERT** (Bidirectional Encoder Representations from Transformers) yra labai didelis daugiasluoksnis transformatoriÅ³ tinklas su 12 sluoksniÅ³ *BERT-base* ir 24 sluoksniais *BERT-large*. Modelis pirmiausia iÅ¡ anksto apmokomas naudojant didelÄ¯ tekstÅ³ korpusÄ… (WikiPedia + knygos) taikant nesupervizuotÄ… mokymÄ… (prognozuojant uÅ¾maskuotus Å¾odÅ¾ius sakinyje). Per iÅ¡ankstinÄ¯ mokymÄ… modelis Ä¯gyja reikÅ¡mingÄ… kalbos supratimÄ…, kurÄ¯ vÄ—liau galima panaudoti su kitais duomenÅ³ rinkiniais taikant smulkÅ³ derinimÄ…. Å is procesas vadinamas **perkÄ—limo mokymu**.

![paveikslas iÅ¡ http://jalammar.github.io/illustrated-bert/](../../../../../translated_images/lt/jalammarBERT-language-modeling-masked-lm.34f113ea5fec4362.webp)

> Vaizdo [Å¡altinis](http://jalammar.github.io/illustrated-bert/)

## âœï¸ Pratimai: Transformatoriai

TÄ™skite mokymÄ…si Å¡iose uÅ¾raÅ¡Å³ knygelÄ—se:

* [Transformatoriai PyTorch](TransformersPyTorch.ipynb)
* [Transformatoriai TensorFlow](TransformersTF.ipynb)

## IÅ¡vada

Å ioje pamokoje suÅ¾inojote apie transformatorius ir dÄ—mesio mechanizmus, kurie yra esminiai NLP Ä¯rankiÅ³ rinkinio elementai. Yra daugybÄ— transformatoriÅ³ architektÅ«rÅ³ variantÅ³, Ä¯skaitant BERT, DistilBERT, BigBird, OpenGPT3 ir daugiau, kuriuos galima smulkiai derinti. [HuggingFace paketas](https://github.com/huggingface/) suteikia galimybÄ™ mokyti daugelÄ¯ Å¡iÅ³ architektÅ«rÅ³ naudojant tiek PyTorch, tiek TensorFlow.

## ğŸš€ IÅ¡Å¡Å«kis

## [Po paskaitos testas](https://ff-quizzes.netlify.app/en/ai/quiz/36)

## ApÅ¾valga ir savarankiÅ¡kas mokymasis

* [TinklaraÅ¡Äio Ä¯raÅ¡as](https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/), paaiÅ¡kinantis klasikinÄ¯ [Attention is all you need](https://arxiv.org/abs/1706.03762) straipsnÄ¯ apie transformatorius.
* [TinklaraÅ¡ÄiÅ³ serija](https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452) apie transformatorius, iÅ¡samiai paaiÅ¡kinanti architektÅ«rÄ….

## [UÅ¾duotis](assignment.md)

---

