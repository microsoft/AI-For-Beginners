<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "7e617f0b8de85a43957a853aba09bfeb",
  "translation_date": "2025-08-31T17:57:20+00:00",
  "source_file": "lessons/5-NLP/18-Transformers/README.md",
  "language_code": "lt"
}
-->
# DÄ—mesio mechanizmai ir transformatoriai

## [PrieÅ¡ paskaitÄ… skirtas testas](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/118)

Vienas svarbiausiÅ³ NLP (natÅ«ralios kalbos apdorojimo) srities uÅ¾daviniÅ³ yra **maÅ¡ininis vertimas**, esminÄ— uÅ¾duotis, kuriÄ… atlieka tokie Ä¯rankiai kaip â€Google Translateâ€œ. Å ioje dalyje mes sutelksime dÄ—mesÄ¯ Ä¯ maÅ¡ininÄ¯ vertimÄ… arba, plaÄiau kalbant, Ä¯ bet kokiÄ… *sekos Ä¯ sekÄ…* uÅ¾duotÄ¯ (dar vadinamÄ… **sakiniÅ³ transformacija**).

Naudojant RNN, sekos Ä¯ sekÄ… uÅ¾duotis Ä¯gyvendinama dviem rekursiniais tinklais: vienas tinklas, vadinamas **koduotoju** (encoder), suspaudÅ¾ia Ä¯vesties sekÄ… Ä¯ paslÄ—ptÄ… bÅ«senÄ…, o kitas tinklas, vadinamas **dekoderiu** (decoder), iÅ¡skleidÅ¾ia Å¡iÄ… paslÄ—ptÄ… bÅ«senÄ… Ä¯ iÅ¡verstÄ… rezultatÄ…. TaÄiau Å¡is metodas turi keletÄ… problemÅ³:

* GalutinÄ— koduotojo tinklo bÅ«sena sunkiai prisimena sakinio pradÅ¾iÄ…, todÄ—l modelis prastai veikia su ilgais sakiniais.
* Visi Å¾odÅ¾iai sekoje turi vienodÄ… Ä¯takÄ… rezultatui. TaÄiau realybÄ—je tam tikri Å¾odÅ¾iai Ä¯vesties sekoje daÅ¾nai turi didesnÄ™ Ä¯takÄ… iÅ¡vesties sekai nei kiti.

**DÄ—mesio mechanizmai** suteikia galimybÄ™ Ä¯vertinti kiekvieno Ä¯vesties vektoriaus kontekstinÄ™ Ä¯takÄ… kiekvienai RNN iÅ¡vesties prognozei. Tai Ä¯gyvendinama sukuriant trumpesnius ryÅ¡ius tarp tarpiniÅ³ Ä¯vesties RNN bÅ«senÅ³ ir iÅ¡vesties RNN bÅ«senÅ³. Tokiu bÅ«du, generuojant iÅ¡vesties simbolÄ¯ y<sub>t</sub>, atsiÅ¾velgiama Ä¯ visas Ä¯vesties paslÄ—ptas bÅ«senas h<sub>i</sub>, su skirtingais svorio koeficientais Î±<sub>t,i</sub>.

![Vaizdas, rodantis koduotojo/dekoderio modelÄ¯ su papildomu dÄ—mesio sluoksniu](../../../../../translated_images/encoder-decoder-attention.7a726296894fb567aa2898c94b17b3289087f6705c11907df8301df9e5eeb3de.lt.png)

> Koduotojo-dekoderio modelis su papildomu dÄ—mesio mechanizmu iÅ¡ [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf), cituota iÅ¡ [Å¡io tinklaraÅ¡Äio Ä¯raÅ¡o](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)

DÄ—mesio matrica {Î±<sub>i,j</sub>} atspindÄ—tÅ³, kokiu mastu tam tikri Ä¯vesties Å¾odÅ¾iai dalyvauja generuojant tam tikrÄ… Å¾odÄ¯ iÅ¡vesties sekoje. Å½emiau pateiktas tokios matricos pavyzdys:

![Vaizdas, rodantis pavyzdinÄ¯ suderinimÄ…, rastÄ… naudojant RNNsearch-50, paimtas iÅ¡ Bahdanau - arviz.org](../../../../../translated_images/bahdanau-fig3.09ba2d37f202a6af11de6c82d2d197830ba5f4528d9ea430eb65fd3a75065973.lt.png)

> Paveikslas iÅ¡ [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) (3 pav.)

DÄ—mesio mechanizmai yra atsakingi uÅ¾ daugelÄ¯ dabartiniÅ³ ar beveik dabartiniÅ³ NLP pasiekimÅ³. TaÄiau dÄ—mesio pridÄ—jimas Å¾ymiai padidina modelio parametrÅ³ skaiÄiÅ³, o tai sukÄ—lÄ— mastelio problemas su RNN. Vienas pagrindiniÅ³ RNN mastelio apribojimÅ³ yra tas, kad modeliÅ³ rekursinis pobÅ«dis apsunkina mokymo partijÅ³ apdorojimÄ… ir lygiagretinimÄ…. RNN kiekvienas sekos elementas turi bÅ«ti apdorojamas nuosekliai, todÄ—l lygiagretinimas tampa sudÄ—tingas.

![Koduotojas-dekoderis su dÄ—mesiu](../../../../../lessons/5-NLP/18-Transformers/images/EncDecAttention.gif)

> Paveikslas iÅ¡ [Google tinklaraÅ¡Äio](https://research.googleblog.com/2016/09/a-neural-network-for-machine.html)

DÄ—mesio mechanizmÅ³ pritaikymas kartu su Å¡iuo apribojimu paskatino sukurti dabartinius paÅ¾angiausius transformatoriÅ³ modelius, tokius kaip BERT ir Open-GPT3.

## TransformatoriÅ³ modeliai

Viena pagrindiniÅ³ transformatoriÅ³ idÄ—jÅ³ yra iÅ¡vengti RNN sekos pobÅ«dÅ¾io ir sukurti modelÄ¯, kuris bÅ«tÅ³ lygiagretinamas mokymo metu. Tai pasiekiama Ä¯gyvendinant dvi idÄ—jas:

* pozicinÄ¯ kodavimÄ…
* savidÄ—mesio mechanizmÄ…, skirtÄ… modeliuoti Å¡ablonus vietoj RNN (arba CNN) (todÄ—l straipsnis, pristatantis transformatorius, vadinasi *[Attention is all you need](https://arxiv.org/abs/1706.03762)*)

### Pozicinis kodavimas/Ä¯terpimas

Pozicinio kodavimo idÄ—ja yra tokia:  
1. Naudojant RNN, Å¾odÅ¾iÅ³ santykinÄ— pozicija yra atspindima Å¾ingsniÅ³ skaiÄiumi, todÄ—l jos nereikia aiÅ¡kiai nurodyti.  
2. TaÄiau perÄ—jus prie dÄ—mesio mechanizmÅ³, reikia Å¾inoti Å¾odÅ¾iÅ³ santykines pozicijas sekoje.  
3. Norint gauti pozicinÄ¯ kodavimÄ…, prie Å¾odÅ¾iÅ³ sekos pridedame jÅ³ pozicijÅ³ sekÄ… (pvz., skaiÄius 0, 1, ...).  
4. Tada pozicijÄ… sumaiÅ¡ome su Å¾odÅ¾io Ä¯terpimo vektoriumi. NorÄ—dami transformuoti pozicijÄ… (sveikÄ…jÄ¯ skaiÄiÅ³) Ä¯ vektoriÅ³, galime naudoti skirtingus metodus:

* MokomÄ…jÄ¯ Ä¯terpimÄ…, panaÅ¡Å³ Ä¯ Å¾odÅ¾iÅ³ Ä¯terpimÄ…. Å Ä¯ metodÄ… Äia ir nagrinÄ—jame. Taikome Ä¯terpimo sluoksnius tiek Å¾odÅ¾iams, tiek jÅ³ pozicijoms, gauname vienodo dydÅ¾io Ä¯terpimo vektorius, kuriuos tada sudedame.
* FiksuotÄ… pozicijos kodavimo funkcijÄ…, kaip siÅ«loma originaliame straipsnyje.

<img src="images/pos-embedding.png" width="50%"/>

> Vaizdas autoriaus

Rezultatas, kurÄ¯ gauname su poziciniu Ä¯terpimu, apima tiek originalÅ³ Å¾odÄ¯, tiek jo pozicijÄ… sekoje.

### Daugiafunkcinis savidÄ—mesys

Toliau reikia uÅ¾fiksuoti tam tikrus Å¡ablonus sekoje. Tam transformatoriai naudoja **savidÄ—mesio** mechanizmÄ…, kuris iÅ¡ esmÄ—s yra dÄ—mesys, taikomas tai paÄiai sekai kaip Ä¯vestis ir iÅ¡vestis. Taikant savidÄ—mesÄ¯, galime atsiÅ¾velgti Ä¯ **kontekstÄ…** sakinyje ir pamatyti, kurie Å¾odÅ¾iai yra tarpusavyje susijÄ™. PavyzdÅ¾iui, tai leidÅ¾ia pamatyti, Ä¯ kÄ… nurodo Ä¯vardÅ¾iai, tokie kaip *jis*, ir taip pat atsiÅ¾velgti Ä¯ kontekstÄ…:

![](../../../../../translated_images/CoreferenceResolution.861924d6d384a7d68d8d0039d06a71a151f18a796b8b1330239d3590bd4947eb.lt.png)

> Vaizdas iÅ¡ [Google tinklaraÅ¡Äio](https://research.googleblog.com/2017/08/transformer-novel-neural-network.html)

Transformatoriuose naudojame **daugiafunkcinÄ¯ dÄ—mesÄ¯**, kad tinklas galÄ—tÅ³ uÅ¾fiksuoti kelis skirtingus priklausomybiÅ³ tipus, pvz., ilgalaikius ir trumpalaikius Å¾odÅ¾iÅ³ ryÅ¡ius, koreferencijas ir kt.

[TensorFlow uÅ¾raÅ¡Å³ knygelÄ—](TransformersTF.ipynb) pateikia daugiau detaliÅ³ apie transformatoriÅ³ sluoksniÅ³ Ä¯gyvendinimÄ….

### Koduotojo-dekoderio dÄ—mesys

Transformatoriuose dÄ—mesys naudojamas dviejose vietose:

* Norint uÅ¾fiksuoti Å¡ablonus Ä¯vesties tekste naudojant savidÄ—mesÄ¯.
* Norint atlikti sekos vertimÄ… â€“ tai yra dÄ—mesio sluoksnis tarp koduotojo ir dekoderio.

Koduotojo-dekoderio dÄ—mesys yra labai panaÅ¡us Ä¯ dÄ—mesio mechanizmÄ…, naudojamÄ… RNN, kaip apraÅ¡yta Å¡ios dalies pradÅ¾ioje. Å i animuota diagrama paaiÅ¡kina koduotojo-dekoderio dÄ—mesio vaidmenÄ¯.

![Animuotas GIF, rodantis, kaip atliekami vertinimai transformatoriÅ³ modeliuose.](../../../../../lessons/5-NLP/18-Transformers/images/transformer-animated-explanation.gif)

Kadangi kiekviena Ä¯vesties pozicija nepriklausomai susiejama su kiekviena iÅ¡vesties pozicija, transformatoriai gali geriau lygiagretinti nei RNN, o tai leidÅ¾ia kurti daug didesnius ir iÅ¡raiÅ¡kingesnius kalbos modelius. Kiekviena dÄ—mesio galvutÄ— gali bÅ«ti naudojama mokytis skirtingÅ³ Å¾odÅ¾iÅ³ santykiÅ³, kurie pagerina NLP uÅ¾duoÄiÅ³ rezultatus.

## BERT

**BERT** (Bidirectional Encoder Representations from Transformers) yra labai didelis daugiasluoksnis transformatoriÅ³ tinklas su 12 sluoksniÅ³ *BERT-base* versijoje ir 24 sluoksniais *BERT-large* versijoje. Modelis pirmiausia iÅ¡ anksto apmokomas naudojant didelÄ¯ tekstÅ³ korpusÄ… (Vikipedija + knygos) taikant nesupervizuotÄ… mokymÄ… (prognozuojant uÅ¾maskuotus Å¾odÅ¾ius sakinyje). Per iÅ¡ankstinÄ¯ mokymÄ… modelis Ä¯gyja reikÅ¡mingÄ… kalbos supratimÄ…, kurÄ¯ vÄ—liau galima pritaikyti kitose duomenÅ³ rinkiniuose naudojant smulkÅ³jÄ¯ derinimÄ…. Å is procesas vadinamas **perkÄ—limo mokymusi**.

![paveikslas iÅ¡ http://jalammar.github.io/illustrated-bert/](../../../../../translated_images/jalammarBERT-language-modeling-masked-lm.34f113ea5fec4362e39ee4381aab7cad06b5465a0b5f053a0f2aa05fbe14e746.lt.png)

> Vaizdo [Å¡altinis](http://jalammar.github.io/illustrated-bert/)

## âœï¸ Pratimai: Transformatoriai

TÄ™skite mokymÄ…si Å¡iose uÅ¾raÅ¡Å³ knygelÄ—se:

* [Transformatoriai PyTorch](TransformersPyTorch.ipynb)
* [Transformatoriai TensorFlow](TransformersTF.ipynb)

## IÅ¡vada

Å ioje pamokoje suÅ¾inojote apie transformatorius ir dÄ—mesio mechanizmus â€“ visus esminius NLP Ä¯rankiÅ³ rinkinio elementus. Yra daugybÄ— transformatoriÅ³ architektÅ«rÅ³ variantÅ³, Ä¯skaitant BERT, DistilBERT, BigBird, OpenGPT3 ir daugiau, kuriuos galima smulkiai derinti. [HuggingFace paketas](https://github.com/huggingface/) suteikia galimybÄ™ treniruoti daugelÄ¯ Å¡iÅ³ architektÅ«rÅ³ tiek su PyTorch, tiek su TensorFlow.

## ğŸš€ IÅ¡Å¡Å«kis

## [Po paskaitos skirtas testas](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/218)

## PerÅ¾iÅ«ra ir savarankiÅ¡kas mokymasis

* [TinklaraÅ¡Äio Ä¯raÅ¡as](https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/), paaiÅ¡kinantis klasikinÄ¯ [Attention is all you need](https://arxiv.org/abs/1706.03762) straipsnÄ¯ apie transformatorius.
* [TinklaraÅ¡ÄiÅ³ serija](https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452) apie transformatorius, iÅ¡samiai paaiÅ¡kinanti architektÅ«rÄ….

## [UÅ¾duotis](assignment.md)

---

**AtsakomybÄ—s apribojimas**:  
Å is dokumentas buvo iÅ¡verstas naudojant AI vertimo paslaugÄ… [Co-op Translator](https://github.com/Azure/co-op-translator). Nors siekiame tikslumo, praÅ¡ome atkreipti dÄ—mesÄ¯, kad automatiniai vertimai gali turÄ—ti klaidÅ³ ar netikslumÅ³. Originalus dokumentas jo gimtÄ…ja kalba turÄ—tÅ³ bÅ«ti laikomas autoritetingu Å¡altiniu. Kritinei informacijai rekomenduojama naudoti profesionalÅ³ Å¾mogaus vertimÄ…. Mes neprisiimame atsakomybÄ—s uÅ¾ nesusipratimus ar klaidingus interpretavimus, atsiradusius dÄ—l Å¡io vertimo naudojimo.