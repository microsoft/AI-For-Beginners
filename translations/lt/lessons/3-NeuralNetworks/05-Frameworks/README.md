<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "2b544f20b796402507fb05a0df893323",
  "translation_date": "2025-08-31T17:51:21+00:00",
  "source_file": "lessons/3-NeuralNetworks/05-Frameworks/README.md",
  "language_code": "lt"
}
-->
# NeuroniniÅ³ tinklÅ³ karkasai

Kaip jau iÅ¡mokome, norint efektyviai treniruoti neuroninius tinklus, reikia atlikti du dalykus:

* Dirbti su tensoriais, pvz., dauginti, sudÄ—ti ir apskaiÄiuoti tam tikras funkcijas, tokias kaip sigmoidÄ— ar softmax
* ApskaiÄiuoti visÅ³ iÅ¡raiÅ¡kÅ³ gradientus, kad bÅ«tÅ³ galima atlikti gradientinio nusileidimo optimizacijÄ…

## [PrieÅ¡ paskaitÄ… atlikite testÄ…](https://ff-quizzes.netlify.app/en/ai/quiz/9)

Nors `numpy` biblioteka gali atlikti pirmÄ…jÄ… dalÄ¯, mums reikia mechanizmo gradientams apskaiÄiuoti. [MÅ«sÅ³ karkase](../04-OwnFramework/OwnFramework.ipynb), kurÄ¯ sukÅ«rÄ—me ankstesniame skyriuje, turÄ—jome rankiniu bÅ«du programuoti visas iÅ¡vestines funkcijas `backward` metode, kuris atlieka atgalinÄ¯ sklidimÄ…. Idealiu atveju karkasas turÄ—tÅ³ suteikti galimybÄ™ apskaiÄiuoti *bet kurios iÅ¡raiÅ¡kos* gradientus, kuriuos galime apibrÄ—Å¾ti.

Kitas svarbus dalykas yra galimybÄ— atlikti skaiÄiavimus GPU arba kitais specializuotais skaiÄiavimo Ä¯renginiais, tokiais kaip [TPU](https://en.wikipedia.org/wiki/Tensor_Processing_Unit). GiliÅ³jÅ³ neuroniniÅ³ tinklÅ³ treniravimas reikalauja *labai daug* skaiÄiavimÅ³, todÄ—l galimybÄ— juos paralelizuoti GPU yra labai svarbi.

> âœ… Terminas â€paralelizuotiâ€œ reiÅ¡kia paskirstyti skaiÄiavimus per kelis Ä¯renginius.

Å iuo metu du populiariausi neuroniniÅ³ tinklÅ³ karkasai yra: [TensorFlow](http://TensorFlow.org) ir [PyTorch](https://pytorch.org/). Abu jie suteikia Å¾emo lygio API darbui su tensoriais tiek CPU, tiek GPU. Be Å¾emo lygio API, taip pat yra aukÅ¡to lygio API, vadinami [Keras](https://keras.io/) ir [PyTorch Lightning](https://pytorchlightning.ai/) atitinkamai.

Å½emo lygio API | [TensorFlow](http://TensorFlow.org) | [PyTorch](https://pytorch.org/)
---------------|-------------------------------------|--------------------------------
AukÅ¡to lygio API| [Keras](https://keras.io/) | [PyTorch Lightning](https://pytorchlightning.ai/)

**Å½emo lygio API** abiejuose karkasuose leidÅ¾ia kurti vadinamuosius **skaiÄiavimo grafus**. Å is grafas apibrÄ—Å¾ia, kaip apskaiÄiuoti iÅ¡vestÄ¯ (daÅ¾niausiai nuostoliÅ³ funkcijÄ…) su pateiktais Ä¯vesties parametrais ir gali bÅ«ti perduotas skaiÄiavimams GPU, jei jis yra prieinamas. Yra funkcijos, leidÅ¾ianÄios diferencijuoti Å¡Ä¯ skaiÄiavimo grafÄ… ir apskaiÄiuoti gradientus, kurie vÄ—liau gali bÅ«ti naudojami modelio parametrÅ³ optimizavimui.

**AukÅ¡to lygio API** traktuoja neuroninius tinklus kaip **sluoksniÅ³ sekÄ…** ir leidÅ¾ia daug lengviau konstruoti daugumÄ… neuroniniÅ³ tinklÅ³. Modelio treniravimas paprastai reikalauja paruoÅ¡ti duomenis ir tada iÅ¡kviesti `fit` funkcijÄ…, kad atliktÅ³ darbÄ….

AukÅ¡to lygio API leidÅ¾ia labai greitai sukurti tipinius neuroninius tinklus, nesirÅ«pinant daugybe detaliÅ³. Tuo paÄiu metu Å¾emo lygio API suteikia daug daugiau kontrolÄ—s treniravimo procesui, todÄ—l jie daÅ¾nai naudojami tyrimuose, kai dirbama su naujomis neuroniniÅ³ tinklÅ³ architektÅ«romis.

Taip pat svarbu suprasti, kad galite naudoti abu API kartu, pvz., galite sukurti savo tinklo sluoksnio architektÅ«rÄ… naudodami Å¾emo lygio API ir tada naudoti jÄ… didesniame tinkle, sukurtame ir treniruotame su aukÅ¡to lygio API. Arba galite apibrÄ—Å¾ti tinklÄ… naudodami aukÅ¡to lygio API kaip sluoksniÅ³ sekÄ… ir tada naudoti savo Å¾emo lygio treniravimo ciklÄ… optimizacijai atlikti. Abu API naudoja tuos paÄius pagrindinius principus ir yra sukurti taip, kad gerai veiktÅ³ kartu.

## Mokymasis

Å iame kurse dauguma turinio pateikiama tiek PyTorch, tiek TensorFlow karkasams. Galite pasirinkti savo mÄ—gstamÄ… karkasÄ… ir perÅ¾iÅ«rÄ—ti tik atitinkamus uÅ¾raÅ¡us. Jei nesate tikri, kurÄ¯ karkasÄ… pasirinkti, perskaitykite diskusijas internete apie **PyTorch vs. TensorFlow**. Taip pat galite perÅ¾iÅ«rÄ—ti abu karkasus, kad geriau juos suprastumÄ—te.

Kai tik Ä¯manoma, naudosime aukÅ¡to lygio API dÄ—l paprastumo. TaÄiau manome, kad svarbu suprasti, kaip neuroniniai tinklai veikia nuo pagrindÅ³, todÄ—l pradÅ¾ioje dirbsime su Å¾emo lygio API ir tensoriais. TaÄiau, jei norite greitai pradÄ—ti ir nenorite skirti daug laiko Å¡ioms detalÄ—ms mokytis, galite praleisti Å¡iÄ… dalÄ¯ ir iÅ¡kart pereiti prie aukÅ¡to lygio API uÅ¾raÅ¡Å³.

## âœï¸ Pratimai: Karkasai

TÄ™skite mokymÄ…si Å¡iuose uÅ¾raÅ¡uose:

Å½emo lygio API | [TensorFlow+Keras UÅ¾raÅ¡ai](IntroKerasTF.ipynb) | [PyTorch](IntroPyTorch.ipynb)
---------------|-------------------------------------|--------------------------------
AukÅ¡to lygio API| [Keras](IntroKeras.ipynb) | *PyTorch Lightning*

Ä®valdÄ™ karkasus, prisiminkime per didelio pritaikymo (overfitting) sÄ…vokÄ….

# Per didelis pritaikymas (Overfitting)

Per didelis pritaikymas yra itin svarbi sÄ…voka maÅ¡ininio mokymosi srityje, ir labai svarbu jÄ… teisingai suprasti!

Apsvarstykime Å¡iÄ… problemÄ…, kai reikia aproksimuoti 5 taÅ¡kus (grafikuose Å¾ymimi kaip `x`):

![linear](../../../../../translated_images/overfit1.f24b71c6f652e59e6bed7245ffbeaecc3ba320e16e2221f6832b432052c4da43.lt.jpg) | ![overfit](../../../../../translated_images/overfit2.131f5800ae10ca5e41d12a411f5f705d9ee38b1b10916f284b787028dd55cc1c.lt.jpg)
-------------------------|--------------------------
**Linijinis modelis, 2 parametrai** | **Nelinijinis modelis, 7 parametrai**
Treniravimo klaida = 5.3 | Treniravimo klaida = 0
Validavimo klaida = 5.1 | Validavimo klaida = 20

* KairÄ—je matome gerÄ… tiesios linijos aproksimacijÄ…. Kadangi parametrÅ³ skaiÄius yra tinkamas, modelis teisingai supranta taÅ¡kÅ³ pasiskirstymo esmÄ™.
* DeÅ¡inÄ—je modelis yra per daug galingas. Kadangi turime tik 5 taÅ¡kus, o modelis turi 7 parametrus, jis gali prisitaikyti taip, kad praeitÅ³ per visus taÅ¡kus, todÄ—l treniravimo klaida tampa 0. TaÄiau tai neleidÅ¾ia modeliui suprasti teisingo duomenÅ³ modelio, todÄ—l validavimo klaida yra labai didelÄ—.

Labai svarbu rasti tinkamÄ… pusiausvyrÄ… tarp modelio sudÄ—tingumo (parametrÅ³ skaiÄiaus) ir treniravimo pavyzdÅ¾iÅ³ skaiÄiaus.

## KodÄ—l atsiranda per didelis pritaikymas

  * Nepakanka treniravimo duomenÅ³
  * Per daug galingas modelis
  * Per daug triukÅ¡mo Ä¯vesties duomenyse

## Kaip aptikti per didelÄ¯ pritaikymÄ…

Kaip matote iÅ¡ aukÅ¡Äiau pateikto grafiko, per didelÄ¯ pritaikymÄ… galima aptikti pagal labai maÅ¾Ä… treniravimo klaidÄ… ir didelÄ™ validavimo klaidÄ…. Paprastai treniravimo metu matysime, kaip tiek treniravimo, tiek validavimo klaidos pradeda maÅ¾Ä—ti, o tada tam tikru momentu validavimo klaida gali nustoti maÅ¾Ä—ti ir pradÄ—ti didÄ—ti. Tai bus Å¾enklas, kad atsirado per didelis pritaikymas, ir indikatorius, kad turÄ—tume sustabdyti treniravimÄ… (arba bent jau iÅ¡saugoti modelio bÅ«senÄ…).

![overfitting](../../../../../translated_images/Overfitting.408ad91cd90b4371d0a81f4287e1409c359751adeb1ae450332af50e84f08c3e.lt.png)

## Kaip iÅ¡vengti per didelio pritaikymo

Jei pastebite, kad atsiranda per didelis pritaikymas, galite atlikti vienÄ… iÅ¡ Å¡iÅ³ veiksmÅ³:

 * Padidinti treniravimo duomenÅ³ kiekÄ¯
 * SumaÅ¾inti modelio sudÄ—tingumÄ…
 * Naudoti tam tikrÄ… [reguliavimo technikÄ…](../../4-ComputerVision/08-TransferLearning/TrainingTricks.md), pvz., [Dropout](../../4-ComputerVision/08-TransferLearning/TrainingTricks.md#Dropout), kuriÄ… aptarsime vÄ—liau.

## Per didelis pritaikymas ir Å¡aliÅ¡kumo-variacijos kompromisas

Per didelis pritaikymas iÅ¡ tikrÅ³jÅ³ yra bendresnÄ—s statistikos problemos, vadinamos [Å¡aliÅ¡kumo-variacijos kompromisu](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff), atvejis. Jei apsvarstysime galimus klaidÅ³ Å¡altinius mÅ«sÅ³ modelyje, galime iÅ¡skirti dviejÅ³ tipÅ³ klaidas:

* **Å aliÅ¡kumo klaidos** atsiranda, kai mÅ«sÅ³ algoritmas negali teisingai uÅ¾fiksuoti ryÅ¡io tarp treniravimo duomenÅ³. Tai gali bÅ«ti dÄ—l to, kad mÅ«sÅ³ modelis nÄ—ra pakankamai galingas (**nepakankamas pritaikymas**).
* **Variacijos klaidos**, kurios atsiranda, kai modelis aproksimuoja triukÅ¡mÄ… Ä¯vesties duomenyse vietoj prasmingo ryÅ¡io (**per didelis pritaikymas**).

Treniravimo metu Å¡aliÅ¡kumo klaida maÅ¾Ä—ja (kai mÅ«sÅ³ modelis mokosi aproksimuoti duomenis), o variacijos klaida didÄ—ja. Svarbu sustabdyti treniravimÄ… - arba rankiniu bÅ«du (kai aptinkame per didelÄ¯ pritaikymÄ…), arba automatiÅ¡kai (Ä¯vedant reguliavimÄ…) - kad iÅ¡vengtume per didelio pritaikymo.

## IÅ¡vada

Å ioje pamokoje suÅ¾inojote apie skirtumus tarp Ä¯vairiÅ³ API dviejuose populiariausiuose AI karkasuose, TensorFlow ir PyTorch. Be to, suÅ¾inojote apie labai svarbiÄ… temÄ… - per didelÄ¯ pritaikymÄ….

## ğŸš€ IÅ¡Å¡Å«kis

Pridedamuose uÅ¾raÅ¡uose rasite â€uÅ¾duotisâ€œ apaÄioje; perÅ¾iÅ«rÄ—kite uÅ¾raÅ¡us ir atlikite uÅ¾duotis.

## [Po paskaitos atlikite testÄ…](https://ff-quizzes.netlify.app/en/ai/quiz/10)

## PerÅ¾iÅ«ra ir savarankiÅ¡kas mokymasis

Atlikite tyrimÄ… Å¡iomis temomis:

- TensorFlow
- PyTorch
- Per didelis pritaikymas

Paklauskite savÄ™s Å¡iÅ³ klausimÅ³:

- Kuo skiriasi TensorFlow ir PyTorch?
- Kuo skiriasi per didelis pritaikymas ir nepakankamas pritaikymas?

## [UÅ¾duotis](lab/README.md)

Å ioje laboratorijoje jÅ«sÅ³ praÅ¡oma iÅ¡sprÄ™sti dvi klasifikavimo problemas naudojant vieno ir keliÅ³ sluoksniÅ³ visiÅ¡kai sujungtus tinklus su PyTorch arba TensorFlow.

* [Instrukcijos](lab/README.md)
* [UÅ¾raÅ¡ai](lab/LabFrameworks.ipynb)

---

**AtsakomybÄ—s apribojimas**:  
Å is dokumentas buvo iÅ¡verstas naudojant AI vertimo paslaugÄ… [Co-op Translator](https://github.com/Azure/co-op-translator). Nors siekiame tikslumo, praÅ¡ome atkreipti dÄ—mesÄ¯, kad automatiniai vertimai gali turÄ—ti klaidÅ³ ar netikslumÅ³. Originalus dokumentas jo gimtÄ…ja kalba turÄ—tÅ³ bÅ«ti laikomas autoritetingu Å¡altiniu. Kritinei informacijai rekomenduojama profesionali Å¾mogaus vertimo paslauga. Mes neprisiimame atsakomybÄ—s uÅ¾ nesusipratimus ar klaidingus interpretavimus, atsiradusius naudojant Å¡Ä¯ vertimÄ….