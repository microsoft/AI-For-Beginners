# NeuroniniÅ³ tinklÅ³ karkasai

Kaip jau iÅ¡mokome, norint efektyviai treniruoti neuroninius tinklus, reikia atlikti du dalykus:

* Dirbti su tensoriais, pvz., dauginti, sudÄ—ti ir skaiÄiuoti tam tikras funkcijas, tokias kaip sigmoidinÄ— ar softmax
* ApskaiÄiuoti visÅ³ iÅ¡raiÅ¡kÅ³ gradientus, kad bÅ«tÅ³ galima atlikti gradientinio nusileidimo optimizacijÄ…

## [PrieÅ¡ paskaitÄ… vykdomas testas](https://ff-quizzes.netlify.app/en/ai/quiz/9)

Nors `numpy` biblioteka gali atlikti pirmÄ…jÄ… dalÄ¯, mums reikia mechanizmo gradientams apskaiÄiuoti. [MÅ«sÅ³ karkase](../04-OwnFramework/OwnFramework.ipynb), kurÄ¯ sukÅ«rÄ—me ankstesniame skyriuje, turÄ—jome rankiniu bÅ«du programuoti visas iÅ¡vestiniÅ³ funkcijas `backward` metode, kuris atlieka atgalinÄ¯ sklidimÄ…. Idealiu atveju karkasas turÄ—tÅ³ suteikti galimybÄ™ apskaiÄiuoti *bet kokios iÅ¡raiÅ¡kos* gradientus, kuriuos galime apibrÄ—Å¾ti.

Kitas svarbus dalykas yra galimybÄ— atlikti skaiÄiavimus GPU arba kitose specializuotose skaiÄiavimo vienetuose, pvz., [TPU](https://en.wikipedia.org/wiki/Tensor_Processing_Unit). GiliÅ³jÅ³ neuroniniÅ³ tinklÅ³ treniravimas reikalauja *daug* skaiÄiavimÅ³, todÄ—l galimybÄ— juos paralelizuoti GPU yra labai svarbi.

> âœ… Terminas â€paralelizuotiâ€œ reiÅ¡kia paskirstyti skaiÄiavimus per kelis Ä¯renginius.

Å iuo metu du populiariausi neuroniniÅ³ tinklÅ³ karkasai yra: [TensorFlow](http://TensorFlow.org) ir [PyTorch](https://pytorch.org/). Abu suteikia Å¾emo lygio API darbui su tensoriais tiek CPU, tiek GPU. Be Å¾emo lygio API, yra ir aukÅ¡to lygio API, vadinami [Keras](https://keras.io/) ir [PyTorch Lightning](https://pytorchlightning.ai) atitinkamai.

Å½emo lygio API | [TensorFlow](http://TensorFlow.org) | [PyTorch](https://pytorch.org/)
---------------|-------------------------------------|--------------------------------
AukÅ¡to lygio API| [Keras](https://keras.io/) | [PyTorch Lightning](https://pytorchlightning.ai/)

**Å½emo lygio API** abiejuose karkasuose leidÅ¾ia kurti vadinamuosius **skaiÄiavimo grafus**. Å is grafas apibrÄ—Å¾ia, kaip apskaiÄiuoti iÅ¡vestÄ¯ (daÅ¾niausiai nuostoliÅ³ funkcijÄ…) su pateiktais Ä¯vesties parametrais, ir gali bÅ«ti perduotas skaiÄiavimui GPU, jei jis yra prieinamas. Yra funkcijos, skirtos diferencijuoti Å¡Ä¯ skaiÄiavimo grafÄ… ir apskaiÄiuoti gradientus, kurie vÄ—liau gali bÅ«ti naudojami modelio parametrÅ³ optimizavimui.

**AukÅ¡to lygio API** daugiausia laiko neuroninius tinklus kaip **sluoksniÅ³ sekÄ…**, todÄ—l daugumos neuroniniÅ³ tinklÅ³ konstravimas tampa daug paprastesnis. Modelio treniravimas paprastai reikalauja paruoÅ¡ti duomenis ir tada iÅ¡kviesti funkcijÄ… `fit`, kad atliktÅ³ darbÄ….

AukÅ¡to lygio API leidÅ¾ia labai greitai sukurti tipinius neuroninius tinklus, nesirÅ«pinant daugybe detaliÅ³. Tuo paÄiu metu Å¾emo lygio API suteikia daug daugiau kontrolÄ—s treniravimo procesui, todÄ—l jie daÅ¾nai naudojami tyrimuose, kai dirbama su naujomis neuroniniÅ³ tinklÅ³ architektÅ«romis.

Taip pat svarbu suprasti, kad galite naudoti abu API kartu, pvz., galite sukurti savo tinklo sluoksnio architektÅ«rÄ… naudodami Å¾emo lygio API, o tada naudoti jÄ… didesniame tinkle, sukurtame ir treniruotame su aukÅ¡to lygio API. Arba galite apibrÄ—Å¾ti tinklÄ… naudodami aukÅ¡to lygio API kaip sluoksniÅ³ sekÄ…, o tada naudoti savo Å¾emo lygio treniravimo ciklÄ… optimizacijai atlikti. Abu API naudoja tuos paÄius pagrindinius konceptus ir yra sukurti taip, kad gerai veiktÅ³ kartu.

## Mokymasis

Å iame kurse siÅ«lome daugumÄ… turinio tiek PyTorch, tiek TensorFlow. Galite pasirinkti savo mÄ—gstamÄ… karkasÄ… ir perÅ¾iÅ«rÄ—ti tik atitinkamus uÅ¾raÅ¡us. Jei nesate tikri, kurÄ¯ karkasÄ… pasirinkti, perskaitykite diskusijas internete apie **PyTorch vs. TensorFlow**. Taip pat galite perÅ¾iÅ«rÄ—ti abu karkasus, kad geriau suprastumÄ—te.

Kai tik Ä¯manoma, naudosime aukÅ¡to lygio API dÄ—l paprastumo. TaÄiau manome, kad svarbu suprasti, kaip neuroniniai tinklai veikia nuo pat pradÅ¾iÅ³, todÄ—l pradÅ¾ioje dirbsime su Å¾emo lygio API ir tensoriais. TaÄiau, jei norite greitai pradÄ—ti ir nenorite skirti daug laiko Å¡iÅ³ detaliÅ³ mokymuisi, galite praleisti Å¡iuos skyrius ir pereiti tiesiai prie aukÅ¡to lygio API uÅ¾raÅ¡Å³.

## âœï¸ Pratimai: Karkasai

TÄ™skite mokymÄ…si Å¡iuose uÅ¾raÅ¡uose:

Å½emo lygio API | [TensorFlow+Keras UÅ¾raÅ¡ai](IntroKerasTF.ipynb) | [PyTorch](IntroPyTorch.ipynb)
---------------|-------------------------------------|--------------------------------
AukÅ¡to lygio API| [Keras](IntroKeras.ipynb) | *PyTorch Lightning*

Ä®valdÄ™ karkasus, apÅ¾velkime per didelio pritaikymo (overfitting) sÄ…vokÄ….

# Per didelis pritaikymas (Overfitting)

Per didelis pritaikymas yra itin svarbi sÄ…voka maÅ¡ininio mokymosi srityje, ir labai svarbu jÄ… suprasti teisingai!

Apsvarstykite Å¡iÄ… problemÄ…, kurioje reikia aproksimuoti 5 taÅ¡kus (grafikuose paÅ¾ymÄ—tus `x`):

![linear](../../../../../translated_images/lt/overfit1.f24b71c6f652e59e.webp) | ![overfit](../../../../../translated_images/lt/overfit2.131f5800ae10ca5e.webp)
-------------------------|--------------------------
**Linijinis modelis, 2 parametrai** | **Nelinijinis modelis, 7 parametrai**
Mokymo klaida = 5.3 | Mokymo klaida = 0
Validacijos klaida = 5.1 | Validacijos klaida = 20

* KairÄ—je matome gerÄ… tiesÄ—s aproksimacijÄ…. Kadangi parametrÅ³ skaiÄius yra tinkamas, modelis teisingai supranta taÅ¡kÅ³ pasiskirstymo idÄ—jÄ….
* DeÅ¡inÄ—je modelis yra per daug galingas. Kadangi turime tik 5 taÅ¡kus, o modelis turi 7 parametrus, jis gali prisitaikyti taip, kad praeitÅ³ per visus taÅ¡kus, todÄ—l mokymo klaida tampa 0. TaÄiau tai neleidÅ¾ia modeliui suprasti teisingo duomenÅ³ modelio, todÄ—l validacijos klaida yra labai didelÄ—.

Labai svarbu rasti tinkamÄ… pusiausvyrÄ… tarp modelio sudÄ—tingumo (parametrÅ³ skaiÄiaus) ir mokymo pavyzdÅ¾iÅ³ skaiÄiaus.

## KodÄ—l atsiranda per didelis pritaikymas

  * Nepakankamai mokymo duomenÅ³
  * Per daug galingas modelis
  * Per daug triukÅ¡mo Ä¯vesties duomenyse

## Kaip aptikti per didelÄ¯ pritaikymÄ…

Kaip matote iÅ¡ aukÅ¡Äiau pateikto grafiko, per didelÄ¯ pritaikymÄ… galima aptikti pagal labai maÅ¾Ä… mokymo klaidÄ… ir didelÄ™ validacijos klaidÄ…. Paprastai mokymo metu matysime, kaip tiek mokymo, tiek validacijos klaidos pradeda maÅ¾Ä—ti, o tada tam tikru momentu validacijos klaida gali nustoti maÅ¾Ä—ti ir pradÄ—ti didÄ—ti. Tai bus per didelio pritaikymo Å¾enklas ir indikatorius, kad turÄ—tume sustabdyti mokymÄ… (arba bent jau iÅ¡saugoti modelio bÅ«senÄ…).

![overfitting](../../../../../translated_images/lt/Overfitting.408ad91cd90b4371.webp)

## Kaip iÅ¡vengti per didelio pritaikymo

Jei pastebite, kad atsiranda per didelis pritaikymas, galite atlikti vienÄ… iÅ¡ Å¡iÅ³ veiksmÅ³:

 * Padidinti mokymo duomenÅ³ kiekÄ¯
 * SumaÅ¾inti modelio sudÄ—tingumÄ…
 * Naudoti tam tikrÄ… [reguliavimo technikÄ…](../../4-ComputerVision/08-TransferLearning/TrainingTricks.md), pvz., [Dropout](../../4-ComputerVision/08-TransferLearning/TrainingTricks.md#Dropout), kuriÄ… aptarsime vÄ—liau.

## Per didelis pritaikymas ir Å¡aliÅ¡kumo-variacijos kompromisas

Per didelis pritaikymas iÅ¡ tiesÅ³ yra bendresnÄ—s statistikos problemos, vadinamos [Å¡aliÅ¡kumo-variacijos kompromisu](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff), atvejis. Jei apsvarstysime galimus klaidÅ³ Å¡altinius mÅ«sÅ³ modelyje, galime matyti dviejÅ³ tipÅ³ klaidas:

* **Å aliÅ¡kumo klaidos** atsiranda dÄ—l to, kad mÅ«sÅ³ algoritmas negali teisingai uÅ¾fiksuoti ryÅ¡io tarp mokymo duomenÅ³. Tai gali bÅ«ti dÄ—l to, kad mÅ«sÅ³ modelis nÄ—ra pakankamai galingas (**nepakankamas pritaikymas**).
* **Variacijos klaidos**, kurios atsiranda dÄ—l to, kad modelis aproksimuoja triukÅ¡mÄ… Ä¯vesties duomenyse, o ne prasmingÄ… ryÅ¡Ä¯ (**per didelis pritaikymas**).

Mokymo metu Å¡aliÅ¡kumo klaida maÅ¾Ä—ja (kai mÅ«sÅ³ modelis mokosi aproksimuoti duomenis), o variacijos klaida didÄ—ja. Svarbu sustabdyti mokymÄ… - arba rankiniu bÅ«du (kai aptinkame per didelÄ¯ pritaikymÄ…), arba automatiÅ¡kai (Ä¯vedant reguliavimÄ…) - kad iÅ¡vengtume per didelio pritaikymo.

## IÅ¡vada

Å ioje pamokoje suÅ¾inojote apie skirtumus tarp Ä¯vairiÅ³ API dviejuose populiariausiuose AI karkasuose, TensorFlow ir PyTorch. Be to, suÅ¾inojote apie labai svarbiÄ… temÄ… - per didelÄ¯ pritaikymÄ….

## ğŸš€ IÅ¡Å¡Å«kis

Pridedamuose uÅ¾raÅ¡uose rasite â€uÅ¾duotisâ€œ apaÄioje; perÅ¾iÅ«rÄ—kite uÅ¾raÅ¡us ir atlikite uÅ¾duotis.

## [Po paskaitos vykdomas testas](https://ff-quizzes.netlify.app/en/ai/quiz/10)

## ApÅ¾valga ir savarankiÅ¡kas mokymasis

Atlikite tyrimÄ… Å¡iomis temomis:

- TensorFlow
- PyTorch
- Per didelis pritaikymas

Paklauskite savÄ™s Å¡iÅ³ klausimÅ³:

- Kuo skiriasi TensorFlow ir PyTorch?
- Kuo skiriasi per didelis pritaikymas ir nepakankamas pritaikymas?

## [UÅ¾duotis](lab/README.md)

Å ioje laboratorijoje jÅ«sÅ³ praÅ¡oma iÅ¡sprÄ™sti dvi klasifikavimo problemas, naudojant vieno ir daugiapakopius visiÅ¡kai sujungtus tinklus su PyTorch arba TensorFlow.

* [Instrukcijos](lab/README.md)
* [UÅ¾raÅ¡ai](lab/LabFrameworks.ipynb)

---

