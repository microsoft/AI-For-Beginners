<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "186bf7eeab776b36f557357ea56d4751",
  "translation_date": "2025-08-31T17:49:36+00:00",
  "source_file": "lessons/3-NeuralNetworks/04-OwnFramework/README.md",
  "language_code": "lt"
}
-->
# Ä®vadas Ä¯ neuroninius tinklus. Daugiasluoksnis perceptronas

Ankstesniame skyriuje suÅ¾inojote apie paprasÄiausiÄ… neuroninio tinklo modelÄ¯ â€“ vieno sluoksnio perceptronÄ…, kuris yra linijinis dviejÅ³ klasiÅ³ klasifikavimo modelis.

Å iame skyriuje iÅ¡plÄ—sime Å¡Ä¯ modelÄ¯ Ä¯ lankstesnÄ™ sistemÄ…, leidÅ¾ianÄiÄ…:

* atlikti **daugiaklasÄ™ klasifikacijÄ…** be dviejÅ³ klasiÅ³
* sprÄ™sti **regresijos problemas** be klasifikavimo
* atskirti klases, kurios nÄ—ra linijiÅ¡kai atskiriamos

Taip pat sukursime savo modulinÄ™ sistemÄ… Python kalboje, kuri leis mums konstruoti Ä¯vairias neuroniniÅ³ tinklÅ³ architektÅ«ras.

## [PrieÅ¡ paskaitÄ… vykdomas testas](https://ff-quizzes.netlify.app/en/ai/quiz/7)

## MaÅ¡ininio mokymosi formalizavimas

PradÄ—kime nuo maÅ¡ininio mokymosi problemos formalizavimo. Tarkime, turime mokymo duomenÅ³ rinkinÄ¯ **X** su Å¾ymÄ—mis **Y**, ir mums reikia sukurti modelÄ¯ *f*, kuris pateiktÅ³ kuo tikslesnes prognozes. PrognoziÅ³ kokybÄ— matuojama **nuostoliÅ³ funkcija** â„’. DaÅ¾nai naudojamos Å¡ios nuostoliÅ³ funkcijos:

* Regresijos problemoms, kai reikia prognozuoti skaiÄiÅ³, galime naudoti **absoliutinÄ™ paklaidÄ…** âˆ‘<sub>i</sub>|f(x<sup>(i)</sup>)-y<sup>(i)</sup>| arba **kvadratinÄ™ paklaidÄ…** âˆ‘<sub>i</sub>(f(x<sup>(i)</sup>)-y<sup>(i)</sup>)<sup>2</sup>
* Klasifikavimui naudojame **0-1 nuostolÄ¯** (kuris iÅ¡ esmÄ—s yra tas pats kaip modelio **tikslumas**) arba **logistinÄ¯ nuostolÄ¯**.

Vieno sluoksnio perceptrono funkcija *f* buvo apibrÄ—Å¾ta kaip linijinÄ— funkcija *f(x)=wx+b* (Äia *w* yra svoriÅ³ matrica, *x* yra Ä¯vesties poÅ¾ymiÅ³ vektorius, o *b* yra poslinkio vektorius). Skirtingoms neuroniniÅ³ tinklÅ³ architektÅ«roms Å¡i funkcija gali bÅ«ti sudÄ—tingesnÄ—.

> Klasifikavimo atveju daÅ¾nai pageidaujama, kad tinklo iÅ¡vestis bÅ«tÅ³ klasiÅ³ tikimybÄ—s. Norint paversti bet kokius skaiÄius Ä¯ tikimybes (pvz., normalizuoti iÅ¡vestÄ¯), daÅ¾nai naudojame **softmax** funkcijÄ… Ïƒ, ir funkcija *f* tampa *f(x)=Ïƒ(wx+b)*

AukÅ¡Äiau pateiktoje *f* apibrÄ—Å¾tyje *w* ir *b* vadinami **parametrais** Î¸=âŸ¨*w,b*âŸ©. TurÄ—dami duomenÅ³ rinkinÄ¯ âŸ¨**X**,**Y**âŸ©, galime apskaiÄiuoti bendrÄ… klaidÄ… visame duomenÅ³ rinkinyje kaip funkcijÄ… nuo parametrÅ³ Î¸.

> âœ… **Neuroninio tinklo mokymo tikslas yra sumaÅ¾inti klaidÄ… keiÄiant parametrus Î¸**

## Gradientinio nusileidimo optimizavimas

Yra gerai Å¾inomas funkcijÅ³ optimizavimo metodas, vadinamas **gradientiniu nusileidimu**. Jo idÄ—ja yra ta, kad galime apskaiÄiuoti nuostoliÅ³ funkcijos iÅ¡vestinÄ™ (daugiamatÄ—je erdvÄ—je vadinamÄ… **gradientu**) pagal parametrus ir keisti parametrus taip, kad klaida sumaÅ¾Ä—tÅ³. Tai galima formalizuoti taip:

* Inicializuokite parametrus atsitiktinÄ—mis reikÅ¡mÄ—mis w<sup>(0)</sup>, b<sup>(0)</sup>
* Kartokite Å¡Ä¯ Å¾ingsnÄ¯ daug kartÅ³:
    - w<sup>(i+1)</sup> = w<sup>(i)</sup>-Î·âˆ‚â„’/âˆ‚w
    - b<sup>(i+1)</sup> = b<sup>(i)</sup>-Î·âˆ‚â„’/âˆ‚b

Mokymo metu optimizavimo Å¾ingsniai turÄ—tÅ³ bÅ«ti skaiÄiuojami atsiÅ¾velgiant Ä¯ visÄ… duomenÅ³ rinkinÄ¯ (prisiminkite, kad nuostoliai skaiÄiuojami kaip suma per visus mokymo pavyzdÅ¾ius). TaÄiau realiame gyvenime mes imame maÅ¾as duomenÅ³ rinkinio dalis, vadinamas **minipartijomis**, ir skaiÄiuojame gradientus remdamiesi duomenÅ³ pogrupiu. Kadangi kiekvienÄ… kartÄ… pogrupis imamas atsitiktinai, toks metodas vadinamas **stochastiniu gradientiniu nusileidimu** (SGD).

## Daugiasluoksniai perceptronai ir atgalinio sklidimo algoritmas

Vieno sluoksnio tinklas, kaip matÄ—me aukÅ¡Äiau, gali klasifikuoti linijiÅ¡kai atskiriamas klases. NorÄ—dami sukurti turtingesnÄ¯ modelÄ¯, galime sujungti kelis tinklo sluoksnius. MatematiÅ¡kai tai reikÅ¡tÅ³, kad funkcija *f* turÄ—tÅ³ sudÄ—tingesnÄ™ formÄ… ir bÅ«tÅ³ skaiÄiuojama keliais etapais:
* z<sub>1</sub>=w<sub>1</sub>x+b<sub>1</sub>
* z<sub>2</sub>=w<sub>2</sub>Î±(z<sub>1</sub>)+b<sub>2</sub>
* f = Ïƒ(z<sub>2</sub>)

ÄŒia Î± yra **nelinijinÄ— aktyvavimo funkcija**, Ïƒ yra softmax funkcija, o parametrai Î¸=<*w<sub>1</sub>,b<sub>1</sub>,w<sub>2</sub>,b<sub>2</sub>*>.

Gradientinio nusileidimo algoritmas iÅ¡liktÅ³ toks pat, taÄiau gradientÅ³ skaiÄiavimas bÅ«tÅ³ sudÄ—tingesnis. Remiantis grandinÄ—s diferencijavimo taisykle, galime apskaiÄiuoti iÅ¡vestines taip:

* âˆ‚â„’/âˆ‚w<sub>2</sub> = (âˆ‚â„’/âˆ‚Ïƒ)(âˆ‚Ïƒ/âˆ‚z<sub>2</sub>)(âˆ‚z<sub>2</sub>/âˆ‚w<sub>2</sub>)
* âˆ‚â„’/âˆ‚w<sub>1</sub> = (âˆ‚â„’/âˆ‚Ïƒ)(âˆ‚Ïƒ/âˆ‚z<sub>2</sub>)(âˆ‚z<sub>2</sub>/âˆ‚Î±)(âˆ‚Î±/âˆ‚z<sub>1</sub>)(âˆ‚z<sub>1</sub>/âˆ‚w<sub>1</sub>)

> âœ… GrandinÄ—s diferencijavimo taisyklÄ— naudojama nuostoliÅ³ funkcijos iÅ¡vestinÄ—ms pagal parametrus apskaiÄiuoti.

Atkreipkite dÄ—mesÄ¯, kad kairiausia visÅ³ Å¡iÅ³ iÅ¡raiÅ¡kÅ³ dalis yra ta pati, todÄ—l galime efektyviai apskaiÄiuoti iÅ¡vestines, pradedant nuo nuostoliÅ³ funkcijos ir einant "atgal" per skaiÄiavimo grafikÄ…. TodÄ—l daugiasluoksnio perceptrono mokymo metodas vadinamas **atgaliniu sklidimu** arba 'backprop'.

<img alt="skaiÄiavimo grafikas" src="images/ComputeGraphGrad.png"/>

> TODO: paveikslÄ—lio citata

> âœ… AtgalinÄ¯ sklidimÄ… aptarsime daug iÅ¡samiau mÅ«sÅ³ uÅ¾raÅ¡Å³ knygelÄ—s pavyzdyje.  

## IÅ¡vada

Å ioje pamokoje sukÅ«rÄ—me savo neuroninio tinklo bibliotekÄ… ir panaudojome jÄ… paprastai dviejÅ³ dimensijÅ³ klasifikavimo uÅ¾duoÄiai.

## ğŸš€ IÅ¡Å¡Å«kis

Pridedamoje uÅ¾raÅ¡Å³ knygelÄ—je Ä¯gyvendinsite savo sistemÄ… daugiasluoksniÅ³ perceptronÅ³ kÅ«rimui ir mokymui. GalÄ—site iÅ¡samiai pamatyti, kaip veikia Å¡iuolaikiniai neuroniniai tinklai.

Pereikite prie [OwnFramework](OwnFramework.ipynb) uÅ¾raÅ¡Å³ knygelÄ—s ir dirbkite su ja.

## [Po paskaitos vykdomas testas](https://ff-quizzes.netlify.app/en/ai/quiz/8)

## ApÅ¾valga ir savarankiÅ¡kas mokymasis

Atgalinis sklidimas yra daÅ¾nai naudojamas algoritmas dirbtinio intelekto ir maÅ¡ininio mokymosi srityse, verta jÄ¯ [iÅ¡samiau iÅ¡nagrinÄ—ti](https://wikipedia.org/wiki/Backpropagation)

## [UÅ¾duotis](lab/README.md)

Å ioje laboratorijoje jÅ«sÅ³ praÅ¡oma panaudoti sistemÄ…, kuriÄ… sukÅ«rÄ—te Å¡ioje pamokoje, MNIST ranka raÅ¡ytÅ³ skaitmenÅ³ klasifikavimo uÅ¾duoÄiai sprÄ™sti.

* [Instrukcijos](lab/README.md)
* [UÅ¾raÅ¡Å³ knygelÄ—](lab/MyFW_MNIST.ipynb)

---

**AtsakomybÄ—s apribojimas**:  
Å is dokumentas buvo iÅ¡verstas naudojant AI vertimo paslaugÄ… [Co-op Translator](https://github.com/Azure/co-op-translator). Nors siekiame tikslumo, praÅ¡ome atkreipti dÄ—mesÄ¯, kad automatiniai vertimai gali turÄ—ti klaidÅ³ ar netikslumÅ³. Originalus dokumentas jo gimtÄ…ja kalba turÄ—tÅ³ bÅ«ti laikomas autoritetingu Å¡altiniu. Kritinei informacijai rekomenduojama naudoti profesionalÅ³ Å¾mogaus vertimÄ…. Mes neprisiimame atsakomybÄ—s uÅ¾ nesusipratimus ar klaidingus interpretavimus, atsiradusius dÄ—l Å¡io vertimo naudojimo.