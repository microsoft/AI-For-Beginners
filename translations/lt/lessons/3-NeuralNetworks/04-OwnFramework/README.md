# Ä®vadas Ä¯ neuroninius tinklus. Daugiasluoksnis perceptronas

Ankstesniame skyriuje suÅ¾inojote apie paprasÄiausiÄ… neuroninio tinklo modelÄ¯ â€“ vienasluoksnÄ¯ perceptronÄ…, kuris yra linijinis dviejÅ³ klasiÅ³ klasifikavimo modelis.

Å iame skyriuje iÅ¡plÄ—sime Å¡Ä¯ modelÄ¯ Ä¯ lankstesnÄ™ sistemÄ…, kuri leis:

* atlikti **daugiaklasÄ™ klasifikacijÄ…** be dviejÅ³ klasiÅ³
* sprÄ™sti **regresijos problemas** be klasifikavimo
* atskirti klases, kurios nÄ—ra linijiÅ¡kai atskiriamos

Taip pat sukursime savo modulinÄ™ sistemÄ… Python kalba, kuri leis konstruoti Ä¯vairias neuroniniÅ³ tinklÅ³ architektÅ«ras.

## [PrieÅ¡ paskaitÄ… â€“ testas](https://ff-quizzes.netlify.app/en/ai/quiz/7)

## MaÅ¡ininio mokymosi formalizavimas

PradÄ—kime nuo maÅ¡ininio mokymosi problemos formalizavimo. Tarkime, turime mokymo duomenÅ³ rinkinÄ¯ **X** su Å¾ymÄ—mis **Y**, ir mums reikia sukurti modelÄ¯ *f*, kuris pateiktÅ³ kuo tikslesnes prognozes. PrognoziÅ³ kokybÄ— matuojama pagal **nuostoliÅ³ funkcijÄ…** &lagran;. DaÅ¾niausiai naudojamos Å¡ios nuostoliÅ³ funkcijos:

* Regresijos problemoms, kai reikia prognozuoti skaiÄiÅ³, galime naudoti **absoliutinÄ™ paklaidÄ…** &sum;<sub>i</sub>|f(x<sup>(i)</sup>)-y<sup>(i)</sup>| arba **kvadratinÄ™ paklaidÄ…** &sum;<sub>i</sub>(f(x<sup>(i)</sup>)-y<sup>(i)</sup>)<sup>2</sup>
* Klasifikavimui naudojame **0-1 nuostolÄ¯** (kuris iÅ¡ esmÄ—s yra tas pats, kas modelio **tikslumas**) arba **logistinÄ¯ nuostolÄ¯**.

Vienasluoksniame perceptrone funkcija *f* buvo apibrÄ—Å¾ta kaip linijinÄ— funkcija *f(x)=wx+b* (Äia *w* yra svoriÅ³ matrica, *x* â€“ Ä¯vesties poÅ¾ymiÅ³ vektorius, o *b* â€“ poslinkio vektorius). Skirtingose neuroniniÅ³ tinklÅ³ architektÅ«rose Å¡i funkcija gali bÅ«ti sudÄ—tingesnÄ—.

> Klasifikavimo atveju daÅ¾nai pageidaujama, kad tinklo iÅ¡vestis bÅ«tÅ³ atitinkamÅ³ klasiÅ³ tikimybÄ—s. Norint paversti bet kokius skaiÄius Ä¯ tikimybes (pvz., normalizuoti iÅ¡vestÄ¯), daÅ¾nai naudojame **softmax** funkcijÄ… &sigma;, ir funkcija *f* tampa *f(x)=&sigma;(wx+b)*.

ApibrÄ—Å¾ime *f* aukÅ¡Äiau, *w* ir *b* vadinami **parametrais** &theta;=âŸ¨*w,b*âŸ©. TurÄ—dami duomenÅ³ rinkinÄ¯ âŸ¨**X**,**Y**âŸ©, galime apskaiÄiuoti bendrÄ… klaidÄ… visam duomenÅ³ rinkiniui kaip funkcijÄ… nuo parametrÅ³ &theta;.

> âœ… **Neuroninio tinklo mokymo tikslas yra sumaÅ¾inti klaidÄ… keiÄiant parametrus &theta;**

## Gradientinio nusileidimo optimizavimas

Yra gerai Å¾inomas funkcijÅ³ optimizavimo metodas, vadinamas **gradientiniu nusileidimu**. Jo idÄ—ja yra ta, kad galime apskaiÄiuoti nuostoliÅ³ funkcijos iÅ¡vestinÄ™ (daugiamatÄ—je erdvÄ—je vadinamÄ… **gradientu**) pagal parametrus ir keisti parametrus taip, kad klaida sumaÅ¾Ä—tÅ³. Tai galima formalizuoti taip:

* Inicializuokite parametrus atsitiktinÄ—mis reikÅ¡mÄ—mis w<sup>(0)</sup>, b<sup>(0)</sup>
* Kartokite Å¡iuos veiksmus daug kartÅ³:
    - w<sup>(i+1)</sup> = w<sup>(i)</sup>-&eta;&part;&lagran;/&part;w
    - b<sup>(i+1)</sup> = b<sup>(i)</sup>-&eta;&part;&lagran;/&part;b

Mokymo metu optimizavimo Å¾ingsniai turÄ—tÅ³ bÅ«ti skaiÄiuojami atsiÅ¾velgiant Ä¯ visÄ… duomenÅ³ rinkinÄ¯ (prisiminkite, kad nuostoliai skaiÄiuojami kaip suma per visus mokymo pavyzdÅ¾ius). TaÄiau realiame gyvenime imame maÅ¾as duomenÅ³ rinkinio dalis, vadinamas **minipartijomis**, ir skaiÄiuojame gradientus remdamiesi duomenÅ³ pogrupiu. Kadangi pogrupis kiekvienÄ… kartÄ… imamas atsitiktinai, toks metodas vadinamas **stochastiniu gradientiniu nusileidimu** (SGD).

## Daugiasluoksniai perceptronai ir atgalinis sklidimas

Vienasluoksnis tinklas, kaip matÄ—me aukÅ¡Äiau, gali klasifikuoti linijiÅ¡kai atskiriamas klases. NorÄ—dami sukurti turtingesnÄ¯ modelÄ¯, galime sujungti kelis tinklo sluoksnius. MatematiÅ¡kai tai reikÅ¡tÅ³, kad funkcija *f* turÄ—tÅ³ sudÄ—tingesnÄ™ formÄ… ir bÅ«tÅ³ skaiÄiuojama keliais etapais:
* z<sub>1</sub>=w<sub>1</sub>x+b<sub>1</sub>
* z<sub>2</sub>=w<sub>2</sub>&alpha;(z<sub>1</sub>)+b<sub>2</sub>
* f = &sigma;(z<sub>2</sub>)

ÄŒia &alpha; yra **ne linijinÄ— aktyvavimo funkcija**, &sigma; yra softmax funkcija, o parametrai &theta;=<*w<sub>1</sub>,b<sub>1</sub>,w<sub>2</sub>,b<sub>2</sub>*>.

Gradientinio nusileidimo algoritmas iÅ¡liktÅ³ toks pat, taÄiau gradientÅ³ skaiÄiavimas bÅ«tÅ³ sudÄ—tingesnis. Remiantis grandinÄ—s diferencijavimo taisykle, galime apskaiÄiuoti iÅ¡vestines taip:

* &part;&lagran;/&part;w<sub>2</sub> = (&part;&lagran;/&part;&sigma;)(&part;&sigma;/&part;z<sub>2</sub>)(&part;z<sub>2</sub>/&part;w<sub>2</sub>)
* &part;&lagran;/&part;w<sub>1</sub> = (&part;&lagran;/&part;&sigma;)(&part;&sigma;/&part;z<sub>2</sub>)(&part;z<sub>2</sub>/&part;&alpha;)(&part;&alpha;/&part;z<sub>1</sub>)(&part;z<sub>1</sub>/&part;w<sub>1</sub>)

> âœ… GrandinÄ—s diferencijavimo taisyklÄ— naudojama nuostoliÅ³ funkcijos iÅ¡vestinÄ—ms pagal parametrus apskaiÄiuoti.

Atkreipkite dÄ—mesÄ¯, kad kairiausia visÅ³ Å¡iÅ³ iÅ¡raiÅ¡kÅ³ dalis yra ta pati, todÄ—l efektyviai galime apskaiÄiuoti iÅ¡vestines, pradedant nuo nuostoliÅ³ funkcijos ir einant "atgal" per skaiÄiavimo grafikÄ…. TodÄ—l daugiasluoksnio perceptrono mokymo metodas vadinamas **atgaliniu sklidimu** arba 'backprop'.

<img alt="skaiÄiavimo grafikas" src="../../../../../translated_images/lt/ComputeGraphGrad.4626252c0de03507.webp"/>

> TODO: paveikslÄ—lio citata

> âœ… AtgalinÄ¯ sklidimÄ… iÅ¡samiau aptarsime mÅ«sÅ³ uÅ¾raÅ¡Å³ knygelÄ—s pavyzdyje.

## IÅ¡vada

Å ioje pamokoje sukÅ«rÄ—me savo neuroninio tinklo bibliotekÄ… ir panaudojome jÄ… paprastai dviejÅ³ dimensijÅ³ klasifikavimo uÅ¾duoÄiai.

## ğŸš€ IÅ¡Å¡Å«kis

Pridedamoje uÅ¾raÅ¡Å³ knygelÄ—je Ä¯gyvendinsite savo sistemÄ… daugiasluoksniÅ³ perceptronÅ³ kÅ«rimui ir mokymui. GalÄ—site detaliai pamatyti, kaip veikia Å¡iuolaikiniai neuroniniai tinklai.

Pereikite prie [OwnFramework](OwnFramework.ipynb) uÅ¾raÅ¡Å³ knygelÄ—s ir jÄ… iÅ¡nagrinÄ—kite.

## [Po paskaitos â€“ testas](https://ff-quizzes.netlify.app/en/ai/quiz/8)

## ApÅ¾valga ir savarankiÅ¡kas mokymasis

Atgalinis sklidimas yra daÅ¾nai naudojamas algoritmas AI ir ML srityse, verta jÄ¯ [iÅ¡samiau iÅ¡nagrinÄ—ti](https://wikipedia.org/wiki/Backpropagation).

## [UÅ¾duotis](lab/README.md)

Å ioje laboratorijoje jÅ«sÅ³ praÅ¡oma panaudoti sistemÄ…, kuriÄ… sukÅ«rÄ—te Å¡ioje pamokoje, MNIST ranka raÅ¡ytÅ³ skaitmenÅ³ klasifikavimo uÅ¾duoÄiai sprÄ™sti.

* [Instrukcijos](lab/README.md)
* [UÅ¾raÅ¡Å³ knygelÄ—](lab/MyFW_MNIST.ipynb)

---

