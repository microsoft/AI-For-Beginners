{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "\n",
    "For di example wey we do before, we dey use high-dimensional bag-of-words vectors wey get length `vocab_size`, and we turn low-dimensional positional representation vectors into sparse one-hot representation. Dis one-hot representation no dey save memory well. Plus, e dey treat each word as if dem no relate, so one-hot encoded vectors no fit show di meaning similarity wey dey between words.\n",
    "\n",
    "For dis unit, we go still dey look di **News AG** dataset. To start, make we load di data and collect some definitions from di last unit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "\n",
    "ds_train, ds_test = tfds.load('ag_news_subset').values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wetin be embedding?\n",
    "\n",
    "Di idea of **embedding** na to use small-small dense vectors wey go show di meaning wey dey inside di word. Later we go talk how to make beta word embeddings, but for now make we just see embeddings as one way to make di word vector no too big.\n",
    "\n",
    "So, embedding layer dey take word as input, and e dey give output vector wey get di `embedding_size` wey you set. E be like `Dense` layer, but e no dey use one-hot encoded vector as input, e fit use word number instead.\n",
    "\n",
    "If we use embedding layer as di first layer for our network, we fit change from bag-of-words to **embedding bag** model. For dis one, we go first change each word for our text to di embedding wey match am, then we go do one kind calculation for all di embeddings, like `sum`, `average` or `max`.\n",
    "\n",
    "![Image wey dey show embedding classifier for five sequence words.](../../../../../translated_images/embedding-classifier-example.b77f021a7ee67eeec8e68bfe11636c5b97d6eaa067515a129bfb1d0034b1ac5b.pcm.png)\n",
    "\n",
    "Our classifier neural network get di following layers:\n",
    "\n",
    "* `TextVectorization` layer, wey dey take string as input, and e dey give tensor of token numbers. We go set one beta vocabulary size `vocab_size`, and we go ignore words wey people no dey use well-well. Di input shape go be 1, and di output shape go be $n$, because we go get $n$ tokens as result, and each one go get numbers from 0 to `vocab_size`.\n",
    "* `Embedding` layer, wey dey take $n$ numbers, and e dey reduce each number to dense vector wey get di length wey you set (100 for our example). So di input tensor wey get shape $n$ go turn $n\\times 100$ tensor.\n",
    "* Aggregation layer, wey dey take di average of di tensor along di first axis. Dis one mean say e go calculate di average of all $n$ input tensors wey match di different words. To do dis layer, we go use `Lambda` layer, and we go pass di function wey go calculate di average. Di output go get shape of 100, and e go be di numeric representation of di whole input sequence.\n",
    "* Final `Dense` linear classifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization (TextVec  (None, None)             0         \n",
      " torization)                                                     \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, None, 100)         3000000   \n",
      "                                                                 \n",
      " lambda (Lambda)             (None, 100)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4)                 404       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,000,404\n",
      "Trainable params: 3,000,404\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 30000\n",
    "batch_size = 128\n",
    "\n",
    "vectorizer = keras.layers.experimental.preprocessing.TextVectorization(max_tokens=vocab_size,input_shape=(1,))\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    vectorizer,    \n",
    "    keras.layers.Embedding(vocab_size,100),\n",
    "    keras.layers.Lambda(lambda x: tf.reduce_mean(x,axis=1)),\n",
    "    keras.layers.Dense(4, activation='softmax')\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For di `summary` wey e print, for di **output shape** column, di first tensor dimension `None` mean di minibatch size, and di second one mean di length of di token sequence. All di token sequence for di minibatch get different length. We go talk how we go handle am for di next section.\n",
    "\n",
    "Now make we train di network:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training vectorizer\n",
      "938/938 [==============================] - 20s 20ms/step - loss: 0.7891 - acc: 0.8155 - val_loss: 0.4470 - val_acc: 0.8642\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22255515100>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_text(x):\n",
    "    return x['title']+' '+x['description']\n",
    "\n",
    "def tupelize(x):\n",
    "    return (extract_text(x),x['label'])\n",
    "\n",
    "print(\"Training vectorizer\")\n",
    "vectorizer.adapt(ds_train.take(500).map(extract_text))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'])\n",
    "model.fit(ds_train.map(tupelize).batch(batch_size),validation_data=ds_test.map(tupelize).batch(batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "> **Note** say we dey build vectorizer based on small part of di data. Dis one na to make di process fast, and e fit mean say no be all di tokens wey dey our text go dey di vocabulary. If dis one happen, dem go ignore di tokens, and e fit make di accuracy small. But for real life, small part of text dey always give better vocabulary estimation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to handle sequence wey get different size\n",
    "\n",
    "Make we understand how training dey happen for minibatches. For di example wey dey up, di input tensor get dimension 1, and we dey use 128-long minibatches, so di real size of di tensor na $128 \\times 1$. But, di number of tokens wey dey each sentence no dey di same. If we use di `TextVectorization` layer for one input, di number of tokens wey e go return go dey different, e go depend on how dem tokenize di text:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 1 45], shape=(2,), dtype=int64)\n",
      "tf.Tensor([ 112 1271    1    3 1747  158], shape=(6,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer('Hello, world!'))\n",
    "print(vectorizer('I am glad to meet you!'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But wen we use di vectorizer for plenti sequences, e go produce tensor wey get rectangular shape, so e go fill di unused elements wit di PAD token (wey for our case na zero):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 6), dtype=int64, numpy=\n",
       "array([[   1,   45,    0,    0,    0,    0],\n",
       "       [ 112, 1271,    1,    3, 1747,  158]], dtype=int64)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer(['Hello, world!','I am glad to meet you!'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we fit see di embeddings:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 1.53059261e-02,  6.80514947e-02,  3.14026810e-02, ...,\n",
       "         -8.92002955e-02,  1.52911525e-04, -5.65562584e-02],\n",
       "        [ 2.57456154e-01,  2.79364467e-01, -2.03605562e-01, ...,\n",
       "         -2.07474351e-01,  8.31158683e-02, -2.03911960e-01],\n",
       "        [ 3.98201384e-02, -8.03454965e-03,  2.39790026e-02, ...,\n",
       "         -7.18549127e-04,  2.66963355e-02, -4.30646613e-02],\n",
       "        [ 3.98201384e-02, -8.03454965e-03,  2.39790026e-02, ...,\n",
       "         -7.18549127e-04,  2.66963355e-02, -4.30646613e-02],\n",
       "        [ 3.98201384e-02, -8.03454965e-03,  2.39790026e-02, ...,\n",
       "         -7.18549127e-04,  2.66963355e-02, -4.30646613e-02],\n",
       "        [ 3.98201384e-02, -8.03454965e-03,  2.39790026e-02, ...,\n",
       "         -7.18549127e-04,  2.66963355e-02, -4.30646613e-02]],\n",
       "\n",
       "       [[ 1.89674050e-01,  2.61548996e-01, -3.67433839e-02, ...,\n",
       "         -2.07366899e-01, -1.05442435e-01, -2.36952081e-01],\n",
       "        [ 6.16133213e-02,  1.80511594e-01,  9.77298319e-02, ...,\n",
       "         -5.46628237e-02, -1.07340455e-01, -1.06589928e-01],\n",
       "        [ 1.53059261e-02,  6.80514947e-02,  3.14026810e-02, ...,\n",
       "         -8.92002955e-02,  1.52911525e-04, -5.65562584e-02],\n",
       "        [-4.84890305e-02, -8.41715634e-02,  1.51529670e-01, ...,\n",
       "          1.28192469e-01, -7.77286515e-02,  1.26041949e-01],\n",
       "        [-4.17212099e-02, -5.60694858e-02,  4.08860669e-02, ...,\n",
       "          8.70475471e-02,  8.92383084e-02,  1.67974353e-01],\n",
       "        [ 2.85779923e-01,  4.57767487e-01,  4.52292450e-02, ...,\n",
       "         -1.97419018e-01, -2.04659685e-01, -2.79758364e-01]]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[1](vectorizer(['Hello, world!','I am glad to meet you!'])).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note**: To reduce how much padding go dey, e fit make sense to arrange all the sequences for the dataset according to how dem dey increase for length (or, make am clear, number of tokens). Dis one go make sure say each minibatch get sequences wey get similar length.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic embeddings: Word2Vec\n",
    "\n",
    "For di example wey we do before, di embedding layer learn how e go fit map words to vector representations, but di representations no get semantic meaning. E go make sense if we fit learn vector representation wey go make similar words or synonyms dey close to each oda based on vector distance (like euclidian distance).\n",
    "\n",
    "To do dis one, we go need to pretrain our embedding model for big text collection using technique like [Word2Vec](https://en.wikipedia.org/wiki/Word2vec). Dis one dey based on two main architectures wey dem dey use to produce distributed representation of words:\n",
    "\n",
    " - **Continuous bag-of-words** (CBoW), na where we dey train di model to predict one word from di surrounding context. If dem give di ngram $(W_{-2},W_{-1},W_0,W_1,W_2)$, di goal of di model na to predict $W_0$ from $(W_{-2},W_{-1},W_1,W_2)$.\n",
    " - **Continuous skip-gram** na di opposite of CBoW. Di model dey use di surrounding window of context words to predict di current word.\n",
    "\n",
    "CBoW fast well, but skip-gram slow small, e dey represent words wey no dey common better.\n",
    "\n",
    "![Image wey dey show both CBoW and Skip-Gram algorithms to convert words to vectors.](../../../../../translated_images/example-algorithms-for-converting-words-to-vectors.fbe9207a726922f6f0f5de66427e8a6eda63809356114e28fb1fa5f4a83ebda7.pcm.png)\n",
    "\n",
    "To test di Word2Vec embedding wey dem don pretrain for Google News dataset, we fit use di **gensim** library. For di example below, we go find di words wey dey most similar to 'neural'.\n",
    "\n",
    "> **Note:** When you first create word vectors, e fit take time to download dem!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "w2v = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neuronal -> 0.7804799675941467\n",
      "neurons -> 0.7326500415802002\n",
      "neural_circuits -> 0.7252851724624634\n",
      "neuron -> 0.7174385190010071\n",
      "cortical -> 0.6941086649894714\n",
      "brain_circuitry -> 0.6923246383666992\n",
      "synaptic -> 0.6699118614196777\n",
      "neural_circuitry -> 0.6638563275337219\n",
      "neurochemical -> 0.6555314064025879\n",
      "neuronal_activity -> 0.6531826257705688\n"
     ]
    }
   ],
   "source": [
    "for w,p in w2v.most_similar('neural'):\n",
    "    print(f\"{w} -> {p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fit extract di vector embedding from di word, to use am train di classification model. Di embedding get 300 components, but for here we go show only di first 20 components of di vector make e clear:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01226807,  0.06225586,  0.10693359,  0.05810547,  0.23828125,\n",
       "        0.03686523,  0.05151367, -0.20703125,  0.01989746,  0.10058594,\n",
       "       -0.03759766, -0.1015625 , -0.15820312, -0.08105469, -0.0390625 ,\n",
       "       -0.05053711,  0.16015625,  0.2578125 ,  0.10058594, -0.25976562],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v['play'][:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Di beta tin wey semantic embeddings dey do be say you fit use di vector encoding based on di meaning. For example, we fit ask make e find one word wey di vector representation dey near di words *king* and *woman*, and e far from di word *man*:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('queen', 0.7118192911148071)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar(positive=['king','woman'],negative=['man'])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Di example wey dey up use some internal GenSym magic, but di logic wey dey under am no too hard. One interesting thing about embeddings be say you fit do normal vector operations for embedding vectors, and e go show operations for word **meanings**. Di example wey dey up fit dey explain with vector operations: we dey calculate di vector wey match **KING-MAN+WOMAN** (di operations `+` and `-` dey happen for vector representations of di words wey dey match), and then we go find di word wey dey closest to dat vector for di dictionary:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'queen'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the vector corresponding to kind-man+woman\n",
    "qvec = w2v['king']-1.7*w2v['man']+1.7*w2v['woman']\n",
    "# find the index of the closest embedding vector \n",
    "d = np.sum((w2v.vectors-qvec)**2,axis=1)\n",
    "min_idx = np.argmin(d)\n",
    "# find the corresponding word\n",
    "w2v.index_to_key[min_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NOTE**: We gatz add small coefficients to *man* and *woman* vectors - try remove am make you see wetin go happen.\n",
    "\n",
    "To find di vector wey dey closest, we dey use TensorFlow machinery to calculate vector of distances between our vector and all di vectors wey dey vocabulary, then we go find di index of di word wey get di smallest distance using `argmin`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While Word2Vec dey look like better way to show wetin word mean, e get plenty wahala, like di ones wey dey below:\n",
    "\n",
    "* Both CBoW and skip-gram models na **predictive embeddings**, and dem dey only use di local context. Word2Vec no dey use di global context at all.\n",
    "* Word2Vec no dey consider word **morphology**, wey mean say e no dey look di way di meaning of word fit depend on di different parts of di word, like di root.\n",
    "\n",
    "**FastText** wan try solve di second problem, and e dey build on top Word2Vec by learning vector representations for each word and di character n-grams wey dey inside di word. Di values of di representations go then dey average into one vector for each training step. Even though dis one go add plenty extra computation for di pretraining, e go make word embeddings fit carry sub-word information.\n",
    "\n",
    "Another method, **GloVe**, dey use different way to do word embeddings, based on di factorization of di word-context matrix. First, e go build one big matrix wey go count how many times word dey appear for different contexts, then e go try represent di matrix for smaller dimensions in a way wey go reduce reconstruction loss.\n",
    "\n",
    "Di gensim library dey support all dis word embeddings, and you fit try dem out by changing di model loading code wey dey above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use pretrained embeddings for Keras\n",
    "\n",
    "We fit change di example wey dey up so dat we go fit put semantic embeddings like Word2Vec for di matrix wey dey our embedding layer. Di vocabularies for di pretrained embedding and di text corpus no go match well, so we go need choose one. For here, we go look di two options wey dey: using di tokenizer vocabulary, and using di vocabulary wey dey Word2Vec embeddings.\n",
    "\n",
    "### Using tokenizer vocabulary\n",
    "\n",
    "If we use di tokenizer vocabulary, some words for di vocabulary go get Word2Vec embeddings wey match dem, and some no go get. As our vocabulary size be `vocab_size`, and di Word2Vec embedding vector length be `embed_size`, di embedding layer go dey represented by weight matrix wey get shape `vocab_size`$\\times$`embed_size`. We go fill dis matrix by checking di vocabulary:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding size: 300\n",
      "Populating matrix, this will take some time...Done, found 4551 words, 784 words missing\n"
     ]
    }
   ],
   "source": [
    "embed_size = len(w2v.get_vector('hello'))\n",
    "print(f'Embedding size: {embed_size}')\n",
    "\n",
    "vocab = vectorizer.get_vocabulary()\n",
    "W = np.zeros((vocab_size,embed_size))\n",
    "print('Populating matrix, this will take some time...',end='')\n",
    "found, not_found = 0,0\n",
    "for i,w in enumerate(vocab):\n",
    "    try:\n",
    "        W[i] = w2v.get_vector(w)\n",
    "        found+=1\n",
    "    except:\n",
    "        # W[i] = np.random.normal(0.0,0.3,size=(embed_size,))\n",
    "        not_found+=1\n",
    "\n",
    "print(f\"Done, found {found} words, {not_found} words missing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For words wey no dey inside Word2Vec vocabulary, we fit either leave dem as zero, or we fit generate random vector for dem.\n",
    "\n",
    "Now, we fit define embedding layer wey get pretrained weights:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = keras.layers.Embedding(vocab_size,embed_size,weights=[W],trainable=False)\n",
    "model = keras.models.Sequential([\n",
    "    vectorizer, emb,\n",
    "    keras.layers.Lambda(lambda x: tf.reduce_mean(x,axis=1)),\n",
    "    keras.layers.Dense(4, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make we train our model now.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 10s 10ms/step - loss: 1.1075 - acc: 0.7822 - val_loss: 0.9134 - val_acc: 0.8175\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2220226ef10>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'])\n",
    "model.fit(ds_train.map(tupelize).batch(batch_size),\n",
    "          validation_data=ds_test.map(tupelize).batch(batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note**: See say we set `trainable=False` when we dey create `Embedding`, e mean say we no dey retrain the Embedding layer. Dis fit make accuracy small low, but e go make training fast.\n",
    "\n",
    "### How to use embedding vocabulary\n",
    "\n",
    "One wahala wey dey with di way we do am before na say di vocabularies wey TextVectorization and Embedding dey use no be di same. To solve dis problem, we fit use one of dis options:\n",
    "* Train di Word2Vec model again with our own vocabulary.\n",
    "* Load our dataset with di vocabulary wey dey di pretrained Word2Vec model. Di vocabularies wey we go use load di dataset fit dey specified when we dey load am.\n",
    "\n",
    "Di second option dey look easier, so make we do am. First, we go create `TextVectorization` layer wey get di vocabulary wey we take from di Word2Vec embeddings:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(w2v.vocab.keys())\n",
    "vectorizer = keras.layers.experimental.preprocessing.TextVectorization(input_shape=(1,))\n",
    "vectorizer.set_vocabulary(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gensim word embeddings library get one beta function, `get_keras_embeddings`, wey go automatically create di Keras embeddings layer for you.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "938/938 [==============================] - 20s 14ms/step - loss: 1.3377 - acc: 0.4978 - val_loss: 1.2995 - val_acc: 0.5647\n",
      "Epoch 2/5\n",
      "938/938 [==============================] - 10s 10ms/step - loss: 1.2587 - acc: 0.5722 - val_loss: 1.2339 - val_acc: 0.5842\n",
      "Epoch 3/5\n",
      "938/938 [==============================] - 10s 10ms/step - loss: 1.1980 - acc: 0.5884 - val_loss: 1.1826 - val_acc: 0.5954\n",
      "Epoch 4/5\n",
      "938/938 [==============================] - 12s 13ms/step - loss: 1.1503 - acc: 0.6002 - val_loss: 1.1417 - val_acc: 0.6018\n",
      "Epoch 5/5\n",
      "938/938 [==============================] - 11s 12ms/step - loss: 1.1120 - acc: 0.6097 - val_loss: 1.1083 - val_acc: 0.6104\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2220ccb81c0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    vectorizer, \n",
    "    w2v.get_keras_embedding(train_embeddings=False),\n",
    "    keras.layers.Lambda(lambda x: tf.reduce_mean(x,axis=1)),\n",
    "    keras.layers.Dense(4, activation='softmax')\n",
    "])\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'])\n",
    "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128),epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of di reasons why we no dey see higher accuracy na because some words from our dataset dey miss for di pretrained GloVe vocabulary, and so dem dey basically ignore dem. To solve dis one, we fit train our own embeddings based on our dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contextual embeddings\n",
    "\n",
    "One big wahala wey dey traditional pretrained embedding representations like Word2Vec be say, even though dem fit sabi small meaning of one word, dem no fit sabi di different meanings wey di word fit get. Dis one fit cause wahala for downstream models.\n",
    "\n",
    "For example, di word 'play' get different meaning for dis two sentences:\n",
    "- I go watch one **play** for di theater.\n",
    "- John wan **play** wit im friends.\n",
    "\n",
    "Di pretrained embeddings wey we don talk about go represent di two meanings of di word 'play' as di same embedding. To solve dis kain wahala, we need to build embeddings wey base on di **language model**, wey dem don train wit plenty text, and e *sabi* how words fit join together for different contexts. To talk about contextual embeddings no dey di scope of dis tutorial, but we go come back to dem when we dey talk about language models for di next unit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n<!-- CO-OP TRANSLATOR DISCLAIMER START -->\n**Disclaimer**:  \nDis dokyument don use AI translation service [Co-op Translator](https://github.com/Azure/co-op-translator) do di translation. Even as we dey try make am accurate, abeg make you sabi say machine translation fit get mistake or no dey correct well. Di original dokyument wey dey for im native language na di main source wey you go fit trust. For important information, e good make professional human translation dey use. We no go fit take blame for any misunderstanding or wrong interpretation wey fit happen because you use dis translation.\n<!-- CO-OP TRANSLATOR DISCLAIMER END -->\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0cb620c6d4b9f7a635928804c26cf22403d89d98d79684e4529119355ee6d5a5"
  },
  "kernel_info": {
   "name": "conda-env-py37_tensorflow-py"
  },
  "kernelspec": {
   "display_name": "py37_tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "coopTranslator": {
   "original_hash": "b859482be7f61d1eadc2c6a2720a37e4",
   "translation_date": "2025-11-18T19:27:10+00:00",
   "source_file": "lessons/5-NLP/14-Embeddings/EmbeddingsTF.ipynb",
   "language_code": "pcm"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}