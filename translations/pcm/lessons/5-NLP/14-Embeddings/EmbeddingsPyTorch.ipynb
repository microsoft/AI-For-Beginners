{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "\n",
    "For di example wey we do before, we dey use high-dimensional bag-of-words vectors wey get length `vocab_size`, and we dey change from low-dimensional positional representation vectors go sparse one-hot representation. Dis one-hot representation no dey use memory well, plus, e dey treat each word as if dem no relate to each oda, meaning say one-hot encoded vectors no dey show any semantic similarity between words.\n",
    "\n",
    "For dis unit, we go still dey look di **News AG** dataset. To start, make we load di data and get some definitions from di notebook wey we use before.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\WORK\\ai-for-beginners\\5-NLP\\14-Embeddings\\data\\train.csv: 29.5MB [00:01, 18.8MB/s]                            \n",
      "d:\\WORK\\ai-for-beginners\\5-NLP\\14-Embeddings\\data\\test.csv: 1.86MB [00:00, 11.2MB/s]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building vocab...\n",
      "Vocab size =  95812\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "import numpy as np\n",
    "from torchnlp import *\n",
    "train_dataset, test_dataset, classes, vocab = load_dataset()\n",
    "vocab_size = len(vocab)\n",
    "print(\"Vocab size = \",vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wetin be embedding?\n",
    "\n",
    "Di idea of **embedding** na to represent words wit vectors wey get lower-dimensional dense format, wey go fit show di meaning wey dey inside di word. Later we go talk how to build word embeddings wey get meaning, but for now, make we just see embeddings as one way to reduce di dimensionality of word vector.\n",
    "\n",
    "So, embedding layer go take one word as input, and e go produce output vector wey get di `embedding_size` wey you set. E dey similar to `Linear` layer, but instead of using one-hot encoded vector, e go fit take di word number as input.\n",
    "\n",
    "If we use embedding layer as di first layer for our network, we fit change from bag-of-words to **embedding bag** model. For dis model, we go first change each word for our text to di embedding wey match am, then we go calculate one aggregate function for all di embeddings, like `sum`, `average` or `max`.\n",
    "\n",
    "![Image showing an embedding classifier for five sequence words.](../../../../../translated_images/embedding-classifier-example.b77f021a7ee67eeec8e68bfe11636c5b97d6eaa067515a129bfb1d0034b1ac5b.pcm.png)\n",
    "\n",
    "Our classifier neural network go start wit embedding layer, then aggregation layer, and linear classifier on top:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbedClassifier(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super().__init__()\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, embed_dim)\n",
    "        self.fc = torch.nn.Linear(embed_dim, num_class)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = torch.mean(x,dim=1)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to Handle Variable Sequence Size\n",
    "\n",
    "Because of how dis kain architecture be, we go need create minibatches for our network in one special way. For di last unit, wen we dey use bag-of-words, all di BoW tensors wey dey inside one minibatch get di same size `vocab_size`, no matter how long di text sequence be. But once we start to use word embeddings, di number of words wey dey each text sample go dey different, and to join all di samples together inside minibatches, we go need add some padding.\n",
    "\n",
    "We fit do dis one by using di same method wey involve providing `collate_fn` function to di datasource:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padify(b):\n",
    "    # b is the list of tuples of length batch_size\n",
    "    #   - first element of a tuple = label, \n",
    "    #   - second = feature (text sequence)\n",
    "    # build vectorized sequence\n",
    "    v = [encode(x[1]) for x in b]\n",
    "    # first, compute max length of a sequence in this minibatch\n",
    "    l = max(map(len,v))\n",
    "    return ( # tuple of two tensors - labels and features\n",
    "        torch.LongTensor([t[0]-1 for t in b]),\n",
    "        torch.stack([torch.nn.functional.pad(torch.tensor(t),(0,l-len(t)),mode='constant',value=0) for t in v])\n",
    "    )\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, collate_fn=padify, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train di embedding classifier\n",
    "\n",
    "Now wey we don set correct dataloader, we fit train di model wit di training function wey we don define for di previous unit:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200: acc=0.6415625\n",
      "6400: acc=0.6865625\n",
      "9600: acc=0.7103125\n",
      "12800: acc=0.726953125\n",
      "16000: acc=0.739375\n",
      "19200: acc=0.75046875\n",
      "22400: acc=0.7572321428571429\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.889799795315499, 0.7623160588611644)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = EmbedClassifier(vocab_size,32,len(classes)).to(device)\n",
    "train_epoch(net,train_loader, lr=1, epoch_size=25000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note**: We dey train only 25k records here (e no reach one full epoch) sake of time, but you fit continue to train, write function wey go train for plenty epochs, and test wit learning rate parameter to get better accuracy. You fit reach accuracy wey go near 90%.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EmbeddingBag Layer and Variable-Length Sequence Representation\n",
    "\n",
    "For di architecture wey we bin dey use before, we need to pad all di sequences make dem get di same length so dem go fit enter minibatch. Dis no be di most efficient way to represent sequences wey get different length - another way na to use **offset** vector, wey go hold di offsets of all di sequences wey dey inside one big vector.\n",
    "\n",
    "![Image wey dey show offset sequence representation](../../../../../translated_images/offset-sequence-representation.eb73fcefb29b46eecfbe74466077cfeb7c0f93a4f254850538a2efbc63517479.pcm.png)\n",
    "\n",
    "> **Note**: For di picture wey dey up, we dey show sequence of characters, but for our example we dey work with sequences of words. But di general principle of how to represent sequences with offset vector still remain di same.\n",
    "\n",
    "To work with offset representation, we dey use [`EmbeddingBag`](https://pytorch.org/docs/stable/generated/torch.nn.EmbeddingBag.html) layer. E be like `Embedding`, but e dey take content vector and offset vector as input, and e also get averaging layer, wey fit be `mean`, `sum` or `max`.\n",
    "\n",
    "Dis na di modified network wey dey use `EmbeddingBag`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbedClassifier(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super().__init__()\n",
    "        self.embedding = torch.nn.EmbeddingBag(vocab_size, embed_dim)\n",
    "        self.fc = torch.nn.Linear(embed_dim, num_class)\n",
    "\n",
    "    def forward(self, text, off):\n",
    "        x = self.embedding(text, off)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prepare di dataset for training, we need to provide one conversion function wey go prepare di offset vector:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def offsetify(b):\n",
    "    # first, compute data tensor from all sequences\n",
    "    x = [torch.tensor(encode(t[1])) for t in b]\n",
    "    # now, compute the offsets by accumulating the tensor of sequence lengths\n",
    "    o = [0] + [len(t) for t in x]\n",
    "    o = torch.tensor(o[:-1]).cumsum(dim=0)\n",
    "    return ( \n",
    "        torch.LongTensor([t[0]-1 for t in b]), # labels\n",
    "        torch.cat(x), # text \n",
    "        o\n",
    "    )\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, collate_fn=offsetify, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, say unlike all di examples wey we don do before, our network now dey accept two parameters: data vector and offset vector, wey get different sizes. Same way, our data loader dey also give us 3 values instead of 2: both text and offset vectors dey provided as features. So, we go need adjust our training function small to handle am:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200: acc=0.6153125\n",
      "6400: acc=0.6615625\n",
      "9600: acc=0.6932291666666667\n",
      "12800: acc=0.715078125\n",
      "16000: acc=0.7270625\n",
      "19200: acc=0.7382291666666667\n",
      "22400: acc=0.7486160714285715\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(22.771553103007037, 0.7551983365323096)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = EmbedClassifier(vocab_size,32,len(classes)).to(device)\n",
    "\n",
    "def train_epoch_emb(net,dataloader,lr=0.01,optimizer=None,loss_fn = torch.nn.CrossEntropyLoss(),epoch_size=None, report_freq=200):\n",
    "    optimizer = optimizer or torch.optim.Adam(net.parameters(),lr=lr)\n",
    "    loss_fn = loss_fn.to(device)\n",
    "    net.train()\n",
    "    total_loss,acc,count,i = 0,0,0,0\n",
    "    for labels,text,off in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        labels,text,off = labels.to(device), text.to(device), off.to(device)\n",
    "        out = net(text, off)\n",
    "        loss = loss_fn(out,labels) #cross_entropy(out,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss+=loss\n",
    "        _,predicted = torch.max(out,1)\n",
    "        acc+=(predicted==labels).sum()\n",
    "        count+=len(labels)\n",
    "        i+=1\n",
    "        if i%report_freq==0:\n",
    "            print(f\"{count}: acc={acc.item()/count}\")\n",
    "        if epoch_size and count>epoch_size:\n",
    "            break\n",
    "    return total_loss.item()/count, acc.item()/count\n",
    "\n",
    "\n",
    "train_epoch_emb(net,train_loader, lr=4, epoch_size=25000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Embeddings: Word2Vec\n",
    "\n",
    "For di example wey we do before, di model embedding layer learn how to map words go vector representation, but di representation no get beta semantical meaning. E go make sense if we fit learn vector representation wey similar words or synonyms go dey close to each oda based on vector distance (like euclidian distance).\n",
    "\n",
    "To do dis one, we go need pre-train our embedding model for one big collection of text in one special way. One of di first way wey dem take train semantic embeddings na [Word2Vec](https://en.wikipedia.org/wiki/Word2vec). E dey based on two main architectures wey dem dey use to produce distributed representation of words:\n",
    "\n",
    "- **Continuous bag-of-words** (CBoW) — for dis architecture, we dey train di model to predict one word from di surrounding context. If dem give di ngram $(W_{-2},W_{-1},W_0,W_1,W_2)$, di goal of di model na to predict $W_0$ from $(W_{-2},W_{-1},W_1,W_2)$.\n",
    "- **Continuous skip-gram** na di opposite of CBoW. Di model dey use di surrounding window of context words to predict di current word.\n",
    "\n",
    "CBoW fast pass, but skip-gram slow small, e dey do better work for words wey no dey common.\n",
    "\n",
    "![Image wey show both CBoW and Skip-Gram algorithms to convert words to vectors.](../../../../../translated_images/example-algorithms-for-converting-words-to-vectors.fbe9207a726922f6f0f5de66427e8a6eda63809356114e28fb1fa5f4a83ebda7.pcm.png)\n",
    "\n",
    "To try word2vec embedding wey dem don pre-train for Google News dataset, we fit use **gensim** library. For di example below, we go find di words wey dey most similar to 'neural'\n",
    "\n",
    "> **Note:** When you first create word vectors, e fit take time to download dem!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "w2v = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neuronal -> 0.7804799675941467\n",
      "neurons -> 0.7326500415802002\n",
      "neural_circuits -> 0.7252851724624634\n",
      "neuron -> 0.7174385190010071\n",
      "cortical -> 0.6941086649894714\n",
      "brain_circuitry -> 0.6923246383666992\n",
      "synaptic -> 0.6699118614196777\n",
      "neural_circuitry -> 0.6638563275337219\n",
      "neurochemical -> 0.6555314064025879\n",
      "neuronal_activity -> 0.6531826257705688\n"
     ]
    }
   ],
   "source": [
    "for w,p in w2v.most_similar('neural'):\n",
    "    print(f\"{w} -> {p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fit also fit vector embeddings from di word, to use am train classification model (we go only show first 20 components of di vector for clearity):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01226807,  0.06225586,  0.10693359,  0.05810547,  0.23828125,\n",
       "        0.03686523,  0.05151367, -0.20703125,  0.01989746,  0.10058594,\n",
       "       -0.03759766, -0.1015625 , -0.15820312, -0.08105469, -0.0390625 ,\n",
       "       -0.05053711,  0.16015625,  0.2578125 ,  0.10058594, -0.25976562],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.word_vec('play')[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beta tin wey dey about semantical embeddings na say you fit use vector encoding take change the meaning. For example, we fit ask make e find one word, wey e vector representation go dey near words *king* and *woman*, but e go far from the word *man*:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('queen', 0.7118192911148071)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar(positive=['king','woman'],negative=['man'])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both CBoW and Skip-Grams na \"predictive\" embeddings, because dem dey only use local context. Word2Vec no dey use global context.\n",
    "\n",
    "**FastText**, dey build ontop Word2Vec by learning vector representations for each word and di character n-grams wey dey inside di word. Di values of di representations go then dey averaged into one vector for each training step. Even though dis one go add plenty extra computation for pre-training, e go make word embeddings fit carry sub-word information.\n",
    "\n",
    "Another method, **GloVe**, dey use di idea of co-occurrence matrix, e dey use neural methods to break di co-occurrence matrix into more expressive and non-linear word vectors.\n",
    "\n",
    "You fit try di example by changing embeddings to FastText and GloVe, because gensim dey support plenty different word embedding models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Use Pre-Trained Embeddings for PyTorch\n",
    "\n",
    "We fit change di example wey dey up to load di matrix for our embedding layer wit semantical embeddings like Word2Vec. But we go need remember say di vocabularies for di pre-trained embedding and di text corpus wey we dey use no go match well, so we go initialize weights for di words wey dey miss wit random values:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding size: 300\n",
      "Populating matrix, this will take some time...Done, found 41080 words, 54732 words missing\n"
     ]
    }
   ],
   "source": [
    "embed_size = len(w2v.get_vector('hello'))\n",
    "print(f'Embedding size: {embed_size}')\n",
    "\n",
    "net = EmbedClassifier(vocab_size,embed_size,len(classes))\n",
    "\n",
    "print('Populating matrix, this will take some time...',end='')\n",
    "found, not_found = 0,0\n",
    "for i,w in enumerate(vocab.get_itos()):\n",
    "    try:\n",
    "        net.embedding.weight[i].data = torch.tensor(w2v.get_vector(w))\n",
    "        found+=1\n",
    "    except:\n",
    "        net.embedding.weight[i].data = torch.normal(0.0,1.0,(embed_size,))\n",
    "        not_found+=1\n",
    "\n",
    "print(f\"Done, found {found} words, {not_found} words missing\")\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make we train di model now. Note say di time wey e go take train di model go long pass di one for di previous example, because di embedding layer size big well-well, and e get plenty parameters. Plus, because of dis one, we fit need train di model with more examples if we wan avoid overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200: acc=0.6359375\n",
      "6400: acc=0.68109375\n",
      "9600: acc=0.7067708333333333\n",
      "12800: acc=0.723671875\n",
      "16000: acc=0.73625\n",
      "19200: acc=0.7463541666666667\n",
      "22400: acc=0.7560714285714286\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(214.1013875559821, 0.7626759436980166)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_epoch_emb(net,train_loader, lr=4, epoch_size=25000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our own case, we no see big improvement for accuracy, wey fit dey because di vocabularies dey different well well.  \n",
    "To solve di wahala of different vocabularies, we fit use one of dis solutions:  \n",
    "* Train word2vec model again wit our own vocabulary  \n",
    "* Load our dataset wit di vocabulary wey dey from di pre-trained word2vec model. Di vocabulary wey dem use load di dataset fit dey specified during di loading.  \n",
    "\n",
    "Di second method dey look easier, especially because PyTorch `torchtext` framework get built-in support for embeddings. For example, we fit create GloVe-based vocabulary like dis:  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 399999/400000 [00:15<00:00, 25411.14it/s]\n"
     ]
    }
   ],
   "source": [
    "vocab = torchtext.vocab.GloVe(name='6B', dim=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loaded vocabulary get dis kain basic operations:  \n",
    "* `vocab.stoi` na dictionary wey go help us change word to im dictionary index  \n",
    "* `vocab.itos` dey do di opposite - e go change number go word  \n",
    "* `vocab.vectors` na di array of embedding vectors, so if we wan get di embedding of one word `s`, we go use `vocab.vectors[vocab.stoi[s]]`  \n",
    "\n",
    "Dis na example of how we fit play wit embeddings to show di equation **kind-man+woman = queen** (I bin adjust di coefficient small make e work):  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'queen'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the vector corresponding to kind-man+woman\n",
    "qvec = vocab.vectors[vocab.stoi['king']]-vocab.vectors[vocab.stoi['man']]+1.3*vocab.vectors[vocab.stoi['woman']]\n",
    "# find the index of the closest embedding vector \n",
    "d = torch.sum((vocab.vectors-qvec)**2,dim=1)\n",
    "min_idx = torch.argmin(d)\n",
    "# find the corresponding word\n",
    "vocab.itos[min_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train di classifier wit di embeddings, we first need to encode our dataset wit GloVe vocabulary:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def offsetify(b):\n",
    "    # first, compute data tensor from all sequences\n",
    "    x = [torch.tensor(encode(t[1],voc=vocab)) for t in b] # pass the instance of vocab to encode function!\n",
    "    # now, compute the offsets by accumulating the tensor of sequence lengths\n",
    "    o = [0] + [len(t) for t in x]\n",
    "    o = torch.tensor(o[:-1]).cumsum(dim=0)\n",
    "    return ( \n",
    "        torch.LongTensor([t[0]-1 for t in b]), # labels\n",
    "        torch.cat(x), # text \n",
    "        o\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we don see for up, all vector embeddings dey store for `vocab.vectors` matrix. E make am super-easy to load di weights enter weights of embedding layer wit simple copying:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = EmbedClassifier(len(vocab),len(vocab.vectors[0]),len(classes))\n",
    "net.embedding.weight.data = vocab.vectors\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make we train our model now see if e go give us better result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200: acc=0.6271875\n",
      "6400: acc=0.68078125\n",
      "9600: acc=0.7030208333333333\n",
      "12800: acc=0.71984375\n",
      "16000: acc=0.7346875\n",
      "19200: acc=0.7455729166666667\n",
      "22400: acc=0.7529464285714286\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(35.53972978646833, 0.7575175943698017)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, collate_fn=offsetify, shuffle=True)\n",
    "train_epoch_emb(net,train_loader, lr=4, epoch_size=25000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of di reasons we no dey see big increase for accuracy na because some words from our dataset dey miss for di pre-trained GloVe vocabulary, and so dem dey basically ignore dem. To solve dis mata, we fit train our own embeddings for our dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contextual Embeddings\n",
    "\n",
    "One big wahala wey dey traditional pretrained embedding like Word2Vec na di problem of word sense disambiguation. Even though pretrained embeddings fit capture some meaning of words for context, dem dey put all di possible meaning of one word inside di same embedding. Dis one fit cause wahala for downstream models, because plenty words like di word 'play' get different meanings depending on di context wey dem dey use am.\n",
    "\n",
    "For example, di word 'play' for dis two different sentences get different meaning:\n",
    "- I go watch **play** for di theatre.\n",
    "- John wan **play** wit im friends.\n",
    "\n",
    "Di pretrained embeddings wey dey above dey represent di two meanings of di word 'play' inside di same embedding. To solve dis wahala, we need to build embeddings wey dey based on di **language model**, wey dem don train on top big corpus of text, and e *sabi* how words fit join together for different contexts. To talk about contextual embeddings no dey di scope of dis tutorial, but we go come back to am when we dey talk about language models for di next unit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n<!-- CO-OP TRANSLATOR DISCLAIMER START -->\n**Disclaimer**:  \nDis dokyument don use AI translation service [Co-op Translator](https://github.com/Azure/co-op-translator) do di translation. Even as we dey try make am accurate, abeg sabi say machine translation fit get mistake or no dey correct well. Di original dokyument wey dey for im native language na di main source wey you go trust. For important information, e better make professional human translator check am. We no go fit take blame for any misunderstanding or wrong interpretation wey fit happen because you use dis translation.\n<!-- CO-OP TRANSLATOR DISCLAIMER END -->\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0cb620c6d4b9f7a635928804c26cf22403d89d98d79684e4529119355ee6d5a5"
  },
  "kernelspec": {
   "display_name": "py37_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "coopTranslator": {
   "original_hash": "f50b026abce5cf36783a560ea72cb9b1",
   "translation_date": "2025-11-18T19:25:50+00:00",
   "source_file": "lessons/5-NLP/14-Embeddings/EmbeddingsPyTorch.ipynb",
   "language_code": "pcm"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}