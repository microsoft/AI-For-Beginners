{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention mechanisms and transformers\n",
    "\n",
    "One big wahala wey dey wit recurrent networks be say all di words wey dey for one sequence dey get di same impact for di result. Dis one dey make di performance no too good wit di normal LSTM encoder-decoder models for sequence to sequence tasks, like Named Entity Recognition and Machine Translation. For real life, some words for di input sequence dey get more impact for di sequential outputs pass others.\n",
    "\n",
    "Make we look sequence-to-sequence model, like machine translation. E dey work wit two recurrent networks, one network (**encoder**) go collapse di input sequence into hidden state, and di other one, **decoder**, go unroll di hidden state into di translated result. Di wahala wit dis method be say di final state of di network go struggle to remember di beginning of di sentence, and e go make di model no perform well for long sentences.\n",
    "\n",
    "**Attention Mechanisms** dey help to give weight to di contextual impact of each input vector for each output prediction of di RNN. Di way dem dey do am na by creating shortcuts between di intermediate states of di input RNN, and di output RNN. So, when we dey generate output symbol $y_t$, we go consider all di input hidden states $h_i$, wit different weight coefficients $\\alpha_{t,i}$. \n",
    "\n",
    "Di image below dey show encoder-decoder model wit additive attention layer:\n",
    "\n",
    "![Image showing an encoder/decoder model with an additive attention layer](../../../../../translated_images/encoder-decoder-attention.7a726296894fb567aa2898c94b17b3289087f6705c11907df8301df9e5eeb3de.pcm.png)\n",
    "*Di encoder-decoder model wit additive attention mechanism for [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf), wey dem take from [dis blog post](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)*\n",
    "\n",
    "Di Attention matrix $\\{\\alpha_{i,j}\\}$ go show how much certain input words dey contribute to di generation of one word for di output sequence. Below na example of di matrix:\n",
    "\n",
    "![Image showing a sample alignment found by RNNsearch-50, taken from Bahdanau - arviz.org](../../../../../translated_images/bahdanau-fig3.09ba2d37f202a6af11de6c82d2d197830ba5f4528d9ea430eb65fd3a75065973.pcm.png)\n",
    "\n",
    "*Di figure dey from [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) (Fig.3)*\n",
    "\n",
    "Attention mechanisms na di reason why Natural Language Processing dey perform well today or dey near di best level. But adding attention dey increase di number of model parameters well well, and e cause scaling wahala wit RNNs. One big problem wit scaling RNNs be say di recurrent nature of di models dey make am hard to batch and parallelize training. For RNN, each element of di sequence need to dey process one by one, so e no fit dey parallelize easily.\n",
    "\n",
    "Di adoption of attention mechanisms plus dis wahala na wetin lead to di creation of di Transformer Models wey dey di best today, like BERT and OpenGPT3.\n",
    "\n",
    "## Transformer models\n",
    "\n",
    "Instead of passing di context of each previous prediction into di next evaluation step, **transformer models** dey use **positional encodings** and attention to capture di context of di input inside di given window of text. Di image below dey show how positional encodings wit attention fit capture context inside di window.\n",
    "\n",
    "![Animated GIF showing how the evaluations are performed in transformer models.](../../../../../lessons/5-NLP/18-Transformers/images/transformer-animated-explanation.gif) \n",
    "\n",
    "Because each input position dey map independently to each output position, transformers fit parallelize better pass RNNs, and e dey allow bigger and more expressive language models. Each attention head fit dey used to learn different relationships between words wey dey improve Natural Language Processing tasks.\n",
    "\n",
    "**BERT** (Bidirectional Encoder Representations from Transformers) na very big multi-layer transformer network wit 12 layers for *BERT-base*, and 24 for *BERT-large*. Di model dey first pre-train wit big corpus of text data (WikiPedia + books) using unsupervised training (predicting masked words for sentence). During di pre-training, di model dey learn plenty language understanding wey fit dey used wit other datasets through fine tuning. Dis process na wetin dem dey call **transfer learning**. \n",
    "\n",
    "![picture from http://jalammar.github.io/illustrated-bert/](../../../../../translated_images/jalammarBERT-language-modeling-masked-lm.34f113ea5fec4362e39ee4381aab7cad06b5465a0b5f053a0f2aa05fbe14e746.pcm.png)\n",
    "\n",
    "Plenty variations of Transformer architectures dey, like BERT, DistilBERT, BigBird, OpenGPT3 and more wey fit dey fine-tuned. Di [HuggingFace package](https://github.com/huggingface/) dey provide repository for training plenty of dis architectures wit PyTorch. \n",
    "\n",
    "## Using BERT for text classification\n",
    "\n",
    "Make we see how we fit use pre-trained BERT model to solve our normal task: sequence classification. We go classify our original AG News dataset.\n",
    "\n",
    "First, make we load HuggingFace library and our dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Building vocab...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "from torchnlp import *\n",
    "import transformers\n",
    "train_dataset, test_dataset, classes, vocab = load_dataset()\n",
    "vocab_len = len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bicos we go dey use pre-trained BERT model, we go need use one specific tokenizer. First, we go load tokenizer wey dey follow pre-trained BERT model.\n",
    "\n",
    "HuggingFace library get one repository of pre-trained models, wey you fit use just by putting dia names as arguments for `from_pretrained` functions. All di binary files wey di model need go download automatic.\n",
    "\n",
    "But sometimes, you go need load your own models. For dat kain case, you fit show di directory wey get all di files wey dey important, like di parameters for tokenizer, `config.json` file wey get di model parameters, binary weights, and di rest.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load the model from Internet repository using model name. \n",
    "# Use this if you are running from your own copy of the notebooks\n",
    "bert_model = 'bert-base-uncased' \n",
    "\n",
    "# To load the model from the directory on disk. Use this for Microsoft Learn module, because we have\n",
    "# prepared all required files for you.\n",
    "bert_model = './bert'\n",
    "\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(bert_model)\n",
    "\n",
    "MAX_SEQ_LEN = 128\n",
    "PAD_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "UNK_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `tokenizer` object get the `encode` function wey fit directly use to encode text:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 1052, 22123, 2953, 2818, 2003, 1037, 2307, 7705, 2005, 17953, 2361, 102]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('PyTorch is a great framework for NLP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Den, make we create iterators wey we go use during training to take access di data. Because BERT dey use im own encoding function, we go need define one padding function wey be like `padify` we don define before:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_bert(b):\n",
    "    # b is the list of tuples of length batch_size\n",
    "    #   - first element of a tuple = label, \n",
    "    #   - second = feature (text sequence)\n",
    "    # build vectorized sequence\n",
    "    v = [tokenizer.encode(x[1]) for x in b]\n",
    "    # compute max length of a sequence in this minibatch\n",
    "    l = max(map(len,v))\n",
    "    return ( # tuple of two tensors - labels and features\n",
    "        torch.LongTensor([t[0] for t in b]),\n",
    "        torch.stack([torch.nn.functional.pad(torch.tensor(t),(0,l-len(t)),mode='constant',value=0) for t in v])\n",
    "    )\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=8, collate_fn=pad_bert, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=8, collate_fn=pad_bert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our own case, we go use pre-trained BERT model wey dem dey call `bert-base-uncased`. Make we load di model wit `BertForSequenceClassfication` package. Dis one go make sure say di model don already get di architecture wey we need for classification, including di final classifier. You go see warning message wey go talk say di weights of di final classifier no dey initialized, and di model go need pre-training - dat one dey okay well well, because na wetin we wan do be dat!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./bert were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./bert and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = transformers.BertForSequenceClassification.from_pretrained(bert_model,num_labels=4).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we don ready to start di training! Because BERT don already pre-train, we go wan start wit small learning rate so we no go scatter di initial weights.\n",
    "\n",
    "Na `BertForSequenceClassification` model dey do all di hard work. Wen we run di model on top di training data, e go return both di loss and di network output for di input minibatch. We dey use di loss for parameter optimization (`loss.backward()` dey do di backward pass), and `out` to calculate di training accuracy by comparing di labels wey we get `labs` (wey we calculate using `argmax`) wit di expected `labels`.\n",
    "\n",
    "To fit control di process, we dey gather di loss and accuracy for plenty iterations, and we dey print dem every `report_freq` training cycles.\n",
    "\n",
    "Dis training fit take plenty time, so we dey limit di number of iterations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 1.1254194641113282, Accuracy = 0.585\n",
      "Loss = 0.6194715118408203, Accuracy = 0.83\n",
      "Loss = 0.46665248870849607, Accuracy = 0.8475\n",
      "Loss = 0.4309701919555664, Accuracy = 0.8575\n",
      "Loss = 0.35427074432373046, Accuracy = 0.8825\n",
      "Loss = 0.3306886291503906, Accuracy = 0.8975\n",
      "Loss = 0.30340143203735354, Accuracy = 0.8975\n",
      "Loss = 0.26139299392700194, Accuracy = 0.915\n",
      "Loss = 0.26708646774291994, Accuracy = 0.9225\n",
      "Loss = 0.3667240524291992, Accuracy = 0.8675\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
    "\n",
    "report_freq = 50\n",
    "iterations = 500 # make this larger to train for longer time!\n",
    "\n",
    "model.train()\n",
    "\n",
    "i,c = 0,0\n",
    "acc_loss = 0\n",
    "acc_acc = 0\n",
    "\n",
    "for labels,texts in train_loader:\n",
    "    labels = labels.to(device)-1 # get labels in the range 0-3         \n",
    "    texts = texts.to(device)\n",
    "    loss, out = model(texts, labels=labels)[:2]\n",
    "    labs = out.argmax(dim=1)\n",
    "    acc = torch.mean((labs==labels).type(torch.float32))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    acc_loss += loss\n",
    "    acc_acc += acc\n",
    "    i+=1\n",
    "    c+=1\n",
    "    if i%report_freq==0:\n",
    "        print(f\"Loss = {acc_loss.item()/c}, Accuracy = {acc_acc.item()/c}\")\n",
    "        c = 0\n",
    "        acc_loss = 0\n",
    "        acc_acc = 0\n",
    "    iterations-=1\n",
    "    if not iterations:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You fit see (specially if you increase di number of iterations and wait well well) say BERT classification dey give us beta accuracy! Na because BERT don already sabi di structure of di language well, and we just need to fine-tune di final classifier. But, because BERT na big model, di whole training process dey take plenty time, and e need strong computational power! (GPU, and e go better if you get more than one).\n",
    "\n",
    "> **Note:** For our example, we dey use one of di smallest pre-trained BERT models. Bigger models dey wey fit give beta results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check how di model dey perform\n",
    "\n",
    "Now we fit check how our model dey perform for di test dataset. Di way we go take check am dey similar to di way we take train am, but make we no forget to change di model to evaluation mode by calling `model.eval()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy: 0.9047029702970297\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "iterations = 100\n",
    "acc = 0\n",
    "i = 0\n",
    "for labels,texts in test_loader:\n",
    "    labels = labels.to(device)-1      \n",
    "    texts = texts.to(device)\n",
    "    _, out = model(texts, labels=labels)[:2]\n",
    "    labs = out.argmax(dim=1)\n",
    "    acc += torch.mean((labs==labels).type(torch.float32))\n",
    "    i+=1\n",
    "    if i>iterations: break\n",
    "        \n",
    "print(f\"Final accuracy: {acc.item()/i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Takeaway\n",
    "\n",
    "For dis unit, we don see how e easy to carry pre-trained language model from **transformers** library and use am for our text classification work. Same way, BERT models fit work for entity extraction, question answering, and other NLP tasks.\n",
    "\n",
    "Transformer models na di current best for NLP, and for most cases, e suppose be di first solution wey you go try when you dey do custom NLP solutions. But, e dey very important to sabi di basic principles of recurrent neural networks wey we talk about for dis module if you wan build advanced neural models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n<!-- CO-OP TRANSLATOR DISCLAIMER START -->\n**Disclaimer**:  \nDis dokyument don use AI translet service [Co-op Translator](https://github.com/Azure/co-op-translator) do di translet. Even as we dey try make am correct, abeg make you sabi say machine translet fit get mistake or no dey accurate well. Di original dokyument wey dey for im native language na di one wey you go take as di correct source. For important mata, e good make you use professional human translet. We no go fit take blame for any misunderstanding or wrong interpretation wey fit happen because you use dis translet.\n<!-- CO-OP TRANSLATOR DISCLAIMER END -->\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37_pytorch",
   "language": "python",
   "name": "conda-env-py37_pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "coopTranslator": {
   "original_hash": "753865967678a92dbce7d7efbd36d980",
   "translation_date": "2025-11-18T19:23:10+00:00",
   "source_file": "lessons/5-NLP/18-Transformers/TransformersPyTorch.ipynb",
   "language_code": "pcm"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}