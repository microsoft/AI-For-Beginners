{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Penyematan\n",
    "\n",
    "Dalam contoh sebelumnya, kita bekerja dengan vektor bag-of-words berdimensi tinggi dengan panjang `vocab_size`, dan secara eksplisit mengubah vektor representasi posisi berdimensi rendah menjadi representasi one-hot yang jarang. Representasi one-hot ini tidak efisien dalam penggunaan memori. Selain itu, setiap kata diperlakukan secara independen satu sama lain, sehingga vektor yang dikodekan one-hot tidak dapat mengungkapkan kesamaan semantik antar kata.\n",
    "\n",
    "Dalam unit ini, kita akan melanjutkan eksplorasi dataset **News AG**. Untuk memulai, mari kita muat data dan mengambil beberapa definisi dari unit sebelumnya.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "\n",
    "ds_train, ds_test = tfds.load('ag_news_subset').values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apa itu embedding?\n",
    "\n",
    "Ide **embedding** adalah merepresentasikan kata-kata menggunakan vektor padat berdimensi rendah yang mencerminkan makna semantik dari kata tersebut. Nanti kita akan membahas bagaimana membangun embedding kata yang bermakna, tetapi untuk saat ini mari kita anggap embedding sebagai cara untuk mengurangi dimensi vektor kata.\n",
    "\n",
    "Jadi, lapisan embedding mengambil sebuah kata sebagai input, dan menghasilkan vektor output dengan ukuran `embedding_size` yang ditentukan. Dalam beberapa hal, ini sangat mirip dengan lapisan `Dense`, tetapi alih-alih mengambil vektor one-hot encoded sebagai input, lapisan ini dapat menerima nomor kata.\n",
    "\n",
    "Dengan menggunakan lapisan embedding sebagai lapisan pertama dalam jaringan kita, kita dapat beralih dari model bag-of-words ke model **embedding bag**, di mana kita pertama-tama mengonversi setiap kata dalam teks kita ke embedding yang sesuai, lalu menghitung fungsi agregat tertentu dari semua embedding tersebut, seperti `sum`, `average`, atau `max`.\n",
    "\n",
    "![Gambar menunjukkan pengklasifikasi embedding untuk lima kata dalam urutan.](../../../../../translated_images/id/embedding-classifier-example.b77f021a7ee67eee.webp)\n",
    "\n",
    "Jaringan neural pengklasifikasi kita terdiri dari lapisan-lapisan berikut:\n",
    "\n",
    "* Lapisan `TextVectorization`, yang mengambil string sebagai input, dan menghasilkan tensor nomor token. Kita akan menentukan ukuran kosakata yang masuk akal `vocab_size`, dan mengabaikan kata-kata yang jarang digunakan. Bentuk input akan menjadi 1, dan bentuk output akan menjadi $n$, karena kita akan mendapatkan $n$ token sebagai hasilnya, masing-masing berisi angka dari 0 hingga `vocab_size`.\n",
    "* Lapisan `Embedding`, yang mengambil $n$ angka, dan mengurangi setiap angka menjadi vektor padat dengan panjang tertentu (100 dalam contoh kita). Dengan demikian, tensor input dengan bentuk $n$ akan diubah menjadi tensor $n\\times 100$. \n",
    "* Lapisan agregasi, yang mengambil rata-rata tensor ini sepanjang sumbu pertama, yaitu akan menghitung rata-rata dari semua $n$ tensor input yang sesuai dengan kata-kata yang berbeda. Untuk mengimplementasikan lapisan ini, kita akan menggunakan lapisan `Lambda`, dan memasukkan fungsi untuk menghitung rata-rata. Output akan memiliki bentuk 100, dan ini akan menjadi representasi numerik dari seluruh urutan input.\n",
    "* Pengklasifikasi linear `Dense` terakhir.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization (TextVec  (None, None)             0         \n",
      " torization)                                                     \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, None, 100)         3000000   \n",
      "                                                                 \n",
      " lambda (Lambda)             (None, 100)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4)                 404       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,000,404\n",
      "Trainable params: 3,000,404\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 30000\n",
    "batch_size = 128\n",
    "\n",
    "vectorizer = keras.layers.experimental.preprocessing.TextVectorization(max_tokens=vocab_size,input_shape=(1,))\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    vectorizer,    \n",
    "    keras.layers.Embedding(vocab_size,100),\n",
    "    keras.layers.Lambda(lambda x: tf.reduce_mean(x,axis=1)),\n",
    "    keras.layers.Dense(4, activation='softmax')\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dalam cetakan `summary`, pada kolom **output shape**, dimensi tensor pertama `None` merujuk pada ukuran minibatch, dan dimensi kedua merujuk pada panjang urutan token. Semua urutan token dalam minibatch memiliki panjang yang berbeda. Kita akan membahas cara mengatasinya di bagian berikutnya.\n",
    "\n",
    "Sekarang, mari kita latih jaringan:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training vectorizer\n",
      "938/938 [==============================] - 20s 20ms/step - loss: 0.7891 - acc: 0.8155 - val_loss: 0.4470 - val_acc: 0.8642\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22255515100>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_text(x):\n",
    "    return x['title']+' '+x['description']\n",
    "\n",
    "def tupelize(x):\n",
    "    return (extract_text(x),x['label'])\n",
    "\n",
    "print(\"Training vectorizer\")\n",
    "vectorizer.adapt(ds_train.take(500).map(extract_text))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'])\n",
    "model.fit(ds_train.map(tupelize).batch(batch_size),validation_data=ds_test.map(tupelize).batch(batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "> **Catatan** bahwa kami sedang membangun vektorisasi berdasarkan subset data. Hal ini dilakukan untuk mempercepat proses, dan mungkin mengakibatkan situasi di mana tidak semua token dari teks kami ada dalam kosakata. Dalam kasus ini, token tersebut akan diabaikan, yang dapat menyebabkan akurasi sedikit lebih rendah. Namun, dalam kehidupan nyata, subset teks sering kali memberikan estimasi kosakata yang baik.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mengatasi ukuran urutan variabel\n",
    "\n",
    "Mari kita pahami bagaimana pelatihan terjadi dalam minibatch. Dalam contoh di atas, tensor input memiliki dimensi 1, dan kita menggunakan minibatch sepanjang 128, sehingga ukuran aktual tensor adalah $128 \\times 1$. Namun, jumlah token dalam setiap kalimat berbeda-beda. Jika kita menerapkan lapisan `TextVectorization` pada satu input, jumlah token yang dikembalikan akan berbeda, tergantung pada bagaimana teks tersebut di-tokenisasi:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 1 45], shape=(2,), dtype=int64)\n",
      "tf.Tensor([ 112 1271    1    3 1747  158], shape=(6,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer('Hello, world!'))\n",
    "print(vectorizer('I am glad to meet you!'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Namun, ketika kita menerapkan vectorizer pada beberapa urutan, vectorizer harus menghasilkan tensor berbentuk persegi panjang, sehingga mengisi elemen yang tidak digunakan dengan token PAD (yang dalam kasus kita adalah nol):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 6), dtype=int64, numpy=\n",
       "array([[   1,   45,    0,    0,    0,    0],\n",
       "       [ 112, 1271,    1,    3, 1747,  158]], dtype=int64)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer(['Hello, world!','I am glad to meet you!'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Di sini kita dapat melihat penyematan:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 1.53059261e-02,  6.80514947e-02,  3.14026810e-02, ...,\n",
       "         -8.92002955e-02,  1.52911525e-04, -5.65562584e-02],\n",
       "        [ 2.57456154e-01,  2.79364467e-01, -2.03605562e-01, ...,\n",
       "         -2.07474351e-01,  8.31158683e-02, -2.03911960e-01],\n",
       "        [ 3.98201384e-02, -8.03454965e-03,  2.39790026e-02, ...,\n",
       "         -7.18549127e-04,  2.66963355e-02, -4.30646613e-02],\n",
       "        [ 3.98201384e-02, -8.03454965e-03,  2.39790026e-02, ...,\n",
       "         -7.18549127e-04,  2.66963355e-02, -4.30646613e-02],\n",
       "        [ 3.98201384e-02, -8.03454965e-03,  2.39790026e-02, ...,\n",
       "         -7.18549127e-04,  2.66963355e-02, -4.30646613e-02],\n",
       "        [ 3.98201384e-02, -8.03454965e-03,  2.39790026e-02, ...,\n",
       "         -7.18549127e-04,  2.66963355e-02, -4.30646613e-02]],\n",
       "\n",
       "       [[ 1.89674050e-01,  2.61548996e-01, -3.67433839e-02, ...,\n",
       "         -2.07366899e-01, -1.05442435e-01, -2.36952081e-01],\n",
       "        [ 6.16133213e-02,  1.80511594e-01,  9.77298319e-02, ...,\n",
       "         -5.46628237e-02, -1.07340455e-01, -1.06589928e-01],\n",
       "        [ 1.53059261e-02,  6.80514947e-02,  3.14026810e-02, ...,\n",
       "         -8.92002955e-02,  1.52911525e-04, -5.65562584e-02],\n",
       "        [-4.84890305e-02, -8.41715634e-02,  1.51529670e-01, ...,\n",
       "          1.28192469e-01, -7.77286515e-02,  1.26041949e-01],\n",
       "        [-4.17212099e-02, -5.60694858e-02,  4.08860669e-02, ...,\n",
       "          8.70475471e-02,  8.92383084e-02,  1.67974353e-01],\n",
       "        [ 2.85779923e-01,  4.57767487e-01,  4.52292450e-02, ...,\n",
       "         -1.97419018e-01, -2.04659685e-01, -2.79758364e-01]]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[1](vectorizer(['Hello, world!','I am glad to meet you!'])).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Catatan**: Untuk meminimalkan jumlah padding, dalam beberapa kasus masuk akal untuk mengurutkan semua urutan dalam dataset berdasarkan panjang yang meningkat (atau, lebih tepatnya, jumlah token). Hal ini akan memastikan bahwa setiap minibatch berisi urutan dengan panjang yang serupa.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic embeddings: Word2Vec\n",
    "\n",
    "Dalam contoh sebelumnya, lapisan embedding belajar memetakan kata-kata ke representasi vektor, namun, representasi tersebut tidak memiliki makna semantik. Akan lebih baik jika kita dapat mempelajari representasi vektor sehingga kata-kata yang serupa atau sinonim memiliki vektor yang dekat satu sama lain berdasarkan jarak vektor tertentu (misalnya jarak euclidean).\n",
    "\n",
    "Untuk mencapai hal tersebut, kita perlu melatih model embedding kita terlebih dahulu pada kumpulan teks yang besar menggunakan teknik seperti [Word2Vec](https://en.wikipedia.org/wiki/Word2vec). Teknik ini didasarkan pada dua arsitektur utama yang digunakan untuk menghasilkan representasi terdistribusi dari kata-kata:\n",
    "\n",
    " - **Continuous bag-of-words** (CBoW), di mana kita melatih model untuk memprediksi sebuah kata berdasarkan konteks di sekitarnya. Diberikan ngram $(W_{-2},W_{-1},W_0,W_1,W_2)$, tujuan model adalah memprediksi $W_0$ dari $(W_{-2},W_{-1},W_1,W_2)$.\n",
    " - **Continuous skip-gram** adalah kebalikan dari CBoW. Model menggunakan jendela kata-kata konteks di sekitar untuk memprediksi kata saat ini.\n",
    "\n",
    "CBoW lebih cepat, sedangkan skip-gram lebih lambat, tetapi skip-gram lebih baik dalam merepresentasikan kata-kata yang jarang muncul.\n",
    "\n",
    "![Gambar menunjukkan algoritma CBoW dan Skip-Gram untuk mengonversi kata-kata menjadi vektor.](../../../../../translated_images/id/example-algorithms-for-converting-words-to-vectors.fbe9207a726922f6.webp)\n",
    "\n",
    "Untuk bereksperimen dengan embedding Word2Vec yang telah dilatih sebelumnya pada dataset Google News, kita dapat menggunakan pustaka **gensim**. Di bawah ini kita menemukan kata-kata yang paling mirip dengan 'neural'.\n",
    "\n",
    "> **Note:** Saat pertama kali membuat vektor kata, proses pengunduhan dapat memakan waktu cukup lama!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "w2v = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neuronal -> 0.7804799675941467\n",
      "neurons -> 0.7326500415802002\n",
      "neural_circuits -> 0.7252851724624634\n",
      "neuron -> 0.7174385190010071\n",
      "cortical -> 0.6941086649894714\n",
      "brain_circuitry -> 0.6923246383666992\n",
      "synaptic -> 0.6699118614196777\n",
      "neural_circuitry -> 0.6638563275337219\n",
      "neurochemical -> 0.6555314064025879\n",
      "neuronal_activity -> 0.6531826257705688\n"
     ]
    }
   ],
   "source": [
    "for w,p in w2v.most_similar('neural'):\n",
    "    print(f\"{w} -> {p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kita juga dapat mengekstrak embedding vektor dari kata, untuk digunakan dalam melatih model klasifikasi. Embedding memiliki 300 komponen, tetapi di sini kami hanya menunjukkan 20 komponen pertama dari vektor untuk kejelasan:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01226807,  0.06225586,  0.10693359,  0.05810547,  0.23828125,\n",
       "        0.03686523,  0.05151367, -0.20703125,  0.01989746,  0.10058594,\n",
       "       -0.03759766, -0.1015625 , -0.15820312, -0.08105469, -0.0390625 ,\n",
       "       -0.05053711,  0.16015625,  0.2578125 ,  0.10058594, -0.25976562],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v['play'][:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hal hebat tentang embedding semantik adalah bahwa Anda dapat memanipulasi pengkodean vektor berdasarkan semantik. Sebagai contoh, kita dapat meminta untuk menemukan kata yang representasi vektornya sedekat mungkin dengan kata *raja* dan *wanita*, serta sejauh mungkin dari kata *pria*:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('queen', 0.7118192911148071)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar(positive=['king','woman'],negative=['man'])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Contoh di atas menggunakan beberapa sihir internal GenSym, tetapi logika dasarnya sebenarnya cukup sederhana. Hal yang menarik tentang embedding adalah bahwa Anda dapat melakukan operasi vektor normal pada vektor embedding, dan itu akan mencerminkan operasi pada **makna** kata. Contoh di atas dapat diekspresikan dalam bentuk operasi vektor: kita menghitung vektor yang sesuai dengan **KING-MAN+WOMAN** (operasi `+` dan `-` dilakukan pada representasi vektor dari kata-kata yang bersangkutan), dan kemudian menemukan kata terdekat dalam kamus dengan vektor tersebut:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'queen'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the vector corresponding to kind-man+woman\n",
    "qvec = w2v['king']-1.7*w2v['man']+1.7*w2v['woman']\n",
    "# find the index of the closest embedding vector \n",
    "d = np.sum((w2v.vectors-qvec)**2,axis=1)\n",
    "min_idx = np.argmin(d)\n",
    "# find the corresponding word\n",
    "w2v.index_to_key[min_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NOTE**: Kami harus menambahkan koefisien kecil pada vektor *man* dan *woman* - coba hapus koefisien tersebut untuk melihat apa yang terjadi.\n",
    "\n",
    "Untuk menemukan vektor terdekat, kami menggunakan mekanisme TensorFlow untuk menghitung vektor jarak antara vektor kami dan semua vektor dalam kosakata, lalu menemukan indeks kata dengan nilai minimum menggunakan `argmin`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Meskipun Word2Vec tampak seperti cara yang hebat untuk mengekspresikan semantik kata, metode ini memiliki banyak kekurangan, termasuk yang berikut:\n",
    "\n",
    "* Baik model CBoW maupun skip-gram adalah **predictive embeddings**, dan hanya mempertimbangkan konteks lokal. Word2Vec tidak memanfaatkan konteks global.\n",
    "* Word2Vec tidak mempertimbangkan **morfologi** kata, yaitu fakta bahwa makna kata dapat bergantung pada bagian-bagian tertentu dari kata, seperti akar kata.\n",
    "\n",
    "**FastText** mencoba mengatasi keterbatasan kedua ini, dan mengembangkan Word2Vec dengan mempelajari representasi vektor untuk setiap kata serta n-gram karakter yang ditemukan dalam setiap kata. Nilai dari representasi ini kemudian dirata-rata menjadi satu vektor pada setiap langkah pelatihan. Meskipun ini menambah banyak perhitungan tambahan selama pretraining, metode ini memungkinkan embeddings kata untuk menyandikan informasi sub-kata.\n",
    "\n",
    "Metode lain, **GloVe**, menggunakan pendekatan yang berbeda untuk embeddings kata, berdasarkan faktorisasi matriks kata-konteks. Pertama, metode ini membangun matriks besar yang menghitung jumlah kemunculan kata dalam berbagai konteks, lalu mencoba merepresentasikan matriks ini dalam dimensi yang lebih rendah dengan cara yang meminimalkan kehilangan rekonstruksi.\n",
    "\n",
    "Perpustakaan gensim mendukung embeddings kata tersebut, dan Anda dapat bereksperimen dengan mengubah kode pemuatan model di atas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Menggunakan embedding yang sudah dilatih sebelumnya di Keras\n",
    "\n",
    "Kita dapat memodifikasi contoh di atas untuk mengisi matriks dalam lapisan embedding kita dengan embedding semantik, seperti Word2Vec. Kosakata dari embedding yang sudah dilatih sebelumnya dan korpus teks kemungkinan besar tidak akan cocok, jadi kita perlu memilih salah satu. Di sini kita akan mengeksplorasi dua opsi yang mungkin: menggunakan kosakata tokenizer, dan menggunakan kosakata dari embedding Word2Vec.\n",
    "\n",
    "### Menggunakan kosakata tokenizer\n",
    "\n",
    "Saat menggunakan kosakata tokenizer, beberapa kata dari kosakata akan memiliki embedding Word2Vec yang sesuai, dan beberapa lainnya akan hilang. Mengingat bahwa ukuran kosakata kita adalah `vocab_size`, dan panjang vektor embedding Word2Vec adalah `embed_size`, lapisan embedding akan direpresentasikan oleh matriks bobot dengan bentuk `vocab_size`$\\times$`embed_size`. Kita akan mengisi matriks ini dengan menelusuri kosakata:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding size: 300\n",
      "Populating matrix, this will take some time...Done, found 4551 words, 784 words missing\n"
     ]
    }
   ],
   "source": [
    "embed_size = len(w2v.get_vector('hello'))\n",
    "print(f'Embedding size: {embed_size}')\n",
    "\n",
    "vocab = vectorizer.get_vocabulary()\n",
    "W = np.zeros((vocab_size,embed_size))\n",
    "print('Populating matrix, this will take some time...',end='')\n",
    "found, not_found = 0,0\n",
    "for i,w in enumerate(vocab):\n",
    "    try:\n",
    "        W[i] = w2v.get_vector(w)\n",
    "        found+=1\n",
    "    except:\n",
    "        # W[i] = np.random.normal(0.0,0.3,size=(embed_size,))\n",
    "        not_found+=1\n",
    "\n",
    "print(f\"Done, found {found} words, {not_found} words missing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Untuk kata-kata yang tidak ada dalam kosakata Word2Vec, kita dapat membiarkannya sebagai nol, atau menghasilkan vektor acak.\n",
    "\n",
    "Sekarang kita dapat mendefinisikan lapisan embedding dengan bobot yang telah dilatih sebelumnya:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = keras.layers.Embedding(vocab_size,embed_size,weights=[W],trainable=False)\n",
    "model = keras.models.Sequential([\n",
    "    vectorizer, emb,\n",
    "    keras.layers.Lambda(lambda x: tf.reduce_mean(x,axis=1)),\n",
    "    keras.layers.Dense(4, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 10s 10ms/step - loss: 1.1075 - acc: 0.7822 - val_loss: 0.9134 - val_acc: 0.8175\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2220226ef10>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'])\n",
    "model.fit(ds_train.map(tupelize).batch(batch_size),\n",
    "          validation_data=ds_test.map(tupelize).batch(batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Catatan**: Perhatikan bahwa kami menetapkan `trainable=False` saat membuat `Embedding`, yang berarti kami tidak melatih ulang lapisan Embedding. Hal ini mungkin menyebabkan akurasi sedikit lebih rendah, tetapi mempercepat proses pelatihan.\n",
    "\n",
    "### Menggunakan kosakata embedding\n",
    "\n",
    "Salah satu masalah dengan pendekatan sebelumnya adalah kosakata yang digunakan dalam TextVectorization dan Embedding berbeda. Untuk mengatasi masalah ini, kita dapat menggunakan salah satu solusi berikut:\n",
    "* Melatih ulang model Word2Vec pada kosakata kita.\n",
    "* Memuat dataset kita dengan kosakata dari model Word2Vec yang telah dilatih sebelumnya. Kosakata yang digunakan untuk memuat dataset dapat ditentukan selama proses pemuatan.\n",
    "\n",
    "Pendekatan kedua tampaknya lebih mudah, jadi mari kita implementasikan. Pertama-tama, kita akan membuat lapisan `TextVectorization` dengan kosakata yang telah ditentukan, diambil dari embedding Word2Vec:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(w2v.vocab.keys())\n",
    "vectorizer = keras.layers.experimental.preprocessing.TextVectorization(input_shape=(1,))\n",
    "vectorizer.set_vocabulary(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perpustakaan word embeddings gensim memiliki fungsi yang praktis, `get_keras_embeddings`, yang secara otomatis akan membuat lapisan embeddings Keras yang sesuai untuk Anda.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "938/938 [==============================] - 20s 14ms/step - loss: 1.3377 - acc: 0.4978 - val_loss: 1.2995 - val_acc: 0.5647\n",
      "Epoch 2/5\n",
      "938/938 [==============================] - 10s 10ms/step - loss: 1.2587 - acc: 0.5722 - val_loss: 1.2339 - val_acc: 0.5842\n",
      "Epoch 3/5\n",
      "938/938 [==============================] - 10s 10ms/step - loss: 1.1980 - acc: 0.5884 - val_loss: 1.1826 - val_acc: 0.5954\n",
      "Epoch 4/5\n",
      "938/938 [==============================] - 12s 13ms/step - loss: 1.1503 - acc: 0.6002 - val_loss: 1.1417 - val_acc: 0.6018\n",
      "Epoch 5/5\n",
      "938/938 [==============================] - 11s 12ms/step - loss: 1.1120 - acc: 0.6097 - val_loss: 1.1083 - val_acc: 0.6104\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2220ccb81c0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    vectorizer, \n",
    "    w2v.get_keras_embedding(train_embeddings=False),\n",
    "    keras.layers.Lambda(lambda x: tf.reduce_mean(x,axis=1)),\n",
    "    keras.layers.Dense(4, activation='softmax')\n",
    "])\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'])\n",
    "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128),epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Salah satu alasan mengapa kita tidak melihat akurasi yang lebih tinggi adalah karena beberapa kata dari dataset kita tidak ada dalam kosakata GloVe yang telah dilatih sebelumnya, sehingga kata-kata tersebut pada dasarnya diabaikan. Untuk mengatasi hal ini, kita dapat melatih embedding kita sendiri berdasarkan dataset kita.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Kontekstual\n",
    "\n",
    "Salah satu keterbatasan utama dari representasi embedding yang dilatih sebelumnya seperti Word2Vec adalah meskipun mereka dapat menangkap sebagian makna dari sebuah kata, mereka tidak dapat membedakan antara makna yang berbeda. Hal ini dapat menyebabkan masalah pada model lanjutan.\n",
    "\n",
    "Sebagai contoh, kata 'play' memiliki makna yang berbeda dalam dua kalimat berikut:\n",
    "- Saya pergi ke sebuah **play** di teater.\n",
    "- John ingin **play** dengan teman-temannya.\n",
    "\n",
    "Embedding yang dilatih sebelumnya yang kita bahas merepresentasikan kedua makna kata 'play' dalam embedding yang sama. Untuk mengatasi keterbatasan ini, kita perlu membangun embedding berdasarkan **model bahasa**, yang dilatih pada korpus teks yang besar, dan *memahami* bagaimana kata-kata dapat disusun dalam berbagai konteks. Membahas embedding kontekstual berada di luar cakupan tutorial ini, tetapi kita akan kembali membahasnya saat berbicara tentang model bahasa di unit berikutnya.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Penafian**:  \nDokumen ini telah diterjemahkan menggunakan layanan penerjemahan AI [Co-op Translator](https://github.com/Azure/co-op-translator). Meskipun kami berusaha untuk memberikan hasil yang akurat, harap diingat bahwa terjemahan otomatis mungkin mengandung kesalahan atau ketidakakuratan. Dokumen asli dalam bahasa aslinya harus dianggap sebagai sumber yang otoritatif. Untuk informasi yang bersifat kritis, disarankan menggunakan jasa penerjemahan profesional oleh manusia. Kami tidak bertanggung jawab atas kesalahpahaman atau penafsiran yang keliru yang timbul dari penggunaan terjemahan ini.\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0cb620c6d4b9f7a635928804c26cf22403d89d98d79684e4529119355ee6d5a5"
  },
  "kernel_info": {
   "name": "conda-env-py37_tensorflow-py"
  },
  "kernelspec": {
   "display_name": "py37_tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "coopTranslator": {
   "original_hash": "b859482be7f61d1eadc2c6a2720a37e4",
   "translation_date": "2025-08-29T16:23:24+00:00",
   "source_file": "lessons/5-NLP/14-Embeddings/EmbeddingsTF.ipynb",
   "language_code": "id"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}