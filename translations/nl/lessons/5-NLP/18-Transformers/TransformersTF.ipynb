{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aandachtsmechanismen en transformers\n",
    "\n",
    "Een belangrijk nadeel van recurrente netwerken is dat alle woorden in een reeks dezelfde invloed hebben op het resultaat. Dit leidt tot suboptimale prestaties bij standaard LSTM encoder-decoder modellen voor sequentie-tot-sequentie taken, zoals Named Entity Recognition en Machinevertaling. In werkelijkheid hebben specifieke woorden in de invoerreeks vaak meer invloed op de opeenvolgende uitvoer dan andere.\n",
    "\n",
    "Laten we een sequentie-tot-sequentie model overwegen, zoals machinevertaling. Dit wordt geïmplementeerd door twee recurrente netwerken, waarbij één netwerk (**encoder**) de invoerreeks samenvat in een verborgen toestand, en een ander netwerk, **decoder**, deze verborgen toestand uitrolt naar het vertaalde resultaat. Het probleem met deze aanpak is dat de eindtoestand van het netwerk moeite heeft om het begin van een zin te onthouden, wat leidt tot een slechte kwaliteit van het model bij lange zinnen.\n",
    "\n",
    "**Aandachtsmechanismen** bieden een manier om het contextuele gewicht van elke invoervector op elke uitvoervoorspelling van het RNN te bepalen. Dit wordt geïmplementeerd door shortcuts te creëren tussen de tussenliggende toestanden van het invoer-RNN en het uitvoer-RNN. Op deze manier houden we bij het genereren van het uitvoersymbool $y_t$ rekening met alle verborgen toestanden van de invoer $h_i$, met verschillende gewichtscoëfficiënten $\\alpha_{t,i}$. \n",
    "\n",
    "![Afbeelding van een encoder/decoder model met een additieve aandachtlaag](../../../../../translated_images/nl/encoder-decoder-attention.7a726296894fb567.webp)\n",
    "*Het encoder-decoder model met additief aandachtsmechanisme in [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf), geciteerd uit [deze blogpost](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)*\n",
    "\n",
    "De aandachtsmatrix $\\{\\alpha_{i,j}\\}$ vertegenwoordigt de mate waarin bepaalde invoerwoorden bijdragen aan de generatie van een specifiek woord in de uitvoerreeks. Hieronder staat een voorbeeld van zo'n matrix:\n",
    "\n",
    "![Afbeelding van een voorbeelduitlijning gevonden door RNNsearch-50, afkomstig uit Bahdanau - arviz.org](../../../../../translated_images/nl/bahdanau-fig3.09ba2d37f202a6af.webp)\n",
    "\n",
    "*Afbeelding afkomstig uit [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) (Fig.3)*\n",
    "\n",
    "Aandachtsmechanismen zijn verantwoordelijk voor veel van de huidige of bijna huidige state-of-the-art in natuurlijke taalverwerking. Het toevoegen van aandacht verhoogt echter aanzienlijk het aantal modelparameters, wat leidde tot schaalproblemen met RNN's. Een belangrijke beperking bij het schalen van RNN's is dat de recurrente aard van de modellen het uitdagend maakt om training te batchen en te paralleliseren. In een RNN moet elk element van een reeks in volgorde worden verwerkt, wat betekent dat het niet eenvoudig parallel kan worden uitgevoerd.\n",
    "\n",
    "De adoptie van aandachtsmechanismen in combinatie met deze beperking leidde tot de creatie van de huidige state-of-the-art Transformer-modellen die we vandaag kennen en gebruiken, van BERT tot OpenGPT3.\n",
    "\n",
    "## Transformer-modellen\n",
    "\n",
    "In plaats van de context van elke vorige voorspelling door te geven aan de volgende evaluatiestap, gebruiken **transformer-modellen** **positionele encoderingen** en **aandacht** om de context van een gegeven invoer vast te leggen binnen een opgegeven tekstvenster. De onderstaande afbeelding laat zien hoe positionele encoderingen met aandacht context kunnen vastleggen binnen een bepaald venster.\n",
    "\n",
    "![Geanimeerde GIF die laat zien hoe de evaluaties worden uitgevoerd in transformer-modellen.](../../../../../lessons/5-NLP/18-Transformers/images/transformer-animated-explanation.gif) \n",
    "\n",
    "Omdat elke invoerpositie onafhankelijk wordt gemapt naar elke uitvoerpositie, kunnen transformers beter paralleliseren dan RNN's, wat veel grotere en expressievere taalmodellen mogelijk maakt. Elke aandachtshoofd kan worden gebruikt om verschillende relaties tussen woorden te leren, wat de prestaties van downstream-taken in natuurlijke taalverwerking verbetert.\n",
    "\n",
    "## Een eenvoudig Transformer-model bouwen\n",
    "\n",
    "Keras bevat geen ingebouwde Transformer-laag, maar we kunnen er zelf een bouwen. Zoals eerder zullen we ons richten op tekstclassificatie van de AG News dataset, maar het is vermeldenswaard dat Transformer-modellen de beste resultaten laten zien bij moeilijkere NLP-taken.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "\n",
    "ds_train, ds_test = tfds.load('ag_news_subset').values()\n",
    "\n",
    "def extract_text(x):\n",
    "    return x['title']+' '+x['description']\n",
    "\n",
    "def tupelize(x):\n",
    "    return (extract_text(x),x['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nieuwe lagen in Keras moeten de `Layer`-klasse subklassen en de `call`-methode implementeren. Laten we beginnen met de **Positional Embedding**-laag. We zullen [wat code uit de officiële Keras-documentatie](https://keras.io/examples/nlp/text_classification_with_transformer/) gebruiken. We gaan ervan uit dat we alle invoersequenties op lengte `maxlen` opvullen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(keras.layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = keras.layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "        self.maxlen = maxlen\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = self.maxlen\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x+positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deze laag bestaat uit twee `Embedding`-lagen: één voor het embedden van tokens (op de manier die we eerder hebben besproken) en één voor het embedden van tokenposities. Tokenposities worden gecreëerd als een reeks natuurlijke getallen van 0 tot `maxlen` met behulp van `tf.range`, en vervolgens door de embedding-laag gehaald. De twee resulterende embedding-vectoren worden vervolgens bij elkaar opgeteld, wat een positioneel-geëmbedde representatie oplevert van de invoer met de vorm `maxlen`$\\times$`embed_dim`.\n",
    "\n",
    "Laten we nu het transformerblok implementeren. Dit blok zal de uitvoer van de eerder gedefinieerde embedding-laag gebruiken:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim, name='attn')\n",
    "        self.ffn = keras.Sequential(\n",
    "            [keras.layers.Dense(ff_dim, activation=\"relu\"), keras.layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = keras.layers.Dropout(rate)\n",
    "        self.dropout2 = keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nu zijn we klaar om het complete transformer-model te definiëren:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "text_vectorization (TextVect (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "token_and_position_embedding (None, 256, 32)           648192    \n",
      "_________________________________________________________________\n",
      "transformer_block (Transform (None, 256, 32)           10656     \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 20)                660       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 84        \n",
      "=================================================================\n",
      "Total params: 659,592\n",
      "Trainable params: 659,592\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 32  # Embedding size for each token\n",
    "num_heads = 2  # Number of attention heads\n",
    "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
    "maxlen = 256\n",
    "vocab_size = 20000\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.experimental.preprocessing.TextVectorization(max_tokens=vocab_size,output_sequence_length=maxlen, input_shape=(1,)),\n",
    "    TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim),\n",
    "    TransformerBlock(embed_dim, num_heads, ff_dim),\n",
    "    keras.layers.GlobalAveragePooling1D(),\n",
    "    keras.layers.Dropout(0.1),\n",
    "    keras.layers.Dense(20, activation=\"relu\"),\n",
    "    keras.layers.Dropout(0.1),\n",
    "    keras.layers.Dense(4, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokenizer\n",
      "938/938 [==============================] - 45s 39ms/step - loss: 0.4978 - acc: 0.8068 - val_loss: 0.2808 - val_acc: 0.9124\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9c2427a0d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Training tokenizer')\n",
    "model.layers[0].adapt(ds_train.map(extract_text))\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'], optimizer='adam')\n",
    "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT Transformer Modellen\n",
    "\n",
    "**BERT** (Bidirectional Encoder Representations from Transformers) is een zeer groot meerlagig transformernetwerk met 12 lagen voor *BERT-base* en 24 lagen voor *BERT-large*. Het model wordt eerst voorgetraind op een grote hoeveelheid tekstdata (WikiPedia + boeken) met behulp van ongesuperviseerd leren (voorspellen van gemaskeerde woorden in een zin). Tijdens de voortraining absorbeert het model een aanzienlijk niveau van taalbegrip, wat vervolgens kan worden benut met andere datasets door middel van fine-tuning. Dit proces wordt **transfer learning** genoemd.\n",
    "\n",
    "![afbeelding van http://jalammar.github.io/illustrated-bert/](../../../../../translated_images/nl/jalammarBERT-language-modeling-masked-lm.34f113ea5fec4362.webp)\n",
    "\n",
    "Er zijn veel variaties van Transformer-architecturen, waaronder BERT, DistilBERT, BigBird, OpenGPT3 en meer, die kunnen worden fijn afgestemd.\n",
    "\n",
    "Laten we eens kijken hoe we een voorgetraind BERT-model kunnen gebruiken om ons traditionele probleem van sequentieclassificatie op te lossen. We zullen het idee en wat code lenen van de [officiële documentatie](https://www.tensorflow.org/text/tutorials/classify_text_with_bert).\n",
    "\n",
    "Om voorgetrainde modellen te laden, zullen we **Tensorflow hub** gebruiken. Laten we eerst de BERT-specifieke vectorizer laden:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_41180/4216669875.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow_text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow_hub\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mhub\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhub\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mKerasLayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_text'"
     ]
    }
   ],
   "source": [
    "import tensorflow_text \n",
    "import tensorflow_hub as hub\n",
    "vectorizer = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_type_ids': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       " array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "       dtype=int32)>,\n",
       " 'input_word_ids': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       " array([[  101,  1045,  2293, 19081,   102,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]], dtype=int32)>,\n",
       " 'input_mask': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       " array([[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "       dtype=int32)>}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer(['I love transformers'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Het is belangrijk dat je dezelfde vectorizer gebruikt als degene waarmee het oorspronkelijke netwerk is getraind. Bovendien retourneert de BERT-vectorizer drie componenten:\n",
    "* `input_word_ids`, wat een reeks tokennummers is voor de invoerzinnen\n",
    "* `input_mask`, die aangeeft welk deel van de reeks daadwerkelijke invoer bevat en welk deel opvulling is. Dit lijkt op de maskering die wordt geproduceerd door de `Masking`-laag\n",
    "* `input_type_ids` wordt gebruikt voor taken met taalmodellering en maakt het mogelijk om twee invoerzinnen in één reeks te specificeren.\n",
    "\n",
    "Vervolgens kunnen we de BERT-feature extractor instantiëren:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = hub.KerasLayer('https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pooled_output -> (1, 128)\n",
      "encoder_outputs -> 4\n",
      "sequence_output -> (1, 128, 128)\n",
      "default -> (1, 128)\n"
     ]
    }
   ],
   "source": [
    "z = bert(vectorizer(['I love transformers']))\n",
    "for i,x in z.items():\n",
    "    print(f\"{i} -> { len(x) if isinstance(x, list) else x.shape }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dus, de BERT-laag levert een aantal nuttige resultaten op:\n",
    "* `pooled_output` is het resultaat van het middelen van alle tokens in de reeks. Je kunt het zien als een intelligente semantische embedding van het hele netwerk. Het is gelijk aan de output van de `GlobalAveragePooling1D`-laag in ons vorige model.\n",
    "* `sequence_output` is de output van de laatste transformerlaag (komt overeen met de output van `TransformerBlock` in ons model hierboven).\n",
    "* `encoder_outputs` zijn de outputs van alle transformerlagen. Aangezien we een BERT-model met 4 lagen hebben geladen (zoals je waarschijnlijk kunt afleiden uit de naam, die `4_H` bevat), heeft het 4 tensors. De laatste is hetzelfde als `sequence_output`.\n",
    "\n",
    "Nu gaan we het end-to-end classificatiemodel definiëren. We zullen gebruik maken van *functionele modeldefinitie*, waarbij we de invoer van het model definiëren en vervolgens een reeks expressies geven om de output te berekenen. We zullen ook de gewichten van het BERT-model niet-trainbaar maken en alleen de uiteindelijke classifier trainen:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer (KerasLayer)        {'input_type_ids': ( 0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer_1 (KerasLayer)      {'pooled_output': (N 4782465     keras_layer[0][0]                \n",
      "                                                                 keras_layer[0][1]                \n",
      "                                                                 keras_layer[0][2]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 128)          0           keras_layer_1[0][5]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 4)            516         dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 4,782,981\n",
      "Trainable params: 516\n",
      "Non-trainable params: 4,782,465\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inp = keras.Input(shape=(),dtype=tf.string)\n",
    "x = vectorizer(inp)\n",
    "x = bert(x)\n",
    "x = keras.layers.Dropout(0.1)(x['pooled_output'])\n",
    "out = keras.layers.Dense(4,activation='softmax')(x)\n",
    "model = keras.models.Model(inp,out)\n",
    "bert.trainable = False\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 528s 559ms/step - loss: 0.8056 - acc: 0.6983 - val_loss: 0.5953 - val_acc: 0.7888\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9bb1e36d00>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'], optimizer='adam')\n",
    "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ondanks het feit dat er weinig trainbare parameters zijn, is het proces behoorlijk traag, omdat de BERT feature extractor veel rekenkracht vereist. Het lijkt erop dat we geen redelijke nauwkeurigheid hebben kunnen bereiken, mogelijk door een gebrek aan training of een tekort aan modelparameters.\n",
    "\n",
    "Laten we proberen de BERT-gewichten te ontgrendelen en deze ook te trainen. Dit vereist een zeer kleine leersnelheid en een meer zorgvuldige trainingsstrategie met **warmup**, waarbij gebruik wordt gemaakt van de **AdamW** optimizer. We zullen het `tf-models-official` pakket gebruiken om de optimizer te creëren:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer (KerasLayer)        {'input_type_ids': ( 0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer_1 (KerasLayer)      {'pooled_output': (N 4782465     keras_layer[0][0]                \n",
      "                                                                 keras_layer[0][1]                \n",
      "                                                                 keras_layer[0][2]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 128)          0           keras_layer_1[0][5]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 4)            516         dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 4,782,981\n",
      "Trainable params: 4,782,980\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n",
      "938/938 [==============================] - 629s 664ms/step - loss: 0.6344 - acc: 0.7658 - val_loss: 0.4876 - val_acc: 0.8247\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9bb0bd0070>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from official.nlp import optimization \n",
    "bert.trainable=True\n",
    "model.summary()\n",
    "epochs = 3\n",
    "opt = optimization.create_optimizer(\n",
    "    init_lr=3e-5,\n",
    "    num_train_steps=epochs*len(ds_train),\n",
    "    num_warmup_steps=0.1*epochs*len(ds_train),\n",
    "    optimizer_type='adamw')\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'], optimizer=opt)\n",
    "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zoals je kunt zien, verloopt de training vrij traag - maar je kunt experimenteren en het model een paar epochs (5-10) trainen om te zien of je een beter resultaat kunt behalen in vergelijking met de methoden die we eerder hebben gebruikt.\n",
    "\n",
    "## Huggingface Transformers-bibliotheek\n",
    "\n",
    "Een andere veelgebruikte (en iets eenvoudigere) manier om Transformer-modellen te gebruiken is de [HuggingFace-pakket](https://github.com/huggingface/), dat eenvoudige bouwstenen biedt voor verschillende NLP-taken. Het is beschikbaar voor zowel Tensorflow als PyTorch, een ander zeer populair neurale netwerkframework.\n",
    "\n",
    "> **Opmerking**: Als je niet geïnteresseerd bent in hoe de Transformers-bibliotheek werkt, kun je naar het einde van dit notebook gaan, omdat je niets wezenlijk anders zult zien dan wat we hierboven hebben gedaan. We zullen dezelfde stappen herhalen om een BERT-model te trainen met een andere bibliotheek en een aanzienlijk groter model. Het proces omvat dus een vrij lange training, dus je kunt er ook voor kiezen om alleen door de code te bladeren.\n",
    "\n",
    "Laten we eens kijken hoe ons probleem kan worden opgelost met behulp van [Huggingface Transformers](http://huggingface.co).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Het eerste wat we moeten doen, is het model kiezen dat we gaan gebruiken. Naast enkele ingebouwde modellen bevat Huggingface een [online modelrepository](https://huggingface.co/models), waar je veel meer vooraf getrainde modellen van de community kunt vinden. Al deze modellen kunnen worden geladen en gebruikt door simpelweg een modelnaam op te geven. Alle benodigde binaire bestanden voor het model worden automatisch gedownload.\n",
    "\n",
    "Soms moet je je eigen modellen laden. In dat geval kun je de map specificeren die alle relevante bestanden bevat, inclusief parameters voor de tokenizer, het `config.json`-bestand met modelparameters, binaire gewichten, enzovoort.\n",
    "\n",
    "Vanuit de modelnaam kunnen we zowel het model als de tokenizer instantiëren. Laten we beginnen met een tokenizer:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "# To load the model from Internet repository using model name. \n",
    "# Use this if you are running from your own copy of the notebooks\n",
    "bert_model = 'bert-base-uncased' \n",
    "\n",
    "# To load the model from the directory on disk. Use this for Microsoft Learn module, because we have\n",
    "# prepared all required files for you.\n",
    "#bert_model = './bert'\n",
    "\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(bert_model)\n",
    "\n",
    "MAX_SEQ_LEN = 128\n",
    "PAD_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "UNK_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Het `tokenizer` object bevat de `encode` functie die direct kan worden gebruikt om tekst te coderen:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 23435, 12314, 2003, 1037, 2307, 7705, 2005, 17953, 2361, 102]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('Tensorflow is a great framework for NLP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We kunnen ook een tokenizer gebruiken om een reeks te coderen op een manier die geschikt is om aan het model door te geven, d.w.z. inclusief `token_ids`, `input_mask` velden, enz. We kunnen ook specificeren dat we Tensorflow-tensoren willen door het argument `return_tensors='tf'` te geven:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[ 101, 7592, 1010, 2045,  102]], dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[0, 0, 0, 0, 0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[1, 1, 1, 1, 1]], dtype=int32)>}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(['Hello, there'],return_tensors='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In ons geval zullen we een vooraf getraind BERT-model gebruiken genaamd `bert-base-uncased`. *Uncased* geeft aan dat het model niet gevoelig is voor hoofdletters en kleine letters.\n",
    "\n",
    "Bij het trainen van het model moeten we een getokeniseerde reeks als invoer leveren, en daarom zullen we een gegevensverwerkingspipeline ontwerpen. Aangezien `tokenizer.encode` een Python-functie is, zullen we dezelfde aanpak gebruiken als in de vorige eenheid door deze aan te roepen met `py_function`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(x):\n",
    "    return tokenizer.encode(x.numpy().decode('utf-8'),return_tensors='tf',padding='max_length',max_length=MAX_SEQ_LEN,truncation=True)[0]\n",
    "\n",
    "def process_fn(x):\n",
    "    s = x['title']+' '+x['description']\n",
    "    e = tf.py_function(process,inp=[s],Tout=(tf.int32))\n",
    "    e.set_shape(MAX_SEQ_LEN)\n",
    "    return e,x['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nu kunnen we het daadwerkelijke model laden met behulp van het `BertForSequenceClassification`-pakket. Dit zorgt ervoor dat ons model al een vereiste architectuur heeft voor classificatie, inclusief de uiteindelijke classifier. Je zult een waarschuwingsbericht zien waarin staat dat de gewichten van de uiteindelijke classifier niet zijn geïnitialiseerd en dat het model pre-training nodig heeft - dat is helemaal prima, want dat is precies wat we gaan doen!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = transformers.TFBertForSequenceClassification.from_pretrained(bert_model,num_labels=4,output_attentions=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  109482240 \n",
      "_________________________________________________________________\n",
      "dropout_75 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  3076      \n",
      "=================================================================\n",
      "Total params: 109,485,316\n",
      "Trainable params: 109,485,316\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zoals je kunt zien in `summary()`, bevat het model bijna 110 miljoen parameters! Vermoedelijk, als we een eenvoudige classificatietaak willen uitvoeren op een relatief kleine dataset, willen we de BERT-baselaag niet trainen:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  109482240 \n",
      "_________________________________________________________________\n",
      "dropout_75 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  3076      \n",
      "=================================================================\n",
      "Total params: 109,485,316\n",
      "Trainable params: 3,076\n",
      "Non-trainable params: 109,482,240\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.layers[0].trainable = False\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nu kunnen we beginnen met trainen!\n",
    "\n",
    "> **Opmerking**: Het trainen van een volledige BERT-model kan erg tijdrovend zijn! Daarom trainen we het alleen voor de eerste 32 batches. Dit is slechts om te laten zien hoe het trainen van het model wordt opgezet. Als je geïnteresseerd bent in volledige training - verwijder gewoon de parameters `steps_per_epoch` en `validation_steps`, en bereid je voor om te wachten!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 142s 4s/step - loss: 1.3896 - acc: 0.2500 - val_loss: 1.3863 - val_acc: 0.2480\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f1d40a4b6a0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile('adam','sparse_categorical_crossentropy',['acc'])\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "model.fit(ds_train.map(process_fn).batch(32),validation_data=ds_test.map(process_fn).batch(32),steps_per_epoch=32,validation_steps=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als je het aantal iteraties verhoogt, lang genoeg wacht en traint over meerdere epochs, kun je verwachten dat BERT-classificatie ons de beste nauwkeurigheid geeft! Dat komt omdat BERT de structuur van de taal al behoorlijk goed begrijpt, en we alleen de laatste classifier hoeven te fine-tunen. Maar omdat BERT een groot model is, duurt het hele trainingsproces lang en vereist het serieuze rekenkracht! (GPU, en bij voorkeur meer dan één).\n",
    "\n",
    "> **Opmerking:** In ons voorbeeld hebben we een van de kleinste voorgetrainde BERT-modellen gebruikt. Er zijn grotere modellen die waarschijnlijk betere resultaten opleveren.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Belangrijkste punten\n",
    "\n",
    "In deze eenheid hebben we zeer recente modelarchitecturen op basis van **transformers** bekeken. We hebben ze toegepast op onze taak voor tekstclassificatie, maar op dezelfde manier kunnen BERT-modellen worden gebruikt voor entiteitsextractie, vraag-antwoord systemen en andere NLP-taken.\n",
    "\n",
    "Transformermodellen vertegenwoordigen de huidige state-of-the-art in NLP, en in de meeste gevallen zou dit de eerste oplossing moeten zijn waarmee je begint te experimenteren bij het implementeren van aangepaste NLP-oplossingen. Het begrijpen van de basisprincipes van recurrente neurale netwerken, zoals besproken in deze module, is echter van cruciaal belang als je geavanceerde neurale modellen wilt bouwen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Disclaimer**:  \nDit document is vertaald met behulp van de AI-vertalingsservice [Co-op Translator](https://github.com/Azure/co-op-translator). Hoewel we streven naar nauwkeurigheid, dient u zich ervan bewust te zijn dat geautomatiseerde vertalingen fouten of onnauwkeurigheden kunnen bevatten. Het originele document in zijn oorspronkelijke taal moet worden beschouwd als de gezaghebbende bron. Voor cruciale informatie wordt professionele menselijke vertaling aanbevolen. Wij zijn niet aansprakelijk voor eventuele misverstanden of verkeerde interpretaties die voortvloeien uit het gebruik van deze vertaling.\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0cb620c6d4b9f7a635928804c26cf22403d89d98d79684e4529119355ee6d5a5"
  },
  "kernelspec": {
   "display_name": "py38_tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "coopTranslator": {
   "original_hash": "ab59c532409774988ab875f2260e8e53",
   "translation_date": "2025-08-28T21:42:09+00:00",
   "source_file": "lessons/5-NLP/18-Transformers/TransformersTF.ipynb",
   "language_code": "nl"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}