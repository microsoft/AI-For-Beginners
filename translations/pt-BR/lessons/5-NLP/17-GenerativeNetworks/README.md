# Redes Generativas

## [Quiz pr√©-aula](https://ff-quizzes.netlify.app/en/ai/quiz/33)

Redes Neurais Recorrentes (RNNs) e suas variantes com c√©lulas controladas, como Long Short Term Memory Cells (LSTMs) e Gated Recurrent Units (GRUs), fornecem um mecanismo para modelagem de linguagem, pois conseguem aprender a ordem das palavras e prever a pr√≥xima palavra em uma sequ√™ncia. Isso nos permite usar RNNs para **tarefas generativas**, como gera√ß√£o de texto comum, tradu√ß√£o autom√°tica e at√© mesmo legendas para imagens.

> ‚úÖ Pense em todas as vezes que voc√™ se beneficiou de tarefas generativas, como a conclus√£o de texto enquanto digita. Pesquise sobre seus aplicativos favoritos para descobrir se eles utilizam RNNs.

Na arquitetura de RNN que discutimos na unidade anterior, cada unidade RNN produzia o pr√≥ximo estado oculto como sa√≠da. No entanto, tamb√©m podemos adicionar outra sa√≠da a cada unidade recorrente, permitindo que ela produza uma **sequ√™ncia** (que tem o mesmo comprimento da sequ√™ncia original). Al√©m disso, podemos usar unidades RNN que n√£o aceitam uma entrada em cada etapa, mas apenas um vetor de estado inicial, e ent√£o produzem uma sequ√™ncia de sa√≠das.

Isso permite diferentes arquiteturas neurais, como mostrado na imagem abaixo:

![Imagem mostrando padr√µes comuns de redes neurais recorrentes.](../../../../../translated_images/pt-BR/unreasonable-effectiveness-of-rnn.541ead816778f42d.webp)

> Imagem do post no blog [Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) por [Andrej Karpaty](http://karpathy.github.io/)

* **Um-para-um** √© uma rede neural tradicional com uma entrada e uma sa√≠da.
* **Um-para-muitos** √© uma arquitetura generativa que aceita um valor de entrada e gera uma sequ√™ncia de valores de sa√≠da. Por exemplo, se quisermos treinar uma rede de **legendas para imagens** que produza uma descri√ß√£o textual de uma imagem, podemos usar uma imagem como entrada, pass√°-la por uma CNN para obter seu estado oculto e, em seguida, usar uma cadeia recorrente para gerar a legenda palavra por palavra.
* **Muitos-para-um** corresponde √†s arquiteturas RNN que descrevemos na unidade anterior, como classifica√ß√£o de texto.
* **Muitos-para-muitos**, ou **sequ√™ncia-para-sequ√™ncia**, corresponde a tarefas como **tradu√ß√£o autom√°tica**, onde temos uma primeira RNN que coleta todas as informa√ß√µes da sequ√™ncia de entrada no estado oculto, e outra cadeia RNN que desenrola esse estado na sequ√™ncia de sa√≠da.

Nesta unidade, focaremos em modelos generativos simples que nos ajudam a gerar texto. Para simplificar, usaremos tokeniza√ß√£o em n√≠vel de caracteres.

Treinaremos esta RNN para gerar texto passo a passo. Em cada etapa, pegaremos uma sequ√™ncia de caracteres de comprimento `nchars` e pediremos √† rede que gere o pr√≥ximo caractere de sa√≠da para cada caractere de entrada:

![Imagem mostrando um exemplo de gera√ß√£o de RNN da palavra 'HELLO'.](../../../../../translated_images/pt-BR/rnn-generate.56c54afb52f9781d.webp)

Ao gerar texto (durante a infer√™ncia), come√ßamos com algum **prompt**, que √© passado pelas c√©lulas RNN para gerar seu estado intermedi√°rio, e ent√£o a gera√ß√£o come√ßa a partir desse estado. Geramos um caractere por vez e passamos o estado e o caractere gerado para outra c√©lula RNN para gerar o pr√≥ximo, at√© que tenhamos gerado caracteres suficientes.

<img src="../../../../../translated_images/pt-BR/rnn-generate-inf.5168dc65e0370eea.webp" width="60%"/>

> Imagem do autor

## ‚úçÔ∏è Exerc√≠cios: Redes Generativas

Continue seu aprendizado nos seguintes notebooks:

* [Redes Generativas com PyTorch](GenerativePyTorch.ipynb)
* [Redes Generativas com TensorFlow](GenerativeTF.ipynb)

## Gera√ß√£o de texto suave e temperatura

A sa√≠da de cada c√©lula RNN √© uma distribui√ß√£o de probabilidade de caracteres. Se sempre escolhermos o caractere com a maior probabilidade como o pr√≥ximo caractere no texto gerado, o texto frequentemente pode se tornar "c√≠clico", repetindo as mesmas sequ√™ncias de caracteres repetidamente, como neste exemplo:

```
today of the second the company and a second the company ...
```

No entanto, se olharmos para a distribui√ß√£o de probabilidade do pr√≥ximo caractere, pode ser que a diferen√ßa entre algumas das maiores probabilidades n√£o seja t√£o grande, por exemplo, um caractere pode ter probabilidade 0,2 e outro 0,19, etc. Por exemplo, ao procurar o pr√≥ximo caractere na sequ√™ncia '*play*', o pr√≥ximo caractere pode ser igualmente um espa√ßo ou **e** (como na palavra *player*).

Isso nos leva √† conclus√£o de que nem sempre √© "justo" selecionar o caractere com maior probabilidade, pois escolher o segundo maior ainda pode levar a um texto significativo. √â mais s√°bio **amostrar** caracteres da distribui√ß√£o de probabilidade fornecida pela sa√≠da da rede. Podemos tamb√©m usar um par√¢metro, **temperatura**, que ajusta a distribui√ß√£o de probabilidade, caso queiramos adicionar mais aleatoriedade ou torn√°-la mais √≠ngreme, se quisermos nos ater mais aos caracteres de maior probabilidade.

Explore como essa gera√ß√£o de texto suave √© implementada nos notebooks mencionados acima.

## Conclus√£o

Embora a gera√ß√£o de texto possa ser √∫til por si s√≥, os maiores benef√≠cios v√™m da capacidade de gerar texto usando RNNs a partir de algum vetor de caracter√≠sticas inicial. Por exemplo, a gera√ß√£o de texto √© usada como parte da tradu√ß√£o autom√°tica (sequ√™ncia-para-sequ√™ncia, neste caso o vetor de estado do *encoder* √© usado para gerar ou *decodificar* a mensagem traduzida) ou para gerar descri√ß√µes textuais de uma imagem (nesse caso, o vetor de caracter√≠sticas viria de um extrator CNN).

## üöÄ Desafio

Fa√ßa algumas li√ß√µes no Microsoft Learn sobre este t√≥pico:

* Gera√ß√£o de Texto com [PyTorch](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-pytorch/6-generative-networks/?WT.mc_id=academic-77998-cacaste)/[TensorFlow](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-tensorflow/5-generative-networks/?WT.mc_id=academic-77998-cacaste)

## [Quiz p√≥s-aula](https://ff-quizzes.netlify.app/en/ai/quiz/34)

## Revis√£o e Autoestudo

Aqui est√£o alguns artigos para expandir seu conhecimento:

* Diferentes abordagens para gera√ß√£o de texto com Cadeia de Markov, LSTM e GPT-2: [post no blog](https://towardsdatascience.com/text-generation-gpt-2-lstm-markov-chain-9ea371820e1e)
* Exemplo de gera√ß√£o de texto na [documenta√ß√£o do Keras](https://keras.io/examples/generative/lstm_character_level_text_generation/)

## [Tarefa](lab/README.md)

Vimos como gerar texto caractere por caractere. No laborat√≥rio, voc√™ explorar√° a gera√ß√£o de texto em n√≠vel de palavras.

---

