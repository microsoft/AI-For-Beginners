# Representando Texto como Tensores

## [Quiz pr√©-aula](https://ff-quizzes.netlify.app/en/ai/quiz/25)

## Classifica√ß√£o de Texto

Na primeira parte desta se√ß√£o, vamos focar na tarefa de **classifica√ß√£o de texto**. Utilizaremos o [AG News](https://www.kaggle.com/amananandrai/ag-news-classification-dataset), um conjunto de dados que cont√©m artigos de not√≠cias como o seguinte:

* Categoria: Ci√™ncia/Tecnologia
* T√≠tulo: Empresa de Ky. Ganha Subs√≠dio para Estudar Pept√≠deos (AP)
* Corpo: AP - Uma empresa fundada por um pesquisador de qu√≠mica da Universidade de Louisville ganhou um subs√≠dio para desenvolver...

Nosso objetivo ser√° classificar o item de not√≠cia em uma das categorias com base no texto.

## Representando texto

Se quisermos resolver tarefas de Processamento de Linguagem Natural (PLN) com redes neurais, precisamos de uma maneira de representar texto como tensores. Os computadores j√° representam caracteres textuais como n√∫meros que mapeiam para fontes na sua tela usando codifica√ß√µes como ASCII ou UTF-8.

<img alt="Imagem mostrando um diagrama que mapeia um caractere para uma representa√ß√£o ASCII e bin√°ria" src="../../../../../translated_images/pt-BR/ascii-character-map.18ed6aa7f3b0a7ff.webp" width="50%"/>

> [Fonte da imagem](https://www.seobility.net/en/wiki/ASCII)

Como humanos, entendemos o que cada letra **representa** e como todos os caracteres se juntam para formar as palavras de uma frase. No entanto, os computadores, por si s√≥, n√£o t√™m esse entendimento, e a rede neural precisa aprender o significado durante o treinamento.

Portanto, podemos usar diferentes abordagens ao representar texto:

* **Representa√ß√£o a n√≠vel de caractere**, onde representamos o texto tratando cada caractere como um n√∫mero. Dado que temos *C* caracteres diferentes em nosso corpus de texto, a palavra *Hello* seria representada por um tensor 5x*C*. Cada letra corresponderia a uma coluna do tensor em codifica√ß√£o one-hot.
* **Representa√ß√£o a n√≠vel de palavra**, na qual criamos um **vocabul√°rio** de todas as palavras em nosso texto e, em seguida, representamos as palavras usando codifica√ß√£o one-hot. Essa abordagem √© um pouco melhor, porque cada letra, por si s√≥, n√£o tem muito significado, e, ao usar conceitos sem√¢nticos de n√≠vel mais alto - palavras - simplificamos a tarefa para a rede neural. No entanto, dado o tamanho do dicion√°rio, precisamos lidar com tensores esparsos de alta dimens√£o.

Independentemente da representa√ß√£o, primeiro precisamos converter o texto em uma sequ√™ncia de **tokens**, sendo um token um caractere, uma palavra ou, √†s vezes, at√© parte de uma palavra. Depois, convertemos o token em um n√∫mero, geralmente usando um **vocabul√°rio**, e esse n√∫mero pode ser alimentado em uma rede neural usando codifica√ß√£o one-hot.

## N-Gramas

Na linguagem natural, o significado preciso das palavras s√≥ pode ser determinado no contexto. Por exemplo, os significados de *rede neural* e *rede de pesca* s√£o completamente diferentes. Uma das maneiras de levar isso em conta √© construir nosso modelo com pares de palavras, considerando os pares de palavras como tokens separados no vocabul√°rio. Dessa forma, a frase *Eu gosto de pescar* ser√° representada pela seguinte sequ√™ncia de tokens: *Eu gosto*, *gosto de*, *de pescar*. O problema com essa abordagem √© que o tamanho do dicion√°rio cresce significativamente, e combina√ß√µes como *de pescar* e *de comprar* s√£o representadas por tokens diferentes, que n√£o compartilham nenhuma semelhan√ßa sem√¢ntica, apesar do mesmo verbo.

Em alguns casos, podemos considerar o uso de tri-gramas -- combina√ß√µes de tr√™s palavras -- tamb√©m. Assim, essa abordagem √© frequentemente chamada de **n-gramas**. Al√©m disso, faz sentido usar n-gramas com representa√ß√£o a n√≠vel de caractere, caso em que os n-gramas corresponder√£o aproximadamente a diferentes s√≠labas.

## Bag-of-Words e TF/IDF

Ao resolver tarefas como classifica√ß√£o de texto, precisamos ser capazes de representar o texto por um vetor de tamanho fixo, que usaremos como entrada para o classificador denso final. Uma das maneiras mais simples de fazer isso √© combinar todas as representa√ß√µes individuais de palavras, por exemplo, somando-as. Se somarmos as codifica√ß√µes one-hot de cada palavra, acabaremos com um vetor de frequ√™ncias, mostrando quantas vezes cada palavra aparece no texto. Essa representa√ß√£o de texto √© chamada de **bag-of-words** (BoW).

<img src="../../../../../translated_images/pt-BR/bow.3811869cff59368d.webp" width="90%"/>

> Imagem do autor

Um BoW essencialmente representa quais palavras aparecem no texto e em quais quantidades, o que pode ser uma boa indica√ß√£o do que o texto trata. Por exemplo, um artigo de not√≠cias sobre pol√≠tica provavelmente conter√° palavras como *presidente* e *pa√≠s*, enquanto uma publica√ß√£o cient√≠fica ter√° algo como *colisor*, *descoberto*, etc. Assim, as frequ√™ncias das palavras podem, em muitos casos, ser um bom indicador do conte√∫do do texto.

O problema com BoW √© que certas palavras comuns, como *e*, *√©*, etc., aparecem na maioria dos textos e t√™m as maiores frequ√™ncias, mascarando as palavras que s√£o realmente importantes. Podemos reduzir a import√¢ncia dessas palavras levando em conta a frequ√™ncia com que elas ocorrem em toda a cole√ß√£o de documentos. Essa √© a ideia principal por tr√°s da abordagem TF/IDF, que √© abordada em mais detalhes nos notebooks anexados a esta li√ß√£o.

No entanto, nenhuma dessas abordagens pode levar totalmente em conta a **sem√¢ntica** do texto. Precisamos de modelos de redes neurais mais poderosos para fazer isso, o que discutiremos mais adiante nesta se√ß√£o.

## ‚úçÔ∏è Exerc√≠cios: Representa√ß√£o de Texto

Continue seu aprendizado nos seguintes notebooks:

* [Representa√ß√£o de Texto com PyTorch](TextRepresentationPyTorch.ipynb)
* [Representa√ß√£o de Texto com TensorFlow](TextRepresentationTF.ipynb)

## Conclus√£o

At√© agora, estudamos t√©cnicas que podem adicionar peso de frequ√™ncia a diferentes palavras. No entanto, elas n√£o conseguem representar o significado ou a ordem. Como o famoso linguista J. R. Firth disse em 1935: "O significado completo de uma palavra √© sempre contextual, e nenhum estudo de significado fora do contexto pode ser levado a s√©rio." Aprenderemos mais adiante no curso como capturar informa√ß√µes contextuais do texto usando modelagem de linguagem.

## üöÄ Desafio

Experimente outros exerc√≠cios usando bag-of-words e diferentes modelos de dados. Voc√™ pode se inspirar nesta [competi√ß√£o no Kaggle](https://www.kaggle.com/competitions/word2vec-nlp-tutorial/overview/part-1-for-beginners-bag-of-words)

## [Quiz p√≥s-aula](https://ff-quizzes.netlify.app/en/ai/quiz/26)

## Revis√£o & Autoestudo

Pratique suas habilidades com embeddings de texto e t√©cnicas de bag-of-words no [Microsoft Learn](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-pytorch/?WT.mc_id=academic-77998-cacaste)

## [Tarefa: Notebooks](assignment.md)

---

