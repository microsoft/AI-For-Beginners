# Mecanismos de Aten√ß√£o e Transformers

## [Quiz pr√©-aula](https://ff-quizzes.netlify.app/en/ai/quiz/35)

Um dos problemas mais importantes no dom√≠nio de PLN (Processamento de Linguagem Natural) √© a **tradu√ß√£o autom√°tica**, uma tarefa essencial que sustenta ferramentas como o Google Tradutor. Nesta se√ß√£o, vamos nos concentrar na tradu√ß√£o autom√°tica ou, mais geralmente, em qualquer tarefa de *sequ√™ncia para sequ√™ncia* (tamb√©m chamada de **transdu√ß√£o de senten√ßas**).

Com RNNs, a tarefa de sequ√™ncia para sequ√™ncia √© implementada por duas redes recorrentes, onde uma rede, o **codificador**, comprime uma sequ√™ncia de entrada em um estado oculto, enquanto outra rede, o **decodificador**, expande esse estado oculto em um resultado traduzido. Existem alguns problemas com essa abordagem:

* O estado final da rede codificadora tem dificuldade em lembrar o in√≠cio de uma senten√ßa, o que causa baixa qualidade do modelo para senten√ßas longas.
* Todas as palavras em uma sequ√™ncia t√™m o mesmo impacto no resultado. Na realidade, no entanto, palavras espec√≠ficas na sequ√™ncia de entrada frequentemente t√™m mais impacto nas sa√≠das sequenciais do que outras.

Os **Mecanismos de Aten√ß√£o** fornecem um meio de ponderar o impacto contextual de cada vetor de entrada em cada previs√£o de sa√≠da da RNN. Isso √© implementado criando atalhos entre os estados intermedi√°rios da RNN de entrada e a RNN de sa√≠da. Dessa forma, ao gerar o s√≠mbolo de sa√≠da y<sub>t</sub>, levamos em conta todos os estados ocultos de entrada h<sub>i</sub>, com diferentes coeficientes de peso &alpha;<sub>t,i</sub>.

![Imagem mostrando um modelo codificador/decodificador com uma camada de aten√ß√£o aditiva](../../../../../translated_images/pt-BR/encoder-decoder-attention.7a726296894fb567.webp)

> O modelo codificador-decodificador com mecanismo de aten√ß√£o aditiva em [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf), citado deste [post no blog](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)

A matriz de aten√ß√£o {&alpha;<sub>i,j</sub>} representaria o grau em que certas palavras de entrada influenciam a gera√ß√£o de uma palavra espec√≠fica na sequ√™ncia de sa√≠da. Abaixo est√° um exemplo de tal matriz:

![Imagem mostrando um alinhamento de exemplo encontrado pelo RNNsearch-50, retirada de Bahdanau - arviz.org](../../../../../translated_images/pt-BR/bahdanau-fig3.09ba2d37f202a6af.webp)

> Figura de [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) (Fig.3)

Os mecanismos de aten√ß√£o s√£o respons√°veis por grande parte do estado da arte atual ou pr√≥ximo ao atual em PLN. No entanto, adicionar aten√ß√£o aumenta significativamente o n√∫mero de par√¢metros do modelo, o que levou a problemas de escalabilidade com RNNs. Uma limita√ß√£o chave na escalabilidade das RNNs √© que a natureza recorrente dos modelos torna desafiador agrupar e paralelizar o treinamento. Em uma RNN, cada elemento de uma sequ√™ncia precisa ser processado em ordem sequencial, o que significa que n√£o pode ser facilmente paralelizado.

![Codificador Decodificador com Aten√ß√£o](../../../../../lessons/5-NLP/18-Transformers/images/EncDecAttention.gif)

> Figura do [Blog do Google](https://research.googleblog.com/2016/09/a-neural-network-for-machine.html)

A ado√ß√£o de mecanismos de aten√ß√£o, combinada com essa limita√ß√£o, levou √† cria√ß√£o dos modelos Transformer, que hoje representam o estado da arte, como BERT e Open-GPT3.

## Modelos Transformer

Uma das principais ideias por tr√°s dos transformers √© evitar a natureza sequencial das RNNs e criar um modelo que seja paraleliz√°vel durante o treinamento. Isso √© alcan√ßado implementando duas ideias:

* codifica√ß√£o posicional
* uso do mecanismo de autoaten√ß√£o para capturar padr√µes em vez de RNNs (ou CNNs) (√© por isso que o artigo que introduz os transformers se chama *[Attention is all you need](https://arxiv.org/abs/1706.03762)*)

### Codifica√ß√£o/Embutimento Posicional

A ideia da codifica√ß√£o posicional √© a seguinte:  
1. Ao usar RNNs, a posi√ß√£o relativa dos tokens √© representada pelo n√∫mero de passos, e, portanto, n√£o precisa ser representada explicitamente.  
2. No entanto, ao mudarmos para aten√ß√£o, precisamos saber as posi√ß√µes relativas dos tokens dentro de uma sequ√™ncia.  
3. Para obter a codifica√ß√£o posicional, aumentamos nossa sequ√™ncia de tokens com uma sequ√™ncia de posi√ß√µes dos tokens na sequ√™ncia (ou seja, uma sequ√™ncia de n√∫meros 0, 1, ...).  
4. Em seguida, misturamos a posi√ß√£o do token com um vetor de embutimento do token. Para transformar a posi√ß√£o (inteiro) em um vetor, podemos usar diferentes abordagens:

* Embutimento trein√°vel, semelhante ao embutimento de tokens. Esta √© a abordagem que consideramos aqui. Aplicamos camadas de embutimento tanto nos tokens quanto em suas posi√ß√µes, resultando em vetores de embutimento de mesmas dimens√µes, que ent√£o somamos.
* Fun√ß√£o fixa de codifica√ß√£o posicional, como proposto no artigo original.

<img src="../../../../../translated_images/pt-BR/pos-embedding.e41ce9b6cf6078af.webp" width="50%"/>

> Imagem do autor

O resultado que obtemos com o embutimento posicional incorpora tanto o token original quanto sua posi√ß√£o dentro de uma sequ√™ncia.

### Autoaten√ß√£o Multi-Cabe√ßa

Em seguida, precisamos capturar alguns padr√µes dentro de nossa sequ√™ncia. Para isso, os transformers usam um mecanismo de **autoaten√ß√£o**, que √© essencialmente aten√ß√£o aplicada √† mesma sequ√™ncia como entrada e sa√≠da. Aplicar autoaten√ß√£o nos permite levar em conta o **contexto** dentro da senten√ßa e ver quais palavras est√£o inter-relacionadas. Por exemplo, isso nos permite identificar quais palavras s√£o referidas por correfer√™ncias, como *it* (ele/ela), e tamb√©m considerar o contexto:

![](../../../../../translated_images/pt-BR/CoreferenceResolution.861924d6d384a7d6.webp)

> Imagem do [Blog do Google](https://research.googleblog.com/2017/08/transformer-novel-neural-network.html)

Nos transformers, usamos **Aten√ß√£o Multi-Cabe√ßa** para dar √† rede o poder de capturar v√°rios tipos diferentes de depend√™ncias, como rela√ß√µes de palavras de longo prazo vs. curto prazo, correfer√™ncia vs. outra coisa, etc.

O [Notebook do TensorFlow](TransformersTF.ipynb) cont√©m mais detalhes sobre a implementa√ß√£o das camadas do transformer.

### Aten√ß√£o Codificador-Decodificador

Nos transformers, a aten√ß√£o √© usada em dois lugares:

* Para capturar padr√µes dentro do texto de entrada usando autoaten√ß√£o.
* Para realizar a tradu√ß√£o de sequ√™ncia - √© a camada de aten√ß√£o entre o codificador e o decodificador.

A aten√ß√£o codificador-decodificador √© muito semelhante ao mecanismo de aten√ß√£o usado em RNNs, como descrito no in√≠cio desta se√ß√£o. Este diagrama animado explica o papel da aten√ß√£o codificador-decodificador.

![GIF animado mostrando como as avalia√ß√µes s√£o realizadas em modelos transformer.](../../../../../lessons/5-NLP/18-Transformers/images/transformer-animated-explanation.gif)

Como cada posi√ß√£o de entrada √© mapeada independentemente para cada posi√ß√£o de sa√≠da, os transformers podem paralelizar melhor do que as RNNs, o que permite modelos de linguagem muito maiores e mais expressivos. Cada cabe√ßa de aten√ß√£o pode ser usada para aprender diferentes rela√ß√µes entre palavras, o que melhora as tarefas de Processamento de Linguagem Natural.

## BERT

**BERT** (Bidirectional Encoder Representations from Transformers) √© uma rede transformer muito grande com v√°rias camadas: 12 camadas para o *BERT-base* e 24 para o *BERT-large*. O modelo √© primeiro pr√©-treinado em um grande corpus de dados textuais (Wikipedia + livros) usando treinamento n√£o supervisionado (prevendo palavras mascaradas em uma senten√ßa). Durante o pr√©-treinamento, o modelo absorve n√≠veis significativos de compreens√£o da linguagem, que podem ser aproveitados com outros conjuntos de dados usando ajuste fino. Esse processo √© chamado de **aprendizado por transfer√™ncia**.

![imagem de http://jalammar.github.io/illustrated-bert/](../../../../../translated_images/pt-BR/jalammarBERT-language-modeling-masked-lm.34f113ea5fec4362.webp)

> Imagem [fonte](http://jalammar.github.io/illustrated-bert/)

## ‚úçÔ∏è Exerc√≠cios: Transformers

Continue seu aprendizado nos seguintes notebooks:

* [Transformers em PyTorch](TransformersPyTorch.ipynb)
* [Transformers em TensorFlow](TransformersTF.ipynb)

## Conclus√£o

Nesta li√ß√£o, voc√™ aprendeu sobre Transformers e Mecanismos de Aten√ß√£o, ferramentas essenciais na caixa de ferramentas de PLN. Existem muitas varia√ß√µes de arquiteturas Transformer, incluindo BERT, DistilBERT, BigBird, OpenGPT3 e mais, que podem ser ajustadas. O pacote [HuggingFace](https://github.com/huggingface/) fornece um reposit√≥rio para treinar muitas dessas arquiteturas com PyTorch e TensorFlow.

## üöÄ Desafio

## [Quiz p√≥s-aula](https://ff-quizzes.netlify.app/en/ai/quiz/36)

## Revis√£o e Autoestudo

* [Post no blog](https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/), explicando o cl√°ssico artigo [Attention is all you need](https://arxiv.org/abs/1706.03762) sobre transformers.
* [Uma s√©rie de posts no blog](https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452) sobre transformers, explicando a arquitetura em detalhes.

## [Tarefa](assignment.md)

---

