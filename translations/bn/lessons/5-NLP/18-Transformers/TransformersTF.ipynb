{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# অ্যাটেনশন মেকানিজম এবং ট্রান্সফর্মার\n",
    "\n",
    "রেকারেন্ট নেটওয়ার্কগুলোর একটি বড় সীমাবদ্ধতা হলো, একটি সিকোয়েন্সের সব শব্দ ফলাফলের উপর সমান প্রভাব ফেলে। এর ফলে সিকোয়েন্স-টু-সিকোয়েন্স কাজের জন্য স্ট্যান্ডার্ড LSTM এনকোডার-ডিকোডার মডেলগুলোতে অপ্টিমাল পারফরম্যান্স হয় না, যেমন নেমড এন্টিটি রিকগনিশন এবং মেশিন ট্রান্সলেশন। বাস্তবে, ইনপুট সিকোয়েন্সের নির্দিষ্ট কিছু শব্দ সিকোয়েন্সিয়াল আউটপুটে অন্য শব্দগুলোর চেয়ে বেশি প্রভাব ফেলে।\n",
    "\n",
    "ধরা যাক একটি সিকোয়েন্স-টু-সিকোয়েন্স মডেল, যেমন মেশিন ট্রান্সলেশন। এটি দুটি রেকারেন্ট নেটওয়ার্ক দ্বারা বাস্তবায়িত হয়, যেখানে একটি নেটওয়ার্ক (**encoder**) ইনপুট সিকোয়েন্সকে হিডেন স্টেটে সংকুচিত করে এবং অন্যটি, **decoder**, এই হিডেন স্টেটকে অনুবাদিত ফলাফলে রূপান্তরিত করে। এই পদ্ধতির সমস্যা হলো, নেটওয়ার্কের চূড়ান্ত স্টেটের জন্য বাক্যের শুরুটা মনে রাখা কঠিন হয়ে যায়, যার ফলে দীর্ঘ বাক্যের ক্ষেত্রে মডেলের গুণগত মান খারাপ হয়।\n",
    "\n",
    "**অ্যাটেনশন মেকানিজম** RNN-এর প্রতিটি আউটপুট প্রেডিকশনে প্রতিটি ইনপুট ভেক্টরের প্রাসঙ্গিক প্রভাবকে ওজন দেওয়ার একটি উপায় প্রদান করে। এটি বাস্তবায়িত হয় ইনপুট RNN-এর মধ্যবর্তী স্টেট এবং আউটপুট RNN-এর মধ্যে শর্টকাট তৈরি করে। এই পদ্ধতিতে, যখন আউটপুট প্রতীক $y_t$ তৈরি করা হয়, তখন আমরা বিভিন্ন ওজন সহগ $\\alpha_{t,i}$ সহ সমস্ত ইনপুট হিডেন স্টেট $h_i$ বিবেচনায় নেব। \n",
    "\n",
    "![এনকোডার/ডিকোডার মডেল একটি অ্যাডিটিভ অ্যাটেনশন লেয়ার সহ](../../../../../translated_images/bn/encoder-decoder-attention.7a726296894fb567.webp)\n",
    "*[Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) থেকে উদ্ধৃত, [এই ব্লগ পোস্ট](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html) থেকে নেওয়া এনকোডার-ডিকোডার মডেল অ্যাডিটিভ অ্যাটেনশন মেকানিজম সহ*\n",
    "\n",
    "অ্যাটেনশন ম্যাট্রিক্স $\\{\\alpha_{i,j}\\}$ একটি নির্দিষ্ট আউটপুট সিকোয়েন্সের একটি শব্দ তৈরিতে কোন ইনপুট শব্দগুলো কতটা ভূমিকা রাখছে তা উপস্থাপন করে। নিচে এমন একটি ম্যাট্রিক্সের উদাহরণ দেওয়া হলো:\n",
    "\n",
    "![RNNsearch-50 দ্বারা পাওয়া একটি নমুনা অ্যালাইনমেন্ট, Bahdanau - arviz.org থেকে নেওয়া](../../../../../translated_images/bn/bahdanau-fig3.09ba2d37f202a6af.webp)\n",
    "\n",
    "*[Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) থেকে নেওয়া (Fig.3)*\n",
    "\n",
    "অ্যাটেনশন মেকানিজম বর্তমান বা প্রায় বর্তমান প্রাকৃতিক ভাষা প্রক্রিয়াকরণের (NLP) স্টেট অফ দ্য আর্টের জন্য দায়ী। তবে অ্যাটেনশন যোগ করার ফলে মডেলের প্যারামিটারের সংখ্যা উল্লেখযোগ্যভাবে বৃদ্ধি পায়, যা RNN-এর স্কেলিং সমস্যার দিকে নিয়ে যায়। RNN-এর স্কেলিংয়ের একটি মূল সীমাবদ্ধতা হলো, মডেলের রেকারেন্ট প্রকৃতি প্রশিক্ষণকে ব্যাচ এবং প্যারালালাইজ করা কঠিন করে তোলে। RNN-এ একটি সিকোয়েন্সের প্রতিটি উপাদানকে ক্রমানুসারে প্রক্রিয়া করতে হয়, যার ফলে এটি সহজে প্যারালালাইজ করা যায় না।\n",
    "\n",
    "অ্যাটেনশন মেকানিজমের গ্রহণযোগ্যতা এবং এই সীমাবদ্ধতার সংমিশ্রণে, আজকের স্টেট অফ দ্য আর্ট ট্রান্সফর্মার মডেলগুলোর সৃষ্টি হয়েছে, যা আমরা BERT থেকে OpenGPT3 পর্যন্ত ব্যবহার করি।\n",
    "\n",
    "## ট্রান্সফর্মার মডেল\n",
    "\n",
    "প্রতিটি পূর্ববর্তী প্রেডিকশনের প্রসঙ্গকে পরবর্তী মূল্যায়ন ধাপে ফরোয়ার্ড করার পরিবর্তে, **ট্রান্সফর্মার মডেলগুলো** **পজিশনাল এনকোডিং** এবং **অ্যাটেনশন** ব্যবহার করে একটি প্রদত্ত টেক্সট উইন্ডোর মধ্যে একটি ইনপুটের প্রসঙ্গ ধারণ করে। নিচের চিত্রটি দেখায় কীভাবে পজিশনাল এনকোডিং এবং অ্যাটেনশন একটি প্রদত্ত উইন্ডোর মধ্যে প্রসঙ্গ ধারণ করতে পারে।\n",
    "\n",
    "![ট্রান্সফর্মার মডেলে মূল্যায়ন কীভাবে সম্পন্ন হয় তা দেখানো একটি অ্যানিমেটেড GIF।](../../../../../lessons/5-NLP/18-Transformers/images/transformer-animated-explanation.gif) \n",
    "\n",
    "যেহেতু প্রতিটি ইনপুট পজিশন স্বাধীনভাবে প্রতিটি আউটপুট পজিশনে ম্যাপ করা হয়, ট্রান্সফর্মারগুলো RNN-এর চেয়ে ভালোভাবে প্যারালালাইজ করতে পারে, যা অনেক বড় এবং আরও এক্সপ্রেসিভ ভাষার মডেল সক্ষম করে। প্রতিটি অ্যাটেনশন হেড বিভিন্ন শব্দের মধ্যে সম্পর্ক শিখতে ব্যবহার করা যেতে পারে, যা ডাউনস্ট্রিম NLP কাজগুলো উন্নত করে।\n",
    "\n",
    "## সহজ ট্রান্সফর্মার মডেল তৈরি\n",
    "\n",
    "Keras-এ বিল্ট-ইন ট্রান্সফর্মার লেয়ার নেই, তবে আমরা নিজেরাই এটি তৈরি করতে পারি। আগের মতো, আমরা AG News ডেটাসেটের টেক্সট ক্লাসিফিকেশনের উপর ফোকাস করব, তবে এটি উল্লেখযোগ্য যে ট্রান্সফর্মার মডেলগুলো আরও কঠিন NLP কাজগুলোতে সেরা ফলাফল দেখায়।\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "\n",
    "ds_train, ds_test = tfds.load('ag_news_subset').values()\n",
    "\n",
    "def extract_text(x):\n",
    "    return x['title']+' '+x['description']\n",
    "\n",
    "def tupelize(x):\n",
    "    return (extract_text(x),x['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "কেরাস-এ নতুন লেয়ারগুলি `Layer` ক্লাসকে সাবক্লাস করা উচিত এবং `call` মেথডটি ইমপ্লিমেন্ট করা উচিত। চলুন **Positional Embedding** লেয়ার দিয়ে শুরু করি। আমরা [কেরাসের অফিসিয়াল ডকুমেন্টেশন থেকে কিছু কোড](https://keras.io/examples/nlp/text_classification_with_transformer/) ব্যবহার করব। আমরা ধরে নেব যে আমরা সমস্ত ইনপুট সিকোয়েন্সকে `maxlen` দৈর্ঘ্যে প্যাড করি।\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(keras.layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = keras.layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "        self.maxlen = maxlen\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = self.maxlen\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x+positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "এই স্তরটি দুটি `Embedding` স্তর নিয়ে গঠিত: একটি টোকেন এমবেড করার জন্য (যেমনটি আমরা আগে আলোচনা করেছি) এবং আরেকটি টোকেন পজিশনের জন্য। টোকেন পজিশনগুলো `tf.range` ব্যবহার করে 0 থেকে `maxlen` পর্যন্ত প্রাকৃতিক সংখ্যার একটি ক্রম হিসেবে তৈরি করা হয় এবং তারপর এমবেডিং স্তরের মাধ্যমে পাঠানো হয়। দুটি প্রাপ্ত এমবেডিং ভেক্টর যোগ করা হয়, যার ফলে ইনপুটের একটি পজিশনালি এমবেড করা উপস্থাপনা তৈরি হয় যার আকার `maxlen`$\\times$`embed_dim`।\n",
    "\n",
    "এখন, চলুন ট্রান্সফরমার ব্লকটি বাস্তবায়ন করি। এটি পূর্বে সংজ্ঞায়িত এমবেডিং স্তরের আউটপুট গ্রহণ করবে:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim, name='attn')\n",
    "        self.ffn = keras.Sequential(\n",
    "            [keras.layers.Dense(ff_dim, activation=\"relu\"), keras.layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = keras.layers.Dropout(rate)\n",
    "        self.dropout2 = keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "এখন আমরা সম্পূর্ণ ট্রান্সফর্মার মডেল সংজ্ঞায়িত করতে প্রস্তুত:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "text_vectorization (TextVect (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "token_and_position_embedding (None, 256, 32)           648192    \n",
      "_________________________________________________________________\n",
      "transformer_block (Transform (None, 256, 32)           10656     \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 20)                660       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 84        \n",
      "=================================================================\n",
      "Total params: 659,592\n",
      "Trainable params: 659,592\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 32  # Embedding size for each token\n",
    "num_heads = 2  # Number of attention heads\n",
    "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
    "maxlen = 256\n",
    "vocab_size = 20000\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.experimental.preprocessing.TextVectorization(max_tokens=vocab_size,output_sequence_length=maxlen, input_shape=(1,)),\n",
    "    TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim),\n",
    "    TransformerBlock(embed_dim, num_heads, ff_dim),\n",
    "    keras.layers.GlobalAveragePooling1D(),\n",
    "    keras.layers.Dropout(0.1),\n",
    "    keras.layers.Dense(20, activation=\"relu\"),\n",
    "    keras.layers.Dropout(0.1),\n",
    "    keras.layers.Dense(4, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokenizer\n",
      "938/938 [==============================] - 45s 39ms/step - loss: 0.4978 - acc: 0.8068 - val_loss: 0.2808 - val_acc: 0.9124\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9c2427a0d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Training tokenizer')\n",
    "model.layers[0].adapt(ds_train.map(extract_text))\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'], optimizer='adam')\n",
    "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT ট্রান্সফর্মার মডেল\n",
    "\n",
    "**BERT** (Bidirectional Encoder Representations from Transformers) একটি অত্যন্ত বড় মাল্টি-লেয়ার ট্রান্সফর্মার নেটওয়ার্ক, যেখানে *BERT-base* এর জন্য ১২টি স্তর এবং *BERT-large* এর জন্য ২৪টি স্তর রয়েছে। এই মডেলটি প্রথমে বিশাল পরিমাণ টেক্সট ডেটা (উইকিপিডিয়া + বই) ব্যবহার করে অ-পর্যবেক্ষণমূলক প্রশিক্ষণের মাধ্যমে (একটি বাক্যে মাস্ক করা শব্দ অনুমান করা) প্রি-ট্রেইন করা হয়। প্রি-ট্রেইনিংয়ের সময় মডেলটি উল্লেখযোগ্য ভাষাগত বোঝাপড়া অর্জন করে, যা পরে অন্যান্য ডেটাসেটের সাথে ফাইন টিউনিংয়ের মাধ্যমে ব্যবহার করা যায়। এই প্রক্রিয়াকে **ট্রান্সফার লার্নিং** বলা হয়।\n",
    "\n",
    "![http://jalammar.github.io/illustrated-bert/ থেকে ছবি](../../../../../translated_images/bn/jalammarBERT-language-modeling-masked-lm.34f113ea5fec4362.webp)\n",
    "\n",
    "ট্রান্সফর্মার আর্কিটেকচারের অনেক বৈচিত্র্য রয়েছে, যেমন BERT, DistilBERT, BigBird, OpenGPT3 এবং আরও অনেক কিছু, যেগুলো ফাইন টিউন করা যেতে পারে।\n",
    "\n",
    "চলুন দেখি কীভাবে আমরা প্রি-ট্রেইন করা BERT মডেল ব্যবহার করে আমাদের প্রচলিত সিকোয়েন্স ক্লাসিফিকেশন সমস্যার সমাধান করতে পারি। আমরা [অফিশিয়াল ডকুমেন্টেশন](https://www.tensorflow.org/text/tutorials/classify_text_with_bert) থেকে ধারণা এবং কিছু কোড ধার করব।\n",
    "\n",
    "প্রি-ট্রেইন করা মডেল লোড করতে আমরা **Tensorflow hub** ব্যবহার করব। প্রথমে, চলুন BERT-স্পেসিফিক ভেক্টরাইজার লোড করি:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_41180/4216669875.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow_text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow_hub\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mhub\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhub\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mKerasLayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_text'"
     ]
    }
   ],
   "source": [
    "import tensorflow_text \n",
    "import tensorflow_hub as hub\n",
    "vectorizer = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_type_ids': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       " array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "       dtype=int32)>,\n",
       " 'input_word_ids': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       " array([[  101,  1045,  2293, 19081,   102,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]], dtype=int32)>,\n",
       " 'input_mask': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       " array([[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "       dtype=int32)>}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer(['I love transformers'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "এটি গুরুত্বপূর্ণ যে আপনি একই ভেক্টরাইজার ব্যবহার করেন যেটি মূল নেটওয়ার্ক প্রশিক্ষণের জন্য ব্যবহৃত হয়েছিল। এছাড়াও, BERT ভেক্টরাইজার তিনটি উপাদান প্রদান করে:\n",
    "* `input_word_ids`, যা ইনপুট বাক্যের জন্য টোকেন নম্বরগুলোর একটি ক্রম\n",
    "* `input_mask`, যা দেখায় ক্রমের কোন অংশটি প্রকৃত ইনপুট এবং কোন অংশটি প্যাডিং। এটি `Masking` লেয়ার দ্বারা উৎপন্ন মাস্কের মতো\n",
    "* `input_type_ids` ভাষা মডেলিং কাজের জন্য ব্যবহৃত হয় এবং একটি ক্রমে দুটি ইনপুট বাক্য নির্দিষ্ট করার অনুমতি দেয়।\n",
    "\n",
    "এরপর, আমরা BERT ফিচার এক্সট্রাক্টর ইনস্ট্যান্সিয়েট করতে পারি:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = hub.KerasLayer('https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pooled_output -> (1, 128)\n",
      "encoder_outputs -> 4\n",
      "sequence_output -> (1, 128, 128)\n",
      "default -> (1, 128)\n"
     ]
    }
   ],
   "source": [
    "z = bert(vectorizer(['I love transformers']))\n",
    "for i,x in z.items():\n",
    "    print(f\"{i} -> { len(x) if isinstance(x, list) else x.shape }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "তাহলে, BERT স্তর বেশ কিছু দরকারী ফলাফল প্রদান করে:\n",
    "* `pooled_output` হলো সিকোয়েন্সের সমস্ত টোকেনের গড় হিসাব। আপনি এটিকে পুরো নেটওয়ার্কের একটি বুদ্ধিমান অর্থবোধক এমবেডিং হিসেবে দেখতে পারেন। এটি আমাদের আগের মডেলের `GlobalAveragePooling1D` স্তরের আউটপুটের সমতুল্য।\n",
    "* `sequence_output` হলো শেষ ট্রান্সফর্মার স্তরের আউটপুট (উপরের মডেলে `TransformerBlock` এর আউটপুটের সাথে মিলে যায়)।\n",
    "* `encoder_outputs` হলো সমস্ত ট্রান্সফর্মার স্তরের আউটপুট। যেহেতু আমরা ৪-স্তরের BERT মডেল লোড করেছি (যা আপনি সম্ভবত নাম থেকে অনুমান করতে পারেন, যেখানে `4_H` রয়েছে), এটি ৪টি টেনসর প্রদান করে। শেষটি `sequence_output` এর সমান।\n",
    "\n",
    "এখন আমরা এন্ড-টু-এন্ড ক্লাসিফিকেশন মডেল সংজ্ঞায়িত করব। আমরা *ফাংশনাল মডেল সংজ্ঞা* ব্যবহার করব, যেখানে আমরা মডেলের ইনপুট সংজ্ঞায়িত করব এবং তারপর এর আউটপুট গণনা করার জন্য একাধিক এক্সপ্রেশন প্রদান করব। আমরা BERT মডেলের ওজনগুলোকে ট্রেনিং-অযোগ্য করব এবং শুধুমাত্র চূড়ান্ত ক্লাসিফায়ারটি ট্রেন করব:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer (KerasLayer)        {'input_type_ids': ( 0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer_1 (KerasLayer)      {'pooled_output': (N 4782465     keras_layer[0][0]                \n",
      "                                                                 keras_layer[0][1]                \n",
      "                                                                 keras_layer[0][2]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 128)          0           keras_layer_1[0][5]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 4)            516         dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 4,782,981\n",
      "Trainable params: 516\n",
      "Non-trainable params: 4,782,465\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inp = keras.Input(shape=(),dtype=tf.string)\n",
    "x = vectorizer(inp)\n",
    "x = bert(x)\n",
    "x = keras.layers.Dropout(0.1)(x['pooled_output'])\n",
    "out = keras.layers.Dense(4,activation='softmax')(x)\n",
    "model = keras.models.Model(inp,out)\n",
    "bert.trainable = False\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 528s 559ms/step - loss: 0.8056 - acc: 0.6983 - val_loss: 0.5953 - val_acc: 0.7888\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9bb1e36d00>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'], optimizer='adam')\n",
    "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "যদিও ট্রেন করার মতো প্যারামিটার খুব কম, প্রক্রিয়াটি বেশ ধীর, কারণ BERT ফিচার এক্সট্রাক্টর কম্পিউটেশনালভাবে ভারী। মনে হচ্ছে আমরা যুক্তিসঙ্গত সঠিকতা অর্জন করতে পারিনি, হয় প্রশিক্ষণের অভাবে, অথবা মডেলের প্যারামিটারগুলির অভাবে।\n",
    "\n",
    "চলুন BERT এর ওজন আনফ্রিজ করে এটিকে ট্রেন করার চেষ্টা করি। এর জন্য খুব ছোট লার্নিং রেট প্রয়োজন, এবং **warmup** সহ আরও সতর্ক প্রশিক্ষণ কৌশল প্রয়োজন, **AdamW** অপ্টিমাইজার ব্যবহার করে। আমরা `tf-models-official` প্যাকেজ ব্যবহার করে অপ্টিমাইজার তৈরি করব:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer (KerasLayer)        {'input_type_ids': ( 0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer_1 (KerasLayer)      {'pooled_output': (N 4782465     keras_layer[0][0]                \n",
      "                                                                 keras_layer[0][1]                \n",
      "                                                                 keras_layer[0][2]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 128)          0           keras_layer_1[0][5]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 4)            516         dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 4,782,981\n",
      "Trainable params: 4,782,980\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n",
      "938/938 [==============================] - 629s 664ms/step - loss: 0.6344 - acc: 0.7658 - val_loss: 0.4876 - val_acc: 0.8247\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9bb0bd0070>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from official.nlp import optimization \n",
    "bert.trainable=True\n",
    "model.summary()\n",
    "epochs = 3\n",
    "opt = optimization.create_optimizer(\n",
    "    init_lr=3e-5,\n",
    "    num_train_steps=epochs*len(ds_train),\n",
    "    num_warmup_steps=0.1*epochs*len(ds_train),\n",
    "    optimizer_type='adamw')\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'], optimizer=opt)\n",
    "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "যেমনটি দেখা যাচ্ছে, প্রশিক্ষণ প্রক্রিয়া বেশ ধীরগতিতে চলছে - তবে আপনি চাইলে কিছু ইপোক (৫-১০) নিয়ে পরীক্ষা করতে পারেন এবং আগের ব্যবহৃত পদ্ধতির সাথে তুলনা করে সেরা ফলাফল পেতে চেষ্টা করতে পারেন।\n",
    "\n",
    "## Huggingface Transformers লাইব্রেরি\n",
    "\n",
    "Transformer মডেল ব্যবহারের আরেকটি খুব সাধারণ (এবং কিছুটা সহজ) উপায় হলো [HuggingFace প্যাকেজ](https://github.com/huggingface/), যা বিভিন্ন NLP কাজের জন্য সহজ বিল্ডিং ব্লক সরবরাহ করে। এটি Tensorflow এবং PyTorch উভয়ের জন্যই উপলব্ধ, যা আরেকটি জনপ্রিয় নিউরাল নেটওয়ার্ক ফ্রেমওয়ার্ক।\n",
    "\n",
    "> **Note**: যদি আপনি Transformers লাইব্রেরি কীভাবে কাজ করে তা দেখতে আগ্রহী না হন - তাহলে আপনি এই নোটবুকের শেষে চলে যেতে পারেন, কারণ এখানে আমরা উপরে যা করেছি তার থেকে মৌলিকভাবে ভিন্ন কিছু দেখতে পাবেন না। আমরা BERT মডেল প্রশিক্ষণের একই ধাপগুলি পুনরায় ব্যবহার করব, তবে একটি ভিন্ন লাইব্রেরি এবং উল্লেখযোগ্যভাবে বড় মডেল নিয়ে। সুতরাং, প্রক্রিয়াটি কিছুটা দীর্ঘ প্রশিক্ষণ জড়িত, তাই আপনি শুধু কোডটি দেখে নিতে চাইতে পারেন।\n",
    "\n",
    "চলুন দেখি আমাদের সমস্যাটি কীভাবে [Huggingface Transformers](http://huggingface.co) ব্যবহার করে সমাধান করা যায়।\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "প্রথমে আমাদের যে মডেলটি ব্যবহার করব তা নির্বাচন করতে হবে। কিছু বিল্ট-ইন মডেলের পাশাপাশি, Huggingface-এ একটি [অনলাইন মডেল সংগ্রহশালা](https://huggingface.co/models) রয়েছে, যেখানে আপনি কমিউনিটির তৈরি আরও অনেক প্রি-ট্রেইনড মডেল খুঁজে পেতে পারেন। এই সমস্ত মডেল শুধুমাত্র মডেলের নাম প্রদান করেই লোড এবং ব্যবহার করা যায়। মডেলের জন্য প্রয়োজনীয় সমস্ত বাইনারি ফাইল স্বয়ংক্রিয়ভাবে ডাউনলোড হয়ে যাবে।\n",
    "\n",
    "কিছু সময়ে আপনাকে নিজের মডেল লোড করতে হতে পারে, সেই ক্ষেত্রে আপনি সেই ডিরেক্টরিটি নির্দিষ্ট করতে পারেন যেখানে সমস্ত প্রাসঙ্গিক ফাইল রয়েছে, যেমন টোকেনাইজারের জন্য প্যারামিটার, `config.json` ফাইলের মডেল প্যারামিটার, বাইনারি ওজন ইত্যাদি।\n",
    "\n",
    "মডেলের নাম থেকে আমরা মডেল এবং টোকেনাইজার উভয়ই ইনস্ট্যানশিয়েট করতে পারি। চলুন টোকেনাইজার দিয়ে শুরু করি:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "# To load the model from Internet repository using model name. \n",
    "# Use this if you are running from your own copy of the notebooks\n",
    "bert_model = 'bert-base-uncased' \n",
    "\n",
    "# To load the model from the directory on disk. Use this for Microsoft Learn module, because we have\n",
    "# prepared all required files for you.\n",
    "#bert_model = './bert'\n",
    "\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(bert_model)\n",
    "\n",
    "MAX_SEQ_LEN = 128\n",
    "PAD_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "UNK_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tokenizer` অবজেক্টে `encode` ফাংশন রয়েছে যা সরাসরি টেক্সট এনকোড করতে ব্যবহার করা যেতে পারে:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 23435, 12314, 2003, 1037, 2307, 7705, 2005, 17953, 2361, 102]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('Tensorflow is a great framework for NLP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "আমরা টোকেনাইজার ব্যবহার করে একটি সিকোয়েন্স এমনভাবে এনকোড করতে পারি যা মডেলে পাঠানোর জন্য উপযুক্ত, যেমন `token_ids`, `input_mask` ফিল্ড ইত্যাদি অন্তর্ভুক্ত করে। আমরা `return_tensors='tf'` আর্গুমেন্ট প্রদান করে টেন্সরফ্লো টেনসর চাইছি তা নির্দিষ্ট করতেও পারি:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[ 101, 7592, 1010, 2045,  102]], dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[0, 0, 0, 0, 0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[1, 1, 1, 1, 1]], dtype=int32)>}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(['Hello, there'],return_tensors='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "আমাদের ক্ষেত্রে, আমরা `bert-base-uncased` নামক প্রি-ট্রেইনড BERT মডেল ব্যবহার করব। *Uncased* নির্দেশ করে যে মডেলটি কেস-সংবেদনশীল নয়।\n",
    "\n",
    "মডেলটি ট্রেইন করার সময়, আমাদের টোকেনাইজড সিকোয়েন্স ইনপুট হিসেবে প্রদান করতে হবে, এবং এজন্য আমরা ডেটা প্রসেসিং পাইপলাইন ডিজাইন করব। যেহেতু `tokenizer.encode` একটি Python ফাংশন, আমরা আগের ইউনিটে ব্যবহৃত পদ্ধতির মতোই এটি কল করার জন্য `py_function` ব্যবহার করব:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(x):\n",
    "    return tokenizer.encode(x.numpy().decode('utf-8'),return_tensors='tf',padding='max_length',max_length=MAX_SEQ_LEN,truncation=True)[0]\n",
    "\n",
    "def process_fn(x):\n",
    "    s = x['title']+' '+x['description']\n",
    "    e = tf.py_function(process,inp=[s],Tout=(tf.int32))\n",
    "    e.set_shape(MAX_SEQ_LEN)\n",
    "    return e,x['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "এখন আমরা `BertForSequenceClassification` প্যাকেজ ব্যবহার করে প্রকৃত মডেলটি লোড করতে পারি। এটি নিশ্চিত করে যে আমাদের মডেলে ইতিমধ্যেই শ্রেণীবিভাজনের জন্য প্রয়োজনীয় আর্কিটেকচার রয়েছে, যার মধ্যে চূড়ান্ত শ্রেণীবিভাজক অন্তর্ভুক্ত। আপনি একটি সতর্কবার্তা বার্তা দেখতে পাবেন যা বলবে যে চূড়ান্ত শ্রেণীবিভাজকের ওজনগুলি প্রাথমিকভাবে নির্ধারিত নয়, এবং মডেলটি প্রাক-প্রশিক্ষণের প্রয়োজন হবে - এটি পুরোপুরি ঠিক আছে, কারণ আমরা ঠিক সেটাই করতে যাচ্ছি!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = transformers.TFBertForSequenceClassification.from_pretrained(bert_model,num_labels=4,output_attentions=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  109482240 \n",
      "_________________________________________________________________\n",
      "dropout_75 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  3076      \n",
      "=================================================================\n",
      "Total params: 109,485,316\n",
      "Trainable params: 109,485,316\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`summary()` থেকে আপনি দেখতে পাচ্ছেন, মডেলটিতে প্রায় ১১০ মিলিয়ন প্যারামিটার রয়েছে! অনুমান করা যায়, যদি আমরা একটি তুলনামূলকভাবে ছোট ডেটাসেটে সহজ শ্রেণীবিন্যাস কাজ করতে চাই, তবে আমরা BERT বেস লেয়ারটি প্রশিক্ষণ দিতে চাই না:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  109482240 \n",
      "_________________________________________________________________\n",
      "dropout_75 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  3076      \n",
      "=================================================================\n",
      "Total params: 109,485,316\n",
      "Trainable params: 3,076\n",
      "Non-trainable params: 109,482,240\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.layers[0].trainable = False\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "এখন আমরা প্রশিক্ষণ শুরু করতে প্রস্তুত!\n",
    "\n",
    "> **Note**: সম্পূর্ণ স্কেলের BERT মডেল প্রশিক্ষণ করতে অনেক সময় লাগতে পারে! তাই আমরা শুধুমাত্র প্রথম ৩২টি ব্যাচের জন্য এটি প্রশিক্ষণ করব। এটি কেবল দেখানোর জন্য যে মডেল প্রশিক্ষণ কীভাবে সেট আপ করা হয়। যদি আপনি সম্পূর্ণ স্কেলের প্রশিক্ষণ চেষ্টা করতে আগ্রহী হন - তাহলে `steps_per_epoch` এবং `validation_steps` প্যারামিটারগুলি সরিয়ে ফেলুন এবং অপেক্ষা করার জন্য প্রস্তুত হন!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 142s 4s/step - loss: 1.3896 - acc: 0.2500 - val_loss: 1.3863 - val_acc: 0.2480\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f1d40a4b6a0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile('adam','sparse_categorical_crossentropy',['acc'])\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "model.fit(ds_train.map(process_fn).batch(32),validation_data=ds_test.map(process_fn).batch(32),steps_per_epoch=32,validation_steps=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "যদি আপনি পুনরাবৃত্তির সংখ্যা বাড়ান এবং যথেষ্ট সময় অপেক্ষা করেন, এবং কয়েকটি epoch ধরে প্রশিক্ষণ চালান, তাহলে আশা করা যায় যে BERT শ্রেণীবিভাজন আমাদের সর্বোচ্চ নির্ভুলতা প্রদান করবে! এর কারণ হলো BERT ইতোমধ্যেই ভাষার গঠন বেশ ভালোভাবে বোঝে, এবং আমাদের কেবলমাত্র চূড়ান্ত শ্রেণীবিভাজকটি fine-tune করতে হয়। তবে, যেহেতু BERT একটি বড় মডেল, পুরো প্রশিক্ষণ প্রক্রিয়াটি অনেক সময় নেয় এবং উল্লেখযোগ্য কম্পিউটেশনাল ক্ষমতা প্রয়োজন হয়! (GPU, এবং সম্ভব হলে একাধিক GPU)।\n",
    "\n",
    "> **Note:** আমাদের উদাহরণে, আমরা সবচেয়ে ছোট প্রি-ট্রেইনড BERT মডেলগুলোর একটি ব্যবহার করেছি। বড় মডেলগুলো সম্ভবত আরও ভালো ফলাফল দিতে পারে।\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## মূল বিষয়বস্তু\n",
    "\n",
    "এই ইউনিটে, আমরা **ট্রান্সফর্মারস** ভিত্তিক অত্যন্ত সাম্প্রতিক মডেল আর্কিটেকচারগুলো দেখেছি। আমরা এগুলো আমাদের টেক্সট শ্রেণীবিন্যাস কাজের জন্য প্রয়োগ করেছি, তবে একইভাবে, BERT মডেলগুলোকে সত্তা নির্ণয়, প্রশ্নোত্তর এবং অন্যান্য NLP কাজের জন্যও ব্যবহার করা যেতে পারে।\n",
    "\n",
    "ট্রান্সফর্মার মডেলগুলো NLP-তে বর্তমানের সর্বাধুনিক অবস্থান উপস্থাপন করে, এবং বেশিরভাগ ক্ষেত্রেই এটি এমন একটি সমাধান হওয়া উচিত যার মাধ্যমে আপনি কাস্টম NLP সমাধান বাস্তবায়নের সময় পরীক্ষা-নিরীক্ষা শুরু করবেন। তবে, উন্নত নিউরাল মডেল তৈরি করতে চাইলে এই মডিউলে আলোচনা করা পুনরাবৃত্ত নিউরাল নেটওয়ার্কের মৌলিক নীতিগুলো বোঝা অত্যন্ত গুরুত্বপূর্ণ।\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**অস্বীকৃতি**:  \nএই নথিটি AI অনুবাদ পরিষেবা [Co-op Translator](https://github.com/Azure/co-op-translator) ব্যবহার করে অনুবাদ করা হয়েছে। আমরা যথাসম্ভব সঠিক অনুবাদ প্রদানের চেষ্টা করি, তবে অনুগ্রহ করে মনে রাখবেন যে স্বয়ংক্রিয় অনুবাদে ত্রুটি বা অসঙ্গতি থাকতে পারে। মূল ভাষায় থাকা নথিটিকে প্রামাণিক উৎস হিসেবে বিবেচনা করা উচিত। গুরুত্বপূর্ণ তথ্যের জন্য, পেশাদার মানব অনুবাদ সুপারিশ করা হয়। এই অনুবাদ ব্যবহারের ফলে কোনো ভুল বোঝাবুঝি বা ভুল ব্যাখ্যা হলে আমরা তার জন্য দায়ী থাকব না।\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0cb620c6d4b9f7a635928804c26cf22403d89d98d79684e4529119355ee6d5a5"
  },
  "kernelspec": {
   "display_name": "py38_tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "coopTranslator": {
   "original_hash": "ab59c532409774988ab875f2260e8e53",
   "translation_date": "2025-08-28T12:11:16+00:00",
   "source_file": "lessons/5-NLP/18-Transformers/TransformersTF.ipynb",
   "language_code": "bn"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}