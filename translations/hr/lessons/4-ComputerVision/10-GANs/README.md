# Generativne suparniÄke mreÅ¾e

U prethodnom dijelu nauÄili smo o **generativnim modelima**: modelima koji mogu generirati nove slike sliÄne onima iz skupa za treniranje. VAE je bio dobar primjer generativnog modela.

## [Kviz prije predavanja](https://ff-quizzes.netlify.app/en/ai/quiz/19)

MeÄ‘utim, ako pokuÅ¡amo generirati neÅ¡to zaista znaÄajno, poput slike visoke rezolucije, koristeÄ‡i VAE, vidjet Ä‡emo da treniranje ne konvergira dobro. Za ovaj sluÄaj trebamo nauÄiti o drugoj arhitekturi koja je posebno usmjerena na generativne modele - **Generativne suparniÄke mreÅ¾e**, ili GAN-ove.

Glavna ideja GAN-a je imati dvije neuronske mreÅ¾e koje se treniraju jedna protiv druge:

<img src="../../../../../translated_images/hr/gan_architecture.8f3a5ab62b8d5d69.webp" width="70%"/>

> Slika: [Dmitry Soshnikov](http://soshnikov.com)

> âœ… Mali rjeÄnik:
> * **Generator** je mreÅ¾a koja uzima neki sluÄajni vektor i kao rezultat proizvodi sliku.
> * **Diskriminator** je mreÅ¾a koja uzima sliku i treba odrediti je li to stvarna slika (iz skupa za treniranje) ili je generirana od strane generatora. U suÅ¡tini, to je klasifikator slika.

### Diskriminator

Arhitektura diskriminatora ne razlikuje se od obiÄne mreÅ¾e za klasifikaciju slika. U najjednostavnijem sluÄaju moÅ¾e biti potpuno povezan klasifikator, ali najvjerojatnije Ä‡e biti [konvolucijska mreÅ¾a](../07-ConvNets/README.md).

> âœ… GAN temeljen na konvolucijskim mreÅ¾ama naziva se [DCGAN](https://arxiv.org/pdf/1511.06434.pdf)

CNN diskriminator sastoji se od sljedeÄ‡ih slojeva: nekoliko konvolucija + pooling (s smanjenjem prostorne veliÄine) i jednog ili viÅ¡e potpuno povezanih slojeva za dobivanje "vektora znaÄajki", te zavrÅ¡nog binarnog klasifikatora.

> âœ… 'Pooling' u ovom kontekstu je tehnika koja smanjuje veliÄinu slike. "Pooling slojevi smanjuju dimenzije podataka kombiniranjem izlaza klastera neurona u jednom sloju u jedan neuron u sljedeÄ‡em sloju." - [izvor](https://wikipedia.org/wiki/Convolutional_neural_network#Pooling_layers)

### Generator

Generator je malo sloÅ¾eniji. MoÅ¾ete ga smatrati obrnutim diskriminatorom. PoÄevÅ¡i od latentnog vektora (umjesto vektora znaÄajki), ima potpuno povezani sloj za pretvaranje u potrebnu veliÄinu/oblik, nakon Äega slijede dekonvolucije + poveÄ‡anje rezolucije. Ovo je sliÄno *dekoderu* dijela [autoenkodera](../09-Autoencoders/README.md).

> âœ… BuduÄ‡i da je konvolucijski sloj implementiran kao linearni filter koji prolazi kroz sliku, dekonvolucija je u suÅ¡tini sliÄna konvoluciji i moÅ¾e se implementirati koristeÄ‡i istu logiku sloja.

<img src="../../../../../translated_images/hr/gan_arch_detail.46b95fd366f8e543.webp" width="70%"/>

> Slika: [Dmitry Soshnikov](http://soshnikov.com)

### Treniranje GAN-a

GAN-ovi se nazivaju **suparniÄkim** jer postoji stalno natjecanje izmeÄ‘u generatora i diskriminatora. Tijekom ovog natjecanja, i generator i diskriminator se poboljÅ¡avaju, Äime mreÅ¾a uÄi proizvoditi sve bolje slike.

Treniranje se odvija u dvije faze:

* **Treniranje diskriminatora**. Ovaj zadatak je priliÄno jednostavan: generiramo batch slika pomoÄ‡u generatora, oznaÄavajuÄ‡i ih s 0, Å¡to oznaÄava laÅ¾nu sliku, i uzimamo batch slika iz ulaznog skupa podataka (s oznakom 1, stvarna slika). Dobivamo neki *gubitak diskriminatora* i provodimo backprop.
* **Treniranje generatora**. Ovo je malo sloÅ¾enije jer ne znamo oÄekivani izlaz za generator izravno. Uzimamo cijelu GAN mreÅ¾u koja se sastoji od generatora praÄ‡enog diskriminatorom, hranimo je nekim sluÄajnim vektorima i oÄekujemo da rezultat bude 1 (Å¡to odgovara stvarnim slikama). Zatim zamrzavamo parametre diskriminatora (ne Å¾elimo da se trenira u ovom koraku) i provodimo backprop.

Tijekom ovog procesa, gubici generatora i diskriminatora ne opadaju znaÄajno. U idealnoj situaciji, trebali bi oscilirati, Å¡to odgovara poboljÅ¡anju performansi obje mreÅ¾e.

## âœï¸ VjeÅ¾be: GAN-ovi

* [GAN Notebook u TensorFlow/Keras](GANTF.ipynb)
* [GAN Notebook u PyTorch](GANPyTorch.ipynb)

### Problemi s treniranjem GAN-ova

GAN-ovi su poznati po tome Å¡to ih je posebno teÅ¡ko trenirati. Evo nekoliko problema:

* **Kolaps moda**. Ovim pojmom oznaÄavamo situaciju kada generator nauÄi proizvoditi jednu uspjeÅ¡nu sliku koja zavarava diskriminator, ali ne i raznolikost razliÄitih slika.
* **Osjetljivost na hiperparametre**. ÄŒesto moÅ¾ete vidjeti da GAN uopÄ‡e ne konvergira, a zatim iznenada smanjenje stope uÄenja dovodi do konvergencije.
* OdrÅ¾avanje **ravnoteÅ¾e** izmeÄ‘u generatora i diskriminatora. U mnogim sluÄajevima gubitak diskriminatora moÅ¾e brzo pasti na nulu, Å¡to rezultira time da generator ne moÅ¾e dalje trenirati. Da bismo to prevladali, moÅ¾emo pokuÅ¡ati postaviti razliÄite stope uÄenja za generator i diskriminator ili preskoÄiti treniranje diskriminatora ako je gubitak veÄ‡ prenizak.
* Treniranje za **visoku rezoluciju**. OdraÅ¾avajuÄ‡i isti problem kao kod autoenkodera, ovaj problem se javlja jer rekonstrukcija previÅ¡e slojeva konvolucijske mreÅ¾e dovodi do artefakata. Ovaj problem se obiÄno rjeÅ¡ava tzv. **progresivnim rastom**, gdje se prvo nekoliko slojeva trenira na slikama niske rezolucije, a zatim se slojevi "otkljuÄavaju" ili dodaju. Drugo rjeÅ¡enje bilo bi dodavanje dodatnih veza izmeÄ‘u slojeva i treniranje nekoliko rezolucija odjednom - pogledajte ovaj [Multi-Scale Gradient GANs rad](https://arxiv.org/abs/1903.06048) za detalje.

## Prijenos stila

GAN-ovi su izvrstan naÄin za generiranje umjetniÄkih slika. Druga zanimljiva tehnika je tzv. **prijenos stila**, koji uzima jednu **sliku sadrÅ¾aja** i ponovno je crta u drugom stilu, primjenjujuÄ‡i filtre iz **slike stila**.

Kako to funkcionira:
* PoÄinjemo s nasumiÄnom slikom Å¡uma (ili sa slikom sadrÅ¾aja, ali radi lakÅ¡eg razumijevanja bolje je poÄeti s nasumiÄnim Å¡umom).
* NaÅ¡ cilj je stvoriti takvu sliku koja bi bila bliska i slici sadrÅ¾aja i slici stila. To se odreÄ‘uje pomoÄ‡u dvije funkcije gubitka:
   - **Gubitak sadrÅ¾aja** se raÄuna na temelju znaÄajki koje CNN izvuÄe na nekim slojevima iz trenutne slike i slike sadrÅ¾aja.
   - **Gubitak stila** se raÄuna izmeÄ‘u trenutne slike i slike stila na pametan naÄin koristeÄ‡i Gram matrice (viÅ¡e detalja u [primjeru notebooka](StyleTransfer.ipynb)).
* Kako bismo sliku uÄinili glaÄ‘om i uklonili Å¡um, uvodimo i **Gubitak varijacije**, koji raÄuna prosjeÄnu udaljenost izmeÄ‘u susjednih piksela.
* Glavna petlja optimizacije prilagoÄ‘ava trenutnu sliku koristeÄ‡i gradijentni spust (ili neki drugi algoritam optimizacije) kako bi minimizirala ukupni gubitak, koji je ponderirani zbroj svih triju gubitaka.

## âœï¸ Primjer: [Prijenos stila](StyleTransfer.ipynb)

## [Kviz nakon predavanja](https://ff-quizzes.netlify.app/en/ai/quiz/20)

## ZakljuÄak

U ovoj lekciji nauÄili ste o GAN-ovima i kako ih trenirati. TakoÄ‘er ste nauÄili o posebnim izazovima s kojima se ovaj tip neuronske mreÅ¾e moÅ¾e suoÄiti i nekim strategijama kako ih prevladati.

## ğŸš€ Izazov

ProÄ‘ite kroz [notebook za prijenos stila](StyleTransfer.ipynb) koristeÄ‡i vlastite slike.

## Pregled i samostalno uÄenje

Za referencu, proÄitajte viÅ¡e o GAN-ovima u ovim resursima:

* Marco Pasini, [10 lekcija koje sam nauÄio trenirajuÄ‡i GAN-ove godinu dana](https://towardsdatascience.com/10-lessons-i-learned-training-generative-adversarial-networks-gans-for-a-year-c9071159628)
* [StyleGAN](https://en.wikipedia.org/wiki/StyleGAN), *de facto* GAN arhitektura koju vrijedi razmotriti
* [Stvaranje generativne umjetnosti koristeÄ‡i GAN-ove na Azure ML](https://soshnikov.com/scienceart/creating-generative-art-using-gan-on-azureml/)

## Zadatak

Ponovno pregledajte jedan od dva notebooka povezana s ovom lekcijom i ponovno trenirajte GAN na vlastitim slikama. Å to moÅ¾ete stvoriti?

---

