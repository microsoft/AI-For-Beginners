# Modeliranje jezika

SemantiÄke ugraÄ‘ene reprezentacije, poput Word2Vec i GloVe, zapravo su prvi korak prema **modeliranju jezika** - stvaranju modela koji na neki naÄin *razumiju* (ili *predstavljaju*) prirodu jezika.

## [Kviz prije predavanja](https://ff-quizzes.netlify.app/en/ai/quiz/29)

Glavna ideja modeliranja jezika je treniranje na nepodacima bez oznaka na nesuperviziran naÄin. Ovo je vaÅ¾no jer imamo ogromne koliÄine teksta bez oznaka, dok je koliÄina teksta s oznakama uvijek ograniÄena trudom koji moÅ¾emo uloÅ¾iti u oznaÄavanje. NajÄeÅ¡Ä‡e moÅ¾emo izgraditi modele jezika koji mogu **predvidjeti nedostajuÄ‡e rijeÄi** u tekstu, jer je lako sakriti nasumiÄnu rijeÄ u tekstu i koristiti je kao uzorak za treniranje.

## Treniranje ugraÄ‘enih reprezentacija

U naÅ¡im prethodnim primjerima koristili smo unaprijed trenirane semantiÄke ugraÄ‘ene reprezentacije, ali zanimljivo je vidjeti kako se te reprezentacije mogu trenirati. Postoji nekoliko moguÄ‡ih ideja koje se mogu koristiti:

* **N-Gram** modeliranje jezika, gdje predviÄ‘amo token gledajuÄ‡i N prethodnih tokena (N-gram)
* **Kontinuirana vreÄ‡a rijeÄi** (CBoW), gdje predviÄ‘amo srednji token $W_0$ u nizu tokena $W_{-N}$, ..., $W_N$.
* **Skip-gram**, gdje predviÄ‘amo skup susjednih tokena {$W_{-N},\dots, W_{-1}, W_1,\dots, W_N$} iz srednjeg tokena $W_0$.

![slika iz rada o pretvaranju rijeÄi u vektore](../../../../../translated_images/hr/example-algorithms-for-converting-words-to-vectors.fbe9207a726922f6.webp)

> Slika iz [ovog rada](https://arxiv.org/pdf/1301.3781.pdf)

## âœï¸ Primjeri biljeÅ¾nica: Treniranje CBoW modela

Nastavite uÄiti kroz sljedeÄ‡e biljeÅ¾nice:

* [Treniranje CBoW Word2Vec s TensorFlowom](CBoW-TF.ipynb)
* [Treniranje CBoW Word2Vec s PyTorchom](CBoW-PyTorch.ipynb)

## ZakljuÄak

U prethodnoj lekciji vidjeli smo da ugraÄ‘ene reprezentacije rijeÄi djeluju kao Äarolija! Sada znamo da treniranje ugraÄ‘enih reprezentacija rijeÄi nije vrlo sloÅ¾en zadatak, i trebali bismo biti u moguÄ‡nosti trenirati vlastite ugraÄ‘ene reprezentacije za tekst specifiÄan za odreÄ‘eno podruÄje ako je potrebno.

## [Kviz nakon predavanja](https://ff-quizzes.netlify.app/en/ai/quiz/30)

## Pregled i samostalno uÄenje

* [SluÅ¾beni PyTorch vodiÄ o modeliranju jezika](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html).
* [SluÅ¾beni TensorFlow vodiÄ o treniranju Word2Vec modela](https://www.TensorFlow.org/tutorials/text/word2vec).
* KoriÅ¡tenje okvira **gensim** za treniranje najÄeÅ¡Ä‡e koriÅ¡tenih ugraÄ‘enih reprezentacija u nekoliko linija koda opisano je [u ovoj dokumentaciji](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html).

## ğŸš€ [Zadatak: Trenirajte Skip-Gram model](lab/README.md)

U laboratorijskom zadatku izazivamo vas da izmijenite kod iz ove lekcije kako biste trenirali Skip-Gram model umjesto CBoW. [ProÄitajte detalje](lab/README.md)

---

