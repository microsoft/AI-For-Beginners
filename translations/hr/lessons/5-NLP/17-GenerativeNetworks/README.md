<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "d9de7847385eeeda67cfdcce1640ab72",
  "translation_date": "2025-08-25T21:46:19+00:00",
  "source_file": "lessons/5-NLP/17-GenerativeNetworks/README.md",
  "language_code": "hr"
}
-->
# Generativne mreÅ¾e

## [Kviz prije predavanja](https://ff-quizzes.netlify.app/en/ai/quiz/33)

Rekurentne neuronske mreÅ¾e (RNN) i njihove varijante s kontroliranim Ä‡elijama, poput Ä‡elija za dugoroÄno kratkoroÄno pamÄ‡enje (LSTM) i kontroliranih rekurentnih jedinica (GRU), omoguÄ‡ile su modeliranje jezika jer mogu nauÄiti redoslijed rijeÄi i pruÅ¾iti predviÄ‘anja za sljedeÄ‡u rijeÄ u nizu. To nam omoguÄ‡uje koriÅ¡tenje RNN-a za **generativne zadatke**, poput generiranja obiÄnog teksta, strojnog prevoÄ‘enja, pa Äak i opisivanja slika.

> âœ… Razmislite o svim situacijama u kojima ste imali koristi od generativnih zadataka, poput dovrÅ¡avanja teksta dok tipkate. IstraÅ¾ite svoje omiljene aplikacije kako biste vidjeli koriste li RNN-e.

U arhitekturi RNN-a koju smo raspravili u prethodnoj jedinici, svaka RNN jedinica proizvodila je sljedeÄ‡e skriveno stanje kao izlaz. MeÄ‘utim, moÅ¾emo dodati joÅ¡ jedan izlaz svakoj rekurentnoj jedinici, Å¡to bi nam omoguÄ‡ilo da generiramo **niz** (jednak duljini izvornog niza). Å toviÅ¡e, moÅ¾emo koristiti RNN jedinice koje ne prihvaÄ‡aju ulaz u svakom koraku, veÄ‡ samo uzimaju poÄetni vektor stanja i zatim proizvode niz izlaza.

To omoguÄ‡uje razliÄite neuronske arhitekture prikazane na slici ispod:

![Slika koja prikazuje uobiÄajene obrasce rekurentnih neuronskih mreÅ¾a.](../../../../../translated_images/unreasonable-effectiveness-of-rnn.541ead816778f42dce6c42d8a56c184729aa2378d059b851be4ce12b993033df.hr.jpg)

> Slika iz blog posta [Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) autora [Andrej Karpaty](http://karpathy.github.io/)

* **Jedan-na-jedan** je tradicionalna neuronska mreÅ¾a s jednim ulazom i jednim izlazom
* **Jedan-na-viÅ¡e** je generativna arhitektura koja prihvaÄ‡a jednu ulaznu vrijednost i generira niz izlaznih vrijednosti. Na primjer, ako Å¾elimo trenirati mreÅ¾u za **opisivanje slika** koja bi proizvela tekstualni opis slike, moÅ¾emo sliku koristiti kao ulaz, proslijediti je kroz CNN kako bismo dobili skriveno stanje, a zatim rekurentni lanac generira opis rijeÄ po rijeÄ
* **ViÅ¡e-na-jedan** odgovara arhitekturama RNN-a koje smo opisali u prethodnoj jedinici, poput klasifikacije teksta
* **ViÅ¡e-na-viÅ¡e**, ili **niz-na-niz**, odgovara zadacima poput **strojnog prevoÄ‘enja**, gdje prvo RNN prikuplja sve informacije iz ulaznog niza u skriveno stanje, a drugi RNN lanac razmotava ovo stanje u izlazni niz.

U ovoj jedinici fokusirat Ä‡emo se na jednostavne generativne modele koji nam pomaÅ¾u generirati tekst. Radi jednostavnosti, koristit Ä‡emo tokenizaciju na razini znakova.

Trenirat Ä‡emo ovaj RNN da generira tekst korak po korak. U svakom koraku uzet Ä‡emo niz znakova duljine `nchars` i traÅ¾iti od mreÅ¾e da generira sljedeÄ‡i izlazni znak za svaki ulazni znak:

![Slika koja prikazuje primjer generiranja rijeÄi 'HELLO' pomoÄ‡u RNN-a.](../../../../../translated_images/rnn-generate.56c54afb52f9781d63a7c16ea9c1b86cb70e6e1eae6a742b56b7b37468576b17.hr.png)

Tijekom generiranja teksta (tijekom inferencije), zapoÄinjemo s nekim **poticajem**, koji se prosljeÄ‘uje kroz RNN Ä‡elije kako bi se generiralo njegovo meÄ‘ustanje, a zatim iz tog stanja zapoÄinje generiranje. Generiramo jedan znak odjednom, prosljeÄ‘ujemo stanje i generirani znak sljedeÄ‡oj RNN Ä‡eliji kako bismo generirali sljedeÄ‡i znak, sve dok ne generiramo dovoljno znakova.

<img src="images/rnn-generate-inf.png" width="60%"/>

> Slika autora

## âœï¸ VjeÅ¾be: Generativne mreÅ¾e

Nastavite uÄiti u sljedeÄ‡im biljeÅ¾nicama:

* [Generativne mreÅ¾e s PyTorchom](../../../../../lessons/5-NLP/17-GenerativeNetworks/GenerativePyTorch.ipynb)
* [Generativne mreÅ¾e s TensorFlowom](../../../../../lessons/5-NLP/17-GenerativeNetworks/GenerativeTF.ipynb)

## Mekano generiranje teksta i temperatura

Izlaz svake RNN Ä‡elije je distribucija vjerojatnosti znakova. Ako uvijek uzmemo znak s najveÄ‡om vjerojatnoÅ¡Ä‡u kao sljedeÄ‡i znak u generiranom tekstu, tekst Äesto moÅ¾e postati "cikliran" izmeÄ‘u istih sekvenci znakova iznova i iznova, kao u ovom primjeru:

```
today of the second the company and a second the company ...
```

MeÄ‘utim, ako pogledamo distribuciju vjerojatnosti za sljedeÄ‡i znak, moÅ¾e se dogoditi da razlika izmeÄ‘u nekoliko najviÅ¡ih vjerojatnosti nije velika, npr. jedan znak moÅ¾e imati vjerojatnost 0.2, drugi - 0.19, itd. Na primjer, kada traÅ¾imo sljedeÄ‡i znak u nizu '*play*', sljedeÄ‡i znak moÅ¾e jednako dobro biti razmak ili **e** (kao u rijeÄi *player*).

To nas dovodi do zakljuÄka da nije uvijek "pravedno" odabrati znak s veÄ‡om vjerojatnoÅ¡Ä‡u, jer odabir drugog najviÅ¡eg i dalje moÅ¾e dovesti do smislenog teksta. Mudrije je **uzorkovati** znakove iz distribucije vjerojatnosti koju daje izlaz mreÅ¾e. TakoÄ‘er moÅ¾emo koristiti parametar, **temperaturu**, koji Ä‡e izravnati distribuciju vjerojatnosti ako Å¾elimo dodati viÅ¡e sluÄajnosti ili je uÄiniti strmijom ako Å¾elimo viÅ¡e slijediti znakove s najveÄ‡om vjerojatnoÅ¡Ä‡u.

IstraÅ¾ite kako je ovo mekano generiranje teksta implementirano u biljeÅ¾nicama povezanima gore.

## ZakljuÄak

Iako generiranje teksta moÅ¾e biti korisno samo po sebi, glavne prednosti dolaze iz moguÄ‡nosti generiranja teksta pomoÄ‡u RNN-a iz nekog poÄetnog vektora znaÄajki. Na primjer, generiranje teksta koristi se kao dio strojnog prevoÄ‘enja (niz-na-niz, u ovom sluÄaju vektor stanja iz *enkodera* koristi se za generiranje ili *dekodiranje* prevedene poruke) ili generiranja tekstualnog opisa slike (u kojem sluÄaju vektor znaÄajki dolazi iz CNN ekstraktora).

## ğŸš€ Izazov

ProuÄite neke lekcije na Microsoft Learn o ovoj temi

* Generiranje teksta s [PyTorchom](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-pytorch/6-generative-networks/?WT.mc_id=academic-77998-cacaste)/[TensorFlowom](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-tensorflow/5-generative-networks/?WT.mc_id=academic-77998-cacaste)

## [Kviz nakon predavanja](https://ff-quizzes.netlify.app/en/ai/quiz/34)

## Pregled i samostalno uÄenje

Evo nekoliko Älanaka za proÅ¡irenje vaÅ¡eg znanja:

* RazliÄiti pristupi generiranju teksta s Markovim lancem, LSTM-om i GPT-2: [blog post](https://towardsdatascience.com/text-generation-gpt-2-lstm-markov-chain-9ea371820e1e)
* Primjer generiranja teksta u [Keras dokumentaciji](https://keras.io/examples/generative/lstm_character_level_text_generation/)

## [Zadatak](lab/README.md)

Vidjeli smo kako generirati tekst znak po znak. U laboratoriju Ä‡ete istraÅ¾iti generiranje teksta na razini rijeÄi.

**Odricanje od odgovornosti**:  
Ovaj dokument je preveden pomoÄ‡u AI usluge za prevoÄ‘enje [Co-op Translator](https://github.com/Azure/co-op-translator). Iako nastojimo osigurati toÄnost, imajte na umu da automatski prijevodi mogu sadrÅ¾avati pogreÅ¡ke ili netoÄnosti. Izvorni dokument na izvornom jeziku treba smatrati mjerodavnim izvorom. Za kljuÄne informacije preporuÄuje se profesionalni prijevod od strane struÄnjaka. Ne preuzimamo odgovornost za bilo kakve nesporazume ili pogreÅ¡ne interpretacije proizaÅ¡le iz koriÅ¡tenja ovog prijevoda.