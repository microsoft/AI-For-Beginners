# Generativne mreÅ¾e

## [Kviz prije predavanja](https://ff-quizzes.netlify.app/en/ai/quiz/33)

Rekurentne neuronske mreÅ¾e (RNN) i njihove varijante s kontroliranim Ä‡elijama, poput Ä‡elija dugog kratkoroÄnog pamÄ‡enja (LSTM) i kontroliranih rekurentnih jedinica (GRU), omoguÄ‡ile su modeliranje jezika jer mogu nauÄiti redoslijed rijeÄi i predvidjeti sljedeÄ‡u rijeÄ u nizu. To nam omoguÄ‡uje koriÅ¡tenje RNN-a za **generativne zadatke**, poput generiranja obiÄnog teksta, strojnog prevoÄ‘enja, pa Äak i opisivanja slika.

> âœ… Razmislite o svim situacijama u kojima ste imali koristi od generativnih zadataka, poput dovrÅ¡avanja teksta dok tipkate. IstraÅ¾ite svoje omiljene aplikacije i provjerite koriste li RNN.

U arhitekturi RNN-a koju smo raspravili u prethodnoj jedinici, svaka RNN jedinica proizvodila je sljedeÄ‡e skriveno stanje kao izlaz. MeÄ‘utim, moÅ¾emo dodati joÅ¡ jedan izlaz svakoj rekurentnoj jedinici, Å¡to bi nam omoguÄ‡ilo da generiramo **niz** (jednake duljine kao originalni niz). Osim toga, moÅ¾emo koristiti RNN jedinice koje ne primaju ulaz na svakom koraku, veÄ‡ samo uzimaju poÄetni vektor stanja i zatim proizvode niz izlaza.

To omoguÄ‡uje razliÄite neuronske arhitekture prikazane na slici ispod:

![Slika koja prikazuje uobiÄajene uzorke rekurentnih neuronskih mreÅ¾a.](../../../../../translated_images/hr/unreasonable-effectiveness-of-rnn.541ead816778f42d.webp)

> Slika iz blog posta [Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) autora [Andrej Karpaty](http://karpathy.github.io/)

* **Jedan-na-jedan** je tradicionalna neuronska mreÅ¾a s jednim ulazom i jednim izlazom
* **Jedan-na-viÅ¡e** je generativna arhitektura koja prima jednu ulaznu vrijednost i generira niz izlaznih vrijednosti. Na primjer, ako Å¾elimo trenirati mreÅ¾u za **opisivanje slika** koja bi proizvela tekstualni opis slike, moÅ¾emo sliku koristiti kao ulaz, proslijediti je kroz CNN kako bismo dobili skriveno stanje, a zatim koristiti rekurentni lanac za generiranje opisa rijeÄ po rijeÄ
* **ViÅ¡e-na-jedan** odgovara RNN arhitekturama koje smo opisali u prethodnoj jedinici, poput klasifikacije teksta
* **ViÅ¡e-na-viÅ¡e**, ili **niz-na-niz**, odgovara zadacima poput **strojnog prevoÄ‘enja**, gdje prvo RNN prikuplja sve informacije iz ulaznog niza u skriveno stanje, a zatim drugi RNN lanac razvija ovo stanje u izlazni niz.

U ovoj jedinici fokusirat Ä‡emo se na jednostavne generativne modele koji nam pomaÅ¾u generirati tekst. Radi jednostavnosti, koristit Ä‡emo tokenizaciju na razini znakova.

Trenirat Ä‡emo ovaj RNN da generira tekst korak po korak. Na svakom koraku uzet Ä‡emo niz znakova duljine `nchars` i traÅ¾iti od mreÅ¾e da generira sljedeÄ‡i izlazni znak za svaki ulazni znak:

![Slika koja prikazuje primjer generiranja rijeÄi 'HELLO' pomoÄ‡u RNN-a.](../../../../../translated_images/hr/rnn-generate.56c54afb52f9781d.webp)

Tijekom generiranja teksta (tijekom inferencije), poÄinjemo s nekim **poticajem**, koji se prosljeÄ‘uje kroz RNN Ä‡elije kako bi se generiralo njegovo meÄ‘ustanje, a zatim iz tog stanja poÄinje generiranje. Generiramo jedan znak po jedan, prosljeÄ‘ujemo stanje i generirani znak sljedeÄ‡oj RNN Ä‡eliji kako bismo generirali sljedeÄ‡i znak, sve dok ne generiramo dovoljno znakova.

<img src="../../../../../translated_images/hr/rnn-generate-inf.5168dc65e0370eea.webp" width="60%"/>

> Slika autora

## âœï¸ VjeÅ¾be: Generativne mreÅ¾e

Nastavite uÄiti u sljedeÄ‡im biljeÅ¾nicama:

* [Generativne mreÅ¾e s PyTorchom](GenerativePyTorch.ipynb)
* [Generativne mreÅ¾e s TensorFlowom](GenerativeTF.ipynb)

## Mekano generiranje teksta i temperatura

Izlaz svake RNN Ä‡elije je distribucija vjerojatnosti znakova. Ako uvijek uzmemo znak s najveÄ‡om vjerojatnoÅ¡Ä‡u kao sljedeÄ‡i znak u generiranom tekstu, tekst Äesto moÅ¾e postati "cikliÄan", ponavljajuÄ‡i iste sekvence znakova iznova i iznova, kao u ovom primjeru:

```
today of the second the company and a second the company ...
```

MeÄ‘utim, ako pogledamo distribuciju vjerojatnosti za sljedeÄ‡i znak, moÅ¾e se dogoditi da razlika izmeÄ‘u nekoliko najveÄ‡ih vjerojatnosti nije velika, npr. jedan znak moÅ¾e imati vjerojatnost 0.2, a drugi 0.19, itd. Na primjer, kada traÅ¾imo sljedeÄ‡i znak u nizu '*play*', sljedeÄ‡i znak moÅ¾e jednako dobro biti razmak ili **e** (kao u rijeÄi *player*).

To nas dovodi do zakljuÄka da nije uvijek "pravedno" odabrati znak s najveÄ‡om vjerojatnoÅ¡Ä‡u, jer odabir drugog po redu takoÄ‘er moÅ¾e dovesti do smislenog teksta. Mudrije je **uzorkovati** znakove iz distribucije vjerojatnosti koju daje izlaz mreÅ¾e. TakoÄ‘er moÅ¾emo koristiti parametar **temperatura**, koji Ä‡e izravnati distribuciju vjerojatnosti ako Å¾elimo dodati viÅ¡e sluÄajnosti, ili je uÄiniti strmijom ako Å¾elimo viÅ¡e se drÅ¾ati znakova s najveÄ‡om vjerojatnoÅ¡Ä‡u.

IstraÅ¾ite kako je ovo mekano generiranje teksta implementirano u biljeÅ¾nicama povezanima gore.

## ZakljuÄak

Iako generiranje teksta moÅ¾e biti korisno samo po sebi, glavne prednosti dolaze iz moguÄ‡nosti generiranja teksta pomoÄ‡u RNN-a iz nekog poÄetnog vektora znaÄajki. Na primjer, generiranje teksta koristi se kao dio strojnog prevoÄ‘enja (niz-na-niz, u ovom sluÄaju vektor stanja iz *enkodera* koristi se za generiranje ili *dekodiranje* prevedene poruke), ili generiranja tekstualnog opisa slike (u kojem sluÄaju vektor znaÄajki dolazi iz CNN ekstraktora).

## ğŸš€ Izazov

ProuÄite neke lekcije na Microsoft Learn na ovu temu

* Generiranje teksta s [PyTorchom](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-pytorch/6-generative-networks/?WT.mc_id=academic-77998-cacaste)/[TensorFlowom](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-tensorflow/5-generative-networks/?WT.mc_id=academic-77998-cacaste)

## [Kviz nakon predavanja](https://ff-quizzes.netlify.app/en/ai/quiz/34)

## Pregled i samostalno uÄenje

Evo nekoliko Älanaka za proÅ¡irenje vaÅ¡eg znanja

* RazliÄiti pristupi generiranju teksta s Markovim lancem, LSTM-om i GPT-2: [blog post](https://towardsdatascience.com/text-generation-gpt-2-lstm-markov-chain-9ea371820e1e)
* Primjer generiranja teksta u [Keras dokumentaciji](https://keras.io/examples/generative/lstm_character_level_text_generation/)

## [Zadatak](lab/README.md)

Vidjeli smo kako generirati tekst znak po znak. U laboratoriju Ä‡ete istraÅ¾iti generiranje teksta na razini rijeÄi.

---

