{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mehanizmi pažnje i transformeri\n",
    "\n",
    "Jedan od glavnih nedostataka rekurentnih mreža je taj što sve riječi u nizu imaju isti utjecaj na rezultat. To uzrokuje suboptimalne performanse kod standardnih LSTM encoder-decoder modela za zadatke prevođenja sekvenci, poput prepoznavanja imenovanih entiteta i strojnog prevođenja. U stvarnosti, određene riječi u ulaznom nizu često imaju veći utjecaj na izlazne sekvence od drugih.\n",
    "\n",
    "Razmotrimo model prevođenja sekvenci, poput strojnog prevođenja. On se implementira pomoću dvije rekurentne mreže, gdje jedna mreža (**encoder**) sažima ulazni niz u skriveno stanje, a druga mreža (**decoder**) razvija to skriveno stanje u prevedeni rezultat. Problem s ovim pristupom je što završno stanje mreže teško pamti početak rečenice, što uzrokuje lošu kvalitetu modela kod dugih rečenica.\n",
    "\n",
    "**Mehanizmi pažnje** omogućuju ponderiranje kontekstualnog utjecaja svakog ulaznog vektora na svaku izlaznu predikciju RNN-a. To se implementira stvaranjem prečaca između međustanja ulaznog RNN-a i izlaznog RNN-a. Na taj način, prilikom generiranja izlaznog simbola $y_t$, uzimamo u obzir sva ulazna skrivena stanja $h_i$, s različitim težinskim koeficijentima $\\alpha_{t,i}$.\n",
    "\n",
    "![Slika koja prikazuje encoder/decoder model s aditivnim slojem pažnje](../../../../../translated_images/hr/encoder-decoder-attention.7a726296894fb567.webp)\n",
    "*Encoder-decoder model s aditivnim mehanizmom pažnje u [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf), citirano iz [ovog blog posta](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)*\n",
    "\n",
    "Matrica pažnje $\\{\\alpha_{i,j}\\}$ predstavlja stupanj u kojem određene ulazne riječi sudjeluju u generiranju određene riječi u izlaznom nizu. Ispod je primjer takve matrice:\n",
    "\n",
    "![Slika koja prikazuje uzorak poravnanja pronađenog pomoću RNNsearch-50, preuzeto iz Bahdanau - arviz.org](../../../../../translated_images/hr/bahdanau-fig3.09ba2d37f202a6af.webp)\n",
    "\n",
    "*Slika preuzeta iz [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) (Slika 3)*\n",
    "\n",
    "Mehanizmi pažnje odgovorni su za velik dio trenutnog ili gotovo trenutnog stanja umjetnosti u obradi prirodnog jezika. Međutim, dodavanje pažnje značajno povećava broj parametara modela, što je dovelo do problema skaliranja s RNN-ovima. Ključno ograničenje skaliranja RNN-ova je to što rekurentna priroda modela otežava grupiranje i paralelizaciju treninga. U RNN-u svaki element sekvence mora se obraditi redoslijedom, što znači da se ne može lako paralelizirati.\n",
    "\n",
    "Usvajanje mehanizama pažnje u kombinaciji s ovim ograničenjem dovelo je do stvaranja sadašnjih transformera, koji predstavljaju stanje umjetnosti, a koje danas poznajemo i koristimo, od BERT-a do OpenGPT3.\n",
    "\n",
    "## Transformer modeli\n",
    "\n",
    "Umjesto prosljeđivanja konteksta svake prethodne predikcije u sljedeći korak evaluacije, **transformer modeli** koriste **pozicijske kodove** i **pažnju** kako bi uhvatili kontekst danog ulaza unutar zadanog prozora teksta. Slika ispod prikazuje kako pozicijski kodovi s pažnjom mogu uhvatiti kontekst unutar zadanog prozora.\n",
    "\n",
    "![Animirani GIF koji prikazuje kako se evaluacije provode u transformer modelima.](../../../../../lessons/5-NLP/18-Transformers/images/transformer-animated-explanation.gif)\n",
    "\n",
    "Budući da se svaka ulazna pozicija neovisno mapira na svaku izlaznu poziciju, transformeri mogu bolje paralelizirati od RNN-ova, što omogućuje mnogo veće i izražajnije jezične modele. Svaka glava pažnje može se koristiti za učenje različitih odnosa između riječi, što poboljšava zadatke obrade prirodnog jezika.\n",
    "\n",
    "## Izgradnja jednostavnog transformer modela\n",
    "\n",
    "Keras ne sadrži ugrađeni sloj za transformere, ali možemo izgraditi vlastiti. Kao i prije, fokusirat ćemo se na klasifikaciju teksta AG News skupa podataka, no vrijedi napomenuti da transformer modeli pokazuju najbolje rezultate na složenijim zadacima obrade prirodnog jezika.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "\n",
    "ds_train, ds_test = tfds.load('ag_news_subset').values()\n",
    "\n",
    "def extract_text(x):\n",
    "    return x['title']+' '+x['description']\n",
    "\n",
    "def tupelize(x):\n",
    "    return (extract_text(x),x['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Novi slojevi u Kerasi trebaju naslijediti klasu `Layer` i implementirati metodu `call`. Počnimo s **Positional Embedding** slojem. Koristit ćemo [neki kod iz službene Keras dokumentacije](https://keras.io/examples/nlp/text_classification_with_transformer/). Pretpostavit ćemo da sve ulazne sekvence popunjavamo do duljine `maxlen`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(keras.layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = keras.layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "        self.maxlen = maxlen\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = self.maxlen\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x+positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ovaj sloj sastoji se od dva sloja `Embedding`: za ugradnju tokena (na način koji smo ranije raspravili) i pozicija tokena. Pozicije tokena stvaraju se kao niz prirodnih brojeva od 0 do `maxlen` koristeći `tf.range`, a zatim se prosljeđuju kroz sloj za ugradnju. Dva dobivena vektora ugradnje se zatim zbrajaju, stvarajući pozicijski ugrađenu reprezentaciju ulaza oblika `maxlen`$\\times$`embed_dim`.\n",
    "\n",
    "Sada, implementirajmo transformer blok. On će uzeti izlaz prethodno definiranog sloja za ugradnju:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim, name='attn')\n",
    "        self.ffn = keras.Sequential(\n",
    "            [keras.layers.Dense(ff_dim, activation=\"relu\"), keras.layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = keras.layers.Dropout(rate)\n",
    "        self.dropout2 = keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer primjenjuje `MultiHeadAttention` na ulaz s pozicijskim kodiranjem kako bi proizveo vektor pažnje dimenzije `maxlen`$\\times$`embed_dim`, koji se zatim miješa s ulazom i normalizira pomoću `LayerNormalization`.\n",
    "\n",
    "> **Napomena**: `LayerNormalization` je sličan `BatchNormalization` koji je obrađen u dijelu *Računalni vid* ovog programa učenja, ali normalizira izlaze prethodnog sloja za svaki uzorak u treningu neovisno, kako bi ih doveo u raspon [-1..1].\n",
    "\n",
    "Izlaz ovog sloja zatim se prosljeđuje kroz `Dense` mrežu (u našem slučaju - perceptron s dva sloja), a rezultat se dodaje konačnom izlazu (koji se ponovno podvrgava normalizaciji).\n",
    "\n",
    "Sada smo spremni definirati kompletan model transformatora:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "text_vectorization (TextVect (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "token_and_position_embedding (None, 256, 32)           648192    \n",
      "_________________________________________________________________\n",
      "transformer_block (Transform (None, 256, 32)           10656     \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 20)                660       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 84        \n",
      "=================================================================\n",
      "Total params: 659,592\n",
      "Trainable params: 659,592\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 32  # Embedding size for each token\n",
    "num_heads = 2  # Number of attention heads\n",
    "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
    "maxlen = 256\n",
    "vocab_size = 20000\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.experimental.preprocessing.TextVectorization(max_tokens=vocab_size,output_sequence_length=maxlen, input_shape=(1,)),\n",
    "    TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim),\n",
    "    TransformerBlock(embed_dim, num_heads, ff_dim),\n",
    "    keras.layers.GlobalAveragePooling1D(),\n",
    "    keras.layers.Dropout(0.1),\n",
    "    keras.layers.Dense(20, activation=\"relu\"),\n",
    "    keras.layers.Dropout(0.1),\n",
    "    keras.layers.Dense(4, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokenizer\n",
      "938/938 [==============================] - 45s 39ms/step - loss: 0.4978 - acc: 0.8068 - val_loss: 0.2808 - val_acc: 0.9124\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9c2427a0d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Training tokenizer')\n",
    "model.layers[0].adapt(ds_train.map(extract_text))\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'], optimizer='adam')\n",
    "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT Transformer modeli\n",
    "\n",
    "**BERT** (Bidirectional Encoder Representations from Transformers) je vrlo velika višeslojna transformacijska mreža s 12 slojeva za *BERT-base* i 24 za *BERT-large*. Model se prvo unaprijed trenira na velikom korpusu tekstualnih podataka (WikiPedia + knjige) koristeći nenadzirano učenje (predviđanje maskiranih riječi u rečenici). Tijekom unaprijednog treniranja model usvaja značajnu razinu razumijevanja jezika, što se kasnije može iskoristiti s drugim skupovima podataka putem finog podešavanja. Ovaj proces naziva se **transferno učenje**.\n",
    "\n",
    "![slika s http://jalammar.github.io/illustrated-bert/](../../../../../translated_images/hr/jalammarBERT-language-modeling-masked-lm.34f113ea5fec4362.webp)\n",
    "\n",
    "Postoji mnogo varijacija transformacijskih arhitektura, uključujući BERT, DistilBERT, BigBird, OpenGPT3 i druge, koje se mogu fino podešavati.\n",
    "\n",
    "Pogledajmo kako možemo koristiti unaprijed trenirani BERT model za rješavanje našeg tradicionalnog problema klasifikacije sekvenci. Posudit ćemo ideju i dio koda iz [službene dokumentacije](https://www.tensorflow.org/text/tutorials/classify_text_with_bert).\n",
    "\n",
    "Za učitavanje unaprijed treniranih modela koristit ćemo **Tensorflow hub**. Prvo, učitajmo BERT-specifični vektorizator:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_41180/4216669875.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow_text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow_hub\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mhub\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhub\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mKerasLayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_text'"
     ]
    }
   ],
   "source": [
    "import tensorflow_text \n",
    "import tensorflow_hub as hub\n",
    "vectorizer = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_type_ids': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       " array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "       dtype=int32)>,\n",
       " 'input_word_ids': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       " array([[  101,  1045,  2293, 19081,   102,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]], dtype=int32)>,\n",
       " 'input_mask': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       " array([[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "       dtype=int32)>}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer(['I love transformers'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Važno je koristiti isti vektorizator koji je korišten za treniranje originalne mreže. Također, BERT vektorizator vraća tri komponente:\n",
    "* `input_word_ids`, što je niz brojeva tokena za ulaznu rečenicu\n",
    "* `input_mask`, koji pokazuje koji dio niza sadrži stvarni unos, a koji je popuna. Slično je maski koju proizvodi sloj `Masking`\n",
    "* `input_type_ids` koristi se za zadatke modeliranja jezika i omogućuje specificiranje dvije ulazne rečenice u jednom nizu.\n",
    "\n",
    "Zatim možemo instancirati BERT ekstraktor značajki:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = hub.KerasLayer('https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pooled_output -> (1, 128)\n",
      "encoder_outputs -> 4\n",
      "sequence_output -> (1, 128, 128)\n",
      "default -> (1, 128)\n"
     ]
    }
   ],
   "source": [
    "z = bert(vectorizer(['I love transformers']))\n",
    "for i,x in z.items():\n",
    "    print(f\"{i} -> { len(x) if isinstance(x, list) else x.shape }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dakle, BERT sloj vraća nekoliko korisnih rezultata:\n",
    "* `pooled_output` je rezultat prosjeka svih tokena u sekvenci. Možete ga smatrati inteligentnim semantičkim ugrađivanjem cijele mreže. Ekvivalentan je izlazu sloja `GlobalAveragePooling1D` u našem prethodnom modelu.\n",
    "* `sequence_output` je izlaz posljednjeg transformacijskog sloja (odgovara izlazu `TransformerBlock` u našem modelu iznad).\n",
    "* `encoder_outputs` su izlazi svih transformacijskih slojeva. Budući da smo učitali BERT model s 4 sloja (kao što vjerojatno možete zaključiti iz imena koje sadrži `4_H`), ima 4 tenzora. Posljednji je isti kao `sequence_output`.\n",
    "\n",
    "Sada ćemo definirati end-to-end model za klasifikaciju. Koristit ćemo *funkcionalnu definiciju modela*, gdje definiramo ulaz modela, a zatim pružamo niz izraza za izračun njegovog izlaza. Također ćemo postaviti težine BERT modela kao ne-trenirajuće i trenirati samo završni klasifikator:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer (KerasLayer)        {'input_type_ids': ( 0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer_1 (KerasLayer)      {'pooled_output': (N 4782465     keras_layer[0][0]                \n",
      "                                                                 keras_layer[0][1]                \n",
      "                                                                 keras_layer[0][2]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 128)          0           keras_layer_1[0][5]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 4)            516         dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 4,782,981\n",
      "Trainable params: 516\n",
      "Non-trainable params: 4,782,465\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inp = keras.Input(shape=(),dtype=tf.string)\n",
    "x = vectorizer(inp)\n",
    "x = bert(x)\n",
    "x = keras.layers.Dropout(0.1)(x['pooled_output'])\n",
    "out = keras.layers.Dense(4,activation='softmax')(x)\n",
    "model = keras.models.Model(inp,out)\n",
    "bert.trainable = False\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 528s 559ms/step - loss: 0.8056 - acc: 0.6983 - val_loss: 0.5953 - val_acc: 0.7888\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9bb1e36d00>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'], optimizer='adam')\n",
    "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unatoč tome što postoji malo parametara za treniranje, proces je prilično spor jer je BERT-ov ekstraktor značajki računalno zahtjevan. Čini se da nismo uspjeli postići zadovoljavajuću točnost, bilo zbog nedostatka treninga ili nedostatka parametara modela.\n",
    "\n",
    "Pokušajmo otključati težine BERT-a i trenirati ga također. Ovo zahtijeva vrlo malu stopu učenja, kao i pažljiviju strategiju treniranja s **zagrijavanjem** koristeći **AdamW** optimizator. Koristit ćemo paket `tf-models-official` za kreiranje optimizatora:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer (KerasLayer)        {'input_type_ids': ( 0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer_1 (KerasLayer)      {'pooled_output': (N 4782465     keras_layer[0][0]                \n",
      "                                                                 keras_layer[0][1]                \n",
      "                                                                 keras_layer[0][2]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 128)          0           keras_layer_1[0][5]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 4)            516         dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 4,782,981\n",
      "Trainable params: 4,782,980\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n",
      "938/938 [==============================] - 629s 664ms/step - loss: 0.6344 - acc: 0.7658 - val_loss: 0.4876 - val_acc: 0.8247\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9bb0bd0070>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from official.nlp import optimization \n",
    "bert.trainable=True\n",
    "model.summary()\n",
    "epochs = 3\n",
    "opt = optimization.create_optimizer(\n",
    "    init_lr=3e-5,\n",
    "    num_train_steps=epochs*len(ds_train),\n",
    "    num_warmup_steps=0.1*epochs*len(ds_train),\n",
    "    optimizer_type='adamw')\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'], optimizer=opt)\n",
    "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kao što možete vidjeti, treniranje ide prilično sporo - ali možda biste željeli eksperimentirati i trenirati model kroz nekoliko epoha (5-10) kako biste vidjeli možete li postići najbolji rezultat u usporedbi s pristupima koje smo koristili ranije.\n",
    "\n",
    "## Huggingface Transformers knjižnica\n",
    "\n",
    "Još jedan vrlo čest (i malo jednostavniji) način korištenja Transformer modela je [HuggingFace paket](https://github.com/huggingface/), koji pruža jednostavne građevne blokove za različite NLP zadatke. Dostupan je i za Tensorflow i za PyTorch, još jedan vrlo popularan okvir za neuronske mreže.\n",
    "\n",
    "> **Napomena**: Ako vas ne zanima kako radi Transformers knjižnica - možete preskočiti na kraj ovog bilježnika, jer nećete vidjeti ništa bitno drugačije od onoga što smo već radili. Ponavljat ćemo iste korake treniranja BERT modela koristeći drugu knjižnicu i znatno veći model. Stoga proces uključuje prilično dugo treniranje, pa možda želite samo pregledati kod.\n",
    "\n",
    "Pogledajmo kako se naš problem može riješiti koristeći [Huggingface Transformers](http://huggingface.co).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prvo što trebamo učiniti je odabrati model koji ćemo koristiti. Osim nekih ugrađenih modela, Huggingface sadrži [online repozitorij modela](https://huggingface.co/models), gdje možete pronaći mnogo više unaprijed treniranih modela koje je kreirala zajednica. Svi ti modeli mogu se učitati i koristiti jednostavnim navođenjem imena modela. Svi potrebni binarni datoteke za model automatski će se preuzeti.\n",
    "\n",
    "Ponekad ćete trebati učitati vlastite modele, u tom slučaju možete navesti direktorij koji sadrži sve relevantne datoteke, uključujući parametre za tokenizer, datoteku `config.json` s parametrima modela, binarne težine itd.\n",
    "\n",
    "Na temelju imena modela, možemo instancirati i model i tokenizer. Započnimo s tokenizerom:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "# To load the model from Internet repository using model name. \n",
    "# Use this if you are running from your own copy of the notebooks\n",
    "bert_model = 'bert-base-uncased' \n",
    "\n",
    "# To load the model from the directory on disk. Use this for Microsoft Learn module, because we have\n",
    "# prepared all required files for you.\n",
    "#bert_model = './bert'\n",
    "\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(bert_model)\n",
    "\n",
    "MAX_SEQ_LEN = 128\n",
    "PAD_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "UNK_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objekt `tokenizer` sadrži funkciju `encode` koja se može izravno koristiti za kodiranje teksta:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 23435, 12314, 2003, 1037, 2307, 7705, 2005, 17953, 2361, 102]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('Tensorflow is a great framework for NLP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Također možemo koristiti tokenizer za kodiranje sekvence na način prikladan za prosljeđivanje modelu, tj. uključujući polja `token_ids`, `input_mask`, itd. Također možemo specificirati da želimo Tensorflow tenzore pružanjem argumenta `return_tensors='tf'`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[ 101, 7592, 1010, 2045,  102]], dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[0, 0, 0, 0, 0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[1, 1, 1, 1, 1]], dtype=int32)>}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(['Hello, there'],return_tensors='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "U ovom slučaju, koristit ćemo unaprijed istrenirani BERT model nazvan `bert-base-uncased`. *Uncased* označava da model nije osjetljiv na velika i mala slova.\n",
    "\n",
    "Prilikom treniranja modela, potrebno je osigurati tokenizirani niz kao ulaz, i stoga ćemo osmisliti procesnu liniju za obradu podataka. Budući da je `tokenizer.encode` Python funkcija, koristit ćemo isti pristup kao u posljednjoj jedinici pozivajući je pomoću `py_function`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(x):\n",
    "    return tokenizer.encode(x.numpy().decode('utf-8'),return_tensors='tf',padding='max_length',max_length=MAX_SEQ_LEN,truncation=True)[0]\n",
    "\n",
    "def process_fn(x):\n",
    "    s = x['title']+' '+x['description']\n",
    "    e = tf.py_function(process,inp=[s],Tout=(tf.int32))\n",
    "    e.set_shape(MAX_SEQ_LEN)\n",
    "    return e,x['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sada možemo učitati stvarni model koristeći paket `BertForSequenceClassification`. Ovo osigurava da naš model već ima potrebnu arhitekturu za klasifikaciju, uključujući završni klasifikator. Vidjet ćete poruku upozorenja koja navodi da težine završnog klasifikatora nisu inicijalizirane i da bi model zahtijevao prethodnu obuku - to je potpuno u redu, jer upravo to namjeravamo učiniti!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = transformers.TFBertForSequenceClassification.from_pretrained(bert_model,num_labels=4,output_attentions=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  109482240 \n",
      "_________________________________________________________________\n",
      "dropout_75 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  3076      \n",
      "=================================================================\n",
      "Total params: 109,485,316\n",
      "Trainable params: 109,485,316\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kao što možete vidjeti iz `summary()`, model sadrži gotovo 110 milijuna parametara! Vjerojatno, ako želimo jednostavan zadatak klasifikacije na relativno malom skupu podataka, ne želimo trenirati osnovni BERT sloj:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  109482240 \n",
      "_________________________________________________________________\n",
      "dropout_75 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  3076      \n",
      "=================================================================\n",
      "Total params: 109,485,316\n",
      "Trainable params: 3,076\n",
      "Non-trainable params: 109,482,240\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.layers[0].trainable = False\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sada smo spremni započeti treniranje!\n",
    "\n",
    "> **Napomena**: Treniranje BERT modela u punom opsegu može biti vrlo dugotrajno! Stoga ćemo ga trenirati samo za prvih 32 serije. Ovo je samo da pokažemo kako se postavlja treniranje modela. Ako ste zainteresirani za treniranje u punom opsegu - samo uklonite parametre `steps_per_epoch` i `validation_steps`, i pripremite se na čekanje!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 142s 4s/step - loss: 1.3896 - acc: 0.2500 - val_loss: 1.3863 - val_acc: 0.2480\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f1d40a4b6a0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile('adam','sparse_categorical_crossentropy',['acc'])\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "model.fit(ds_train.map(process_fn).batch(32),validation_data=ds_test.map(process_fn).batch(32),steps_per_epoch=32,validation_steps=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ako povećate broj iteracija i pričekate dovoljno dugo, te trenirate kroz nekoliko epoha, možete očekivati da će BERT klasifikacija pružiti najbolju točnost! To je zato što BERT već prilično dobro razumije strukturu jezika, pa je potrebno samo fino prilagoditi završni klasifikator. Međutim, budući da je BERT veliki model, cijeli proces treniranja traje dugo i zahtijeva ozbiljnu računalnu snagu! (GPU, i po mogućnosti više od jednog).\n",
    "\n",
    "> **Napomena:** U našem primjeru koristili smo jedan od najmanjih unaprijed istreniranih BERT modela. Postoje veći modeli koji vjerojatno daju bolje rezultate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ključne točke\n",
    "\n",
    "U ovoj jedinici smo se upoznali s vrlo recentnim arhitekturama modela temeljenim na **transformerima**. Primijenili smo ih za naš zadatak klasifikacije teksta, ali na sličan način BERT modeli mogu se koristiti za ekstrakciju entiteta, odgovaranje na pitanja i druge NLP zadatke.\n",
    "\n",
    "Transformerski modeli predstavljaju trenutno najnaprednije stanje u NLP-u, i u većini slučajeva trebali bi biti prvo rješenje s kojim započinjete eksperimentiranje prilikom implementacije prilagođenih NLP rješenja. Međutim, razumijevanje osnovnih principa rekurentnih neuronskih mreža, o kojima smo raspravljali u ovom modulu, izuzetno je važno ako želite izgraditi napredne neuronske modele.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Odricanje od odgovornosti**:  \nOvaj dokument je preveden pomoću AI usluge za prevođenje [Co-op Translator](https://github.com/Azure/co-op-translator). Iako nastojimo osigurati točnost, imajte na umu da automatski prijevodi mogu sadržavati pogreške ili netočnosti. Izvorni dokument na izvornom jeziku treba smatrati autoritativnim izvorom. Za ključne informacije preporučuje se profesionalni prijevod od strane ljudskog prevoditelja. Ne preuzimamo odgovornost za bilo kakve nesporazume ili pogrešne interpretacije koje proizlaze iz korištenja ovog prijevoda.\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0cb620c6d4b9f7a635928804c26cf22403d89d98d79684e4529119355ee6d5a5"
  },
  "kernelspec": {
   "display_name": "py38_tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "coopTranslator": {
   "original_hash": "ab59c532409774988ab875f2260e8e53",
   "translation_date": "2025-08-30T08:05:38+00:00",
   "source_file": "lessons/5-NLP/18-Transformers/TransformersTF.ipynb",
   "language_code": "hr"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}