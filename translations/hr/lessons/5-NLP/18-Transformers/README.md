# Mehanizmi paÅ¾nje i transformeri

## [Pre-kviz predavanja](https://ff-quizzes.netlify.app/en/ai/quiz/35)

Jedan od najvaÅ¾nijih problema u podruÄju NLP-a je **strojno prevoÄ‘enje**, kljuÄni zadatak koji stoji iza alata poput Google Translatea. U ovom dijelu usredotoÄit Ä‡emo se na strojno prevoÄ‘enje, ili opÄ‡enitije, na bilo koji zadatak *sekvenca-u-sekvencu* (koji se takoÄ‘er naziva **transdukcija reÄenica**).

Kod RNN-ova, sekvenca-u-sekvencu se implementira pomoÄ‡u dvije rekurentne mreÅ¾e, gdje jedna mreÅ¾a, **enkoder**, saÅ¾ima ulaznu sekvencu u skriveno stanje, dok druga mreÅ¾a, **dekoder**, razmotava to skriveno stanje u prevedeni rezultat. Postoji nekoliko problema s ovim pristupom:

* ZavrÅ¡no stanje enkoderske mreÅ¾e teÅ¡ko pamti poÄetak reÄenice, Å¡to uzrokuje loÅ¡u kvalitetu modela za duge reÄenice.
* Sve rijeÄi u sekvenci imaju isti utjecaj na rezultat. MeÄ‘utim, u stvarnosti, odreÄ‘ene rijeÄi u ulaznoj sekvenci Äesto imaju veÄ‡i utjecaj na izlazne sekvence od drugih.

**Mehanizmi paÅ¾nje** omoguÄ‡uju ponderiranje kontekstualnog utjecaja svakog ulaznog vektora na svaku izlaznu predikciju RNN-a. To se implementira stvaranjem preÄaca izmeÄ‘u meÄ‘ustanja ulaznog RNN-a i izlaznog RNN-a. Na taj naÄin, pri generiranju izlaznog simbola y<sub>t</sub>, uzet Ä‡emo u obzir sva skrivena stanja ulaza h<sub>i</sub>, s razliÄitim teÅ¾inskim koeficijentima &alpha;<sub>t,i</sub>.

![Slika koja prikazuje enkoder/dekoder model s aditivnim slojem paÅ¾nje](../../../../../translated_images/hr/encoder-decoder-attention.7a726296894fb567.webp)

> Enkoder-dekoder model s aditivnim mehanizmom paÅ¾nje u [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf), citirano iz [ovog blog posta](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)

Matrica paÅ¾nje {&alpha;<sub>i,j</sub>} predstavlja stupanj u kojem odreÄ‘ene ulazne rijeÄi sudjeluju u generiranju odreÄ‘ene rijeÄi u izlaznoj sekvenci. Ispod je primjer takve matrice:

![Slika koja prikazuje uzorak poravnanja pronaÄ‘en od strane RNNsearch-50, preuzeto iz Bahdanau - arviz.org](../../../../../translated_images/hr/bahdanau-fig3.09ba2d37f202a6af.webp)

> Slika iz [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) (Slika 3)

Mehanizmi paÅ¾nje odgovorni su za velik dio trenutnog ili bliskog trenutnom stanju umjetne inteligencije u NLP-u. Dodavanje paÅ¾nje znaÄajno poveÄ‡ava broj parametara modela, Å¡to je dovelo do problema skaliranja kod RNN-ova. KljuÄna ograniÄenja skaliranja RNN-ova su rekurentna priroda modela koja oteÅ¾ava grupiranje i paralelizaciju treninga. Kod RNN-a svaki element sekvence mora se obraditi redoslijedom, Å¡to znaÄi da se ne moÅ¾e lako paralelizirati.

![Enkoder Dekoder s PaÅ¾njom](../../../../../lessons/5-NLP/18-Transformers/images/EncDecAttention.gif)

> Slika iz [Googleovog bloga](https://research.googleblog.com/2016/09/a-neural-network-for-machine.html)

Usvajanje mehanizama paÅ¾nje u kombinaciji s ovim ograniÄenjem dovelo je do stvaranja sadaÅ¡njih vrhunskih Transformer modela koje poznajemo i koristimo danas, poput BERT-a i Open-GPT3.

## Transformer modeli

Jedna od glavnih ideja iza transformera je izbjegavanje sekvencijalne prirode RNN-ova i stvaranje modela koji se moÅ¾e paralelizirati tijekom treninga. To se postiÅ¾e implementacijom dviju ideja:

* pozicijsko kodiranje
* koriÅ¡tenje mehanizma samopozornosti za hvatanje uzoraka umjesto RNN-ova (ili CNN-ova) (zbog Äega se rad koji uvodi transformere naziva *[Attention is all you need](https://arxiv.org/abs/1706.03762)*)

### Pozicijsko kodiranje/ugraÄ‘ivanje

Ideja pozicijskog kodiranja je sljedeÄ‡a.  
1. Kod koriÅ¡tenja RNN-ova, relativni poloÅ¾aj tokena predstavlja se brojem koraka, i stoga ne mora biti eksplicitno predstavljen.  
2. MeÄ‘utim, kada se prebacimo na paÅ¾nju, moramo znati relativne poloÅ¾aje tokena unutar sekvence.  
3. Da bismo dobili pozicijsko kodiranje, proÅ¡irujemo naÅ¡u sekvencu tokena sekvencom pozicija tokena u sekvenci (tj. sekvencom brojeva 0,1, ...).  
4. Zatim mijeÅ¡amo poziciju tokena s vektorom ugraÄ‘ivanja tokena. Za transformaciju pozicije (cijelog broja) u vektor, moÅ¾emo koristiti razliÄite pristupe:

* UgraÄ‘ivanje koje se trenira, sliÄno ugraÄ‘ivanju tokena. Ovo je pristup koji ovdje razmatramo. Primjenjujemo slojeve ugraÄ‘ivanja na tokene i njihove pozicije, Å¡to rezultira vektorima ugraÄ‘ivanja iste dimenzije, koje zatim zbrajamo.
* Fiksna funkcija pozicijskog kodiranja, kako je predloÅ¾eno u originalnom radu.

<img src="../../../../../translated_images/hr/pos-embedding.e41ce9b6cf6078af.webp" width="50%"/>

> Slika autora

Rezultat koji dobivamo s pozicijskim ugraÄ‘ivanjem ukljuÄuje i originalni token i njegov poloÅ¾aj unutar sekvence.

### ViÅ¡eglava samopozornost

Zatim trebamo uhvatiti neke uzorke unutar naÅ¡e sekvence. Da bismo to uÄinili, transformeri koriste mehanizam **samopozornosti**, koji je u osnovi paÅ¾nja primijenjena na istu sekvencu kao ulaz i izlaz. Primjena samopozornosti omoguÄ‡uje nam uzimanje u obzir **konteksta** unutar reÄenice i uvid u meÄ‘usobne odnose izmeÄ‘u rijeÄi. Na primjer, omoguÄ‡uje nam da vidimo na koje rijeÄi se odnose zamjenice poput *to*, i takoÄ‘er uzimamo kontekst u obzir:

![](../../../../../translated_images/hr/CoreferenceResolution.861924d6d384a7d6.webp)

> Slika iz [Googleovog bloga](https://research.googleblog.com/2017/08/transformer-novel-neural-network.html)

Kod transformera koristimo **viÅ¡eglavu paÅ¾nju** kako bismo mreÅ¾i dali moÄ‡ hvatanja razliÄitih vrsta ovisnosti, npr. dugoroÄnih naspram kratkoroÄnih odnosa izmeÄ‘u rijeÄi, koreferencija naspram neÄeg drugog itd.

[TensorFlow Notebook](TransformersTF.ipynb) sadrÅ¾i viÅ¡e detalja o implementaciji slojeva transformera.

### PaÅ¾nja enkoder-dekoder

Kod transformera paÅ¾nja se koristi na dva mjesta:

* Za hvatanje uzoraka unutar ulaznog teksta pomoÄ‡u samopozornosti
* Za izvoÄ‘enje prevoÄ‘enja sekvenci - to je sloj paÅ¾nje izmeÄ‘u enkodera i dekodera.

PaÅ¾nja enkoder-dekoder vrlo je sliÄna mehanizmu paÅ¾nje koriÅ¡tenom kod RNN-ova, kako je opisano na poÄetku ovog dijela. Ovaj animirani dijagram objaÅ¡njava ulogu paÅ¾nje enkoder-dekoder.

![Animirani GIF koji prikazuje kako se evaluacije provode u transformer modelima.](../../../../../lessons/5-NLP/18-Transformers/images/transformer-animated-explanation.gif)

BuduÄ‡i da se svaka ulazna pozicija neovisno mapira na svaku izlaznu poziciju, transformeri se mogu bolje paralelizirati od RNN-ova, Å¡to omoguÄ‡uje mnogo veÄ‡e i izraÅ¾ajnije jeziÄne modele. Svaka glava paÅ¾nje moÅ¾e se koristiti za uÄenje razliÄitih odnosa izmeÄ‘u rijeÄi, Å¡to poboljÅ¡ava zadatke obrade prirodnog jezika.

## BERT

**BERT** (Bidirectional Encoder Representations from Transformers) je vrlo velika viÅ¡eslojna transformer mreÅ¾a s 12 slojeva za *BERT-base*, i 24 za *BERT-large*. Model se prvo unaprijed trenira na velikom korpusu tekstualnih podataka (WikiPedia + knjige) koristeÄ‡i nenadzirano uÄenje (predviÄ‘anje maskiranih rijeÄi u reÄenici). Tijekom unaprijed treniranja model usvaja znaÄajne razine razumijevanja jezika koje se zatim mogu iskoristiti s drugim skupovima podataka pomoÄ‡u finog podeÅ¡avanja. Ovaj proces naziva se **transferno uÄenje**.

![slika s http://jalammar.github.io/illustrated-bert/](../../../../../translated_images/hr/jalammarBERT-language-modeling-masked-lm.34f113ea5fec4362.webp)

> Slika [izvor](http://jalammar.github.io/illustrated-bert/)

## âœï¸ VjeÅ¾be: Transformeri

Nastavite svoje uÄenje u sljedeÄ‡im biljeÅ¾nicama:

* [Transformeri u PyTorch-u](TransformersPyTorch.ipynb)
* [Transformeri u TensorFlow-u](TransformersTF.ipynb)

## ZakljuÄak

U ovoj lekciji nauÄili ste o Transformerima i mehanizmima paÅ¾nje, kljuÄnim alatima u NLP alatu. Postoji mnogo varijacija arhitektura Transformera, ukljuÄujuÄ‡i BERT, DistilBERT, BigBird, OpenGPT3 i druge, koje se mogu fino podeÅ¡avati. Paket [HuggingFace](https://github.com/huggingface/) pruÅ¾a repozitorij za treniranje mnogih od ovih arhitektura s PyTorch-om i TensorFlow-om.

## ğŸš€ Izazov

## [Post-kviz predavanja](https://ff-quizzes.netlify.app/en/ai/quiz/36)

## Pregled i samostalno uÄenje

* [Blog post](https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/), koji objaÅ¡njava klasiÄni rad [Attention is all you need](https://arxiv.org/abs/1706.03762) o transformerima.
* [Serija blog postova](https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452) o transformerima, koja detaljno objaÅ¡njava arhitekturu.

## [Zadatak](assignment.md)

---

