# UgraÄ‘ivanja

## [Pre-lecture kviz](https://ff-quizzes.netlify.app/en/ai/quiz/27)

Kod treniranja klasifikatora temeljenih na BoW ili TF/IDF, radili smo s visokodimenzionalnim vektorima vreÄ‡e rijeÄi duljine `vocab_size`, i eksplicitno smo pretvarali niskodimenzionalne vektore pozicijskog prikaza u rijetke one-hot prikaze. MeÄ‘utim, ovaj one-hot prikaz nije memorijski uÄinkovit. Osim toga, svaka rijeÄ se tretira neovisno o drugima, tj. one-hot kodirani vektori ne izraÅ¾avaju nikakvu semantiÄku sliÄnost izmeÄ‘u rijeÄi.

Ideja **ugraÄ‘ivanja** je predstavljati rijeÄi pomoÄ‡u niskodimenzionalnih gustih vektora koji na neki naÄin odraÅ¾avaju semantiÄko znaÄenje rijeÄi. Kasnije Ä‡emo raspravljati o tome kako izgraditi znaÄajna ugraÄ‘ivanja rijeÄi, ali za sada razmislimo o ugraÄ‘ivanjima kao naÄinu smanjenja dimenzionalnosti vektora rijeÄi.

Dakle, sloj za ugraÄ‘ivanje uzima rijeÄ kao ulaz i proizvodi izlazni vektor odreÄ‘ene veliÄine `embedding_size`. Na neki naÄin, vrlo je sliÄan sloju `Linear`, ali umjesto da uzima one-hot kodirani vektor, moÅ¾e uzeti broj rijeÄi kao ulaz, omoguÄ‡ujuÄ‡i nam da izbjegnemo stvaranje velikih one-hot kodiranih vektora.

KoristeÄ‡i sloj za ugraÄ‘ivanje kao prvi sloj u naÅ¡oj mreÅ¾i klasifikatora, moÅ¾emo se prebaciti s modela vreÄ‡e rijeÄi na model **vreÄ‡e ugraÄ‘ivanja**, gdje prvo svaku rijeÄ u naÅ¡em tekstu pretvaramo u odgovarajuÄ‡e ugraÄ‘ivanje, a zatim izraÄunavamo neku agregatnu funkciju preko svih tih ugraÄ‘ivanja, poput `sum`, `average` ili `max`.  

![Slika koja prikazuje klasifikator ugraÄ‘ivanja za pet rijeÄi u nizu.](../../../../../translated_images/hr/embedding-classifier-example.b77f021a7ee67eee.webp)

> Slika autora

## âœï¸ VjeÅ¾be: UgraÄ‘ivanja

Nastavite uÄiti u sljedeÄ‡im biljeÅ¾nicama:
* [UgraÄ‘ivanja s PyTorch](EmbeddingsPyTorch.ipynb)
* [UgraÄ‘ivanja s TensorFlow](EmbeddingsTF.ipynb)

## SemantiÄka ugraÄ‘ivanja: Word2Vec

Iako je sloj za ugraÄ‘ivanje nauÄio mapirati rijeÄi u vektorski prikaz, taj prikaz nije nuÅ¾no imao puno semantiÄkog znaÄenja. Bilo bi korisno nauÄiti vektorski prikaz takav da sliÄne rijeÄi ili sinonimi odgovaraju vektorima koji su blizu jedni drugima prema nekoj vektorskoj udaljenosti (npr. Euklidskoj udaljenosti).

Da bismo to postigli, trebamo unaprijed trenirati naÅ¡ model za ugraÄ‘ivanje na velikoj zbirci teksta na specifiÄan naÄin. Jedan naÄin treniranja semantiÄkih ugraÄ‘ivanja naziva se [Word2Vec](https://en.wikipedia.org/wiki/Word2vec). Temelji se na dvije glavne arhitekture koje se koriste za stvaranje distribuiranog prikaza rijeÄi:

 - **Kontinuirana vreÄ‡a rijeÄi** (CBoW) â€” u ovoj arhitekturi treniramo model da predvidi rijeÄ iz okolnog konteksta. S obzirom na ngram $(W_{-2},W_{-1},W_0,W_1,W_2)$, cilj modela je predvidjeti $W_0$ iz $(W_{-2},W_{-1},W_1,W_2)$.
 - **Kontinuirani skip-gram** je suprotan CBoW-u. Model koristi okolni prozor rijeÄi iz konteksta kako bi predvidio trenutnu rijeÄ.

CBoW je brÅ¾i, dok je skip-gram sporiji, ali bolje predstavlja rijetke rijeÄi.

![Slika koja prikazuje algoritme CBoW i Skip-Gram za pretvaranje rijeÄi u vektore.](../../../../../translated_images/hr/example-algorithms-for-converting-words-to-vectors.fbe9207a726922f6.webp)

> Slika iz [ovog rada](https://arxiv.org/pdf/1301.3781.pdf)

Unaprijed trenirana ugraÄ‘ivanja Word2Vec (kao i drugi sliÄni modeli, poput GloVe) takoÄ‘er se mogu koristiti umjesto sloja za ugraÄ‘ivanje u neuronskim mreÅ¾ama. MeÄ‘utim, moramo se nositi s rjeÄnicima, jer se rjeÄnik koriÅ¡ten za unaprijed treniranje Word2Vec/GloVe vjerojatno razlikuje od rjeÄnika u naÅ¡em tekstualnom korpusu. Pogledajte gore navedene biljeÅ¾nice kako biste vidjeli kako se ovaj problem moÅ¾e rijeÅ¡iti.

## Kontekstualna ugraÄ‘ivanja

Jedno kljuÄno ograniÄenje tradicionalnih unaprijed treniranih prikaza ugraÄ‘ivanja poput Word2Vec je problem razluÄivanja znaÄenja rijeÄi. Iako unaprijed trenirana ugraÄ‘ivanja mogu uhvatiti dio znaÄenja rijeÄi u kontekstu, svako moguÄ‡e znaÄenje rijeÄi kodirano je u isto ugraÄ‘ivanje. To moÅ¾e uzrokovati probleme u modelima koji dolaze nakon, jer mnoge rijeÄi, poput rijeÄi 'play', imaju razliÄita znaÄenja ovisno o kontekstu u kojem se koriste.

Na primjer, rijeÄ 'play' u ove dvije reÄenice ima sasvim razliÄita znaÄenja:

- IÅ¡ao sam na **predstavu** u kazaliÅ¡tu.
- John Å¾eli **igrati** s prijateljima.

Unaprijed trenirana ugraÄ‘ivanja gore predstavljaju oba znaÄenja rijeÄi 'play' u istom ugraÄ‘ivanju. Da bismo prevladali ovo ograniÄenje, trebamo izgraditi ugraÄ‘ivanja temeljena na **jeziÄnom modelu**, koji je treniran na velikom korpusu teksta i *zna* kako se rijeÄi mogu slagati u razliÄitim kontekstima. Rasprava o kontekstualnim ugraÄ‘ivanjima je izvan dosega ovog vodiÄa, ali Ä‡emo se vratiti na njih kada budemo govorili o jeziÄnim modelima kasnije u teÄaju.

## ZakljuÄak

U ovoj lekciji otkrili ste kako izgraditi i koristiti slojeve za ugraÄ‘ivanje u TensorFlowu i PyTorchu kako biste bolje odrazili semantiÄka znaÄenja rijeÄi.

## ğŸš€ Izazov

Word2Vec je koriÅ¡ten za neke zanimljive primjene, ukljuÄujuÄ‡i generiranje stihova pjesama i poezije. Pogledajte [ovaj Älanak](https://www.politetype.com/blog/word2vec-color-poems) koji objaÅ¡njava kako je autor koristio Word2Vec za generiranje poezije. Pogledajte i [ovaj video Dana Shiffmanna](https://www.youtube.com/watch?v=LSS_bos_TPI&ab_channel=TheCodingTrain) kako biste otkrili drugaÄije objaÅ¡njenje ove tehnike. Zatim pokuÅ¡ajte primijeniti ove tehnike na vlastiti tekstualni korpus, moÅ¾da preuzet s Kagglea.

## [Post-lecture kviz](https://ff-quizzes.netlify.app/en/ai/quiz/28)

## Pregled i samostalno uÄenje

ProÄitajte ovaj rad o Word2Vecu: [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf)

## [Zadatak: BiljeÅ¾nice](assignment.md)

---

