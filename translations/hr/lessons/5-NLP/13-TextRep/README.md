<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "4522e22e150be0845e03aa41209a39d5",
  "translation_date": "2025-08-25T21:51:51+00:00",
  "source_file": "lessons/5-NLP/13-TextRep/README.md",
  "language_code": "hr"
}
-->
# Predstavljanje teksta kao tenzora

## [Kviz prije predavanja](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/113)

## Klasifikacija teksta

U prvom dijelu ovog poglavlja fokusirat Ä‡emo se na zadatak **klasifikacije teksta**. Koristit Ä‡emo [AG News](https://www.kaggle.com/amananandrai/ag-news-classification-dataset) dataset, koji sadrÅ¾i vijesti poput sljedeÄ‡e:

* Kategorija: Znanost/Tehnologija  
* Naslov: Ky. tvrtka dobila potporu za prouÄavanje peptida (AP)  
* Tekst: AP - Tvrtka koju je osnovao istraÅ¾ivaÄ kemije sa SveuÄiliÅ¡ta u Louisvilleu dobila je potporu za razvoj...

NaÅ¡ cilj bit Ä‡e klasificirati vijest u jednu od kategorija na temelju teksta.

## Predstavljanje teksta

Ako Å¾elimo rjeÅ¡avati zadatke obrade prirodnog jezika (NLP) pomoÄ‡u neuronskih mreÅ¾a, trebamo naÄin za predstavljanje teksta kao tenzora. RaÄunala veÄ‡ predstavljaju tekstualne znakove kao brojeve koji se mapiraju na fontove na vaÅ¡em ekranu koristeÄ‡i kodiranja poput ASCII ili UTF-8.

<img alt="Slika koja prikazuje dijagram mapiranja znaka na ASCII i binarnu reprezentaciju" src="images/ascii-character-map.png" width="50%"/>

> [Izvor slike](https://www.seobility.net/en/wiki/ASCII)

Kao ljudi, razumijemo Å¡to svako slovo **predstavlja** i kako se svi znakovi spajaju u rijeÄi reÄenice. MeÄ‘utim, raÄunala sama po sebi nemaju takvo razumijevanje, i neuronska mreÅ¾a mora nauÄiti znaÄenje tijekom treniranja.

Stoga moÅ¾emo koristiti razliÄite pristupe za predstavljanje teksta:

* **Reprezentacija na razini znakova**, gdje tekst predstavljamo tretirajuÄ‡i svaki znak kao broj. Ako imamo *C* razliÄitih znakova u naÅ¡em tekstualnom korpusu, rijeÄ *Hello* bila bi predstavljena kao 5x*C* tenzor. Svako slovo odgovara stupcu tenzora u one-hot kodiranju.  
* **Reprezentacija na razini rijeÄi**, gdje stvaramo **rjeÄnik** svih rijeÄi u naÅ¡em tekstu, a zatim predstavljamo rijeÄi koristeÄ‡i one-hot kodiranje. Ovaj pristup je donekle bolji jer svako slovo samo po sebi nema puno znaÄenja, pa koriÅ¡tenjem semantiÄki viÅ¡ih koncepata - rijeÄi - pojednostavljujemo zadatak za neuronsku mreÅ¾u. MeÄ‘utim, zbog velike veliÄine rjeÄnika, moramo se nositi s visokodimenzionalnim rijetkim tenzorima.

Bez obzira na naÄin predstavljanja, prvo trebamo pretvoriti tekst u niz **tokena**, pri Äemu je jedan token znak, rijeÄ ili ponekad Äak dio rijeÄi. Zatim token pretvaramo u broj, obiÄno koristeÄ‡i **rjeÄnik**, a taj broj moÅ¾e se unijeti u neuronsku mreÅ¾u koristeÄ‡i one-hot kodiranje.

## N-grami

U prirodnom jeziku precizno znaÄenje rijeÄi moÅ¾e se odrediti samo u kontekstu. Na primjer, znaÄenja izraza *neuronska mreÅ¾a* i *ribiÄka mreÅ¾a* potpuno su razliÄita. Jedan od naÄina da to uzmemo u obzir je izgradnja modela na parovima rijeÄi, tretirajuÄ‡i parove rijeÄi kao zasebne tokene u rjeÄniku. Na taj naÄin, reÄenica *Volim iÄ‡i na pecanje* bit Ä‡e predstavljena sljedeÄ‡im nizom tokena: *Volim iÄ‡i*, *iÄ‡i na*, *na pecanje*. Problem s ovim pristupom je Å¡to veliÄina rjeÄnika znaÄajno raste, a kombinacije poput *na pecanje* i *na kupovinu* predstavljene su razliÄitim tokenima, koji ne dijele nikakvu semantiÄku sliÄnost unatoÄ istom glagolu.

U nekim sluÄajevima moÅ¾emo razmotriti koriÅ¡tenje tri-grama -- kombinacija od tri rijeÄi -- takoÄ‘er. Ovaj pristup Äesto se naziva **n-grami**. TakoÄ‘er, ima smisla koristiti n-grame s reprezentacijom na razini znakova, gdje n-grami otprilike odgovaraju razliÄitim slogovima.

## Bag-of-Words i TF/IDF

Kod rjeÅ¡avanja zadataka poput klasifikacije teksta, trebamo biti u moguÄ‡nosti predstaviti tekst jednim vektorom fiksne veliÄine, koji Ä‡emo koristiti kao ulaz za zavrÅ¡ni gusti klasifikator. Jedan od najjednostavnijih naÄina za to je kombiniranje svih pojedinaÄnih reprezentacija rijeÄi, npr. njihovim zbrajanjem. Ako zbrojimo one-hot kodiranja svake rijeÄi, dobit Ä‡emo vektor frekvencija, koji pokazuje koliko se puta svaka rijeÄ pojavljuje u tekstu. Takva reprezentacija teksta naziva se **bag-of-words** (BoW).

<img src="images/bow.png" width="90%"/>

> Slika autora

BoW u osnovi predstavlja koje se rijeÄi pojavljuju u tekstu i u kojim koliÄinama, Å¡to moÅ¾e biti dobar pokazatelj o Äemu se tekst radi. Na primjer, Älanak o politici vjerojatno sadrÅ¾i rijeÄi poput *predsjednik* i *drÅ¾ava*, dok bi znanstvena publikacija imala neÅ¡to poput *sudaraÄ*, *otkriveno*, itd. Dakle, frekvencije rijeÄi u mnogim sluÄajevima mogu biti dobar pokazatelj sadrÅ¾aja teksta.

Problem s BoW-om je Å¡to se odreÄ‘ene uobiÄajene rijeÄi, poput *i*, *je*, itd., pojavljuju u veÄ‡ini tekstova i imaju najviÅ¡e frekvencije, maskirajuÄ‡i rijeÄi koje su zaista vaÅ¾ne. MoÅ¾emo smanjiti vaÅ¾nost tih rijeÄi uzimajuÄ‡i u obzir uÄestalost pojavljivanja rijeÄi u cijeloj kolekciji dokumenata. Ovo je glavna ideja iza TF/IDF pristupa, koji je detaljnije obraÄ‘en u priloÅ¾enim biljeÅ¾nicama ovog poglavlja.

MeÄ‘utim, nijedan od ovih pristupa ne moÅ¾e u potpunosti uzeti u obzir **semantiku** teksta. Za to su nam potrebni snaÅ¾niji modeli neuronskih mreÅ¾a, o kojima Ä‡emo raspravljati kasnije u ovom poglavlju.

## âœï¸ VjeÅ¾be: Predstavljanje teksta

Nastavite uÄiti u sljedeÄ‡im biljeÅ¾nicama:

* [Predstavljanje teksta s PyTorchom](../../../../../lessons/5-NLP/13-TextRep/TextRepresentationPyTorch.ipynb)  
* [Predstavljanje teksta s TensorFlowom](../../../../../lessons/5-NLP/13-TextRep/TextRepresentationTF.ipynb)  

## ZakljuÄak

Do sada smo prouÄavali tehnike koje mogu dodati teÅ¾inu frekvencijama razliÄitih rijeÄi. MeÄ‘utim, one nisu sposobne predstaviti znaÄenje ili redoslijed. Kao Å¡to je poznati lingvist J. R. Firth rekao 1935. godine: "Potpuno znaÄenje rijeÄi uvijek je kontekstualno, i nijedno prouÄavanje znaÄenja izvan konteksta ne moÅ¾e se smatrati ozbiljnim." Kasnije u teÄaju nauÄit Ä‡emo kako uhvatiti kontekstualne informacije iz teksta koristeÄ‡i modele jezika.

## ğŸš€ Izazov

Isprobajte neke druge vjeÅ¾be koristeÄ‡i bag-of-words i razliÄite modele podataka. MoÅ¾ete se inspirirati ovim [natjecanjem na Kaggleu](https://www.kaggle.com/competitions/word2vec-nlp-tutorial/overview/part-1-for-beginners-bag-of-words).

## [Kviz nakon predavanja](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/213)

## Pregled i samostalno uÄenje

UvjeÅ¾bajte svoje vjeÅ¡tine s tehnikama ugraÄ‘ivanja teksta i bag-of-words na [Microsoft Learn](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-pytorch/?WT.mc_id=academic-77998-cacaste).

## [Zadatak: BiljeÅ¾nice](assignment.md)

**Odricanje od odgovornosti**:  
Ovaj dokument je preveden pomoÄ‡u AI usluge za prevoÄ‘enje [Co-op Translator](https://github.com/Azure/co-op-translator). Iako nastojimo osigurati toÄnost, imajte na umu da automatski prijevodi mogu sadrÅ¾avati pogreÅ¡ke ili netoÄnosti. Izvorni dokument na izvornom jeziku treba smatrati autoritativnim izvorom. Za kljuÄne informacije preporuÄuje se profesionalni prijevod od strane Äovjeka. Ne preuzimamo odgovornost za nesporazume ili pogreÅ¡na tumaÄenja koja mogu proizaÄ‡i iz koriÅ¡tenja ovog prijevoda.