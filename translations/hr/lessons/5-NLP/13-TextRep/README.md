# Predstavljanje teksta kao tenzora

## [Kviz prije predavanja](https://ff-quizzes.netlify.app/en/ai/quiz/25)

## Klasifikacija teksta

U prvom dijelu ovog poglavlja fokusirat Ä‡emo se na zadatak **klasifikacije teksta**. Koristit Ä‡emo [AG News](https://www.kaggle.com/amananandrai/ag-news-classification-dataset) dataset, koji sadrÅ¾i vijesti poput sljedeÄ‡e:

* Kategorija: Znanost/Tehnologija  
* Naslov: Ky. Company Wins Grant to Study Peptides (AP)  
* Tekst: AP - Tvrtka koju je osnovao istraÅ¾ivaÄ kemije na SveuÄiliÅ¡tu Louisville dobila je potporu za razvoj...

NaÅ¡ cilj bit Ä‡e klasificirati vijest u jednu od kategorija na temelju teksta.

## Predstavljanje teksta

Ako Å¾elimo rjeÅ¡avati zadatke obrade prirodnog jezika (NLP) pomoÄ‡u neuronskih mreÅ¾a, trebamo naÄin za predstavljanje teksta kao tenzora. RaÄunala veÄ‡ predstavljaju tekstualne znakove kao brojeve koji se mapiraju na fontove na vaÅ¡em ekranu koristeÄ‡i kodiranja poput ASCII ili UTF-8.

<img alt="Slika koja prikazuje dijagram mapiranja znaka na ASCII i binarnu reprezentaciju" src="../../../../../translated_images/hr/ascii-character-map.18ed6aa7f3b0a7ff.webp" width="50%"/>

> [Izvor slike](https://www.seobility.net/en/wiki/ASCII)

Kao ljudi, razumijemo Å¡to svako slovo **predstavlja** i kako se svi znakovi spajaju u rijeÄi reÄenice. MeÄ‘utim, raÄunala sama po sebi nemaju takvo razumijevanje, a neuronska mreÅ¾a mora nauÄiti znaÄenje tijekom treninga.

Stoga moÅ¾emo koristiti razliÄite pristupe za predstavljanje teksta:

* **Reprezentacija na razini znakova**, gdje tekst predstavljamo tretirajuÄ‡i svaki znak kao broj. Ako imamo *C* razliÄitih znakova u naÅ¡em korpusu teksta, rijeÄ *Hello* bila bi predstavljena kao 5x*C* tenzor. Svako slovo odgovara stupcu tenzora u one-hot kodiranju.  
* **Reprezentacija na razini rijeÄi**, gdje stvaramo **vokabular** svih rijeÄi u naÅ¡em tekstu i zatim predstavljamo rijeÄi koristeÄ‡i one-hot kodiranje. Ovaj pristup je donekle bolji jer samo slovo nema puno znaÄenja, pa koriÅ¡tenjem viÅ¡ih semantiÄkih koncepata - rijeÄi - pojednostavljujemo zadatak za neuronsku mreÅ¾u. MeÄ‘utim, zbog velikog vokabulara moramo se nositi s visokodimenzionalnim rijetkim tenzorima.

Bez obzira na naÄin reprezentacije, prvo moramo pretvoriti tekst u niz **tokena**, pri Äemu je jedan token znak, rijeÄ ili ponekad Äak dio rijeÄi. Zatim token pretvaramo u broj, obiÄno koristeÄ‡i **vokabular**, a taj broj moÅ¾e se unijeti u neuronsku mreÅ¾u koristeÄ‡i one-hot kodiranje.

## N-Gramovi

U prirodnom jeziku precizno znaÄenje rijeÄi moÅ¾e se odrediti samo u kontekstu. Na primjer, znaÄenja *neuronska mreÅ¾a* i *ribarska mreÅ¾a* potpuno su razliÄita. Jedan od naÄina da to uzmemo u obzir je da gradimo naÅ¡ model na parovima rijeÄi, tretirajuÄ‡i parove rijeÄi kao zasebne tokene vokabulara. Na taj naÄin reÄenica *Volim iÄ‡i na pecanje* bit Ä‡e predstavljena sljedeÄ‡im nizom tokena: *Volim iÄ‡i*, *iÄ‡i na*, *na pecanje*. Problem s ovim pristupom je Å¡to vokabular znaÄajno raste, a kombinacije poput *na pecanje* i *na kupovinu* predstavljene su razliÄitim tokenima, koji ne dijele nikakvu semantiÄku sliÄnost unatoÄ istom glagolu.

U nekim sluÄajevima moÅ¾emo razmotriti koriÅ¡tenje tri-grama -- kombinacija tri rijeÄi -- takoÄ‘er. Ovaj pristup Äesto se naziva **n-gramovi**. TakoÄ‘er, ima smisla koristiti n-gramove s reprezentacijom na razini znakova, gdje n-gramovi otprilike odgovaraju razliÄitim slogovima.

## Bag-of-Words i TF/IDF

Kod rjeÅ¡avanja zadataka poput klasifikacije teksta, trebamo biti u moguÄ‡nosti predstaviti tekst jednim vektorom fiksne veliÄine, koji Ä‡emo koristiti kao ulaz za zavrÅ¡ni gusti klasifikator. Jedan od najjednostavnijih naÄina za to je kombiniranje svih pojedinaÄnih reprezentacija rijeÄi, npr. njihovim zbrajanjem. Ako zbrojimo one-hot kodiranja svake rijeÄi, dobit Ä‡emo vektor frekvencija, koji pokazuje koliko se puta svaka rijeÄ pojavljuje unutar teksta. Takva reprezentacija teksta naziva se **bag-of-words** (BoW).

<img src="../../../../../translated_images/hr/bow.3811869cff59368d.webp" width="90%"/>

> Slika autora

BoW u osnovi predstavlja koje rijeÄi se pojavljuju u tekstu i u kojim koliÄinama, Å¡to moÅ¾e biti dobar pokazatelj o Äemu se tekst radi. Na primjer, Älanak o politici vjerojatno Ä‡e sadrÅ¾avati rijeÄi poput *predsjednik* i *drÅ¾ava*, dok Ä‡e znanstvena publikacija imati neÅ¡to poput *sudaraÄ*, *otkriveno*, itd. Dakle, frekvencije rijeÄi u mnogim sluÄajevima mogu biti dobar pokazatelj sadrÅ¾aja teksta.

Problem s BoW je Å¡to se odreÄ‘ene uobiÄajene rijeÄi, poput *i*, *je*, itd., pojavljuju u veÄ‡ini tekstova i imaju najviÅ¡e frekvencije, zasjenjujuÄ‡i rijeÄi koje su zaista vaÅ¾ne. MoÅ¾emo smanjiti vaÅ¾nost tih rijeÄi uzimajuÄ‡i u obzir uÄestalost pojavljivanja rijeÄi u cijeloj kolekciji dokumenata. Ovo je glavna ideja iza TF/IDF pristupa, koji je detaljnije objaÅ¡njen u priloÅ¾enim biljeÅ¾nicama uz ovo poglavlje.

MeÄ‘utim, nijedan od ovih pristupa ne moÅ¾e u potpunosti uzeti u obzir **semantiku** teksta. Za to su nam potrebni moÄ‡niji modeli neuronskih mreÅ¾a, o kojima Ä‡emo raspravljati kasnije u ovom poglavlju.

## âœï¸ VjeÅ¾be: Reprezentacija teksta

Nastavite uÄiti u sljedeÄ‡im biljeÅ¾nicama:

* [Reprezentacija teksta s PyTorchom](TextRepresentationPyTorch.ipynb)  
* [Reprezentacija teksta s TensorFlowom](TextRepresentationTF.ipynb)  

## ZakljuÄak

Do sada smo prouÄavali tehnike koje mogu dodati teÅ¾inu frekvencijama razliÄitih rijeÄi. MeÄ‘utim, one nisu sposobne predstaviti znaÄenje ili redoslijed. Kao Å¡to je poznati lingvist J. R. Firth rekao 1935. godine: "Potpuno znaÄenje rijeÄi uvijek je kontekstualno, i nijedno prouÄavanje znaÄenja izvan konteksta ne moÅ¾e se smatrati ozbiljnim." Kasnije u ovom teÄaju nauÄit Ä‡emo kako izvuÄ‡i kontekstualne informacije iz teksta koristeÄ‡i jeziÄno modeliranje.

## ğŸš€ Izazov

Isprobajte neke druge vjeÅ¾be koristeÄ‡i bag-of-words i razliÄite modele podataka. MoÅ¾ete se inspirirati ovim [natjecanjem na Kaggleu](https://www.kaggle.com/competitions/word2vec-nlp-tutorial/overview/part-1-for-beginners-bag-of-words)

## [Kviz nakon predavanja](https://ff-quizzes.netlify.app/en/ai/quiz/26)

## Pregled i samostalno uÄenje

VjeÅ¾bajte svoje vjeÅ¡tine s tehnikama ugraÄ‘ivanja teksta i bag-of-words na [Microsoft Learn](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-pytorch/?WT.mc_id=academic-77998-cacaste)

## [Zadatak: BiljeÅ¾nice](assignment.md)

---

