# Rekurentne neuronske mreÅ¾e

## [Kviz prije predavanja](https://ff-quizzes.netlify.app/en/ai/quiz/31)

U prethodnim odjeljcima koristili smo bogate semantiÄke reprezentacije teksta i jednostavan linearni klasifikator na vrhu ugraÄ‘enih vektora. Ova arhitektura hvata agregirano znaÄenje rijeÄi u reÄenici, ali ne uzima u obzir **redoslijed** rijeÄi, jer operacija agregacije na ugraÄ‘enim vektorima uklanja ovu informaciju iz izvornog teksta. BuduÄ‡i da ovi modeli ne mogu modelirati redoslijed rijeÄi, nisu sposobni rijeÅ¡iti sloÅ¾enije ili dvosmislene zadatke poput generiranja teksta ili odgovaranja na pitanja.

Kako bismo uhvatili znaÄenje sekvenci teksta, trebamo koristiti drugu arhitekturu neuronske mreÅ¾e, koja se naziva **rekurentna neuronska mreÅ¾a** ili RNN. U RNN-u, reÄenicu prosljeÄ‘ujemo kroz mreÅ¾u jedan simbol po simbol, a mreÅ¾a proizvodi odreÄ‘eno **stanje**, koje zatim ponovno prosljeÄ‘ujemo mreÅ¾i s iduÄ‡im simbolom.

![RNN](../../../../../translated_images/hr/rnn.27f5c29c53d727b5.webp)

> Slika autora

Za danu ulaznu sekvencu tokena X<sub>0</sub>,...,X<sub>n</sub>, RNN stvara sekvencu blokova neuronske mreÅ¾e i trenira ovu sekvencu od poÄetka do kraja koristeÄ‡i unatrag Å¡irenje pogreÅ¡ke (backpropagation). Svaki blok mreÅ¾e uzima par (X<sub>i</sub>,S<sub>i</sub>) kao ulaz i proizvodi S<sub>i+1</sub> kao rezultat. KonaÄno stanje S<sub>n</sub> ili (izlaz Y<sub>n</sub>) ide u linearni klasifikator kako bi proizveo rezultat. Svi blokovi mreÅ¾e dijele iste teÅ¾ine i treniraju se od poÄetka do kraja koristeÄ‡i jedan prolaz unatrag Å¡irenja pogreÅ¡ke.

BuduÄ‡i da se vektori stanja S<sub>0</sub>,...,S<sub>n</sub> prosljeÄ‘uju kroz mreÅ¾u, ona moÅ¾e nauÄiti sekvencijalne ovisnosti izmeÄ‘u rijeÄi. Na primjer, kada se rijeÄ *ne* pojavi negdje u sekvenci, mreÅ¾a moÅ¾e nauÄiti negirati odreÄ‘ene elemente unutar vektora stanja, Å¡to rezultira negacijom.

> âœ… BuduÄ‡i da su teÅ¾ine svih RNN blokova na slici gore zajedniÄke, ista slika moÅ¾e se prikazati kao jedan blok (desno) s povratnom petljom, koja prosljeÄ‘uje izlazno stanje mreÅ¾e natrag na ulaz.

## Anatomija RNN Ä‡elije

Pogledajmo kako je organizirana jednostavna RNN Ä‡elija. Ona prihvaÄ‡a prethodno stanje S<sub>i-1</sub> i trenutni simbol X<sub>i</sub> kao ulaze, te mora proizvesti izlazno stanje S<sub>i</sub> (a ponekad nas zanima i neki drugi izlaz Y<sub>i</sub>, kao u sluÄaju generativnih mreÅ¾a).

Jednostavna RNN Ä‡elija ima dvije matrice teÅ¾ina unutar sebe: jedna transformira ulazni simbol (nazovimo je W), a druga transformira ulazno stanje (H). U ovom sluÄaju izlaz mreÅ¾e se raÄuna kao &sigma;(W&times;X<sub>i</sub>+H&times;S<sub>i-1</sub>+b), gdje je &sigma; funkcija aktivacije, a b dodatna pristranost.

<img alt="Anatomija RNN Ä‡elije" src="../../../../../translated_images/hr/rnn-anatomy.79ee3f3920b3294b.webp" width="50%"/>

> Slika autora

U mnogim sluÄajevima, ulazni tokeni prolaze kroz sloj ugraÄ‘ivanja prije nego uÄ‘u u RNN kako bi se smanjila dimenzionalnost. U ovom sluÄaju, ako je dimenzija ulaznih vektora *emb_size*, a vektor stanja *hid_size* - veliÄina W je *emb_size*&times;*hid_size*, a veliÄina H je *hid_size*&times;*hid_size*.

## Long Short Term Memory (LSTM)

Jedan od glavnih problema klasiÄnih RNN-ova je takozvani problem **nestajuÄ‡ih gradijenata**. BuduÄ‡i da se RNN-ovi treniraju od poÄetka do kraja u jednom prolazu unatrag Å¡irenja pogreÅ¡ke, teÅ¡ko je propagirati pogreÅ¡ku do prvih slojeva mreÅ¾e, pa mreÅ¾a ne moÅ¾e nauÄiti odnose izmeÄ‘u udaljenih tokena. Jedan od naÄina za izbjegavanje ovog problema je uvoÄ‘enje **eksplicitnog upravljanja stanjem** pomoÄ‡u takozvanih **vrata**. Postoje dvije poznate arhitekture ovog tipa: **Long Short Term Memory** (LSTM) i **Gated Relay Unit** (GRU).

![Slika koja prikazuje primjer Ä‡elije Long Short Term Memory](../../../../../lessons/5-NLP/16-RNN/images/long-short-term-memory-cell.svg)

> Izvor slike TBD

LSTM mreÅ¾a je organizirana na naÄin sliÄan RNN-u, ali postoje dva stanja koja se prenose iz sloja u sloj: stvarno stanje C i skriveni vektor H. U svakoj jedinici, skriveni vektor H<sub>i</sub> se spaja s ulazom X<sub>i</sub>, i oni kontroliraju Å¡to se dogaÄ‘a sa stanjem C putem **vrata**. Svaka vrata su neuronska mreÅ¾a sa sigmoidnom aktivacijom (izlaz u rasponu [0,1]), koja se moÅ¾e smatrati maskom po bitovima kada se pomnoÅ¾i s vektorom stanja. Postoje sljedeÄ‡a vrata (s lijeva na desno na slici gore):

* **Vrata zaborava** uzimaju skriveni vektor i odreÄ‘uju koje komponente vektora C trebamo zaboraviti, a koje proslijediti dalje.
* **Ulazna vrata** uzimaju neke informacije iz ulaznog i skrivenog vektora i ubacuju ih u stanje.
* **Izlazna vrata** transformiraju stanje putem linearnog sloja s *tanh* aktivacijom, zatim odabiru neke od njegovih komponenti koristeÄ‡i skriveni vektor H<sub>i</sub> kako bi proizveli novo stanje C<sub>i+1</sub>.

Komponente stanja C mogu se smatrati zastavicama koje se mogu ukljuÄiti i iskljuÄiti. Na primjer, kada u sekvenci naiÄ‘emo na ime *Alice*, moÅ¾emo pretpostaviti da se odnosi na Å¾enski lik i podiÄ‡i zastavicu u stanju da imamo Å¾enski imenicu u reÄenici. Kada dalje naiÄ‘emo na frazu *i Tom*, podiÄ‡i Ä‡emo zastavicu da imamo mnoÅ¾insku imenicu. Tako manipulacijom stanjem moÅ¾emo pratiti gramatiÄka svojstva dijelova reÄenice.

> âœ… Izvrstan resurs za razumijevanje unutarnjeg funkcioniranja LSTM-a je ovaj odliÄan Älanak [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) Christophera Olaha.

## Dvosmjerni i viÅ¡eslojni RNN-ovi

Razgovarali smo o rekurentnim mreÅ¾ama koje djeluju u jednom smjeru, od poÄetka sekvence do kraja. To se Äini prirodnim, jer podsjeÄ‡a na naÄin na koji Äitamo i sluÅ¡amo govor. MeÄ‘utim, buduÄ‡i da u mnogim praktiÄnim sluÄajevima imamo nasumiÄan pristup ulaznoj sekvenci, moÅ¾e imati smisla pokrenuti rekurentni izraÄun u oba smjera. Takve mreÅ¾e nazivaju se **dvosmjerni** RNN-ovi. Kada radimo s dvosmjernom mreÅ¾om, trebamo dva skrivena vektora stanja, po jedan za svaki smjer.

Rekurentna mreÅ¾a, bilo jednosmjerna ili dvosmjerna, hvata odreÄ‘ene uzorke unutar sekvence i moÅ¾e ih pohraniti u vektor stanja ili proslijediti u izlaz. Kao i kod konvolucijskih mreÅ¾a, moÅ¾emo izgraditi drugi rekurentni sloj na vrhu prvog kako bismo uhvatili uzorke viÅ¡eg nivoa i izgradili na temelju uzoraka niÅ¾eg nivoa koje je izvukao prvi sloj. To nas dovodi do pojma **viÅ¡eslojnog RNN-a**, koji se sastoji od dva ili viÅ¡e rekurentnih mreÅ¾a, gdje se izlaz prethodnog sloja prosljeÄ‘uje sljedeÄ‡em sloju kao ulaz.

![Slika koja prikazuje viÅ¡eslojni LSTM RNN](../../../../../translated_images/hr/multi-layer-lstm.dd975e29bb2a59fe.webp)

*Slika iz [ovog izvrsnog posta](https://towardsdatascience.com/from-a-lstm-cell-to-a-multilayer-lstm-network-with-pytorch-2899eb5696f3) Fernanda LÃ³peza*

## âœï¸ VjeÅ¾be: UgraÄ‘ivanja

Nastavite uÄiti u sljedeÄ‡im biljeÅ¾nicama:

* [RNN-ovi s PyTorchom](RNNPyTorch.ipynb)
* [RNN-ovi s TensorFlowom](RNNTF.ipynb)

## ZakljuÄak

U ovoj jedinici vidjeli smo da se RNN-ovi mogu koristiti za klasifikaciju sekvenci, ali zapravo mogu obraditi mnogo viÅ¡e zadataka, poput generiranja teksta, strojnog prevoÄ‘enja i drugih. Te zadatke Ä‡emo razmotriti u sljedeÄ‡oj jedinici.

## ğŸš€ Izazov

ProÄitajte literaturu o LSTM-ovima i razmotrite njihove primjene:

- [Grid Long Short-Term Memory](https://arxiv.org/pdf/1507.01526v1.pdf)
- [Show, Attend and Tell: Neural Image Caption
Generation with Visual Attention](https://arxiv.org/pdf/1502.03044v2.pdf)

## [Kviz nakon predavanja](https://ff-quizzes.netlify.app/en/ai/quiz/32)

## Pregled i samostalno uÄenje

- [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) Christophera Olaha.

## [Zadatak: BiljeÅ¾nice](assignment.md)

---

