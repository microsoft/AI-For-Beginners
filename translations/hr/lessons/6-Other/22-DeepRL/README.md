# Duboko pojaÄano uÄenje

PojaÄano uÄenje (RL) smatra se jednim od osnovnih paradigmi strojnog uÄenja, uz nadzirano uÄenje i nenadzirano uÄenje. Dok se u nadziranom uÄenju oslanjamo na skup podataka s poznatim ishodima, RL se temelji na **uÄenju kroz rad**. Na primjer, kada prvi put vidimo raÄunalnu igru, poÄinjemo igrati, Äak i bez poznavanja pravila, i ubrzo poboljÅ¡avamo svoje vjeÅ¡tine samo kroz proces igranja i prilagoÄ‘avanja ponaÅ¡anja.

## [Kviz prije predavanja](https://ff-quizzes.netlify.app/en/ai/quiz/43)

Za provoÄ‘enje RL-a potrebni su nam:

* **OkruÅ¾enje** ili **simulator** koji postavlja pravila igre. Trebali bismo moÄ‡i provoditi eksperimente u simulatoru i promatrati rezultate.
* Neka **funkcija nagrade**, koja pokazuje koliko je naÅ¡ eksperiment bio uspjeÅ¡an. U sluÄaju uÄenja igranja raÄunalne igre, nagrada bi bila naÅ¡ konaÄni rezultat.

Na temelju funkcije nagrade trebali bismo moÄ‡i prilagoditi svoje ponaÅ¡anje i poboljÅ¡ati svoje vjeÅ¡tine kako bismo sljedeÄ‡i put igrali bolje. Glavna razlika izmeÄ‘u drugih vrsta strojnog uÄenja i RL-a je ta Å¡to u RL-u obiÄno ne znamo hoÄ‡emo li pobijediti ili izgubiti dok ne zavrÅ¡imo igru. Dakle, ne moÅ¾emo reÄ‡i je li odreÄ‘eni potez sam po sebi dobar ili ne - nagradu dobivamo tek na kraju igre.

Tijekom RL-a obiÄno provodimo mnogo eksperimenata. Tijekom svakog eksperimenta moramo balansirati izmeÄ‘u praÄ‡enja optimalne strategije koju smo dosad nauÄili (**eksploatacija**) i istraÅ¾ivanja novih moguÄ‡ih stanja (**eksploracija**).

## OpenAI Gym

OdliÄan alat za RL je [OpenAI Gym](https://gym.openai.com/) - **simulacijsko okruÅ¾enje** koje moÅ¾e simulirati mnoga razliÄita okruÅ¾enja, poÄevÅ¡i od Atari igara do fizike balansiranja Å¡tapa. To je jedno od najpopularnijih simulacijskih okruÅ¾enja za treniranje algoritama pojaÄanog uÄenja, a odrÅ¾ava ga [OpenAI](https://openai.com/).

> **Napomena**: Sve dostupne okoline iz OpenAI Gym-a moÅ¾ete vidjeti [ovdje](https://gym.openai.com/envs/#classic_control).

## Balansiranje CartPole-a

Vjerojatno ste svi vidjeli moderne ureÄ‘aje za balansiranje poput *Segwaya* ili *Gyroscootera*. Oni se automatski balansiraju prilagoÄ‘avanjem svojih kotaÄa na temelju signala iz akcelerometra ili Å¾iroskopa. U ovom Ä‡emo dijelu nauÄiti kako rijeÅ¡iti sliÄan problem - balansiranje Å¡tapa. To je sliÄno situaciji kada cirkuski izvoÄ‘aÄ mora balansirati Å¡tap na svojoj ruci - ali ovo balansiranje Å¡tapa odvija se samo u 1D.

Pojednostavljena verzija balansiranja poznata je kao problem **CartPole**. U svijetu CartPole-a imamo horizontalni klizaÄ koji se moÅ¾e kretati lijevo ili desno, a cilj je balansirati vertikalni Å¡tap na vrhu klizaÄa dok se kreÄ‡e.

<img alt="cartpole" src="../../../../../translated_images/hr/cartpole.f52a67f27e058170.webp" width="200"/>

Za stvaranje i koriÅ¡tenje ovog okruÅ¾enja potrebno je nekoliko linija Python koda:

```python
import gym
env = gym.make("CartPole-v1")

env.reset()
done = False
total_reward = 0
while not done:
   env.render()
   action = env.action_space.sample()
   observaton, reward, done, info = env.step(action)
   total_reward += reward

print(f"Total reward: {total_reward}")
```

Svako okruÅ¾enje moÅ¾e se pristupiti na isti naÄin:
* `env.reset` zapoÄinje novi eksperiment
* `env.step` izvodi simulacijski korak. Prima **akciju** iz **prostora akcija** i vraÄ‡a **promatranje** (iz prostora promatranja), kao i nagradu i zastavicu za zavrÅ¡etak.

U gornjem primjeru izvodimo nasumiÄnu akciju u svakom koraku, zbog Äega je trajanje eksperimenta vrlo kratko:

![cartpole bez balansiranja](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-nobalance.gif)

Cilj RL algoritma je trenirati model - tzv. **politiku** &pi; - koja Ä‡e vratiti akciju kao odgovor na dano stanje. Politiku takoÄ‘er moÅ¾emo smatrati probabilistiÄkom, npr. za bilo koje stanje *s* i akciju *a* vratit Ä‡e vjerojatnost &pi;(*a*|*s*) da poduzmemo *a* u stanju *s*.

## Algoritam gradijenata politike

Najjednostavniji naÄin za modeliranje politike je stvaranje neuronske mreÅ¾e koja Ä‡e uzimati stanja kao ulaz i vraÄ‡ati odgovarajuÄ‡e akcije (ili radije vjerojatnosti svih akcija). Na neki naÄin, to bi bilo sliÄno normalnom zadatku klasifikacije, s jednom velikom razlikom - unaprijed ne znamo koje akcije trebamo poduzeti u svakom koraku.

Ideja je ovdje procijeniti te vjerojatnosti. Gradimo vektor **kumulativnih nagrada** koji pokazuje naÅ¡u ukupnu nagradu u svakom koraku eksperimenta. TakoÄ‘er primjenjujemo **diskontiranje nagrada** mnoÅ¾enjem ranijih nagrada s nekim koeficijentom &gamma;=0.99, kako bismo umanjili ulogu ranijih nagrada. Zatim pojaÄavamo one korake duÅ¾ eksperimentalnog puta koji donose veÄ‡e nagrade.

> Saznajte viÅ¡e o algoritmu gradijenata politike i pogledajte ga u akciji u [primjeru biljeÅ¾nice](CartPole-RL-TF.ipynb).

## Algoritam Actor-Critic

PoboljÅ¡ana verzija pristupa gradijenata politike naziva se **Actor-Critic**. Glavna ideja iza njega je da se neuronska mreÅ¾a trenira da vraÄ‡a dvije stvari:

* Politiku koja odreÄ‘uje koju akciju poduzeti. Ovaj dio se naziva **actor**.
* Procjenu ukupne nagrade koju moÅ¾emo oÄekivati u ovom stanju - ovaj dio se naziva **critic**.

Na neki naÄin, ova arhitektura podsjeÄ‡a na [GAN](../../4-ComputerVision/10-GANs/README.md), gdje imamo dvije mreÅ¾e koje se treniraju jedna protiv druge. U modelu actor-critic, actor predlaÅ¾e akciju koju trebamo poduzeti, a critic pokuÅ¡ava biti kritiÄan i procijeniti rezultat. MeÄ‘utim, naÅ¡ cilj je trenirati te mreÅ¾e ujedinjeno.

BuduÄ‡i da znamo i stvarne kumulativne nagrade i rezultate koje critic vraÄ‡a tijekom eksperimenta, relativno je lako izgraditi funkciju gubitka koja Ä‡e minimizirati razliku izmeÄ‘u njih. To nam daje **critic loss**. **Actor loss** moÅ¾emo izraÄunati koristeÄ‡i isti pristup kao u algoritmu gradijenata politike.

Nakon pokretanja jednog od tih algoritama, moÅ¾emo oÄekivati da Ä‡e naÅ¡ CartPole izgledati ovako:

![balansirajuÄ‡i cartpole](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-balance.gif)

## âœï¸ VjeÅ¾be: Gradijenti politike i Actor-Critic RL

Nastavite uÄiti u sljedeÄ‡im biljeÅ¾nicama:

* [RL u TensorFlowu](CartPole-RL-TF.ipynb)
* [RL u PyTorchu](CartPole-RL-PyTorch.ipynb)

## Ostali RL zadaci

PojaÄano uÄenje danas je brzo rastuÄ‡e podruÄje istraÅ¾ivanja. Neki od zanimljivih primjera pojaÄanog uÄenja su:

* PoduÄavanje raÄunala da igra **Atari igre**. Izazovni dio ovog problema je Å¡to nemamo jednostavno stanje predstavljeno kao vektor, veÄ‡ snimku zaslona - i moramo koristiti CNN za pretvaranje slike zaslona u vektor znaÄajki ili za izdvajanje informacija o nagradi. Atari igre dostupne su u Gymu.
* PoduÄavanje raÄunala da igra druÅ¡tvene igre, poput Å¡aha i Goa. Nedavno su programi poput **Alpha Zero** postigli vrhunske rezultate trenirajuÄ‡i se od nule, gdje dva agenta igraju jedan protiv drugog i poboljÅ¡avaju se u svakom koraku.
* U industriji, RL se koristi za stvaranje sustava upravljanja iz simulacije. Usluga nazvana [Bonsai](https://azure.microsoft.com/services/project-bonsai/?WT.mc_id=academic-77998-cacaste) posebno je dizajnirana za to.

## ZakljuÄak

Sada smo nauÄili kako trenirati agente da postignu dobre rezultate samo pruÅ¾anjem funkcije nagrade koja definira Å¾eljeno stanje igre i omoguÄ‡avanjem inteligentnog istraÅ¾ivanja prostora pretraÅ¾ivanja. UspjeÅ¡no smo isprobali dva algoritma i postigli dobar rezultat u relativno kratkom vremenskom razdoblju. MeÄ‘utim, ovo je tek poÄetak vaÅ¡eg putovanja u RL, i svakako biste trebali razmotriti pohaÄ‘anje zasebnog teÄaja ako Å¾elite dublje istraÅ¾iti.

## ğŸš€ Izazov

IstraÅ¾ite primjene navedene u odjeljku 'Ostali RL zadaci' i pokuÅ¡ajte implementirati jednu!

## [Kviz nakon predavanja](https://ff-quizzes.netlify.app/en/ai/quiz/44)

## Pregled i samostalno uÄenje

Saznajte viÅ¡e o klasiÄnom pojaÄanom uÄenju u naÅ¡em [kurikulumu za poÄetnike u strojnome uÄenju](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/README.md).

Pogledajte [ovaj odliÄan video](https://www.youtube.com/watch?v=qv6UVOQ0F44) o tome kako raÄunalo moÅ¾e nauÄiti igrati Super Mario.

## Zadatak: [Trenirajte Mountain Car](lab/README.md)

VaÅ¡ cilj tijekom ovog zadatka bit Ä‡e trenirati drugaÄije Gym okruÅ¾enje - [Mountain Car](https://www.gymlibrary.ml/environments/classic_control/mountain_car/).

---

