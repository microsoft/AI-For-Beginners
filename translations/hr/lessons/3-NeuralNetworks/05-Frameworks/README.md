# Okviri za neuronske mreÅ¾e

Kao Å¡to smo veÄ‡ nauÄili, da bismo uÄinkovito trenirali neuronske mreÅ¾e, moramo uÄiniti dvije stvari:

* Raditi s tenzorima, npr. mnoÅ¾iti, zbrajati i izraÄunavati funkcije poput sigmoid ili softmax
* IzraÄunati gradijente svih izraza kako bismo mogli provesti optimizaciju gradijentnog spuÅ¡tanja

## [Prethodni kviz](https://ff-quizzes.netlify.app/en/ai/quiz/9)

Iako biblioteka `numpy` moÅ¾e obaviti prvi dio, potrebna nam je neka metoda za izraÄun gradijenata. U [naÅ¡em okviru](../04-OwnFramework/OwnFramework.ipynb) koji smo razvili u prethodnom dijelu morali smo ruÄno programirati sve funkcije derivacije unutar metode `backward`, koja provodi povratnu propagaciju. Idealno bi bilo da nam okvir omoguÄ‡uje izraÄun gradijenata *bilo kojeg izraza* koji moÅ¾emo definirati.

JoÅ¡ jedna vaÅ¾na stvar je moguÄ‡nost izvoÄ‘enja izraÄuna na GPU-u ili drugim specijaliziranim jedinicama za izraÄun, poput [TPU](https://en.wikipedia.org/wiki/Tensor_Processing_Unit). Treniranje dubokih neuronskih mreÅ¾a zahtijeva *puno* izraÄuna, a moguÄ‡nost paralelizacije tih izraÄuna na GPU-ima je vrlo vaÅ¾na.

> âœ… Pojam 'paralelizacija' znaÄi raspodjelu izraÄuna na viÅ¡e ureÄ‘aja.

Trenutno su dva najpopularnija okvira za neuronske mreÅ¾e: [TensorFlow](http://TensorFlow.org) i [PyTorch](https://pytorch.org/). Oba pruÅ¾aju niskorazinski API za rad s tenzorima na CPU-u i GPU-u. Povrh niskorazinskog API-ja, postoji i visokorazinski API, nazvan [Keras](https://keras.io/) i [PyTorch Lightning](https://pytorchlightning.ai/) za odgovarajuÄ‡e okvire.

Niskorazinski API | [TensorFlow](http://TensorFlow.org) | [PyTorch](https://pytorch.org/)
------------------|-------------------------------------|--------------------------------
Visokorazinski API| [Keras](https://keras.io/) | [PyTorch Lightning](https://pytorchlightning.ai/)

**Niskorazinski API-ji** u oba okvira omoguÄ‡uju izgradnju takozvanih **raÄunalnih grafova**. Ovaj graf definira kako izraÄunati izlaz (obiÄno funkciju gubitka) s obzirom na ulazne parametre i moÅ¾e se poslati na GPU za izraÄun, ako je dostupan. Postoje funkcije za diferenciranje ovog raÄunalnog grafa i izraÄun gradijenata, koji se zatim mogu koristiti za optimizaciju parametara modela.

**Visokorazinski API-ji** tretiraju neuronske mreÅ¾e kao **sekvencu slojeva** i olakÅ¡avaju konstrukciju veÄ‡ine neuronskih mreÅ¾a. Treniranje modela obiÄno zahtijeva pripremu podataka, a zatim pozivanje funkcije `fit` za obavljanje posla.

Visokorazinski API omoguÄ‡uje brzu konstrukciju tipiÄnih neuronskih mreÅ¾a bez brige o mnogim detaljima. Istovremeno, niskorazinski API pruÅ¾a mnogo veÄ‡u kontrolu nad procesom treniranja, pa se Äesto koristi u istraÅ¾ivanjima, kada se radi s novim arhitekturama neuronskih mreÅ¾a.

TakoÄ‘er je vaÅ¾no razumjeti da moÅ¾ete koristiti oba API-ja zajedno, npr. moÅ¾ete razviti vlastitu arhitekturu sloja mreÅ¾e koristeÄ‡i niskorazinski API, a zatim je koristiti unutar veÄ‡e mreÅ¾e konstruirane i trenirane visokorazinskim API-jem. Ili moÅ¾ete definirati mreÅ¾u koristeÄ‡i visokorazinski API kao sekvencu slojeva, a zatim koristiti vlastitu niskorazinsku petlju za treniranje kako biste proveli optimizaciju. Oba API-ja koriste iste osnovne koncepte i dizajnirani su da dobro suraÄ‘uju.

## UÄenje

U ovom teÄaju nudimo veÄ‡inu sadrÅ¾aja za PyTorch i TensorFlow. MoÅ¾ete odabrati svoj preferirani okvir i proÄ‡i samo odgovarajuÄ‡e biljeÅ¾nice. Ako niste sigurni koji okvir odabrati, proÄitajte neke rasprave na internetu o **PyTorch vs. TensorFlow**. TakoÄ‘er moÅ¾ete pogledati oba okvira kako biste stekli bolji uvid.

Gdje je moguÄ‡e, koristit Ä‡emo visokorazinske API-je radi jednostavnosti. MeÄ‘utim, vjerujemo da je vaÅ¾no razumjeti kako neuronske mreÅ¾e funkcioniraju od temelja, pa na poÄetku poÄinjemo raditi s niskorazinskim API-jem i tenzorima. MeÄ‘utim, ako Å¾elite brzo krenuti i ne Å¾elite troÅ¡iti puno vremena na uÄenje ovih detalja, moÅ¾ete preskoÄiti te dijelove i odmah prijeÄ‡i na biljeÅ¾nice s visokorazinskim API-jem.

## âœï¸ VjeÅ¾be: Okviri

Nastavite svoje uÄenje u sljedeÄ‡im biljeÅ¾nicama:

Niskorazinski API | [TensorFlow+Keras BiljeÅ¾nica](IntroKerasTF.ipynb) | [PyTorch](IntroPyTorch.ipynb)
------------------|-------------------------------------|--------------------------------
Visokorazinski API| [Keras](IntroKeras.ipynb) | *PyTorch Lightning*

Nakon Å¡to savladate okvire, ponovimo pojam pretreniranja.

# Pretreniranje

Pretreniranje je iznimno vaÅ¾an koncept u strojnom uÄenju, i vrlo je vaÅ¾no razumjeti ga!

Razmotrimo sljedeÄ‡i problem aproksimacije 5 toÄaka (prikazanih kao `x` na grafovima dolje):

![linear](../../../../../translated_images/hr/overfit1.f24b71c6f652e59e.webp) | ![overfit](../../../../../translated_images/hr/overfit2.131f5800ae10ca5e.webp)
-------------------------|--------------------------
**Linearni model, 2 parametra** | **Nelinearni model, 7 parametara**
PogreÅ¡ka na treningu = 5.3 | PogreÅ¡ka na treningu = 0
PogreÅ¡ka na validaciji = 5.1 | PogreÅ¡ka na validaciji = 20

* Na lijevoj strani vidimo dobru aproksimaciju ravnom linijom. BuduÄ‡i da je broj parametara adekvatan, model ispravno razumije raspodjelu toÄaka.
* Na desnoj strani model je previÅ¡e moÄ‡an. BuduÄ‡i da imamo samo 5 toÄaka, a model ima 7 parametara, moÅ¾e se prilagoditi tako da prolazi kroz sve toÄke, Äime pogreÅ¡ka na treningu postaje 0. MeÄ‘utim, to sprjeÄava model da razumije pravi obrazac podataka, pa je pogreÅ¡ka na validaciji vrlo visoka.

Vrlo je vaÅ¾no pronaÄ‡i ispravnu ravnoteÅ¾u izmeÄ‘u sloÅ¾enosti modela (broja parametara) i broja uzoraka za treniranje.

## ZaÅ¡to dolazi do pretreniranja

  * Nedovoljno podataka za treniranje
  * PreviÅ¡e moÄ‡an model
  * PreviÅ¡e Å¡uma u ulaznim podacima

## Kako otkriti pretreniranje

Kao Å¡to moÅ¾ete vidjeti na grafu iznad, pretreniranje se moÅ¾e otkriti vrlo niskom pogreÅ¡kom na treningu i visokom pogreÅ¡kom na validaciji. ObiÄno tijekom treninga vidimo kako pogreÅ¡ke na treningu i validaciji poÄinju opadati, a zatim u nekom trenutku pogreÅ¡ka na validaciji prestaje opadati i poÄinje rasti. To Ä‡e biti znak pretreniranja i pokazatelj da bismo trebali prestati trenirati u tom trenutku (ili barem napraviti snimku modela).

![pretreniranje](../../../../../translated_images/hr/Overfitting.408ad91cd90b4371.webp)

## Kako sprijeÄiti pretreniranje

Ako primijetite da dolazi do pretreniranja, moÅ¾ete uÄiniti sljedeÄ‡e:

 * PoveÄ‡ati koliÄinu podataka za treniranje
 * Smanjiti sloÅ¾enost modela
 * Koristiti neku [tehniku regularizacije](../../4-ComputerVision/08-TransferLearning/TrainingTricks.md), poput [Dropout](../../4-ComputerVision/08-TransferLearning/TrainingTricks.md#Dropout), koju Ä‡emo kasnije razmotriti.

## Pretreniranje i kompromis pristranosti-varijance

Pretreniranje je zapravo sluÄaj opÄ‡enitijeg problema u statistici nazvanog [kompromis pristranosti-varijance](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff). Ako razmotrimo moguÄ‡e izvore pogreÅ¡ke u naÅ¡em modelu, moÅ¾emo vidjeti dvije vrste pogreÅ¡aka:

* **PogreÅ¡ke pristranosti** uzrokovane su time Å¡to naÅ¡ algoritam ne moÅ¾e pravilno uhvatiti odnos izmeÄ‘u podataka za treniranje. To moÅ¾e biti rezultat Äinjenice da naÅ¡ model nije dovoljno moÄ‡an (**podtreniranje**).
* **PogreÅ¡ke varijance**, koje su uzrokovane time Å¡to model aproksimira Å¡um u ulaznim podacima umjesto znaÄajnog odnosa (**pretreniranje**).

Tijekom treninga, pogreÅ¡ka pristranosti opada (kako naÅ¡ model uÄi aproksimirati podatke), a pogreÅ¡ka varijance raste. VaÅ¾no je zaustaviti trening - bilo ruÄno (kada otkrijemo pretreniranje) ili automatski (uvoÄ‘enjem regularizacije) - kako bismo sprijeÄili pretreniranje.

## ZakljuÄak

U ovoj lekciji nauÄili ste razlike izmeÄ‘u razliÄitih API-ja za dva najpopularnija AI okvira, TensorFlow i PyTorch. Osim toga, nauÄili ste o vrlo vaÅ¾noj temi, pretreniranje.

## ğŸš€ Izazov

U prateÄ‡im biljeÅ¾nicama pronaÄ‡i Ä‡ete 'zadatke' na dnu; proÄ‘ite kroz biljeÅ¾nice i dovrÅ¡ite zadatke.

## [Kviz nakon predavanja](https://ff-quizzes.netlify.app/en/ai/quiz/10)

## Pregled i samostalno uÄenje

IstraÅ¾ite sljedeÄ‡e teme:

- TensorFlow
- PyTorch
- Pretreniranje

Postavite si sljedeÄ‡a pitanja:

- Koja je razlika izmeÄ‘u TensorFlow-a i PyTorch-a?
- Koja je razlika izmeÄ‘u pretreniranja i podtreniranja?

## [Zadatak](lab/README.md)

U ovom laboratoriju traÅ¾i se da rijeÅ¡ite dva problema klasifikacije koristeÄ‡i jednostavne i viÅ¡eslojne potpuno povezane mreÅ¾e koristeÄ‡i PyTorch ili TensorFlow.

* [Upute](lab/README.md)
* [BiljeÅ¾nica](lab/LabFrameworks.ipynb)

---

