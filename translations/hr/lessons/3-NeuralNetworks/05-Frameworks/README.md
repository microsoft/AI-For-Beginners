<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "2b544f20b796402507fb05a0df893323",
  "translation_date": "2025-08-25T23:52:59+00:00",
  "source_file": "lessons/3-NeuralNetworks/05-Frameworks/README.md",
  "language_code": "hr"
}
-->
# Okviri za neuronske mreÅ¾e

Kao Å¡to smo veÄ‡ nauÄili, da bismo uÄinkovito trenirali neuronske mreÅ¾e, moramo uÄiniti dvije stvari:

* Raditi s tenzorima, npr. mnoÅ¾iti, zbrajati i izraÄunavati funkcije poput sigmoid ili softmax
* IzraÄunati gradijente svih izraza kako bismo mogli provesti optimizaciju gradijentnog spuÅ¡tanja

## [Pre-lecture quiz](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/105)

Iako biblioteka `numpy` moÅ¾e obaviti prvi dio, potrebna nam je neka metoda za izraÄunavanje gradijenata. U [naÅ¡em okviru](../../../../../lessons/3-NeuralNetworks/04-OwnFramework/OwnFramework.ipynb) koji smo razvili u prethodnom dijelu morali smo ruÄno programirati sve funkcije derivacija unutar metode `backward`, koja provodi povratnu propagaciju. Idealno, okvir bi nam trebao omoguÄ‡iti izraÄunavanje gradijenata *bilo kojeg izraza* koji moÅ¾emo definirati.

JoÅ¡ jedna vaÅ¾na stvar je moguÄ‡nost izvoÄ‘enja izraÄuna na GPU-u ili drugim specijaliziranim jedinicama za izraÄunavanje, poput [TPU](https://en.wikipedia.org/wiki/Tensor_Processing_Unit). Treniranje dubokih neuronskih mreÅ¾a zahtijeva *puno* izraÄuna, a moguÄ‡nost paralelizacije tih izraÄuna na GPU-ima je vrlo vaÅ¾na.

> âœ… Pojam 'paralelizacija' znaÄi raspodjelu izraÄuna na viÅ¡e ureÄ‘aja.

Trenutno su dva najpopularnija okvira za neuronske mreÅ¾e: [TensorFlow](http://TensorFlow.org) i [PyTorch](https://pytorch.org/). Oba pruÅ¾aju niskorazinski API za rad s tenzorima na CPU-u i GPU-u. Povrh niskorazinskog API-ja, postoji i visokorazinski API, nazvan [Keras](https://keras.io/) i [PyTorch Lightning](https://pytorchlightning.ai/) odgovarajuÄ‡e.

Niskorazinski API | [TensorFlow](http://TensorFlow.org) | [PyTorch](https://pytorch.org/)
------------------|-------------------------------------|--------------------------------
Visokorazinski API| [Keras](https://keras.io/) | [PyTorch Lightning](https://pytorchlightning.ai/)

**Niskorazinski API-ji** u oba okvira omoguÄ‡uju izgradnju tzv. **grafova izraÄuna**. Ovaj graf definira kako izraÄunati izlaz (obiÄno funkciju gubitka) s obzirom na ulazne parametre i moÅ¾e se poslati na GPU za izraÄunavanje, ako je dostupan. Postoje funkcije za diferenciranje ovog grafa izraÄuna i izraÄunavanje gradijenata, koji se zatim mogu koristiti za optimizaciju parametara modela.

**Visokorazinski API-ji** tretiraju neuronske mreÅ¾e kao **sekvencu slojeva** i olakÅ¡avaju konstrukciju veÄ‡ine neuronskih mreÅ¾a. Treniranje modela obiÄno zahtijeva pripremu podataka, a zatim pozivanje funkcije `fit` za obavljanje posla.

Visokorazinski API omoguÄ‡uje brzu konstrukciju tipiÄnih neuronskih mreÅ¾a bez brige o mnogim detaljima. Istovremeno, niskorazinski API pruÅ¾a mnogo viÅ¡e kontrole nad procesom treniranja, pa se Äesto koristi u istraÅ¾ivanju, kada se radi s novim arhitekturama neuronskih mreÅ¾a.

TakoÄ‘er je vaÅ¾no razumjeti da moÅ¾ete koristiti oba API-ja zajedno, npr. moÅ¾ete razviti vlastitu arhitekturu sloja mreÅ¾e koristeÄ‡i niskorazinski API, a zatim je koristiti unutar veÄ‡e mreÅ¾e konstruirane i trenirane s visokorazinskim API-jem. Ili moÅ¾ete definirati mreÅ¾u koristeÄ‡i visokorazinski API kao sekvencu slojeva, a zatim koristiti vlastitu niskorazinsku petlju treniranja za optimizaciju. Oba API-ja koriste iste osnovne koncepte i dizajnirani su da dobro suraÄ‘uju.

## UÄenje

U ovom teÄaju nudimo veÄ‡inu sadrÅ¾aja za PyTorch i TensorFlow. MoÅ¾ete odabrati svoj preferirani okvir i proÄ‡i samo odgovarajuÄ‡e biljeÅ¾nice. Ako niste sigurni koji okvir odabrati, proÄitajte neke rasprave na internetu o **PyTorch vs. TensorFlow**. TakoÄ‘er moÅ¾ete pogledati oba okvira kako biste stekli bolji uvid.

Gdje je moguÄ‡e, koristit Ä‡emo visokorazinske API-je radi jednostavnosti. MeÄ‘utim, vjerujemo da je vaÅ¾no razumjeti kako neuronske mreÅ¾e funkcioniraju od temelja, pa Ä‡emo na poÄetku raditi s niskorazinskim API-jem i tenzorima. MeÄ‘utim, ako Å¾elite brzo krenuti i ne Å¾elite troÅ¡iti puno vremena na uÄenje ovih detalja, moÅ¾ete preskoÄiti te dijelove i odmah prijeÄ‡i na biljeÅ¾nice s visokorazinskim API-jem.

## âœï¸ VjeÅ¾be: Okviri

Nastavite svoje uÄenje u sljedeÄ‡im biljeÅ¾nicama:

Niskorazinski API | [TensorFlow+Keras Notebook](../../../../../lessons/3-NeuralNetworks/05-Frameworks/IntroKerasTF.ipynb) | [PyTorch](../../../../../lessons/3-NeuralNetworks/05-Frameworks/IntroPyTorch.ipynb)
------------------|-------------------------------------|--------------------------------
Visokorazinski API| [Keras](../../../../../lessons/3-NeuralNetworks/05-Frameworks/IntroKeras.ipynb) | *PyTorch Lightning*

Nakon Å¡to savladate okvire, ponovimo pojam pretreniranja.

# Pretreniranje

Pretreniranje je iznimno vaÅ¾an koncept u strojnom uÄenju i vrlo je vaÅ¾no razumjeti ga!

Razmotrimo sljedeÄ‡i problem aproksimacije 5 toÄaka (predstavljenih s `x` na grafovima dolje):

![linear](../../../../../translated_images/overfit1.f24b71c6f652e59e6bed7245ffbeaecc3ba320e16e2221f6832b432052c4da43.hr.jpg) | ![overfit](../../../../../translated_images/overfit2.131f5800ae10ca5e41d12a411f5f705d9ee38b1b10916f284b787028dd55cc1c.hr.jpg)
-------------------------|--------------------------
**Linearni model, 2 parametra** | **Nelinearni model, 7 parametara**
PogreÅ¡ka na treningu = 5.3 | PogreÅ¡ka na treningu = 0
PogreÅ¡ka na validaciji = 5.1 | PogreÅ¡ka na validaciji = 20

* Na lijevoj strani vidimo dobru aproksimaciju ravnom linijom. BuduÄ‡i da je broj parametara adekvatan, model ispravno razumije raspodjelu toÄaka.
* Na desnoj strani model je previÅ¡e moÄ‡an. BuduÄ‡i da imamo samo 5 toÄaka, a model ima 7 parametara, moÅ¾e se prilagoditi tako da prolazi kroz sve toÄke, ÄineÄ‡i pogreÅ¡ku na treningu 0. MeÄ‘utim, to sprjeÄava model da razumije ispravan obrazac podataka, pa je pogreÅ¡ka na validaciji vrlo visoka.

Vrlo je vaÅ¾no postiÄ‡i ispravnu ravnoteÅ¾u izmeÄ‘u bogatstva modela (broja parametara) i broja uzoraka za trening.

## ZaÅ¡to dolazi do pretreniranja

  * Nedovoljno podataka za trening
  * PreviÅ¡e moÄ‡an model
  * PreviÅ¡e Å¡uma u ulaznim podacima

## Kako otkriti pretreniranje

Kao Å¡to moÅ¾ete vidjeti na grafu iznad, pretreniranje se moÅ¾e otkriti vrlo niskom pogreÅ¡kom na treningu i visokom pogreÅ¡kom na validaciji. ObiÄno tijekom treninga vidimo kako pogreÅ¡ke na treningu i validaciji poÄinju opadati, a zatim u nekom trenutku pogreÅ¡ka na validaciji moÅ¾e prestati opadati i poÄeti rasti. To Ä‡e biti znak pretreniranja i pokazatelj da bismo trebali zaustaviti trening u tom trenutku (ili barem napraviti snimku modela).

![overfitting](../../../../../translated_images/Overfitting.408ad91cd90b4371d0a81f4287e1409c359751adeb1ae450332af50e84f08c3e.hr.png)

## Kako sprijeÄiti pretreniranje

Ako primijetite da dolazi do pretreniranja, moÅ¾ete uÄiniti sljedeÄ‡e:

 * PoveÄ‡ati koliÄinu podataka za trening
 * Smanjiti sloÅ¾enost modela
 * Koristiti neku [tehniku regularizacije](../../4-ComputerVision/08-TransferLearning/TrainingTricks.md), poput [Dropout](../../4-ComputerVision/08-TransferLearning/TrainingTricks.md#Dropout), koju Ä‡emo kasnije razmotriti.

## Pretreniranje i kompromis pristranosti-varijance

Pretreniranje je zapravo sluÄaj opÄ‡enitijeg problema u statistici nazvanog [kompromis pristranosti-varijance](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff). Ako razmotrimo moguÄ‡e izvore pogreÅ¡ke u naÅ¡em modelu, moÅ¾emo vidjeti dvije vrste pogreÅ¡aka:

* **PogreÅ¡ke pristranosti** uzrokovane su time Å¡to naÅ¡ algoritam ne moÅ¾e ispravno uhvatiti odnos izmeÄ‘u podataka za trening. To moÅ¾e biti rezultat Äinjenice da naÅ¡ model nije dovoljno moÄ‡an (**nedovoljno treniranje**).
* **PogreÅ¡ke varijance**, koje su uzrokovane time Å¡to model aproksimira Å¡um u ulaznim podacima umjesto znaÄajnog odnosa (**pretreniranje**).

Tijekom treninga, pogreÅ¡ka pristranosti opada (kako naÅ¡ model uÄi aproksimirati podatke), a pogreÅ¡ka varijance raste. VaÅ¾no je zaustaviti trening - bilo ruÄno (kada otkrijemo pretreniranje) ili automatski (uvoÄ‘enjem regularizacije) - kako bismo sprijeÄili pretreniranje.

## ZakljuÄak

U ovoj lekciji nauÄili ste o razlikama izmeÄ‘u razliÄitih API-ja za dva najpopularnija AI okvira, TensorFlow i PyTorch. Osim toga, nauÄili ste o vrlo vaÅ¾noj temi, pretreniranje.

## ğŸš€ Izazov

U prateÄ‡im biljeÅ¾nicama pronaÄ‡i Ä‡ete 'zadaci' na dnu; proÄ‘ite kroz biljeÅ¾nice i dovrÅ¡ite zadatke.

## [Post-lecture quiz](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/205)

## Pregled i samostalno uÄenje

Provedite istraÅ¾ivanje o sljedeÄ‡im temama:

- TensorFlow
- PyTorch
- Pretreniranje

Postavite si sljedeÄ‡a pitanja:

- Koja je razlika izmeÄ‘u TensorFlow i PyTorch?
- Koja je razlika izmeÄ‘u pretreniranja i nedovoljnog treniranja?

## [Zadatak](lab/README.md)

U ovom laboratoriju traÅ¾i se da rijeÅ¡ite dva problema klasifikacije koristeÄ‡i jednostavne i viÅ¡eslojne potpuno povezane mreÅ¾e koristeÄ‡i PyTorch ili TensorFlow.

* [Upute](lab/README.md)
* [BiljeÅ¾nica](../../../../../lessons/3-NeuralNetworks/05-Frameworks/lab/LabFrameworks.ipynb)

**Odricanje od odgovornosti**:  
Ovaj dokument je preveden pomoÄ‡u AI usluge za prevoÄ‘enje [Co-op Translator](https://github.com/Azure/co-op-translator). Iako nastojimo osigurati toÄnost, imajte na umu da automatski prijevodi mogu sadrÅ¾avati pogreÅ¡ke ili netoÄnosti. Izvorni dokument na izvornom jeziku treba smatrati autoritativnim izvorom. Za kritiÄne informacije preporuÄuje se profesionalni prijevod od strane Äovjeka. Ne preuzimamo odgovornost za nesporazume ili pogreÅ¡na tumaÄenja koja mogu proizaÄ‡i iz koriÅ¡tenja ovog prijevoda.