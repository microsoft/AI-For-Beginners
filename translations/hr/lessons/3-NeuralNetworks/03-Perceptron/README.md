<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "0c37770bba4fff3c71dc00eb261ee61b",
  "translation_date": "2025-08-25T23:58:43+00:00",
  "source_file": "lessons/3-NeuralNetworks/03-Perceptron/README.md",
  "language_code": "hr"
}
-->
# Uvod u neuronske mreÅ¾e: Perceptron

## [Kviz prije predavanja](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/103)

Jedan od prvih pokuÅ¡aja implementacije neÄega sliÄnog modernoj neuronskoj mreÅ¾i napravio je Frank Rosenblatt iz Cornell Aeronautical Laboratory 1957. godine. Bila je to hardverska implementacija nazvana "Mark-1", dizajnirana za prepoznavanje primitivnih geometrijskih oblika, poput trokuta, kvadrata i krugova.

|      |      |
|--------------|-----------|
|<img src='images/Rosenblatt-wikipedia.jpg' alt='Frank Rosenblatt'/> | <img src='images/Mark_I_perceptron_wikipedia.jpg' alt='The Mark 1 Perceptron' />|

> Slike [s Wikipedije](https://en.wikipedia.org/wiki/Perceptron)

Ulazna slika predstavljena je nizom fotocelija dimenzija 20x20, tako da je neuronska mreÅ¾a imala 400 ulaza i jedan binarni izlaz. Jednostavna mreÅ¾a sadrÅ¾avala je jedan neuron, koji se takoÄ‘er naziva **jedinica logiÄkog praga**. TeÅ¾ine neuronske mreÅ¾e djelovale su poput potenciometara koji su zahtijevali ruÄno podeÅ¡avanje tijekom faze treniranja.

> âœ… Potenciometar je ureÄ‘aj koji omoguÄ‡uje korisniku podeÅ¡avanje otpora u strujnom krugu.

> The New York Times je tada pisao o perceptronu: *embrij elektroniÄkog raÄunala za koje [mornarica] oÄekuje da Ä‡e moÄ‡i hodati, govoriti, vidjeti, pisati, reproducirati se i biti svjesno svog postojanja.*

## Model perceptrona

Pretpostavimo da imamo N znaÄajki u naÅ¡em modelu, u kojem sluÄaju bi ulazni vektor bio vektor veliÄine N. Perceptron je model za **binarnu klasifikaciju**, tj. moÅ¾e razlikovati dvije klase ulaznih podataka. Pretpostavit Ä‡emo da za svaki ulazni vektor x izlaz naÅ¡eg perceptrona moÅ¾e biti ili +1 ili -1, ovisno o klasi. Izlaz Ä‡e se izraÄunati pomoÄ‡u formule:

y(x) = f(w<sup>T</sup>x)

gdje je f funkcija aktivacije koraka

<!-- img src="http://www.sciweavers.org/tex2img.php?eq=f%28x%29%20%3D%20%5Cbegin%7Bcases%7D%0A%20%20%20%20%20%20%20%20%20%2B1%20%26%20x%20%5Cgeq%200%20%5C%5C%0A%20%20%20%20%20%20%20%20%20-1%20%26%20x%20%3C%200%0A%20%20%20%20%20%20%20%5Cend%7Bcases%7D%20%5C%5C%0A&bc=White&fc=Black&im=jpg&fs=12&ff=arev&edit=0" align="center" border="0" alt="f(x) = \begin{cases} +1 & x \geq 0 \\ -1 & x < 0 \end{cases} \\" width="154" height="50" / -->
<img src="images/activation-func.png"/>

## Treniranje perceptrona

Za treniranje perceptrona potrebno je pronaÄ‡i vektor teÅ¾ina w koji klasificira veÄ‡inu vrijednosti ispravno, tj. rezultira najmanjom **pogreÅ¡kom**. Ova pogreÅ¡ka E definirana je **kriterijem perceptrona** na sljedeÄ‡i naÄin:

E(w) = -âˆ‘w<sup>T</sup>x<sub>i</sub>t<sub>i</sub>

gdje:

* zbroj se uzima za one toÄke skupa za treniranje i koje rezultiraju pogreÅ¡nom klasifikacijom
* x<sub>i</sub> su ulazni podaci, a t<sub>i</sub> je ili -1 ili +1 za negativne i pozitivne primjere.

Ovaj kriterij se smatra funkcijom teÅ¾ina w, i potrebno ga je minimizirati. ÄŒesto se koristi metoda nazvana **gradijentni spust**, u kojoj zapoÄinjemo s nekim poÄetnim teÅ¾inama w<sup>(0)</sup>, a zatim u svakom koraku aÅ¾uriramo teÅ¾ine prema formuli:

w<sup>(t+1)</sup> = w<sup>(t)</sup> - Î·âˆ‡E(w)

Ovdje je Î· tzv. **stopa uÄenja**, a âˆ‡E(w) oznaÄava **gradijent** funkcije E. Nakon izraÄuna gradijenta, dobivamo:

w<sup>(t+1)</sup> = w<sup>(t)</sup> + âˆ‘Î·x<sub>i</sub>t<sub>i</sub>

Algoritam u Pythonu izgleda ovako:

```python
def train(positive_examples, negative_examples, num_iterations = 100, eta = 1):

    weights = [0,0,0] # Initialize weights (almost randomly :)
        
    for i in range(num_iterations):
        pos = random.choice(positive_examples)
        neg = random.choice(negative_examples)

        z = np.dot(pos, weights) # compute perceptron output
        if z < 0: # positive example classified as negative
            weights = weights + eta*weights.shape

        z  = np.dot(neg, weights)
        if z >= 0: # negative example classified as positive
            weights = weights - eta*weights.shape

    return weights
```

## ZakljuÄak

U ovoj lekciji nauÄili ste o perceptronu, koji je model za binarnu klasifikaciju, i kako ga trenirati koristeÄ‡i vektor teÅ¾ina.

## ğŸš€ Izazov

Ako Å¾elite pokuÅ¡ati izraditi vlastiti perceptron, isprobajte [ovu radionicu na Microsoft Learn](https://docs.microsoft.com/en-us/azure/machine-learning/component-reference/two-class-averaged-perceptron?WT.mc_id=academic-77998-cacaste) koja koristi [Azure ML designer](https://docs.microsoft.com/en-us/azure/machine-learning/concept-designer?WT.mc_id=academic-77998-cacaste).

## [Kviz nakon predavanja](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/203)

## Pregled i samostalno uÄenje

Da biste vidjeli kako moÅ¾emo koristiti perceptron za rjeÅ¡avanje jednostavnih problema, kao i problema iz stvarnog Å¾ivota, i nastavili uÄiti - pogledajte biljeÅ¾nicu [Perceptron](../../../../../lessons/3-NeuralNetworks/03-Perceptron/Perceptron.ipynb).

Evo zanimljivog [Älanka o perceptronima](https://towardsdatascience.com/what-is-a-perceptron-basics-of-neural-networks-c4cfea20c590).

## [Zadatak](lab/README.md)

U ovoj lekciji implementirali smo perceptron za zadatak binarne klasifikacije i koristili ga za klasifikaciju izmeÄ‘u dvije rukom pisane znamenke. U ovom laboratoriju od vas se traÅ¾i da rijeÅ¡ite problem klasifikacije znamenki u potpunosti, tj. odredite koja znamenka najvjerojatnije odgovara danoj slici.

* [Upute](lab/README.md)
* [BiljeÅ¾nica](../../../../../lessons/3-NeuralNetworks/03-Perceptron/lab/PerceptronMultiClass.ipynb)

**Odricanje od odgovornosti**:  
Ovaj dokument je preveden pomoÄ‡u AI usluge za prevoÄ‘enje [Co-op Translator](https://github.com/Azure/co-op-translator). Iako nastojimo osigurati toÄnost, imajte na umu da automatski prijevodi mogu sadrÅ¾avati pogreÅ¡ke ili netoÄnosti. Izvorni dokument na izvornom jeziku treba smatrati autoritativnim izvorom. Za kljuÄne informacije preporuÄuje se profesionalni prijevod od strane Äovjeka. Ne preuzimamo odgovornost za nesporazume ili pogreÅ¡na tumaÄenja koja mogu proizaÄ‡i iz koriÅ¡tenja ovog prijevoda.