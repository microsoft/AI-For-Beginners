<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "186bf7eeab776b36f557357ea56d4751",
  "translation_date": "2025-08-25T23:47:04+00:00",
  "source_file": "lessons/3-NeuralNetworks/04-OwnFramework/README.md",
  "language_code": "hr"
}
-->
# Uvod u neuronske mreÅ¾e. ViÅ¡eslojni perceptron

U prethodnom dijelu nauÄili ste o najjednostavnijem modelu neuronske mreÅ¾e - jednoslojnom perceptronu, linearnom modelu za klasifikaciju s dvije klase.

U ovom dijelu proÅ¡irit Ä‡emo ovaj model u fleksibilniji okvir koji nam omoguÄ‡uje:

* izvoÄ‘enje **viÅ¡eklasne klasifikacije** uz klasifikaciju s dvije klase
* rjeÅ¡avanje **regresijskih problema** uz klasifikaciju
* razdvajanje klasa koje nisu linearno razdvojive

TakoÄ‘er Ä‡emo razviti vlastiti modularni okvir u Pythonu koji Ä‡e nam omoguÄ‡iti konstrukciju razliÄitih arhitektura neuronskih mreÅ¾a.

## [Kviz prije predavanja](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/104)

## Formalizacija strojnog uÄenja

PoÄnimo s formalizacijom problema strojnog uÄenja. Pretpostavimo da imamo skup podataka za treniranje **X** s oznakama **Y**, i trebamo izgraditi model *f* koji Ä‡e davati najtoÄnija predviÄ‘anja. Kvaliteta predviÄ‘anja mjeri se pomoÄ‡u **funkcije gubitka** â„’. SljedeÄ‡e funkcije gubitka Äesto se koriste:

* Za regresijske probleme, kada trebamo predvidjeti broj, moÅ¾emo koristiti **apsolutnu pogreÅ¡ku** âˆ‘<sub>i</sub>|f(x<sup>(i)</sup>)-y<sup>(i)</sup>| ili **kvadratnu pogreÅ¡ku** âˆ‘<sub>i</sub>(f(x<sup>(i)</sup>)-y<sup>(i)</sup>)<sup>2</sup>
* Za klasifikaciju koristimo **0-1 gubitak** (Å¡to je u suÅ¡tini isto kao i **toÄnost** modela) ili **logistiÄki gubitak**.

Za jednoslojni perceptron, funkcija *f* definirana je kao linearna funkcija *f(x)=wx+b* (gdje je *w* matrica teÅ¾ina, *x* vektor ulaznih znaÄajki, a *b* vektor pomaka). Za razliÄite arhitekture neuronskih mreÅ¾a, ova funkcija moÅ¾e poprimiti sloÅ¾eniji oblik.

> U sluÄaju klasifikacije, Äesto je poÅ¾eljno dobiti vjerojatnosti odgovarajuÄ‡ih klasa kao izlaz mreÅ¾e. Kako bismo proizvoljne brojeve pretvorili u vjerojatnosti (npr. normalizirali izlaz), Äesto koristimo funkciju **softmax** Ïƒ, pa funkcija *f* postaje *f(x)=Ïƒ(wx+b)*.

U gore navedenoj definiciji *f*, *w* i *b* nazivaju se **parametri** Î¸=âŸ¨*w,b*âŸ©. S obzirom na skup podataka âŸ¨**X**,**Y**âŸ©, moÅ¾emo izraÄunati ukupnu pogreÅ¡ku na cijelom skupu podataka kao funkciju parametara Î¸.

> âœ… **Cilj treniranja neuronske mreÅ¾e je minimizirati pogreÅ¡ku mijenjanjem parametara Î¸**

## Optimizacija gradijentnim spuÅ¡tanjem

Postoji dobro poznata metoda optimizacije funkcija nazvana **gradijentno spuÅ¡tanje**. Ideja je da moÅ¾emo izraÄunati derivaciju (u viÅ¡edimenzionalnom sluÄaju nazvanu **gradijent**) funkcije gubitka u odnosu na parametre i mijenjati parametre na naÄin da se pogreÅ¡ka smanji. Ovo se moÅ¾e formalizirati na sljedeÄ‡i naÄin:

* Inicijalizirajte parametre nekim sluÄajnim vrijednostima w<sup>(0)</sup>, b<sup>(0)</sup>
* Ponavljajte sljedeÄ‡i korak viÅ¡e puta:
    - w<sup>(i+1)</sup> = w<sup>(i)</sup>-Î·âˆ‚â„’/âˆ‚w
    - b<sup>(i+1)</sup> = b<sup>(i)</sup>-Î·âˆ‚â„’/âˆ‚b

Tijekom treniranja, koraci optimizacije trebali bi se raÄunati uzimajuÄ‡i u obzir cijeli skup podataka (sjetite se da se gubitak raÄuna kao zbroj kroz sve uzorke za treniranje). MeÄ‘utim, u stvarnosti uzimamo male dijelove skupa podataka nazvane **minibatch-evi** i raÄunamo gradijente na temelju podskupa podataka. BuduÄ‡i da se podskup uzima sluÄajno svaki put, takva metoda naziva se **stohastiÄko gradijentno spuÅ¡tanje** (SGD).

## ViÅ¡eslojni perceptroni i povratna propagacija

Jednoslojna mreÅ¾a, kao Å¡to smo vidjeli, moÅ¾e klasificirati linearno razdvojive klase. Kako bismo izgradili bogatiji model, moÅ¾emo kombinirati nekoliko slojeva mreÅ¾e. MatematiÄki, to bi znaÄilo da funkcija *f* ima sloÅ¾eniji oblik i raÄuna se u nekoliko koraka:
* z<sub>1</sub>=w<sub>1</sub>x+b<sub>1</sub>
* z<sub>2</sub>=w<sub>2</sub>Î±(z<sub>1</sub>)+b<sub>2</sub>
* f = Ïƒ(z<sub>2</sub>)

Ovdje je Î± **nelinearna aktivacijska funkcija**, Ïƒ je softmax funkcija, a parametri Î¸=<*w<sub>1</sub>,b<sub>1</sub>,w<sub>2</sub>,b<sub>2</sub>*>.

Algoritam gradijentnog spuÅ¡tanja ostaje isti, ali postaje sloÅ¾enije izraÄunati gradijente. Prema pravilu lanÄane derivacije, moÅ¾emo izraÄunati derivacije kao:

* âˆ‚â„’/âˆ‚w<sub>2</sub> = (âˆ‚â„’/âˆ‚Ïƒ)(âˆ‚Ïƒ/âˆ‚z<sub>2</sub>)(âˆ‚z<sub>2</sub>/âˆ‚w<sub>2</sub>)
* âˆ‚â„’/âˆ‚w<sub>1</sub> = (âˆ‚â„’/âˆ‚Ïƒ)(âˆ‚Ïƒ/âˆ‚z<sub>2</sub>)(âˆ‚z<sub>2</sub>/âˆ‚Î±)(âˆ‚Î±/âˆ‚z<sub>1</sub>)(âˆ‚z<sub>1</sub>/âˆ‚w<sub>1</sub>)

> âœ… Pravilo lanÄane derivacije koristi se za izraÄunavanje derivacija funkcije gubitka u odnosu na parametre.

Primijetite da je lijevi dio svih tih izraza isti, pa moÅ¾emo uÄinkovito izraÄunavati derivacije poÄevÅ¡i od funkcije gubitka i iduÄ‡i "unatrag" kroz raÄunski graf. Stoga se metoda treniranja viÅ¡eslojnog perceptrona naziva **povratna propagacija**, ili 'backprop'.

<img alt="raÄunski graf" src="images/ComputeGraphGrad.png"/>

> TODO: citiranje slike

> âœ… Povratnu propagaciju obradit Ä‡emo detaljnije u naÅ¡em primjeru u biljeÅ¾nici.  

## ZakljuÄak

U ovoj lekciji izgradili smo vlastitu biblioteku za neuronske mreÅ¾e i koristili je za jednostavan zadatak klasifikacije u dvodimenzionalnom prostoru.

## ğŸš€ Izazov

U prateÄ‡oj biljeÅ¾nici implementirat Ä‡ete vlastiti okvir za izgradnju i treniranje viÅ¡eslojnih perceptrona. MoÄ‡i Ä‡ete detaljno vidjeti kako moderne neuronske mreÅ¾e funkcioniraju.

PrijeÄ‘ite na biljeÅ¾nicu [OwnFramework](../../../../../lessons/3-NeuralNetworks/04-OwnFramework/OwnFramework.ipynb) i proÄ‘ite kroz nju.

## [Kviz nakon predavanja](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/204)

## Pregled i samostalno uÄenje

Povratna propagacija je uobiÄajeni algoritam koji se koristi u umjetnoj inteligenciji i strojnome uÄenju, vrijedan detaljnijeg prouÄavanja [ovdje](https://wikipedia.org/wiki/Backpropagation).

## [Zadatak](lab/README.md)

U ovom laboratoriju traÅ¾i se da koristite okvir koji ste izgradili u ovoj lekciji za rjeÅ¡avanje klasifikacije rukom pisanih znamenki iz skupa podataka MNIST.

* [Upute](lab/README.md)
* [BiljeÅ¾nica](../../../../../lessons/3-NeuralNetworks/04-OwnFramework/lab/MyFW_MNIST.ipynb)

**Odricanje od odgovornosti**:  
Ovaj dokument je preveden pomoÄ‡u AI usluge za prevoÄ‘enje [Co-op Translator](https://github.com/Azure/co-op-translator). Iako nastojimo osigurati toÄnost, imajte na umu da automatski prijevodi mogu sadrÅ¾avati pogreÅ¡ke ili netoÄnosti. Izvorni dokument na izvornom jeziku treba smatrati autoritativnim izvorom. Za kritiÄne informacije preporuÄuje se profesionalni prijevod od strane Äovjeka. Ne preuzimamo odgovornost za nesporazume ili pogreÅ¡na tumaÄenja koja mogu proizaÄ‡i iz koriÅ¡tenja ovog prijevoda.