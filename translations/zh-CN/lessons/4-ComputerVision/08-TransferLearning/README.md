# 预训练网络与迁移学习

训练 CNN 需要耗费大量时间，并且需要大量数据。然而，大部分时间都花在学习网络可以用来从图像中提取模式的最佳低级滤波器上。一个自然的问题是：我们是否可以使用一个在某个数据集上训练过的神经网络，并将其适配到不同的图像分类任务，而无需完整的训练过程？

## [课前测验](https://ff-quizzes.netlify.app/en/ai/quiz/15)

这种方法被称为**迁移学习**，因为我们将一个神经网络模型的一些知识转移到另一个模型中。在迁移学习中，我们通常从一个预训练模型开始，该模型已经在某个大型图像数据集（例如 **ImageNet**）上进行了训练。这些模型已经能够很好地从通用图像中提取不同的特征，在许多情况下，仅仅在这些提取的特征之上构建一个分类器就可以取得不错的效果。

> ✅ 迁移学习是一个在其他学术领域（如教育）中也会遇到的术语，它指的是将一个领域的知识应用到另一个领域的过程。

## 预训练模型作为特征提取器

我们在上一节中讨论的卷积网络包含多个层，每一层都旨在从图像中提取一些特征，从低级像素组合（例如水平/垂直线或笔画）到高级特征组合（例如火焰的眼睛）。如果我们在足够大的通用和多样化图像数据集上训练 CNN，网络应该能够学习提取这些常见特征。

Keras 和 PyTorch 都包含函数，可以轻松加载一些常见架构的预训练神经网络权重，这些权重大多是在 ImageNet 图像上训练的。最常用的架构在上一课的 [CNN Architectures](../07-ConvNets/CNN_Architectures.md) 页面中有描述。特别是，你可以考虑使用以下模型之一：

* **VGG-16/VGG-19** 是相对简单的模型，但仍然具有良好的准确性。通常使用 VGG 作为初步尝试是一个不错的选择，可以看看迁移学习的效果如何。
* **ResNet** 是微软研究院在 2015 年提出的一系列模型。它们有更多的层，因此需要更多资源。
* **MobileNet** 是一系列尺寸较小的模型，适合移动设备。如果资源有限并且可以牺牲一些准确性，可以使用它们。

以下是 VGG-16 网络从一张猫的图片中提取的示例特征：

![VGG-16 提取的特征](../../../../../translated_images/zh-CN/features.6291f9c7ba3a0b95.webp)

## 猫与狗数据集

在这个例子中，我们将使用一个[猫与狗](https://www.microsoft.com/download/details.aspx?id=54765&WT.mc_id=academic-77998-cacaste)数据集，这非常接近真实的图像分类场景。

## ✍️ 练习：迁移学习

让我们在相关的笔记本中看看迁移学习的实际应用：

* [迁移学习 - PyTorch](TransferLearningPyTorch.ipynb)
* [迁移学习 - TensorFlow](TransferLearningTF.ipynb)

## 可视化对抗性猫

预训练的神经网络在其“脑海”中包含了不同的模式，包括**理想猫**（以及理想狗、理想斑马等）的概念。我们可以尝试以某种方式**可视化这个图像**。然而，这并不简单，因为这些模式分布在网络权重的各个部分，并且以层次结构组织。

我们可以采取的一种方法是从一个随机图像开始，然后尝试使用**梯度下降优化**技术调整该图像，使网络认为它是一只猫。

![图像优化循环](../../../../../translated_images/zh-CN/ideal-cat-loop.999fbb8ff306e044.webp)

然而，如果我们这样做，我们会得到一些非常类似于随机噪声的东西。这是因为*有很多方法可以让网络认为输入图像是一只猫*，包括一些在视觉上没有意义的方式。虽然这些图像包含了许多典型的猫的模式，但没有任何约束使它们在视觉上具有辨识度。

为了改善结果，我们可以在损失函数中添加另一个项，称为**变化损失**。它是一种度量，显示图像中相邻像素的相似程度。最小化变化损失可以使图像更平滑，并消除噪声，从而揭示更具视觉吸引力的模式。以下是一些这样的“理想”图像示例，它们被高概率分类为猫和斑马：

![理想猫](../../../../../translated_images/zh-CN/ideal-cat.203dd4597643d6b0.webp) | ![理想斑马](../../../../../translated_images/zh-CN/ideal-zebra.7f70e8b54ee15a7a.webp)
-----|-----
*理想猫* | *理想斑马*

类似的方法可以用来对神经网络进行所谓的**对抗性攻击**。假设我们想欺骗一个神经网络，让一只狗看起来像一只猫。如果我们拿一张被网络识别为狗的狗的图片，然后稍微调整它，直到网络开始将其分类为猫：

![狗的图片](../../../../../translated_images/zh-CN/original-dog.8f68a67d2fe0911f.webp) | ![被分类为猫的狗的图片](../../../../../translated_images/zh-CN/adversarial-dog.d9fc7773b0142b89.webp)
-----|-----
*狗的原始图片* | *被分类为猫的狗的图片*

查看以下笔记本中的代码以重现上述结果：

* [理想与对抗性猫 - TensorFlow](AdversarialCat_TF.ipynb)

## 总结

通过迁移学习，你可以快速构建一个用于自定义对象分类任务的分类器，并获得较高的准确性。可以看到，我们现在解决的更复杂任务需要更高的计算能力，无法轻松在 CPU 上完成。在下一单元中，我们将尝试使用更轻量化的实现来训练相同的模型，使用较低的计算资源，结果仅稍微降低准确性。

## 🚀 挑战

在配套的笔记本中，底部有关于迁移知识在某些相似训练数据（例如新类型的动物）中效果最佳的注释。尝试使用完全不同类型的图像进行实验，看看你的迁移知识模型表现得如何。

## [课后测验](https://ff-quizzes.netlify.app/en/ai/quiz/16)

## 复习与自学

阅读 [TrainingTricks.md](TrainingTricks.md)，深入了解其他训练模型的方法。

## [作业](lab/README.md)

在这个实验中，我们将使用真实的 [Oxford-IIIT](https://www.robots.ox.ac.uk/~vgg/data/pets/) 宠物数据集，其中包含 35 种猫和狗的品种，并构建一个迁移学习分类器。

---

