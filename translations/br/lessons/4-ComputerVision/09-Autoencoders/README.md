<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "0b306c04f5337b6e7430e5c0b16bb5c0",
  "translation_date": "2025-08-26T09:08:22+00:00",
  "source_file": "lessons/4-ComputerVision/09-Autoencoders/README.md",
  "language_code": "br"
}
-->
# Autoencoders

Ao treinar CNNs, um dos problemas √© que precisamos de muitos dados rotulados. No caso de classifica√ß√£o de imagens, precisamos separar as imagens em diferentes classes, o que exige esfor√ßo manual.

## [Pre-lecture quiz](https://ff-quizzes.netlify.app/en/ai/quiz/17)

No entanto, podemos querer usar dados brutos (n√£o rotulados) para treinar extratores de caracter√≠sticas de CNN, o que √© chamado de **aprendizado auto-supervisionado**. Em vez de r√≥tulos, usaremos imagens de treinamento como entrada e sa√≠da da rede. A ideia principal do **autoencoder** √© que teremos uma **rede codificadora** que converte a imagem de entrada em algum **espa√ßo latente** (normalmente √© apenas um vetor de tamanho menor), e ent√£o uma **rede decodificadora**, cujo objetivo ser√° reconstruir a imagem original.

> ‚úÖ Um [autoencoder](https://wikipedia.org/wiki/Autoencoder) √© "um tipo de rede neural artificial usada para aprender codifica√ß√µes eficientes de dados n√£o rotulados."

Como estamos treinando um autoencoder para capturar o m√°ximo de informa√ß√µes da imagem original poss√≠vel para uma reconstru√ß√£o precisa, a rede tenta encontrar a melhor **representa√ß√£o** das imagens de entrada para capturar seu significado.

![Diagrama do Autoencoder](../../../../../translated_images/autoencoder_schema.5e6fc9ad98a5eb6197f3513cf3baf4dfbe1389a6ae74daebda64de9f1c99f142.br.jpg)

> Imagem do [blog do Keras](https://blog.keras.io/building-autoencoders-in-keras.html)

## Cen√°rios para o uso de Autoencoders

Embora reconstruir imagens originais possa n√£o parecer √∫til por si s√≥, existem alguns cen√°rios em que os autoencoders s√£o especialmente √∫teis:

* **Reduzir a dimensionalidade de imagens para visualiza√ß√£o** ou **treinar representa√ß√µes de imagens**. Geralmente, autoencoders oferecem melhores resultados do que PCA, porque levam em conta a natureza espacial das imagens e caracter√≠sticas hier√°rquicas.
* **Remo√ß√£o de ru√≠do**, ou seja, eliminar ru√≠dos da imagem. Como o ru√≠do carrega muitas informa√ß√µes in√∫teis, o autoencoder n√£o consegue encaixar tudo no espa√ßo latente relativamente pequeno e, assim, captura apenas a parte importante da imagem. Ao treinar removedores de ru√≠do, come√ßamos com imagens originais e usamos imagens com ru√≠do artificialmente adicionado como entrada para o autoencoder.
* **Super-resolu√ß√£o**, aumentando a resolu√ß√£o da imagem. Come√ßamos com imagens de alta resolu√ß√£o e usamos a imagem com resolu√ß√£o mais baixa como entrada do autoencoder.
* **Modelos generativos**. Uma vez treinado o autoencoder, a parte decodificadora pode ser usada para criar novos objetos a partir de vetores latentes aleat√≥rios.

## Autoencoders Variacionais (VAE)

Autoencoders tradicionais reduzem a dimensionalidade dos dados de entrada de alguma forma, identificando as caracter√≠sticas importantes das imagens de entrada. No entanto, os vetores latentes frequentemente n√£o fazem muito sentido. Em outras palavras, usando o conjunto de dados MNIST como exemplo, identificar quais d√≠gitos correspondem a diferentes vetores latentes n√£o √© uma tarefa f√°cil, porque vetores latentes pr√≥ximos n√£o necessariamente correspondem aos mesmos d√≠gitos.

Por outro lado, para treinar modelos *generativos*, √© melhor ter algum entendimento do espa√ßo latente. Essa ideia nos leva ao **autoencoder variacional** (VAE).

O VAE √© um autoencoder que aprende a prever a *distribui√ß√£o estat√≠stica* dos par√¢metros latentes, chamada de **distribui√ß√£o latente**. Por exemplo, podemos querer que os vetores latentes sejam distribu√≠dos normalmente com uma m√©dia z<sub>mean</sub> e um desvio padr√£o z<sub>sigma</sub> (tanto a m√©dia quanto o desvio padr√£o s√£o vetores de alguma dimensionalidade d). O codificador no VAE aprende a prever esses par√¢metros, e ent√£o o decodificador pega um vetor aleat√≥rio dessa distribui√ß√£o para reconstruir o objeto.

Resumindo:

 * A partir do vetor de entrada, prevemos `z_mean` e `z_log_sigma` (em vez de prever o desvio padr√£o diretamente, prevemos seu logaritmo)
 * Amostramos um vetor `sample` da distribui√ß√£o N(z<sub>mean</sub>,exp(z<sub>log\_sigma</sub>))
 * O decodificador tenta decodificar a imagem original usando `sample` como vetor de entrada

 <img src="images/vae.png" width="50%">

> Imagem deste [post no blog](https://ijdykeman.github.io/ml/2016/12/21/cvae.html) por Isaak Dykeman

Autoencoders variacionais usam uma fun√ß√£o de perda complexa que consiste em duas partes:

* **Perda de reconstru√ß√£o** √© a fun√ß√£o de perda que mostra o qu√£o pr√≥xima a imagem reconstru√≠da est√° do alvo (pode ser o Erro Quadr√°tico M√©dio, ou MSE). √â a mesma fun√ß√£o de perda usada em autoencoders normais.
* **Perda KL**, que garante que as distribui√ß√µes das vari√°veis latentes permane√ßam pr√≥ximas da distribui√ß√£o normal. √â baseada na no√ß√£o de [diverg√™ncia de Kullback-Leibler](https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained) - uma m√©trica para estimar qu√£o semelhantes duas distribui√ß√µes estat√≠sticas s√£o.

Uma vantagem importante dos VAEs √© que eles permitem gerar novas imagens com relativa facilidade, porque sabemos de qual distribui√ß√£o amostrar os vetores latentes. Por exemplo, se treinarmos um VAE com vetor latente 2D no MNIST, podemos variar os componentes do vetor latente para obter diferentes d√≠gitos:

<img alt="vaemnist" src="images/vaemnist.png" width="50%"/>

> Imagem por [Dmitry Soshnikov](http://soshnikov.com)

Observe como as imagens se misturam, √† medida que come√ßamos a obter vetores latentes de diferentes partes do espa√ßo de par√¢metros latentes. Tamb√©m podemos visualizar esse espa√ßo em 2D:

<img alt="vaemnist cluster" src="images/vaemnist-diag.png" width="50%"/> 

> Imagem por [Dmitry Soshnikov](http://soshnikov.com)

## ‚úçÔ∏è Exerc√≠cios: Autoencoders

Saiba mais sobre autoencoders nestes notebooks correspondentes:

* [Autoencoders no TensorFlow](../../../../../lessons/4-ComputerVision/09-Autoencoders/AutoencodersTF.ipynb)
* [Autoencoders no PyTorch](../../../../../lessons/4-ComputerVision/09-Autoencoders/AutoEncodersPyTorch.ipynb)

## Propriedades dos Autoencoders

* **Espec√≠ficos para os Dados** - eles funcionam bem apenas com o tipo de imagens nos quais foram treinados. Por exemplo, se treinarmos uma rede de super-resolu√ß√£o em flores, ela n√£o funcionar√° bem em retratos. Isso ocorre porque a rede pode produzir imagens de maior resolu√ß√£o aproveitando os detalhes finos aprendidos a partir do conjunto de dados de treinamento.
* **Com perdas** - a imagem reconstru√≠da n√£o √© id√™ntica √† imagem original. A natureza da perda √© definida pela *fun√ß√£o de perda* usada durante o treinamento.
* Funciona com **dados n√£o rotulados**

## [Post-lecture quiz](https://ff-quizzes.netlify.app/en/ai/quiz/18)

## Conclus√£o

Nesta li√ß√£o, voc√™ aprendeu sobre os v√°rios tipos de autoencoders dispon√≠veis para o cientista de IA. Aprendeu como constru√≠-los e como us√°-los para reconstruir imagens. Tamb√©m aprendeu sobre o VAE e como us√°-lo para gerar novas imagens.

## üöÄ Desafio

Nesta li√ß√£o, voc√™ aprendeu sobre o uso de autoencoders para imagens. Mas eles tamb√©m podem ser usados para m√∫sica! Confira o projeto [MusicVAE](https://magenta.tensorflow.org/music-vae) do Magenta, que usa autoencoders para aprender a reconstruir m√∫sica. Fa√ßa alguns [experimentos](https://colab.research.google.com/github/magenta/magenta-demos/blob/master/colab-notebooks/Multitrack_MusicVAE.ipynb) com esta biblioteca para ver o que voc√™ pode criar.

## [Post-lecture quiz](https://ff-quizzes.netlify.app/en/ai/quiz/16)

## Revis√£o e Autoestudo

Para refer√™ncia, leia mais sobre autoencoders nestes recursos:

* [Construindo Autoencoders no Keras](https://blog.keras.io/building-autoencoders-in-keras.html)
* [Post no blog NeuroHive](https://neurohive.io/ru/osnovy-data-science/variacionnyj-avtojenkoder-vae/)
* [Autoencoders Variacionais Explicados](https://kvfrans.com/variational-autoencoders-explained/)
* [Autoencoders Variacionais Condicionais](https://ijdykeman.github.io/ml/2016/12/21/cvae.html)

## Tarefa

No final deste [notebook usando TensorFlow](../../../../../lessons/4-ComputerVision/09-Autoencoders/AutoencodersTF.ipynb), voc√™ encontrar√° uma 'tarefa' - use-a como sua tarefa.

**Aviso Legal**:  
Este documento foi traduzido utilizando o servi√ßo de tradu√ß√£o por IA [Co-op Translator](https://github.com/Azure/co-op-translator). Embora nos esforcemos para garantir a precis√£o, esteja ciente de que tradu√ß√µes autom√°ticas podem conter erros ou imprecis√µes. O documento original em seu idioma nativo deve ser considerado a fonte oficial. Para informa√ß√µes cr√≠ticas, recomenda-se a tradu√ß√£o profissional realizada por humanos. N√£o nos responsabilizamos por quaisquer mal-entendidos ou interpreta√ß√µes equivocadas decorrentes do uso desta tradu√ß√£o.